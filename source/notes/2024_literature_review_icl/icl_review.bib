@misc{abernethyMechanismSampleEfficientInContext2023,
  title = {A {{Mechanism}} for {{Sample-Efficient In-Context Learning}} for {{Sparse Retrieval Tasks}}},
  author = {Abernethy, Jacob and Agarwal, Alekh and Marinov, Teodor V. and Warmuth, Manfred K.},
  year = {2023},
  month = may,
  number = {arXiv:2305.17040},
  eprint = {2305.17040},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {We study the phenomenon of {\textbackslash}textit\{in-context learning\} (ICL) exhibited by large language models, where they can adapt to a new learning task, given a handful of labeled examples, without any explicit parameter optimization. Our goal is to explain how a pre-trained transformer model is able to perform ICL under reasonable assumptions on the pre-training process and the downstream tasks. We posit a mechanism whereby a transformer can achieve the following: (a) receive an i.i.d. sequence of examples which have been converted into a prompt using potentially-ambiguous delimiters, (b) correctly segment the prompt into examples and labels, (c) infer from the data a {\textbackslash}textit\{sparse linear regressor\} hypothesis, and finally (d) apply this hypothesis on the given test example and return a predicted label. We establish that this entire procedure is implementable using the transformer mechanism, and we give sample complexity guarantees for this learning framework. Our empirical findings validate the challenge of segmentation, and we show a correspondence between our posited mechanisms and observed attention maps for step (c).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Abernethy et al_2023_A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks.pdf}
}

@misc{ahnLinearAttentionMaybe2024,
  title = {Linear Attention Is (Maybe) All You Need (to Understand Transformer Optimization)},
  author = {Ahn, Kwangjun and Cheng, Xiang and Song, Minhak and Yun, Chulhee and Jadbabaie, Ali and Sra, Suvrit},
  year = {2024},
  month = mar,
  number = {arXiv:2310.01082},
  eprint = {2310.01082},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.{\textasciitilde}von Oswald et al.{\textasciitilde}(ICML 2023), and K.{\textasciitilde}Ahn et al.{\textasciitilde}(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {G:\Other computers\My Laptop\Zotero Files\Ahn et al_2024_Linear attention is (maybe) all you need (to understand transformer.pdf}
}

@article{ahnTransformersLearnImplement2024,
  title = {Transformers Learn to Implement Preconditioned Gradient Descent for In-Context Learning},
  author = {Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  urldate = {2024-04-18},
  file = {G:\Other computers\My Laptop\Zotero Files\Ahn et al_2024_Transformers learn to implement preconditioned gradient descent for in-context.pdf}
}

@misc{ahujaCloserLookInContext2023,
  title = {A {{Closer Look}} at {{In-Context Learning}} under {{Distribution Shifts}}},
  author = {Ahuja, Kartik and {Lopez-Paz}, David},
  year = {2023},
  month = may,
  number = {arXiv:2305.16704},
  eprint = {2305.16704},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler architectures at performing in-context learning under varying distribution shifts? To compare transformers, we propose to use a simple architecture based on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based MLPs exhibit in-context learning under in-distribution evaluations, but transformers more closely emulate the performance of ordinary least squares (OLS). Transformers also display better resilience to mild distribution shifts, where set-based MLPs falter. However, under severe distribution shifts, both models' in-context learning abilities diminish.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Ahuja_Lopez-Paz_2023_A Closer Look at In-Context Learning under Distribution Shifts.pdf}
}

@inproceedings{ahujaTransformersCanLearn2023,
  title = {Transformers {{Can Learn To Solve Linear-Inverse Problems In-Context}}},
  booktitle = {{{NeurIPS}} 2023 {{Workshop}} on {{Deep Learning}} and {{Inverse Problems}}},
  author = {Ahuja, Kabir and Panwar, Madhur and Goyal, Navin},
  year = {2023},
  month = nov,
  urldate = {2024-04-18},
  abstract = {In-context learning is one of the surprising and useful features of large language models. How it works is an active area of research. Recently, stylized meta-learning-like setups have been devised that train these models on a sequence of input-output pairs \$(x, f(x))\$ from a function class using the language modeling loss and observe generalization to unseen functions from the same class. One of the main discoveries in this line of research has been that for several problems such as linear regression, trained transformers (TFs) learn algorithms for learning functions in context. We extend this setup to different types of linear-inverse problems and show that TFs are able to in-context learn these problems as well. Additionally, we show that TFs are able to recover the solutions in fewer-measurements than the number of unknowns, leveraging the structure of these problems and are in accordance with the recovery bounds. Finally, we also discuss the multi-task setup, where the TF is pre-trained on multiple types of linear-inverse problems at once and show that at inference time, given the measurements, they are able to identify the correct problem structure and solve the inverse problem efficiently.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Ahuja et al_2023_Transformers Can Learn To Solve Linear-Inverse Problems In-Context.pdf}
}

@misc{akyurekWhatLearningAlgorithm2023,
  title = {What Learning Algorithm Is In-Context Learning? {{Investigations}} with Linear Models},
  shorttitle = {What Learning Algorithm Is In-Context Learning?},
  author = {Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  year = {2023},
  month = may,
  number = {arXiv:2211.15661},
  eprint = {2211.15661},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples \$(x, f(x))\$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at https://github.com/ekinakyurek/google-research/blob/master/incontext.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Aky√ºrek et al_2023_What learning algorithm is in-context learning.pdf}
}

@article{baiTransformersStatisticiansProvable2023,
  title = {Transformers as {{Statisticians}}: {{Provable In-Context Learning}} with {{In-Context Algorithm Selection}}},
  shorttitle = {Transformers as {{Statisticians}}},
  author = {Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {57125--57211},
  urldate = {2024-04-18},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Bai et al_2023_Transformers as Statisticians.pdf}
}

@misc{bhasinHowDoesMultiTask2024,
  title = {How Does {{Multi-Task Training Affect Transformer In-Context Capabilities}}? {{Investigations}} with {{Function Classes}}},
  shorttitle = {How Does {{Multi-Task Training Affect Transformer In-Context Capabilities}}?},
  author = {Bhasin, Harmon and Ossowski, Timothy and Zhong, Yiqiao and Hu, Junjie},
  year = {2024},
  month = apr,
  number = {arXiv:2404.03558},
  eprint = {2404.03558},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL). While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples. We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence. Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work. Our code and models are available at https://github.com/harmonbhasin/curriculum\_learning\_icl .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Bhasin et al_2024_How does Multi-Task Training Affect Transformer In-Context Capabilities.pdf}
}

@misc{bhattamishraUnderstandingInContextLearning2023,
  title = {Understanding {{In-Context Learning}} in {{Transformers}} and {{LLMs}} by {{Learning}} to {{Learn Discrete Functions}}},
  author = {Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03016},
  eprint = {2310.03016},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a teaching sequence, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement two distinct algorithms to solve a single task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Bhattamishra et al_2023_Understanding In-Context Learning in Transformers and LLMs by Learning to Learn.pdf}
}

@inproceedings{bousquetAlgorithmicStabilityGeneralization2000,
  title = {Algorithmic {{Stability}} and {{Generalization Performance}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bousquet, Olivier and Elisseeff, Andr{\'e}},
  year = {2000},
  volume = {13},
  publisher = {MIT Press},
  urldate = {2024-04-18},
  abstract = {We  present a novel way of obtaining PAC-style bounds on the gen(cid:173) eralization error of learning algorithms, explicitly using their stabil(cid:173) ity properties.  A stable learner is one for which the learned solution  does  not  change much with small  changes in the training set.  The  bounds we  obtain do not depend on any measure of the complexity  of the hypothesis space  (e.g.  VC  dimension)  but rather depend on  how  the  learning  algorithm  searches  this  space,  and  can  thus  be  applied  even when the VC  dimension is  infinite.  We  demonstrate  that regularization networks possess the required stability property  and apply our method to obtain new bounds on their generalization  performance.},
  file = {G:\Other computers\My Laptop\Zotero Files\Bousquet_Elisseeff_2000_Algorithmic Stability and Generalization Performance.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2022-06-09},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {G:\Other computers\My Laptop\Zotero Files\Brown et al_2020_Language Models are Few-Shot Learners.pdf}
}

@article{chanDataDistributionalProperties2022,
  title = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  author = {Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {18878--18891},
  urldate = {2024-04-18},
  file = {G:\Other computers\My Laptop\Zotero Files\Chan et al_2022_Data distributional properties drive emergent in-context learning in.pdf}
}

@misc{chengTransformersImplementFunctional2024,
  title = {Transformers {{Implement Functional Gradient Descent}} to {{Learn Non-Linear Functions In Context}}},
  author = {Cheng, Xiang and Chen, Yuxin and Sra, Suvrit},
  year = {2024},
  month = feb,
  number = {arXiv:2312.06528},
  eprint = {2312.06528},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Cheng et al_2024_Transformers Implement Functional Gradient Descent to Learn Non-Linear.pdf}
}

@misc{chenTrainingDynamicsMultiHead2024,
  title = {Training {{Dynamics}} of {{Multi-Head Softmax Attention}} for {{In-Context Learning}}: {{Emergence}}, {{Convergence}}, and {{Optimality}}},
  shorttitle = {Training {{Dynamics}} of {{Multi-Head Softmax Attention}} for {{In-Context Learning}}},
  author = {Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
  year = {2024},
  month = feb,
  number = {arXiv:2402.19442},
  eprint = {2402.19442},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting "task allocation" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models. The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Chen et al_2024_Training Dynamics of Multi-Head Softmax Attention for In-Context Learning.pdf}
}

@misc{cuiSuperiorityMultiHeadAttention2024,
  title = {Superiority of {{Multi-Head Attention}} in {{In-Context Linear Regression}}},
  author = {Cui, Yingqian and Ren, Jie and He, Pengfei and Tang, Jiliang and Xing, Yue},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17426},
  eprint = {2401.17426},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head attention in the transformer architecture.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Cui et al_2024_Superiority of Multi-Head Attention in In-Context Linear Regression.pdf}
}

@misc{dingCausalLMNotOptimal2024,
  title = {{{CausalLM}} Is Not Optimal for In-Context Learning},
  author = {Ding, Nan and Levinboim, Tomer and Wu, Jialin and Goodman, Sebastian and Soricut, Radu},
  year = {2024},
  month = feb,
  number = {arXiv:2308.06912},
  eprint = {2308.06912},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.06912},
  urldate = {2024-04-18},
  abstract = {Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Ding et al_2024_CausalLM is not optimal for in-context learning.pdf}
}

@misc{fuTransformersLearnHigherOrder2023,
  title = {Transformers {{Learn Higher-Order Optimization Methods}} for {{In-Context Learning}}: {{A Study}} with {{Linear Models}}},
  shorttitle = {Transformers {{Learn Higher-Order Optimization Methods}} for {{In-Context Learning}}},
  author = {Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17086},
  eprint = {2310.17086},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of convergence with high-order methods such as Iterative Newton, which are exponentially faster than Gradient Descent. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, we show theoretical results which support our empirical findings and have a close correspondence with them: we prove that Transformers can implement \$k\$ iterations of Newton's method with \${\textbackslash}mathcal\{O\}(k)\$ layers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Fu et al_2023_Transformers Learn Higher-Order Optimization Methods for In-Context Learning.pdf}
}

@misc{gaoExpressivePowerVariant2024,
  title = {On the {{Expressive Power}} of a {{Variant}} of the {{Looped Transformer}}},
  author = {Gao, Yihang and Zheng, Chuanyang and Xie, Enze and Shi, Han and Hu, Tianyang and Li, Yu and Ng, Michael K. and Li, Zhenguo and Liu, Zhaoqiang},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13572},
  eprint = {2402.13572},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for task pre-processing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to be smarter than human-designed algorithms. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some challenging tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {G:\Other computers\My Laptop\Zotero Files\Gao et al_2024_On the Expressive Power of a Variant of the Looped Transformer.pdf}
}

@phdthesis{gargNatureLearningLearning2023,
  title = {Nature of {{Learning}} and {{Learning}} of {{Nature}}},
  author = {Garg, Shivam},
  year = {2023},
  address = {United States -- California},
  urldate = {2024-04-18},
  abstract = {This thesis explores questions surrounding the foundations of intelligence, both artificial and natural. The first part focuses on the algorithmic and statistical underpinnings of modern machine learning systems. First, we discuss a clean framework for investigating the surprising ability of large language models to learn in-context: the apparent ability to solve new tasks given just a text prompt that provides examples. Further, motivated by concerns around the insatiable data appetite of modern machine learning systems, we discuss the problem of "sample amplification", where we formalize the seemingly naive question of how hard it is to create new data and contrast the hardness of this task to that of learning the data-generating distribution. The second part considers the algorithmic basis of intelligence in nature, specifically in ant colonies and the brain. We examine how arboreal turtle ants solve variants of the shortest path problem without any central control and with minimal computational resources. In the context of the brain, we study how it manages to train its neural network despite its structural limitations. Specifically, we investigate a biologically plausible learning algorithm and contrast it with gradient descent, arguably the only known algorithm for training large-scale artificial neural networks.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798381021509},
  langid = {english},
  school = {Stanford University},
  keywords = {Artificial intelligence,Feedback,Neural networks}
}

@article{gargWhatCanTransformers2022,
  title = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
  shorttitle = {What Can Transformers Learn In-Context?},
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy S. and Valiant, Gregory},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {30583--30598},
  urldate = {2024-04-17},
  file = {G:\Other computers\My Laptop\Zotero Files\Garg et al_2022_What can transformers learn in-context.pdf}
}

@misc{grazziMambaCapableInContext2024,
  title = {Is {{Mamba Capable}} of {{In-Context Learning}}?},
  author = {Grazzi, Riccardo and Siems, Julien and Schrodi, Simon and Brox, Thomas and Hutter, Frank},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03170},
  eprint = {2402.03170},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Grazzi et al_2024_Is Mamba Capable of In-Context Learning.pdf}
}

@misc{guoHowTransformersLearn2023,
  title = {How {{Do Transformers Learn In-Context Beyond Simple Functions}}? {{A Case Study}} on {{Learning}} with {{Representations}}},
  shorttitle = {How {{Do Transformers Learn In-Context Beyond Simple Functions}}?},
  author = {Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10616},
  eprint = {2310.10616},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with representations. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but fixed representation function, composed with a linear function that differs in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size. Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Guo et al_2023_How Do Transformers Learn In-Context Beyond Simple Functions.pdf}
}

@misc{hanExplainingEmergentInContext2023,
  title = {Explaining {{Emergent In-Context Learning}} as {{Kernel Regression}}},
  author = {Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  year = {2023},
  month = oct,
  number = {arXiv:2305.12766},
  eprint = {2305.12766},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression \${\textbackslash}hat y = {\textbackslash}sum\_i y\_i K(x, x\_i)/{\textbackslash}sum\_i K(x, x\_i)\$ as the number of in-context demonstrations grows. Then, we empirically investigate the in-context behaviors of language models. We find that during ICL, the attention and hidden features in LLMs match the behaviors of a kernel regression. Finally, our theory provides insights into multiple phenomena observed in the ICL field: why retrieving demonstrative samples similar to test samples can help, why ICL performance is sensitive to the output formats, and why ICL accuracy benefits from selecting in-distribution and representative samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Han et al_2023_Explaining Emergent In-Context Learning as Kernel Regression.pdf}
}

@misc{huangInContextConvergenceTransformers2023,
  title = {In-{{Context Convergence}} of {{Transformers}}},
  author = {Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05249},
  eprint = {2310.05249},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Huang et al_2023_In-Context Convergence of Transformers.pdf}
}

@misc{kimTransformersLearnNonlinear2024,
  title = {Transformers {{Learn Nonlinear Features In Context}}: {{Nonconvex Mean-field Dynamics}} on the {{Attention Landscape}}},
  shorttitle = {Transformers {{Learn Nonlinear Features In Context}}},
  author = {Kim, Juno and Suzuki, Taiji},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01258},
  eprint = {2402.01258},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Kim_Suzuki_2024_Transformers Learn Nonlinear Features In Context.pdf}
}

@article{kojimaLargeLanguageModels2022,
  title = {Large Language Models Are Zero-Shot Reasoners},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  journal = {Advances in neural information processing systems},
  volume = {35},
  pages = {22199--22213},
  urldate = {2024-04-19},
  file = {G:\Other computers\My Laptop\Zotero Files\Kojima et al_2022_Large language models are zero-shot reasoners.pdf}
}

@misc{liDissectingChainofThoughtCompositionality2023,
  title = {Dissecting {{Chain-of-Thought}}: {{Compositionality}} through {{In-Context Filtering}} and {{Learning}}},
  shorttitle = {Dissecting {{Chain-of-Thought}}},
  author = {Li, Yingcong and Sreenivasan, Kartik and Giannou, Angeliki and Papailiopoulos, Dimitris and Oymak, Samet},
  year = {2023},
  month = nov,
  number = {arXiv:2305.18869},
  eprint = {2305.18869},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). In this setting, we find that the success of CoT can be attributed to breaking down in-context learning of a compositional function into two distinct phases: focusing on and filtering data related to each step of the composition and in-context learning the single-step composition function. Through both experimental and theoretical evidence, we demonstrate how CoT significantly reduces the sample complexity of in-context learning (ICL) and facilitates the learning of complex functions that non-CoT methods struggle with. Furthermore, we illustrate how transformers can transition from vanilla in-context learning to mastering a compositional function with CoT by simply incorporating additional layers that perform the necessary data-filtering for CoT via the attention mechanism. In addition to these test-time benefits, we show CoT helps accelerate pretraining by learning shortcuts to represent complex functions and filtering plays an important role in this process. These findings collectively provide insights into the mechanics of CoT, inviting further investigation of its role in complex reasoning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Li et al_2023_Dissecting Chain-of-Thought.pdf}
}

@misc{linDualOperatingModes2024,
  title = {Dual {{Operating Modes}} of {{In-Context Learning}}},
  author = {Lin, Ziqian and Lee, Kangwook},
  year = {2024},
  month = feb,
  number = {arXiv:2402.18819},
  eprint = {2402.18819},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL. Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples. Our model offers a plausible explanation for this "early ascent" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels. Lastly, we validate our findings and predictions via experiments involving Transformers and large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Lin_Lee_2024_Dual Operating Modes of In-Context Learning.pdf}
}

@inproceedings{liTransformersAlgorithmsGeneralization2023,
  title = {Transformers as {{Algorithms}}: {{Generalization}} and {{Stability}} in {{In-context Learning}}},
  shorttitle = {Transformers as {{Algorithms}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  year = {2023},
  month = jul,
  pages = {19565--19594},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-18},
  abstract = {In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Li et al_2023_Transformers as Algorithms.pdf}
}

@inproceedings{mahdaviRevisitingEquivalenceInContext2024,
  title = {Revisiting the {{Equivalence}} of {{In-Context Learning}} and {{Gradient Descent}}: {{The Impact}} of {{Data Distribution}}},
  shorttitle = {Revisiting the {{Equivalence}} of {{In-Context Learning}} and {{Gradient Descent}}},
  booktitle = {{{ICASSP}} 2024 - 2024 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Mahdavi, Sadegh and Liao, Renjie and Thrampoulidis, Christos},
  year = {2024},
  month = apr,
  pages = {7410--7414},
  publisher = {IEEE},
  address = {Seoul, Korea, Republic of},
  doi = {10.1109/ICASSP48485.2024.10446522},
  urldate = {2024-04-18},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350344851},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Mahdavi et al_2024_Revisiting the Equivalence of In-Context Learning and Gradient Descent.pdf}
}

@misc{minNoisyChannelLanguage2022,
  title = {Noisy {{Channel Language Model Prompting}} for {{Few-Shot Text Classification}}},
  author = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = mar,
  number = {arXiv:2108.04106},
  eprint = {2108.04106},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-19},
  abstract = {We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive methods (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {G:\Other computers\My Laptop\Zotero Files\Min et al_2022_Noisy Channel Language Model Prompting for Few-Shot Text Classification.pdf}
}

@misc{minRethinkingRoleDemonstrations2022,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = oct,
  number = {arXiv:2202.12837},
  eprint = {2202.12837},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {G:\Other computers\My Laptop\Zotero Files\Min et al_2022_Rethinking the Role of Demonstrations.pdf}
}

@misc{panwarInContextLearningBayesian2024,
  title = {In-{{Context Learning}} through the {{Bayesian Prism}}},
  author = {Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
  year = {2024},
  month = apr,
  number = {arXiv:2306.04891},
  eprint = {2306.04891},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs \$(x, f(x))\$. The function \$f\$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. In this paper we empirically examine how far this Bayesian perspective can help us understand ICL. To this end, we generalize the previous meta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple task families. We instantiate this setup on a diverse range of linear and nonlinear function families and find that transformers can do ICL in this setting as well. Where Bayesian inference is tractable, we find evidence that high-capacity transformers mimic the Bayesian predictor. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks. We also find that transformers can learn to generalize to new function classes that were not seen during pretraining. This involves deviation from the Bayesian predictor. We examine these deviations in more depth offering new insights and hypotheses.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Panwar et al_2024_In-Context Learning through the Bayesian Prism.pdf}
}

@inproceedings{panwarSurprisingDeviationsBayesian2023,
  title = {Surprising {{Deviations}} from {{Bayesian View}} in {{In-Context Learning}}},
  booktitle = {I {{Can}}'t {{Believe It}}'s {{Not Better Workshop}}: {{Failure Modes}} in the {{Age}} of {{Foundation Models}}},
  author = {Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
  year = {2023},
  month = dec,
  urldate = {2024-04-18},
  file = {G:\Other computers\My Laptop\Zotero Files\Panwar et al_Surprising Deviations from Bayesian View in In-Context Learning.pdf}
}

@misc{pathakTransformersCanOptimally2023,
  title = {Transformers Can Optimally Learn Regression Mixture Models},
  author = {Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08362},
  eprint = {2311.08362},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low mean-squared error on data generated via this process. By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Pathak et al_2023_Transformers can optimally learn regression mixture models.pdf}
}

@article{raventosPretrainingTaskDiversity2024,
  title = {Pretraining Task Diversity and the Emergence of Non-{{Bayesian}} in-Context Learning for Regression},
  author = {Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  urldate = {2024-04-18},
  file = {G:\Other computers\My Laptop\Zotero Files\Ravent√≥s et al_2024_Pretraining task diversity and the emergence of non-Bayesian in-context.pdf}
}

@misc{razeghiImpactPretrainingTerm2022,
  title = {Impact of {{Pretraining Term Frequencies}} on {{Few-Shot Reasoning}}},
  author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
  year = {2022},
  month = may,
  number = {arXiv:2202.07206},
  eprint = {2202.07206},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above \$70{\textbackslash}\%\$ (absolute) more accurate on the top 10{\textbackslash}\% frequent terms in comparison to the bottom 10{\textbackslash}\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Razeghi et al_2022_Impact of Pretraining Term Frequencies on Few-Shot Reasoning.pdf}
}

@misc{reddyMechanisticBasisData2023,
  title = {The Mechanistic Basis of Data Dependence and Abrupt Learning in an In-Context Classification Task},
  author = {Reddy, Gautam},
  year = {2023},
  month = dec,
  number = {arXiv:2312.03002},
  eprint = {2312.03002},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence. In-context learning contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Reddy_2023_The mechanistic basis of data dependence and abrupt learning in an in-context.pdf}
}

@misc{renIncontextLearningTransformer2023,
  title = {In-Context {{Learning}} with {{Transformer Is Really Equivalent}} to a {{Contrastive Learning Pattern}}},
  author = {Ren, Ruifeng and Liu, Yong},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13220},
  eprint = {2310.13220},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the first to provide the understanding of ICL from the perspective of contrastive learning and has the potential to facilitate future model design by referring to related works on contrastive learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Ren_Liu_2023_In-context Learning with Transformer Is Really Equivalent to a Contrastive.pdf}
}

@misc{shenRevisitingHypothesisPretrained2024,
  title = {Revisiting the {{Hypothesis}}: {{Do}} Pretrained {{Transformers Learn In-Context}} by {{Gradient Descent}}?},
  shorttitle = {Revisiting the {{Hypothesis}}},
  author = {Shen, Lingfeng and Mishra, Aayush and Khashabi, Daniel},
  year = {2024},
  month = feb,
  number = {arXiv:2310.08540},
  eprint = {2310.08540},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {The emergence of In-Context Learning (ICL) in LLMs remains a significant phenomenon with little understanding. To explain ICL, recent studies try to theoretically connect it to Gradient Descent (GD). We ask, does this connection hold up in actual pre-trained models? We highlight the limiting assumptions in prior works that make their context considerably different from the practical context in which language models are trained. For example, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. Furthermore, their experimental verification uses ICL objective (training models explicitly for ICL), which differs from the emergent ICL in the wild. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that the equivalence between ICL and GD remains an open hypothesis and calls for further studies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Shen et al_2024_Revisiting the Hypothesis.pdf}
}

@book{sreenivasanUnderstandingChallengesScaling2023,
  title = {Towards {{Understanding}} the {{Challenges}} in {{Scaling Frontier Machine Learning Models}}},
  author = {Sreenivasan, Kartik},
  year = {2023},
  publisher = {The University of Wisconsin-Madison},
  urldate = {2024-04-18}
}

@misc{vladymyrovLinearTransformersAre2024,
  title = {Linear {{Transformers}} Are {{Versatile In-Context Learners}}},
  author = {Vladymyrov, Max and {von Oswald}, Johannes and Sandler, Mark and Ge, Rong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.14180},
  eprint = {2402.14180},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Vladymyrov et al_2024_Linear Transformers are Versatile In-Context Learners.pdf}
}

@inproceedings{vonoswaldTransformersLearnIncontext2023,
  title = {Transformers Learn In-Context by Gradient Descent},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  year = {2023},
  pages = {35151--35174},
  publisher = {PMLR},
  urldate = {2024-04-18},
  file = {G:\Other computers\My Laptop\Zotero Files\Von Oswald et al_2023_Transformers learn in-context by gradient descent.pdf}
}

@misc{weiFinetunedLanguageModels2022,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2022},
  month = feb,
  number = {arXiv:2109.01652},
  eprint = {2109.01652},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-19},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {G:\Other computers\My Laptop\Zotero Files\Wei et al_2022_Finetuned Language Models Are Zero-Shot Learners.pdf}
}

@inproceedings{wibisonoRoleUnstructuredTraining2023,
  title = {On the {{Role}} of {{Unstructured Training Data}} in {{Transformers}}' {{In-Context Learning Capabilities}}},
  booktitle = {{{NeurIPS}} 2023 {{Workshop}} on {{Mathematics}} of {{Modern Machine Learning}}},
  author = {Wibisono, Kevin Christian and Wang, Yixin},
  year = {2023},
  month = nov,
  urldate = {2024-04-18},
  abstract = {Transformers have exhibited impressive in-context learning (ICL) capabilities: they can generate predictions for new query inputs based on sequences of inputs and outputs (i.e., prompts) without parameter updates. Efforts to provide theoretical explanations for the emergence of these abilities have primarily focused on the structured data setting, where input-output pairings in the training data are known. This scenario can enable simplified transformers (e.g., ones comprising a single attention layer without the softmax activation) to achieve notable ICL performance. However, transformers are primarily trained on unstructured data that rarely include such input-output pairings. To better understand how ICL emerges, we propose to study transformers that are trained on unstructured data, namely data that lack prior knowledge of input-output pairings. This new setting elucidates the pivotal role of softmax attention in the robust ICL abilities of transformers, particularly those with a single attention layer. We posit that the significance of the softmax activation partially stems from the equivalence of softmax-based attention models with mixtures of experts, facilitating the implicit inference of input-output pairings in the test prompts. Additionally, a probing analysis reveals where these pairings are learned within the model. While subsequent layers predictably encode more information about these pairings, we find that even the first attention layer contains a significant amount of pairing information.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Wibisono_Wang_2023_On the Role of Unstructured Training Data in Transformers' In-Context Learning.pdf}
}

@misc{wuHowManyPretraining2024,
  title = {How {{Many Pretraining Tasks Are Needed}} for {{In-Context Learning}} of {{Linear Regression}}?},
  author = {Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L.},
  year = {2024},
  month = mar,
  number = {arXiv:2310.08391},
  eprint = {2310.08391},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical task complexity bound for the attention model pretraining, showing that effective pretraining only requires a small number of independent tasks. Furthermore, we prove that the pretrained model closely matches the Bayes optimal algorithm, i.e., optimally tuned ridge regression, by achieving nearly Bayes optimal risk on unseen tasks under a fixed context length. These theoretical findings complement prior experimental research and shed light on the statistical foundations of ICL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Wu et al_2024_How Many Pretraining Tasks Are Needed for In-Context Learning of Linear.pdf}
}

@misc{xieExplanationIncontextLearning2022,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  year = {2022},
  month = jul,
  number = {arXiv:2111.02080},
  eprint = {2111.02080},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Xie et al_2022_An Explanation of In-context Learning as Implicit Bayesian Inference.pdf}
}

@misc{xingBenefitsTransformerInContext2024,
  title = {Benefits of {{Transformer}}: {{In-Context Learning}} in {{Linear Regression Tasks}} with {{Unstructured Data}}},
  shorttitle = {Benefits of {{Transformer}}},
  author = {Xing, Yue and Lin, Xiaofeng and Suh, Namjoon and Song, Qifan and Cheng, Guang},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00743},
  eprint = {2402.00743},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., {\textbackslash}citet\{zhang2023trained,huang2023context\}, provide theoretical explanations on this in-context learning ability, they assume the input \$x\_i\$ and the output \$y\_i\$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data {\textbackslash}cite\{wibisono2023role\}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if \$y\_i\$ is in the token next to \$x\_i\$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Xing et al_2024_Benefits of Transformer.pdf}
}

@inproceedings{xuLargeLanguageModels2024,
  title = {Do {{Large Language Models Have Compositional Ability}}? {{An Investigation}} into {{Limitations}} and {{Scalability}}},
  shorttitle = {Do {{Large Language Models Have Compositional Ability}}?},
  booktitle = {{{ICLR}} 2024 {{Workshop}} on {{Mathematical}} and {{Empirical Understanding}} of {{Foundation Models}}},
  author = {Xu, Zhuoyan and Shi, Zhenmei and Liang, Yingyu},
  year = {2024},
  month = apr,
  urldate = {2024-04-18},
  abstract = {Large language models (LLM) have emerged as a powerful tool exhibiting remarkable in-context learning (ICL) capabilities. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks that include logical and linguistic challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that contains different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks that involving sequentiaL reasoning, models typically underperform, and scaling up provide no improvements. We offer theoretical analysis in a simplified setting. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code is available at \{{\textbackslash}small {\textbackslash}url\{https://github.com/OliverXUZY/LLM\_Compose\}\}.},
  langid = {english},
  file = {G:\Other computers\My Laptop\Zotero Files\Xu et al_2024_Do Large Language Models Have Compositional Ability.pdf}
}

@misc{yadlowskyPretrainingDataMixtures2023,
  title = {Pretraining {{Data Mixtures Enable Narrow Model Selection Capabilities}} in {{Transformer Models}}},
  author = {Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  year = {2023},
  month = nov,
  number = {arXiv:2311.00871},
  eprint = {2311.00871},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of \$(x, f(x))\$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Yadlowsky et al_2023_Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in.pdf}
}

@misc{yangLoopedTransformersAre2024,
  title = {Looped {{Transformers}} Are {{Better}} at {{Learning Learning Algorithms}}},
  author = {Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris},
  year = {2024},
  month = mar,
  number = {arXiv:2311.12424},
  eprint = {2311.12424},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.12424},
  urldate = {2024-04-18},
  abstract = {Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10\% of the parameter count.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {G:\Other computers\My Laptop\Zotero Files\Yang et al_2024_Looped Transformers are Better at Learning Learning Algorithms.pdf}
}

@misc{zhangInContextLearningLinear2024,
  title = {In-{{Context Learning}} of a {{Linear Transformer Block}}: {{Benefits}} of the {{MLP Component}} and {{One-Step GD Initialization}}},
  shorttitle = {In-{{Context Learning}} of a {{Linear Transformer Block}}},
  author = {Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.14951},
  eprint = {2402.14951},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {We study the {\textbackslash}emph\{in-context learning\} (ICL) ability of a {\textbackslash}emph\{Linear Transformer Block\} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a {\textbackslash}emph\{non-zero mean\}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization (\${\textbackslash}mathsf\{GD\}{\textbackslash}text\{-\}{\textbackslash}mathbf\{{\textbackslash}beta\}\$), in the sense that every \${\textbackslash}mathsf\{GD\}{\textbackslash}text\{-\}{\textbackslash}mathbf\{{\textbackslash}beta\}\$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a \${\textbackslash}mathsf\{GD\}{\textbackslash}text\{-\}{\textbackslash}mathbf\{{\textbackslash}beta\}\$ estimator. Finally, we show that \${\textbackslash}mathsf\{GD\}{\textbackslash}text\{-\}{\textbackslash}mathbf\{{\textbackslash}beta\}\$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective. Our results reveal that LTB achieves ICL by implementing \${\textbackslash}mathsf\{GD\}{\textbackslash}text\{-\}{\textbackslash}mathbf\{{\textbackslash}beta\}\$, and they highlight the role of MLP layers in reducing approximation error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Zhang et al_2024_In-Context Learning of a Linear Transformer Block.pdf}
}

@article{zhangTrainedTransformersLearn2024,
  title = {Trained {{Transformers Learn Linear Models In-Context}}},
  author = {Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L.},
  year = {2024},
  journal = {Journal of Machine Learning Research},
  volume = {25},
  number = {49},
  pages = {1--55},
  issn = {1533-7928},
  urldate = {2024-04-18},
  abstract = {Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.},
  file = {G:\Other computers\My Laptop\Zotero Files\Zhang et al_2024_Trained Transformers Learn Linear Models In-Context.pdf}
}

@misc{zhangWhatHowDoes2023,
  title = {What and {{How}} Does {{In-Context Learning Learn}}? {{Bayesian Model Averaging}}, {{Parameterization}}, and {{Generalization}}},
  shorttitle = {What and {{How}} Does {{In-Context Learning Learn}}?},
  author = {Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  year = {2023},
  month = oct,
  number = {arXiv:2305.19420},
  eprint = {2305.19420},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-18},
  abstract = {In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned by large language models? (b) What is a proper performance metric for ICL and what is the error rate? (c) How does the transformer architecture enable ICL? To answer these questions, we adopt a Bayesian view and formulate ICL as a problem of predicting the response corresponding to the current covariate, given a number of examples drawn from a latent variable model. To answer (a), we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm, which is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a \${\textbackslash}mathcal\{O\}(1/T)\$ regret bound for perfectly pretrained ICL, where \$T\$ is the number of examples in the prompt. To answer (c), we show that, in addition to encoding Bayesian model averaging via attention, the transformer architecture also enables a fine-grained statistical analysis of pretraining under realistic assumptions. In particular, we prove that the error of pretrained model is bounded by a sum of an approximation error and a generalization error, where the former decays to zero exponentially as the depth grows, and the latter decays to zero sublinearly with the number of tokens in the pretraining dataset. Our results provide a unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization, which deepens our knowledge of these essential aspects of modern language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G:\Other computers\My Laptop\Zotero Files\Zhang et al_2023_What and How does In-Context Learning Learn.pdf}
}
