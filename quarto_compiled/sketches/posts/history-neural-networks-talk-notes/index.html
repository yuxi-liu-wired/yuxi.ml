<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2024-08-01">
<meta name="description" content="Notes for a talk on the history of neural networks.">

<title>Notes for a talk on the history of neural networks – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Notes for a talk on the history of neural networks – Yuxi on the Wired">
<meta property="og:description" content="Notes for a talk on the history of neural networks.">
<meta property="og:image" content="https://yuxi.ml/sketches/posts/history-neural-networks-talk-notes/figure/Cajal_RNN_original.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="700">
<meta property="og:image:width" content="632">
<meta name="twitter:title" content="Notes for a talk on the history of neural networks – Yuxi on the Wired">
<meta name="twitter:description" content="Notes for a talk on the history of neural networks.">
<meta name="twitter:image" content="https://yuxi.ml/sketches/posts/history-neural-networks-talk-notes/figure/Cajal_RNN_original.png">
<meta name="twitter:image-height" content="700">
<meta name="twitter:image-width" content="632">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Notes for a talk on the history of neural networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Notes for a talk on the history of neural networks.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">history</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">cybernetics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 1, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-this-essay-is-about" id="toc-what-this-essay-is-about" class="nav-link active" data-scroll-target="#what-this-essay-is-about">What this essay is about</a></li>
  <li><a href="#prehistory" id="toc-prehistory" class="nav-link" data-scroll-target="#prehistory">Prehistory</a>
  <ul class="collapse">
  <li><a href="#santiago-ramón-y-cajal" id="toc-santiago-ramón-y-cajal" class="nav-link" data-scroll-target="#santiago-ramón-y-cajal">Santiago Ramón y Cajal</a></li>
  <li><a href="#rafael-lorente-de-nó" id="toc-rafael-lorente-de-nó" class="nav-link" data-scroll-target="#rafael-lorente-de-nó">Rafael Lorente de Nó</a></li>
  <li><a href="#donald-hebb" id="toc-donald-hebb" class="nav-link" data-scroll-target="#donald-hebb">Donald Hebb</a></li>
  </ul></li>
  <li><a href="#cybernetics" id="toc-cybernetics" class="nav-link" data-scroll-target="#cybernetics">Cybernetics</a>
  <ul class="collapse">
  <li><a href="#wwii" id="toc-wwii" class="nav-link" data-scroll-target="#wwii">WWII</a></li>
  <li><a href="#the-problem-of-fire-control" id="toc-the-problem-of-fire-control" class="nav-link" data-scroll-target="#the-problem-of-fire-control">The problem of fire control</a></li>
  <li><a href="#commentary" id="toc-commentary" class="nav-link" data-scroll-target="#commentary">Commentary</a></li>
  <li><a href="#mcculloch-and-pitts-1943" id="toc-mcculloch-and-pitts-1943" class="nav-link" data-scroll-target="#mcculloch-and-pitts-1943">McCulloch and Pitts (1943)</a></li>
  </ul></li>
  <li><a href="#first-neural-network-period-19501970" id="toc-first-neural-network-period-19501970" class="nav-link" data-scroll-target="#first-neural-network-period-19501970">First neural network period (1950–1970)</a>
  <ul class="collapse">
  <li><a href="#snarc" id="toc-snarc" class="nav-link" data-scroll-target="#snarc">SNARC</a></li>
  <li><a href="#frank-rosenblatt" id="toc-frank-rosenblatt" class="nav-link" data-scroll-target="#frank-rosenblatt">Frank Rosenblatt</a></li>
  <li><a href="#stanford-research-institute" id="toc-stanford-research-institute" class="nav-link" data-scroll-target="#stanford-research-institute">Stanford Research Institute</a></li>
  <li><a href="#adaline" id="toc-adaline" class="nav-link" data-scroll-target="#adaline">ADALINE</a></li>
  </ul></li>
  <li><a href="#neural-network-winter-1970s" id="toc-neural-network-winter-1970s" class="nav-link" data-scroll-target="#neural-network-winter-1970s">Neural network winter (1970s)</a>
  <ul class="collapse">
  <li><a href="#the-xor-myth" id="toc-the-xor-myth" class="nav-link" data-scroll-target="#the-xor-myth">The XOR myth</a></li>
  <li><a href="#possible-reasons-for-the-winter" id="toc-possible-reasons-for-the-winter" class="nav-link" data-scroll-target="#possible-reasons-for-the-winter">Possible reasons for the winter</a></li>
  <li><a href="#minsky-and-papert-against-large-neural-networks" id="toc-minsky-and-papert-against-large-neural-networks" class="nav-link" data-scroll-target="#minsky-and-papert-against-large-neural-networks">Minsky and Papert against large neural networks</a></li>
  <li><a href="#years-of-people-refusing-to-use-gradient-descent" id="toc-years-of-people-refusing-to-use-gradient-descent" class="nav-link" data-scroll-target="#years-of-people-refusing-to-use-gradient-descent">30 years of people refusing to use gradient descent</a></li>
  </ul></li>
  <li><a href="#some-bitter-lessons-i-guess" id="toc-some-bitter-lessons-i-guess" class="nav-link" data-scroll-target="#some-bitter-lessons-i-guess">Some bitter lessons I guess</a>
  <ul class="collapse">
  <li><a href="#first-attempt" id="toc-first-attempt" class="nav-link" data-scroll-target="#first-attempt">First attempt</a></li>
  <li><a href="#second-attempt" id="toc-second-attempt" class="nav-link" data-scroll-target="#second-attempt">Second attempt</a></li>
  <li><a href="#the-idea-follows-the-compute" id="toc-the-idea-follows-the-compute" class="nav-link" data-scroll-target="#the-idea-follows-the-compute">The idea follows the compute</a></li>
  <li><a href="#the-bitter-lesson" id="toc-the-bitter-lesson" class="nav-link" data-scroll-target="#the-bitter-lesson">The bitter lesson</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="what-this-essay-is-about" class="level2">
<h2 class="anchored" data-anchor-id="what-this-essay-is-about">What this essay is about</h2>
<p>This essay is written as a companion piece of a lecture on the history of neural networks. It is not exactly an essay, but more of an extended scrapbook of quotations. The lecture slides are <a href="code/ML history presentation.zip">here</a>.</p>
<ul>
<li><a href="https://yuxi-liu-wired.github.io/essays/posts/cybernetic-artificial-intelligence/">Yuxi on the Wired - Cybernetic Artificial Intelligence</a></li>
<li><a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/">Yuxi on the Wired - Reading Perceptrons</a></li>
<li><a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/">Yuxi on the Wired - The Backstory of Backpropagation</a></li>
<li><a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/">Yuxi on the Wired - The Perceptron Controversy</a></li>
</ul>
</section>
<section id="prehistory" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="prehistory">Prehistory</h2>
<blockquote class="blockquote">
<p>The thunderbolt steers all things. [τὰ δὲ πάντα οἰακίζει κεραυνός]</p>
<p>— Heraclitus</p>
</blockquote>
<section id="santiago-ramón-y-cajal" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="santiago-ramón-y-cajal">Santiago Ramón y Cajal</h3>
<p>Cajal’s neural network diagrams are always feedforward. Cajal never depicted feedback except once, in a diagram of a connection in the cerebellum. In fact, according to de No, who was a student of Cajal, Cajal was vehemently <em>against</em> feedbacks. More on this later.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cajal_RNN_original.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="ramonycajalHistologieSystemeNerveux1909">(<a href="#ref-ramonycajalHistologieSystemeNerveux1909" role="doc-biblioref">Ramón y Cajal 1909, II:149</a>, figure 103)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cajal_RNN.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Colored version highlighting the recurrent loop.</figcaption>
</figure>
</div>
<p>The original caption:</p>
<blockquote class="blockquote">
<p>Voie courte des cellules de Golgi.</p>
<p>Fig. 103. — Schéma destiné à montrer la marche du courant apporté par les fibres moussues et la part que prennent à ce courant les cellules de Golgi.</p>
<p>A, fibres moussues ; — B, cylindre-axe de Purkinje ; — n, grains ; — b, fibres parallèles; — c, cellule de Golgi ; — d, cellule de Purkinje vue de champ.</p>
</blockquote>
<p>Translation:</p>
<blockquote class="blockquote">
<p>Short pathway of the Golgi cells.</p>
<p>Fig. 103. — Diagram intended to show the course of the current brought by the mossy fibers and the part played by the Golgi cells in this current.</p>
<p>A, mossy fibers; — B, Purkinje axis-cylinder; — n, grains; — b, parallel fibers; — c, Golgi cell; — d, Purkinje cell seen from the field.</p>
</blockquote>
</section>
<section id="rafael-lorente-de-nó" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="rafael-lorente-de-nó">Rafael Lorente de Nó</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Vestibulo%E2%80%93ocular_reflex">vestibulo-ocular reflex (VOR)</a> is a reflex that acts to stabilize gaze during head movement.</p>
<p>Lorente de No discovered recurrent circuits in the brain, mainly in the VOR, but also in the cortex <span class="citation" data-cites="larriva-sahdPredictionsRafaelLorente2014">(<a href="#ref-larriva-sahdPredictionsRafaelLorente2014" role="doc-biblioref">Larriva-Sahd 2014</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Lorente de No_RNN_VOR.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A recurrent neural network that explains nystagmus. <span class="citation" data-cites="denoAnalysisActivityChains1938">(<a href="#ref-denoAnalysisActivityChains1938" role="doc-biblioref">de Nó 1938, fig. 13</a>)</span> Original caption: Diagram explaining the production of rhythm during vestibular nystagmus. Fiber f is sup- posed to carry the continuous series of impulses started at the cristae of the semicircular canals which set up the nystagmus. Fibers ft are supposed to be maintaining the tonus of the antagonistic muscle; la, Ib, Za, Zb, 3a, 3b are branches of the axons of cells 1, 2 and 3,4a, 4b, 4d,, 4e are branches of the axon of cell 4; fa, fb, fc are branches of fiber f.&nbsp;II. Diagram of the rhythmic succession of con- tractions and relaxations of the antagonistic muscles during the nystagmus explained by diagram I. Rising of the line indica tes contraction . The interval between turning points a and b is never less than 3 to 4 msec.; – the interval between 50 msec. long.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Lorente de No_ResNet_1.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Simplified diagram of the interneurons in the vestibulo-ocular reflex, showing both residual feedforward connections and recurrent connections. <span class="citation" data-cites="denoAnalysisActivityChains1938">(<a href="#ref-denoAnalysisActivityChains1938" role="doc-biblioref">de Nó 1938, fig. 2</a>)</span> Original caption: Diagram of the pathways connecting the internuncial cells among themselves and with the ocular motoneurons. V, vestibular nerve; 1 to 6, cells in the primary vestibular nuclei; 7, 8, 9, cells in the reticular formation in the medullar (Med.) and pons (P.); 10, 11, 12, cells in the reticular nuclei in the midbrain (M.b.); Oc.n., oculomotor nuclei; f.l.p., fasciculus longitudinalis posterior and similar pathways; i, internuncial pathways; Fl, F2 and Col., position of the stimulating electrodes. The diagrams below indicate the two types of chains formed by internuncial cells; IV, multiple and C, closed chain.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Lorente de No_ResNet_2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Oculomotor of the rabbit. <span class="citation" data-cites="denoAnalysisActivityChains1938">(<a href="#ref-denoAnalysisActivityChains1938" role="doc-biblioref">de Nó 1938, fig. 3</a>)</span> Original caption: i1, i2, i3, i4, intemuncial paths. Passage of a synapse means a delay of about 0.6 msec. Note that each fiber has several synaptic knobs on the neuron, an arrangement increasing the possibility of spatial summation.</figcaption>
</figure>
</div>
<p>Lorente de No actually discovered those during the mid-1920s, but Cajal told him that he shouldn’t publish those, because other neuroscientists would think it is crazy, damaging de No’s career. So he published in 1934, immediately after Cajal died.</p>
<blockquote class="blockquote">
<p>It is easier to sweep this complexity under the rug, which I do now by resorting to anecdote. Three years ago in a workshop at Irvine I presented some data on the properties of mitral cells in the olfactory bulb, from which I inferred that they formed a mutually excitatory neural population (i.e., one having positive feedback). I cited this his as confirmation of Ramón y Cajal’s (1909) hypothesis of “avalanche conduction.” in which a weak olfactory stimulus might undergo “amplification” (as we would say now). Rafael Lorente de Nó. in the audience. stated that I was in error, Cajal had in mind feed-forward recruitment of mitral cells and disavowed the notion of feedback. Lorente later recalled (personal communication) that in the mid-1920s he prepared a manuscript on the cytoarchitecture of the cerebral cortex, In which he concluded that feedback relations were a prominent feature. After reading the manuscript. Cajal strongly urged Lorente not to publish it because it would be unacceptable to the scientific community and might blight his career. Out of respect for his mentor. Lorente elected not to publish the material while Cajal lived, when he did publish (Lorente de Nó. 1934), the work established itself as one of the enduring classics in neuroanatomy.</p>
<p><span class="citation" data-cites="freemanPremisesNeurophysiologicalStudies1984">(<a href="#ref-freemanPremisesNeurophysiologicalStudies1984" role="doc-biblioref">Freeman 1984</a>)</span></p>
</blockquote>
<p>Lorente de No was at the founding of cybernetics, and went to the <a href="https://en.wikipedia.org/wiki/Macy_conferences">Macy conferences</a> a lot. He consistently pointed out that recurrent networks exist in the brain and is possibly responsible for transient memory. He had influenced many of the early cyberneticians, including Hebb, McCulloch, and Pitts. <span class="citation" data-cites="espinosa-sanchezImportanceCajalLorente2023">(<a href="#ref-espinosa-sanchezImportanceCajalLorente2023" role="doc-biblioref">Espinosa-Sanchez, Gomez-Marin, and de Castro 2023</a>)</span></p>
</section>
<section id="donald-hebb" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="donald-hebb">Donald Hebb</h3>
<p>Hebbian learning of synapses, also “reverberation”. In short, if A fired shortly before B fired, then all A-to-B synapses would increase in strength. It is not just “neurons that fire together wire together”, since the neuron before the synapse must fire just before the neuron after the synapse. In modern-day language, it is <a href="https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity">spike-timing-dependent plasticity</a>.</p>
<p>In Hebb’s theory, the brain is a large neuron network, with long parallel fibers (like long-range undersea cables) connecting small clusters of neurons (“cell assemblies”). The long parallel fibers are hardwired, but the small clusters are formed by Hebbian learning. The cell assemblies are recurrent, allowing long-time reverberations, allowing things like after-image, imagination, sensory integration, and other things that require the brain to assemble some information together and “keep it in mind” for a while, despite the lack of external stimulus. He also wanted to use this theory for explaining pathologies like hallucination, phantom pain, etc.</p>
<blockquote class="blockquote">
<p>Let us assume that the persistence or repetition of a reverberatory activity (or “trace”) tends to induce lasting cellular changes that add to its stability. … When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.</p>
<p><span class="citation" data-cites="hebbOrganizationBehaviorNeuropsychological2002">(<a href="#ref-hebbOrganizationBehaviorNeuropsychological2002" role="doc-biblioref">Hebb 2002, 62</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>an indefinite reverberation in the structure might be possible, so long as the background activity in other cells in the same gross region remained the same. It would not of course remain the same for long, especially with changes of visual fixa tion; but such considerations make it possible to conceive of “alternating” reverberation which might frequently last for periods of time as great as half a second or a second.</p>
<p><span class="citation" data-cites="hebbOrganizationBehaviorNeuropsychological2002">(<a href="#ref-hebbOrganizationBehaviorNeuropsychological2002" role="doc-biblioref">Hebb 2002, 73–74</a>)</span></p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Hebb_RNN.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The Hebb cell assembly. <span class="citation" data-cites="hebbOrganizationBehaviorNeuropsychological2002">(<a href="#ref-hebbOrganizationBehaviorNeuropsychological2002" role="doc-biblioref">Hebb 2002, 73</a>, figure 10)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Hebb_RNN.gif" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Animated version. The “reverberation” is clear in this case.</figcaption>
</figure>
</div>
<p>In proposing the reverberating circuitry, Hebb was influenced by de No <span class="citation" data-cites="espinosa-sanchezImportanceCajalLorente2023">(<a href="#ref-espinosa-sanchezImportanceCajalLorente2023" role="doc-biblioref">Espinosa-Sanchez, Gomez-Marin, and de Castro 2023</a>)</span></p>
<blockquote class="blockquote">
<p>[The cell assembly theory] certainly looked improbable to its author–me–when it was first conceived [because it makes the ease of perception of common objects the result of a long process of learning]. The problem of perception remained intractable for about five years (1939 to 1944) and as a result I made no progress in my attempt to understand concepts and thought. It seemed obvious that concepts, like images, must derive from perception, and I could think of no mechanism of perception that corresponded to my preconceptions. In fact, by 1944 I had given up trying to solve the problem. What happened then was that I became aware of some recent work of Lorente de No in conjunction with some observations of Hilgard and Marquis (1940) which led me to think about the problem from a different point of view… The essential basis of an alternative view was provided by Lorente de No, who showed that the cortex is throughout its extent largely composed of enormously complex closed or re-entrant paths, rather than linear connections only between more distant points… When an excitation reaches the cortex, instead of having to be transmitted at once to a motor path, or else die out, it may travel round and round in these closed paths and may continue to do so after the original sensory stimulation has ceased.</p>
<p>Hebb DO. 1980. Essay on mind. Hillsdale, NJ: Lawrence Erlbaum</p>
</blockquote>
</section>
</section>
<section id="cybernetics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="cybernetics">Cybernetics</h2>
<section id="wwii" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="wwii">WWII</h3>
<blockquote class="blockquote">
<p>War is father of all and king of all; and some he manifested as gods, some as men; some he made slaves, some free. [Πόλεμος πάντων μὲν πατήρ ἐστι πάντων δὲ βασιλεύς, καὶ τοὺς μὲν θεοὺς ἔδειξε τοὺς δὲ ἀνθρώπους, τοὺς μὲν δούλους ἐποίησε τοὺς δὲ ἐλευθέρους.]</p>
<p>— Heraclitus</p>
</blockquote>
<p>I have gone a bit overboard with the WWII pictures, but I think war has a high concentration of functionalist beauty. Anything that is optimized to the hilt, like a TSMC fab, or a modern ICBM, is intrinsically beautiful. See <a href="https://gwern.net/larping">Why Do Hipsters Steal Stuff? · Gwern.net</a></p>
<p><img src="figure/war_is_the_father_of_all.png" class="img-fluid"></p>
<p>For a good tutorial on the problem of land-based anti-aircraft fire control during WWII, see the <a href="https://archive.org/details/TF1-3389Flak">1944 United States Army Air Forces film #TF 1-3389, “Flak”</a>. For another one from the perspective of naval fire control, see the <a href="https://www.youtube.com/watch?v=s1i-dnAH9Y4">1953 U.S. Navy training film (MN-6783a) Basic Mechanisms In Fire Control Computers</a>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Tokyo bombing map_damage assessment report no 20.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Damage Assessment Report No.&nbsp;20. Mosaic map showing target area and extent of <a href="https://en.wikipedia.org/wiki/Bombing_of_Tokyo">Tokyo bombing</a> caused by the 1945-03-10 raid – Source: U.S. National Archives, Record Group 243, Series 59, Box 6.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Goncharenko_1963_Tu-123.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><em>Cybernetics in Military Affairs</em>. USSR, Moskva, “<a href="https://en.wikipedia.org/wiki/DOSAAF">DOSAAF</a>” (M. N. Goncharenko, 1963). The image shows a <a href="https://en.wikipedia.org/wiki/Tupolev_Tu-123">Tupolev Tu-123 reconnaissance drone</a>. Книга М. Н. Гончаренко “Кибернетика в военном деле”. СССР, Москва, “ДОСААФ” 1963 год. The original caption says “Puc. 39. Беспилотный разведывательный реактивный самолет AN/VSD-5 многократного использования.” [Fig. 39. AN/VSD-5 reusable unmanned reconnaissance jet aircraft.]</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/white phosphorus bomb_Iwo Jima 1945.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Japanese Mitsubishi A6M Zero drop white phosphorous air-burst bombs on B-24 Bombers over Iwo Jima, 1945-02. Apparently those were used during the Battle of Iwo Jima as a crude anti-bomber attack. The information is scarce, but there is a video <a href="https://www.youtube.com/watch?v=lezeyKuWuPQ"><em>Pacific Jap Phosphorous Bombs</em></a> demonstrating it.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Kammhuber Line map.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Map of the <a href="https://en.wikipedia.org/wiki/Kammhuber_Line">Kammhuber line</a>, a defensive front against British bombers. It consisted of a series of control sectors equipped with radars and searchlights and an associated night fighter. Each sector would direct the night fighter into visual range to target intruding bombers.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Kammhuber_Line_Map_-_Agent_Tegal.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A map of the Kammhuber line stolen by a spy in 1942.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Operation Gomorrah_final plot.webp" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Scatterplot of photos taken from bombers during <a href="https://en.wikipedia.org/wiki/Operation_Gomorra">Operation Gomorrah</a> bombing of Hamburg, 1943-07. It was standard practice to take photos from bombers to validate the bombing accuracy, benchmark the loss function, measure the ablation of the target, and plan the next attack.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Operation Gomorrah_sine curves.jpeg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Operation Gomorrah with <a href="https://en.wikipedia.org/wiki/Avro_Lancaster">Avro Lancaster bomber</a>, 30/31 January 1943-01-30. The bright sine-like curves are flares, while the smoke and explosions provide diffuse lighting. Source: <a href="https://en.wikipedia.org/wiki/File:Attack_on_Hamburg.jpg">C 3371 of the Imperial War Museums.</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Hannover_linear scatterplot.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Multiple explosions/fires in the Sudstadt district of Hannover and a Lancaster silhouetted well below bottom left. Source: <a href="https://ibccdigitalarchive.lincoln.ac.uk/omeka/collections/document/47703">Hannover · IBCC Digital Archive</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Bombing Bremen_searchlights.webp" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A night <a href="https://en.wikipedia.org/wiki/Bombing_of_Bremen_in_World_War_II">bombing raid on Bremen</a>. A British bomber has been caught in the searchlight cone and heavy anti-aircraft fire is converging on the aircraft. Source: <a href="https://www.awm.gov.au/collection/044856">Australian War Memorial</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Kassel 1943-10-22--23 Outward route.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Planning map for the <a href="https://en.wikipedia.org/wiki/Bombing_of_Kassel_in_World_War_II">bombing of Kassel</a>, 1943-10-22–23. <span class="citation" data-cites="murrayStrategyDefeatLuftwaffe1986">(<a href="#ref-murrayStrategyDefeatLuftwaffe1986" role="doc-biblioref">Murray 1986, 211</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Moscow bombing_1941.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><a href="https://en.wikipedia.org/wiki/Battle_of_Moscow">Bombardment of Moscow</a>, 1941-07-26. Photo taken by Margaret Bourke-White.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/war_curve_Moscow_2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The same photo of the Bombardment of Moscow, with a scatterplot overlaid to bring out the similarity.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/war_curve_Moscow_1.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The scatterplot itself. The Python code is as follows.</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the points</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>num_points <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>x_points <span class="op">=</span> np.random.normal(<span class="fl">0.5</span>, <span class="fl">0.5</span>, num_points)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y_points <span class="op">=</span> np.exp(x_points) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>, num_points)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the curves</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>num_curves <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>x_curves <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>r_values <span class="op">=</span> np.random.normal(<span class="dv">1</span>, <span class="fl">0.1</span>, num_curves)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, num_curves)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the points</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_points, y_points, color<span class="op">=</span><span class="st">'white'</span>, s<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the curves</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_curves):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.plot(x_curves, np.exp(r_values[i] <span class="op">*</span> x_curves) <span class="op">+</span> k_values[i], color<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Set plot limits and background color</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">4</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> np.zeros((<span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">3</span>), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">256</span>):</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>gradient[:, i, :] <span class="op">=</span> (i,i,i)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> (<span class="dv">256</span> <span class="op">-</span> gradient) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.imshow(gradient, extent<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">4</span>], aspect<span class="op">=</span><span class="st">'auto'</span>, origin<span class="op">=</span><span class="st">'lower'</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'war_curve.svg'</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/B-17.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A German instructional poster for training the Luftwaffe. The shapes indicate areas of fire from the 50mm gunner positions aboard the B-17. Also noted are susceptible locations, specifically the fuel and oil tanks on the Flying Fortress. The text is murky but what I can make out are “Darstellung der von den Bordwaffen bestrichenen Räume”, “Darstellung des Uberschneidens der Feuerabschnitte”, “Viermotoriges Kampfflugzeug Boeing B-17 Flying Fortress”. [“Representation of the areas covered by the on-board weapons”, “Representation of the overlap of the fire zones”, “Four-engine fighter aircraft Boeing B-17 Flying Fortress”].</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/B-24.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Essentially the same diagram, but for B-24 “Liberator”. <a href="https://www.b24.net/2ndADA-Newsletters/1979-Jun.pdf">Source: Journal of the Second Air Division Association, Newsletter Vol. 18, No.2, 1979-06</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Gunner's Information File_1944-05_S-18.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Illustration in a training manual for American bomber gunners. This shows how to lead the target by a crude form of linear regression that every gunner performs. Source: <a href="https://digitalcollections.museumofflight.org/items/show/50156">Gunner’s Information File - Flexible Gunnery. Air Force Manual No.&nbsp;20, May 1944</a></figcaption>
</figure>
</div>
<p>Bonus: scaling plot?</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/scaling_law_war_and_peace.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Top image: Learning curve of the production of B-29 airframes at the Boeing Wichita division during WWII. Page 75 from <a href="https://purl.stanford.edu/dg041ny3484">Source Book of World War II Basic Data - Airframe Industry. Volume 1. Direct Man-Hours - Progress Curves</a>. Bottom image: Scaling curves for LLM. <span class="citation" data-cites="kaplanScalingLawsNeural2020">(<a href="#ref-kaplanScalingLawsNeural2020" role="doc-biblioref">Kaplan et al. 2020</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="the-problem-of-fire-control" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-problem-of-fire-control">The problem of fire control</h3>
<p>Wiener treated fire control as a problem of linear regression.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/missile control linear geometry_1957.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The linear geometry of missile flight control. Page 21 of <a href="https://maritime.org/doc/symbols/index.php"><em>Standard Fire Control Symbols for Missile Related Quantities</em>, OP 1700 Volume 3, (1957)</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/missile control curved geometry_1957.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The curved geometry of missile flight control. Page 53 of <a href="https://maritime.org/doc/symbols/index.php"><em>Standard Fire Control Symbols for Missile Related Quantities</em>, OP 1700 Volume 3, (1957)</a></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/fire control VOR.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The vestibulo-ocular reflex of the fire control. Section 15 of <a href="https://maritime.org/doc/firecontrol/partg.php">Fire Control Fundamentals, NAVPERS 91900 (1953), Part G</a>. Original caption: To measure director elevation in a vertical plane, the optical equipment and radar antenna must be stabilized so that they elevate in a vertical plane. This is accomplished by continuously supplying the director with crosslevel as measured at the stable element. An automatic positioning device continuously rotates the telescopes, rangefinder and radar antenna in response to the crosslevel signal, to keep their axes horizontal. Thus target elevation is always measured in a vertical plane.</figcaption>
</figure>
</div>
<p>I think it is no coincidence that negative feedback circuit was first discovered by Lorente de No in the VOR, and constructed during WWII in the artificial-VOR. In both cases, the eyes needs to be stabilized cheaply and fast.</p>
<p>Norbert Wiener decided he had to contribute to the war, and since his background was in stochastic process and statistics, he decided to work on aircraft predictor for anti-aircraft fire control. The following quote is from a classified memorandum in 1942. It was connected with sensitive wartime efforts to improve radar communication and became a founding document for cybernetics and information theory.</p>
<blockquote class="blockquote">
<p>This book represents an attempt to unite the theory and practice of two fields of work which are of vital importance in the present emergency, and which have a complete natural methodological unity, but which have up to the present drawn their inspiration from two entirely distinct traditions, and which are widely different in their vocabulary and the training of their personnel. These two fields are those of time series in statistics and of communication engineering.</p>
<p><span class="citation" data-cites="norbertwienerExtrapolationInterpolationSmoothing1966">(<a href="#ref-norbertwienerExtrapolationInterpolationSmoothing1966" role="doc-biblioref">Norbert Wiener 1966, 1</a>)</span></p>
</blockquote>
<p>This quote from its introduction shows how Wiener thought of statistical communication theory. Prediction of aircraft trajectory is understood as communication: The past trajectory of the aircraft is a noisy channel that is trying to communicate (despite the best efforts of the pilot) the future trajectory of the aircraft.</p>
<p>Wiener thought of antiaircraft fire as a problem of feedback control. The loss function is the squared distance between the aircraft and the bullet. The aircraft is killed by minimizing the loss function.</p>
<p>The control mechanism is a least-squares predictor. <span class="citation" data-cites="yeangFilteringNoiseAntiaircraft2023">(<a href="#ref-yeangFilteringNoiseAntiaircraft2023" role="doc-biblioref">Yeang 2023</a>)</span></p>
<blockquote class="blockquote">
<p>in order to obtain as complete a mathematical treatment as possible of the over-all control problem, it is necessary to assimilate the different parts of the system to a single basis, either human or mechanical. Since our understanding of the mechanical elements of gun pointing appeared to us to be far ahead of our psychological understanding, we chose to try to find a mechanical analogue of the gun pointer and the airplane pilot.</p>
<p><span class="citation" data-cites="wienerNorbertWienerLife2017">(<a href="#ref-wienerNorbertWienerLife2017" role="doc-biblioref">Wiener 2017, 407</a>)</span></p>
</blockquote>
</section>
<section id="commentary" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="commentary">Commentary</h3>
<p>Why did it take 50 years for feedback to appear? I don’t know, but I have some theories.</p>
<ul>
<li>Feedback networks are much harder to solve mathematically.</li>
<li>Feedback threatens the idea of cause-then-effect.</li>
<li>Reflexes are simpler to study, so neuroscientists thought that the brain is made of nothing but reflexes.</li>
<li>Behaviorism.</li>
</ul>
<p>Also, I think this shows that there <em>is</em> progress in theoretical neuroscience.</p>
<p>Freud in the 19th century analogized the brain as a steam engine (thus there is “psychic pressure” that builds up and must be “vented”). In the early 20th century, the brain was often analogized to a telephone system. Later, it was analogized to an artificial neural network. Some commentators have pointed at this, and sneered that, scientists always mistakenly use whatever analogy is fashionable to study what is fundamentally out of their reach.</p>
<p>What this does not note is that telephone networks are feedforward, while neural networks can be feedback. A telephone network does not work if there is feedback (it would be ringing pretty loudly), but some artificial neural networks work precisely because there is feedback.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/telephone switching.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Simplified diagram of telephone switching. <a href="https://www.calling315.com/basics-of-telephone-exchanges">Source</a>.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/manual crossbar switch_1903.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Crossbar switching in a 1903 patent. <a href="https://en.wikipedia.org/wiki/File:Telephone_switchboard_cross-switching_(Rankin_Kennedy,_Electrical_Installations,_Vol_V,_1903).jpg">Source</a>.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Crossbar_NY_1938-05.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Crossbar switching at a New York telephone exchange center in 1938-05. <a href="https://ethw.org/File:-1_Crossbar_NY_May_1938.jpg">Source</a>.</figcaption>
</figure>
</div>
</section>
<section id="mcculloch-and-pitts-1943" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="mcculloch-and-pitts-1943">McCulloch and Pitts (1943)</h3>
<p>The paper <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span> is the foundational paper for artificial neural networks. Often cited, rarely read. In modern language, the paper considered neural networks with integer weights, integer biases, 0-1 activation functions, and constant time-delays. The constant time-delay means that the neural network operates in clock-ticks, so that the state of a neuron at time <span class="math inline">\(t\)</span> is decided by the inputs to it at time <span class="math inline">\(t-1\)</span>. This is necessary for two things:</p>
<ol type="1">
<li>Formalized Hebbian learning: if A has a potential synapse to B, and A fired at time <span class="math inline">\(t\)</span>, then B fired at time <span class="math inline">\(t+1\)</span>, then the potential synapse becomes a real synapse.</li>
<li>To make recurrent neural networks work.</li>
</ol>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/MP_neuron.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A single MP neuron. It has integer weights and biases. A positive weight is symbolized by a flat end, and a negative weight is symbolized by a circular end. The bias (firing threshold) is written on the neuron itself.</figcaption>
</figure>
</div>
<p>They proved the following theorems:</p>
<ol type="1">
<li>Any boolean function is implemented by a purely feedforward network (“nets without circles”). Thus, it can be used as the controller in any Turing machine.</li>
</ol>
<blockquote class="blockquote">
<p>first, that every net, if furnished with a tape, scanners connected to afferents, and suitable efferents to perform the necessary motor-operations, can compute only such numbers as can a Turing machine; second, that each of the latter numbers can be computed by such a net…</p>
</blockquote>
<ol start="2" type="1">
<li><p>A recurrent network is equivalent to a Turing machine with a finite tape.</p></li>
<li><p>A recurrent network can perform Hebbian learning, even if all its weights and biases remain unchanged.</p></li>
</ol>
<blockquote class="blockquote">
<p>at any time the neuron fires, and the axonal terminations are simultaneously excited, they become synapses… capable of exciting the neuron… THEOREM VII. Alterable synapses can be replaced by circles.</p>
</blockquote>
<p>The proof is as shown below:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/MP_recurrent_learning.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption class="margin-caption">The dashed synapse can become a real synapse by Hebbian learning.</figcaption>
</figure>
</div>
<p>The paper had two sources: pathological states where the mind folds back onto itself (tinnitus, hallucination, etc), and healthy states where the mind is influenced by its past (memory).</p>
<blockquote class="blockquote">
<p>There is no theory we may hold and no observation we can make that will retain so much as its old defective reference to the facts if the net be altered. Tinnitus, paraesthesias, hallucinations, delusions, confusions and disorientations intervene. Thus empiry confirms that if our nets are undefined, our facts are undefined, and to the “real” we can attribute not so much as one quality or “form”. With determination of the net, the unknowable object of knowledge, the “thing in itself,” ceases to be unknowable. <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>In 1941 I presented my notions on the flow of information through ranks of neurons to Rashevsky’s seminar in the Committee on Mathematical Biology of the University of Chicago and met Walter Pitts, who then was about seventeen years old. He was working on a mathematical theory of learning and I was much impressed.</p>
<p>He was interested in problems of circularity, how to handle regenerative nervous activity in closed loops. I had had to suppose such loops to account for epileptic activity of surgically isolated brain and even of undercut cortex. Lorente de No had shown their significance in vestibular nystagmus. I wanted them to account for causalgia persisting after amputation of a painful limb and even after section of the spinothalamic tract; I wanted them to account for the early stages of memory and conditioning. I wanted them to account for compulsive behavior, for anxiety and for the effects of shock therapy. These appeared to be processes that once started seemed to run on in various ways. Since there obviously were negative feedbacks within the brain, why not regenerative ones? For two years Walter and I worked on these problems whose solution depended upon modular mathematics of which I knew nothing, but Walter did. We needed a rigorous terminology and Walter had it from Carnap, with whom he had been studying. We, I should say Walter Pitts, finally got it in proper form and we published in 1943, A Logical Calculus of the Ideas Immanent in Nervous Activity. H.D. Landahi immediately joined us in a note applying the logical calculus statistically. The crucial third part of our first article is rigorous but opaque and there is an error in subscript. In substance what it proved via its three theorems is that a net made of threshold devices, formal neurons, can compute those and only those numbers that a Turing machine can compute with a finite tape.</p>
<p><span class="citation" data-cites="mccullochRecollectionsManySources1974">(<a href="#ref-mccullochRecollectionsManySources1974" role="doc-biblioref">McCulloch 1974</a>)</span></p>
</blockquote>
</section>
</section>
<section id="first-neural-network-period-19501970" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="first-neural-network-period-19501970">First neural network period (1950–1970)</h2>
<section id="snarc" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="snarc">SNARC</h3>
<p>There were some neural networks in the early 1950s before Rosenblatt. See <span class="citation" data-cites="nilssonQuestArtificialIntelligence2009">(<a href="#ref-nilssonQuestArtificialIntelligence2009" role="doc-biblioref">Nilsson 2009, chap. 4</a>)</span> for a review. The most notable one is Minsky’s <a href="https://en.wikipedia.org/wiki/Stochastic_Neural_Analog_Reinforcement_Calculator">SNARC</a>. Details are in <a href="https://yuxi-liu-wired.github.io/sketches/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work">Yuxi on the Wired - A Scrapbook of Neural Network Lores</a> and <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky">Yuxi on the Wired - The Perceptron Controversy</a>.</p>
<p>Apparently it was a neural network version of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Shannon’s robot rat “Theseus”</a>.</p>
<p>SNARC had 40 neurons, but it was lost. The report <span class="citation" data-cites="minskyNeuralanalogueCalculatorBased1952">(<a href="#ref-minskyNeuralanalogueCalculatorBased1952" role="doc-biblioref">Minsky 1952</a>)</span> was lost too. I emailed the Harvard library for a copy, but they did not find it.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/SNARC_neuron.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Pictured: Minsky’s last remaining neuron. I like to joke that Minsky has very few neurons (see his 1981 comment about unable to afford “thousands or millions of neurons”), and this picture is the only one remaining.</figcaption>
</figure>
</div>
<p>His PhD thesis <span class="citation" data-cites="minskyTheoryNeuralanalogReinforcement1954">(<a href="#ref-minskyTheoryNeuralanalogReinforcement1954" role="doc-biblioref">Minsky 1954</a>)</span> contained lots and lots of neural networks doing all kinds of Turing complete things. No way it would require <em>Perceptrons</em> (1969) to reveal to the world that, surprisingly, single-layered perceptrons can’t do XOR.</p>
</section>
<section id="frank-rosenblatt" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="frank-rosenblatt">Frank Rosenblatt</h3>
<p>Started research in 1957 under “Project PARA” (“Perceiving and Recognition Automata”). First published in a report <span class="citation" data-cites="rosenblattPerceptronProbabilisticModel1958">(<a href="#ref-rosenblattPerceptronProbabilisticModel1958" role="doc-biblioref">Rosenblatt 1958</a>)</span>, later published in a whole book (1961).</p>
<p><span class="citation" data-cites="nagyNeuralNetworksthenNow1991">(<a href="#ref-nagyNeuralNetworksthenNow1991" role="doc-biblioref">Nagy 1991</a>)</span> describes some more of Rosenblatt’s works.</p>
<p>Generally, Rosenblatt investigated in two ways: computer simulation (on IBM machines usually) and special hardwares (Perceptron Mark I, Tobermory). After 1965, the computer simulations became much faster than special hardwares. The tragedy of ASIC neuromorphic computing, no doubt…</p>
<p>Interesting results in the book: (More in <a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/index.html#frank-rosenblatt">Yuxi on the Wired - The Backstory of Backpropagation</a>)</p>
<p>Continuous activation functions (section 10), what he called “transmission functions”.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Rosenblatt_1960_Hopfield.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">First sighting of the Hopfield network. <span class="citation" data-cites="rosenblattPerceptualGeneralizationTransformation1960">(<a href="#ref-rosenblattPerceptualGeneralizationTransformation1960" role="doc-biblioref">Rosenblatt 1960, 74</a>)</span></figcaption>
</figure>
</div>
<p>Recurrent network that can perform a kind of selective visual attention (chapter 21) and memorize output sequences (chapter 22)</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Rosenblatt_1962_Hopfield.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Hopfield network, again. <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, fig. 47</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Rosenblatt_1962_recurrent selective attention.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Recurrent network that can perform a kind of selective visual attention. <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, fig. 63</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Rosenblatt_1962_multimodal.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The first multimodal neural network? Associative learning of image and audio inputs (Figure 58)</figcaption>
</figure>
</div>
<p>The most interesting example is the “back-propagating error-correction procedure” network with residual connections and weight decay, pictured in Figure 42.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Rosenblatt_1962_MLP.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The 4-layered MLP in Rosenblatt. (Figure 42)</figcaption>
</figure>
</div>
<p>Its structure is <code>(fixed linear)-(activation)-(residual linear)-activation-linear</code>:</p>
<ul>
<li>The first layer is fixed (randomly wired).</li>
<li>The second layer has both a residual connection and a learned connection. The learned weights increase by Hebbian learning, and the weights decay exponentially. Weight decay is a practical necessity because Hebbian learning only increases weights, never decreasing them.</li>
<li>The third layer is trained by the perceptron learning rule, as usual.</li>
</ul>
<p>His last great project was Tobermory (1961–1967), for speech recognition. 4 layers with 12,000 weights (magnetic cores). By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines. It occupied an entire room.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Tobermory.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Tobermory schematic. From <a href="https://apps.dtic.mil/sti/trecms/pdf/AD0607459.pdf">System and Circuit Designs for the Tobermory Perceptron: Preliminary Report on Phase I By GEORGE NAGY 1 September, 1963 Prepared Under Contract No.&nbsp;NONR 401 (40) and NSF GP-971</a></figcaption>
</figure>
</div>
</section>
<section id="stanford-research-institute" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="stanford-research-institute">Stanford Research Institute</h3>
<blockquote class="blockquote">
<p>When I first interviewed for a position at SRI in 1961, I was warned by one researcher there against joining research on neural networks. Such research, he claimed, was “premature,” and my involvement in it could damage my reputation.</p>
<p><span class="citation" data-cites="nilssonQuestArtificialIntelligence2009">(<a href="#ref-nilssonQuestArtificialIntelligence2009" role="doc-biblioref">Nilsson 2009, chap. 24.2</a>)</span></p>
</blockquote>
<p>The weight matrix of the MINOS II pattern recognition system, consisting of a grid of magnetic cores. The magnetization of a core is proportional to its matrix weight. Matrix weights that are quite literally weighty. The weight matrices are swappable.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/MINOS_II.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="rosenResearchDevelopmentProgram1965">(<a href="#ref-rosenResearchDevelopmentProgram1965" role="doc-biblioref">Rosen, Nilsson, and Adams 1965</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="adaline" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="adaline">ADALINE</h3>
<p>ADALINE (adaptive linear) and MADALINE (many ADALINE).</p>
<p>The ADALINE is a single perceptron <span class="math display">\[
\theta\left(\sum_i w_i x_i + b\right)
\]</span></p>
<p>and its learning rule is gradient descent on squared error <span class="math inline">\((\sum_i w_i x_i + b - y)^2\)</span>.</p>
<p><img src="figure/ADALINE.png" class="img-fluid"></p>
<p>It used <em>manual input</em> by flipping 16 switches <em>by hand</em>. The first version (“knobby ADALINE”) had weights implemented as rheostats, with knobs turned <em>by hand</em>!!! Later versions used memistors that do gradient descent automatically.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/knobby_ADALINE.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Widrow doing brain surgery on ADALINE.</figcaption>
</figure>
</div>
<p>They got up to 1000 weights in a 2-layered MADALINE, but the learning rule was really hacky, and it was a deadend. <span class="citation" data-cites="widrow30YearsAdaptive1990">(<a href="#ref-widrow30YearsAdaptive1990" role="doc-biblioref">Widrow and Lehr 1990</a>)</span> A video of the MADALINE in action is found in <a href="https://www.youtube.com/watch?v=skfNlwEbqck">The LMS algorithm and ADALINE. Part II - ADALINE and memistor ADALINE - YouTube</a></p>
<p>More information in <a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff">Yuxi on the Wired - The Backstory of Backpropagation</a>.</p>
<p>I don’t know where to put this anecdote, but I want you to read about it anyway.</p>
<blockquote class="blockquote">
<p>We discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called MADALINE I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail.</p>
<p>B. Widrow, “Adaline: Smarter than Sweet”, Stanford Today, Autumn 1963.</p>
</blockquote>
<p>See more of these amusing stories at <a href="https://yuxi-liu-wired.github.io/sketches/posts/neural-network-scrapbook/#neural-networks-want-to-work">Yuxi on the Wired - A Scrapbook of Neural Network Lores</a>.</p>
</section>
</section>
<section id="neural-network-winter-1970s" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="neural-network-winter-1970s">Neural network winter (1970s)</h2>
<section id="the-xor-myth" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-xor-myth">The XOR myth</h3>
<p>The XOR myth can’t be true, for the following reasons:</p>
<ul>
<li>It was widely known that binary perceptron networks are Turing-complete. <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span> already proved it.</li>
<li>Minsky wrote his PhD thesis <span class="citation" data-cites="minskyTheoryNeuralanalogReinforcement1954">(<a href="#ref-minskyTheoryNeuralanalogReinforcement1954" role="doc-biblioref">Minsky 1954</a>)</span> describing all kinds of specific neural networks for computing specific boolean functions.</li>
<li>MLP was studied as “linear threshold logic” by electric engineers. They wouldn’t have been so interested if they couldn’t even do XOR. <span class="citation" data-cites="dertouzosThresholdLogicSynthesis1965">(<a href="#ref-dertouzosThresholdLogicSynthesis1965" role="doc-biblioref">Dertouzos 1965</a>)</span></li>
</ul>
<p>Still, the XOR myth has some power. What exactly is it? To explain this mess, I will draw a flowchart.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/XOR_problem_flowchart.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">XOR flowchart</figcaption>
</figure>
</div>
<p>Let’s follow all the points in the flowchart:</p>
<ul>
<li>Can your neural network learn XOR? If you answer “No”, then your neural network is patently useless.</li>
<li>To learn XOR, the neural network must have at least 2 layers. But how is it made? If you made it manually, then it is not really machine learning, is it?</li>
<li>If it is learned, then what algorithm?</li>
<li>The Perceptron learning algorithm only works for featurized linear regression. So where did you get your features? If you designed it manually, by, for example, reading neuroscience papers, then it is not really machine learning, is it? If they are randomly generated, then it will not scale because of combinatorial explosion.</li>
<li>If you used a hack like the MADALINE learning rule, or whatever Rosenblatt tried, then you have no doubt noticed how fiddly and unscalable they are, and how dependent they are on the hyperparameters. Each particular problem required a particular setting of hyperparameters.</li>
</ul>
<p>After backprop became prevalent, Minsky and Papert wrote their updated Perceptron (1988) with 43 pages to dunk on everything that is the new connectionism. This allows us to add a few more blocks to the XOR myth flowchart:</p>
<ul>
<li>If you use gradient descent, then it would just get stuck in local minima. You can’t just draw a scaling plot and hope it will continue, without proving it theoretically!</li>
<li>In conclusion, there is no general and scalable learning algorithm.</li>
</ul>
<p>As an example of the kind of MLP that Minsky approves: a hand-designed deep network <span class="citation" data-cites="fukushimaVisualFeatureExtraction1969">(<a href="#ref-fukushimaVisualFeatureExtraction1969" role="doc-biblioref">Fukushima 1969</a>)</span>. Here is someone who actually tried to understand the problem instead of just tossing it to the computer and expect things to magically work.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Fukushima_1969.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="fukushimaVisualFeatureExtraction1969">(<a href="#ref-fukushimaVisualFeatureExtraction1969" role="doc-biblioref">Fukushima 1969</a>)</span></figcaption>
</figure>
</div>
<p>Minsky can’t afford a few million neurons:</p>
<blockquote class="blockquote">
<p>I had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that.</p>
<p><span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
</section>
<section id="possible-reasons-for-the-winter" class="level3">
<h3 class="anchored" data-anchor-id="possible-reasons-for-the-winter">Possible reasons for the winter</h3>
<p>Minsky and Papert’s book. At least, Minsky thought so.</p>
<blockquote class="blockquote">
<p>There had been several thousand papers published on Perceptrons up to 1969, but our book put a stop to those. It had, in all modesty, some beautiful mathematics in it–it’s really nineteenth-century mathematics.</p>
<p><span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Minsky’s early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community.</p>
<p>— Robert Hecht-Nielsen <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>Lack of funding</p>
<blockquote class="blockquote">
<p>Many people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. — Michael A. Arbib <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>Lack of people</p>
<ul>
<li>McCulloch (1969) Pitts (1969) Rosenblatt (1971) died</li>
<li>Widrow cut his losses and looked for applications for a single neuron, revolutionizing adaptive noise filtering.</li>
<li>Ted Hoff went to Intel to create microprocessors.</li>
<li>SRI turned to logical AI. Duda and Hart wrote their classic textbook on “pattern classification” which had little neural networks, and half of the book was devoted to “scene analysis”. <span class="citation" data-cites="dudaPatternClassificationScene1973">(<a href="#ref-dudaPatternClassificationScene1973" role="doc-biblioref">Duda and Hart 1973</a>)</span></li>
</ul>
<p>And of course, no backpropagation.</p>
</section>
<section id="minsky-and-papert-against-large-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="minsky-and-papert-against-large-neural-networks">Minsky and Papert against large neural networks</h3>
<p>See <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/">Yuxi on the Wired - The Perceptron Controversy</a> for details.</p>
</section>
<section id="years-of-people-refusing-to-use-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="years-of-people-refusing-to-use-gradient-descent">30 years of people refusing to use gradient descent</h3>
<p>See <a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/">Yuxi on the Wired - The Backstory of Backpropagation</a>.</p>
<p>Rosenblatt did not even consider gradient descent. (1960s)</p>
<p>Minsky repeatedly shooting down gradient descent. (1960s – 1990s at least)</p>
<blockquote class="blockquote">
<p>The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea… It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It’s certainly trivial. The idea is how you do gradient descent. I didn’t consider it practical.</p>
<p><span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 249</a>)</span></p>
</blockquote>
<p>Widrow and Hoff could not generalize one-layered gradient descent to two-layered gradient descent and gave up (1960s).</p>
<blockquote class="blockquote">
<p>The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic first layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.</p>
<p>Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; <strong>you have to have a smooth nonlinearity … no one knew anything about it at that time.</strong> This was long before Paul Werbos. <strong>Backprop to me is almost miraculous</strong>. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>The SRI group never worked it out.</p>
<blockquote class="blockquote">
<p>During the 1960s, neural net researchers employed various methods for changing a network’s adjustable weights so that the entire network made appropriate output responses to a set of “training” inputs. For example, Frank Rosenblatt at Cornell adjusted weight values in the final layer of what he called the three-layer alpha-perceptron. Bill Ridgway (one of Bernard Widrow’s Stanford students) adjusted weights in the first layer of what he called a MADALINE. We had a similar scheme for adjusting weights in the first layer of the MINOS II and MINOS III neural network machines at SRI. Others used various statistical techniques to set weight values. But what stymied us all was how to change weights in more than one layer of multilayer networks. (I recall Charles Rosen, the leader of our group, sitting in his office with yellow quadrille tablets hand-simulating his ever-inventive schemes for making weight changes; none seemed to work out.)</p>
<p><span class="citation" data-cites="nilssonQuestArtificialIntelligence2009">(<a href="#ref-nilssonQuestArtificialIntelligence2009" role="doc-biblioref">Nilsson 2009, sec. 29.4</a>)</span></p>
</blockquote>
<p>Paul Werbos spending 10 years (1972–1982) unable to publish backpropagation, and almost starved.</p>
<p>He developed the backprop in 1971 or 1972 for his PhD thesis, as a mathematical version of Freudian psychoanalysis.</p>
<blockquote class="blockquote">
<p>The model in Figure 1 is actually just a mathematical version of Freud’s model of psychodynamics, where the derivative of <span class="math inline">\(R_i\)</span> represents what Freud called the “cathexis” (or affect) or emotional charge or emotional energy attached to the object which <span class="math inline">\(R_i\)</span> represents. In other words, I came up with backpropagation by not just laughing at Freud’s “nonscientific” model, but by translating it into mathematics and showing that it works. <span class="citation" data-cites="werbosNeuralNetworksPath2011">(<a href="#ref-werbosNeuralNetworksPath2011" role="doc-biblioref">Werbos 2011</a>)</span></p>
</blockquote>
<p>The PhD committee was unconvinced, so he had to find a supporting advisor.</p>
<blockquote class="blockquote">
<p>“But look, the mathematics is straightforward.”</p>
<p>“Yeah, yeah, but you know, <strong>we’re not convinced it’s so straightforward</strong>. You’ve got to prove some theorems first.”</p>
</blockquote>
<p>So he went to Stephen Grossberg…</p>
<blockquote class="blockquote">
<p>this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.</p>
</blockquote>
<p>Then to Marvin Minsky…</p>
<blockquote class="blockquote">
<p>Look, <strong>everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists</strong>. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.</p>
</blockquote>
<p>Unable to find a supporting advisor, he faced the committee again. He decided to simplify his work to just backpropagating through piecewise linear activation functions.</p>
<blockquote class="blockquote">
<p>I handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, <strong>but this is too trivial</strong> and simple to be worthy of a Harvard Ph.D.&nbsp;thesis.”</p>
</blockquote>
<p>So he lost funding, so he lived in the slums and became malnutritioned.</p>
<blockquote class="blockquote">
<p>… they had discontinued support because they were not interested… There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.</p>
</blockquote>
<p>Anyway, he did get his PhD thesis and it did contain the backpropagation algorithm, but then promptly got sucked into a decade of misadventures with federal bureaucracy. The net effect is that he only managed to publish the backprop algorithm in 1982.</p>
<p>Geoffrey Hinton spent several years (1982–1985) refusing to try backpropagation, since he was too invested in the Boltzmann machines.</p>
<blockquote class="blockquote">
<p>I first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule … The next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.</p>
</blockquote>
<p>But it turned out that Boltzmann machines just never worked in practice, and desperate times called for desperate measures (backpropagation). Amazingly, it just worked.</p>
<blockquote class="blockquote">
<p>I almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.</p>
</blockquote>
<p>In a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.</p>
<p>He still thinks Boltzmann machines are better though!</p>
<blockquote class="blockquote">
<p>That was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation.</p>
</blockquote>
</section>
</section>
<section id="some-bitter-lessons-i-guess" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="some-bitter-lessons-i-guess">Some bitter lessons I guess</h2>
<section id="first-attempt" class="level3">
<h3 class="anchored" data-anchor-id="first-attempt">First attempt</h3>
<blockquote class="blockquote">
<p>I hate everything that merely instructs me without increasing or directly quickening my activity.</p>
<p>— Goethe, <a href="https://www.friedrich-schiller-archiv.de/briefwechsel-von-schiller-und-goethe/1798/552-an-schiller-19-dezember-1798/">Letter to Friedrich Schiller, 1798-12-19</a>. Quoted in Nietzsche’s <em>On the Use and Abuse of History for Life</em>.</p>
</blockquote>
<blockquote class="blockquote">
<p>We do need history, but quite differently from the jaded idlers in the garden of knowledge, however grandly they may look down on our rude and unpicturesque requirements. In other words, we need it for life and action, not as a convenient way to avoid life and action, or to excuse a selfish life and a cowardly or base action. We would serve history only so far as it serves life; but to value its study beyond a certain point mutilates and degrades life.</p>
<p>— Nietzsche, <a href="https://en.wikisource.org/wiki/On_the_Use_and_Abuse_of_History_for_Life"><em>On the Use and Abuse of History for Life</em></a> (1874)</p>
</blockquote>
<p>Nihilism: You can draw lessons from history, but everyone else also can draw the opposite ones, so there is no take-home lesson.</p>
<p>Elitism: First-rate minds make history. Second-rate minds study history. Getting history “right” is for second-rate minds: old professors (that’s Schmidhuber) and procrastinating PhD students (that’s me).</p>
<p>Those who don’t know their history, repeat it. Those who know their history, are too busy remembering to make it.</p>
<p>Emotivism: The proper function of history is to fortify your weights and to harden your biases against naysayers, so that you can go forth and make some history.</p>
<blockquote class="blockquote">
<p>The function of a mathematician is to do something, to prove new theorems, to add to mathematics, and not to talk about what he or other mathematicians have done. Statesmen despise publicists, painters despise art-critics, and physiologists, physicists, or mathematicians have usually similar feelings: there is no scorn more profound, or on the whole more justifiable, than that of the men who make for the men who explain. Exposition, criticism, appreciation, is work for second-rate minds.</p>
<p><span class="citation" data-cites="hardyMathematicianApology1940">(<a href="#ref-hardyMathematicianApology1940" role="doc-biblioref">Hardy 1940</a>)</span></p>
</blockquote>
</section>
<section id="second-attempt" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="second-attempt">Second attempt</h3>
<p>This is where I was going to end, but GPT told me that I must include <em>positive</em> lessons, so I made some up. I fully expect you to ignore my lessons and reinterpret the history I dug up to your lessons.</p>
<p>To facilitate this freedom of reinterpretation, I will write my weights and biases in bold letters, so that it is not a secret agenda:</p>
<center>
<strong>VINCIT OMNIA COMPUTATIO</strong>
</center>
<p>With this, you can now subtract my latent vector and add yours, in full Word2Vec fashion.</p>
<section id="nihilism" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="nihilism">Nihilism</h4>
<p>History is useless for doing things, because ideas are cheap and can’t be lost. People will keep reinventing the same ideas until it starts working, then it will not be lost again. The idea is a patient seed, waiting for the soil to moisten.</p>
<p>In particular, it is useless to give correct attribution to “the real inventor” of an idea, because there are so many re-inventors. It is much more interesting to study each re-invention on its own, to find out how they invented. The point is not to respect the dead, but to find out how they did their things, so that we can do more things. It is useless to know who “really” invented backpropagation, but useful to know how each re-inventor managed to invent it in their individual, different <a href="https://plato.stanford.edu/entries/scientific-discovery/#DistBetwContDiscContJust">contexts of rediscovery</a>.</p>
<p>Those who don’t know their history, repeat it. If everyone knows their history, there will be a replication crisis.</p>
<p>Residual networks were described by Lorente de No, then Rosenblatt. DenseNet <span class="citation" data-cites="huangDenselyConnectedConvolutional2018">(<a href="#ref-huangDenselyConnectedConvolutional2018" role="doc-biblioref">Huang et al. 2018</a>)</span> was already in 1988 <span class="citation" data-cites="langLearningTellTwo1988">(<a href="#ref-langLearningTellTwo1988" role="doc-biblioref">Lang and Witbrock 1988</a>)</span>. People didn’t know it, but this did not matter.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/ResNet_history.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Reinventions/rediscoveries of ResNet. Lorente de No (1930s), Rosenblatt (1961), <span class="citation" data-cites="langLearningTellTwo1988">(<a href="#ref-langLearningTellTwo1988" role="doc-biblioref">Lang and Witbrock 1988</a>)</span>, ResNet (2015), DenseNet (2016).</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/RNN_history.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Reinventions/rediscoveries of RNN. Cajal (1900s), Lorente de No (1930s), McCulloch and Pitts (1943).</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Hopfield_history.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Reinventions/rediscoveries of Hopfield network. <span class="citation" data-cites="rosenblattPerceptualGeneralizationTransformation1960">(<a href="#ref-rosenblattPerceptualGeneralizationTransformation1960" role="doc-biblioref">Rosenblatt 1960</a>)</span>, <span class="citation" data-cites="nakanoLearningProcessModel1971">(<a href="#ref-nakanoLearningProcessModel1971" role="doc-biblioref">Nakano 1971</a>)</span>, <span class="citation" data-cites="hopfieldNeuralNetworksPhysical1982">(<a href="#ref-hopfieldNeuralNetworksPhysical1982" role="doc-biblioref">Hopfield 1982</a>)</span>. Note that because Hopfield’s paper has no pretty pictures, I got the picture from <span class="citation" data-cites="tankCollectiveComputationNeuronlike1987">(<a href="#ref-tankCollectiveComputationNeuronlike1987" role="doc-biblioref">Tank and Hopfield 1987</a>)</span> instead.</figcaption>
</figure>
</div>
<p>The only place where the lack of a good idea really hurt is backpropagation. But even if they had backpropagation, what could they have made in the 1970s? A personal tragedy for Widrow and Rosenblatt. Hardly a tragedy for the study of neural networks itself. Ted Hoff was right to leave neural networks to develop the microprocessor.</p>
</section>
<section id="empiricism" class="level4">
<h4 class="anchored" data-anchor-id="empiricism">Empiricism</h4>
<p>Elegant ideas matter little, because there are always opposing elegant ideas. Ideas must be tested by experiments. Benchmarks are the <em>experimentum crucis</em>. In mensuris veritas.</p>
<p>30 years of elegant arguments against backpropagation amounted to nothing but jokes for us.</p>
<p>This disjunction between theory and experiments is particularly acute in reinforcement learning. If you have read RL papers, you would notice how often they describe some elegant ideas and giant formulas, then proceeded to talk about the experiments. I always find the elegant ideas suspicious, and only trust the benchmarks. If the idea is really elegant and stands on its own, why all the ponderous benchmarks? Furthermore, it is often the case that the theory works only in the sense of “eventually almost anywhere if the functional space is large enough”.</p>
<p>Similar comments apply for the PAC-learning literature, the no-free-lunch literature, and the neural network universality literature. It is surely comforting to know that neural networks are universal function approximators, but so are Turing machines. Neural networks work not because they are universal, but because their inductive biases happen to be very attuned to physically interesting problems. Why is that? That… is unclear, but <span class="citation" data-cites="linWhyDoesDeep2017">(<a href="#ref-linWhyDoesDeep2017" role="doc-biblioref">Lin, Tegmark, and Rolnick 2017</a>)</span> has a guess. Basically, it states that neural networks are particularly good at learning trajectories generated by quadratic Lagrangian mechanical systems.</p>
</section>
</section>
<section id="the-idea-follows-the-compute" class="level3">
<h3 class="anchored" data-anchor-id="the-idea-follows-the-compute">The idea follows the compute</h3>
<p>It is not just that compute allows good ideas to be filtered out, and that testing ideas requires expensive compute. Sometimes compute is necessary to even approach the good idea. Without compute experiments, one would be left to wander the vast space of possible ideas. This is why ablation studies are so important, though it lacks the pedigreed prestige of theory. Ablations need compute, making it a choice for the <em>nouveau riche</em> – <a href="https://www.latent.space/p/semianalysis">the GPU-rich</a>, that is.</p>
<p>Two examples. One is ResNet</p>
<blockquote class="blockquote">
<p>For months, they toyed with various ways to add more layers and still get accurate results. After a lot of trial and error, the researchers hit on a system they dubbed “deep residual networks”.</p>
<p><span class="citation" data-cites="linnMicrosoftResearchersWin2015">(<a href="#ref-linnMicrosoftResearchersWin2015" role="doc-biblioref">Linn 2015</a>)</span></p>
</blockquote>
<p>Another is Transformer</p>
<blockquote class="blockquote">
<p>There was every possible combination of tricks and modules—which one helps, which doesn’t help. Let’s rip it out. Let’s replace it with this. Why is the model behaving in this counterintuitive way? Oh, it’s because we didn’t remember to do the masking properly. Does it work yet? OK, move on to the next. All of these components of what we now call the transformer were the output of this extremely high-paced, iterative trial and error.</p>
<p><span class="citation" data-cites="levyGoogleEmployeesInvented2024">(<a href="#ref-levyGoogleEmployeesInvented2024" role="doc-biblioref">Levy 2024</a>)</span></p>
</blockquote>
</section>
<section id="the-bitter-lesson" class="level3">
<h3 class="anchored" data-anchor-id="the-bitter-lesson">The bitter lesson</h3>
<blockquote class="blockquote">
<p>By convention sweet is sweet, bitter is bitter, hot is hot, cold is cold, color is color; but in truth there are only atoms and the void. [νόμωι (γάρ φησι) γλυκὺ καὶ νόμωι πικρόν, νόμωι θερμόν, νόμωι ψυχρόν, νόμωι χροιή, ἐτεῆι δὲ ἄτομα καὶ κενόν]</p>
<p>— Democritus, quoted in <em>Sextus Empiricus Against the Mathematicians</em> VII 135. I like the funny title.</p>
</blockquote>
<blockquote class="blockquote">
<p>A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die and a new generation grows up that is familiar with it …</p>
<p>— Max Planck, <em>Scientific Autobiography</em> (1950), page 33. In simpler terms, “Science progresses one funeral at a time.”.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="1">
<li><p>AI researchers have often tried to build knowledge into their agents,</p></li>
<li><p>this always helps in the short term, and is personally satisfying to the researcher, but</p></li>
<li><p>in the long run it plateaus and even inhibits further progress, and</p></li>
<li><p>breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.</p></li>
</ol>
<p>The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</p>
<p><span class="citation" data-cites="suttonBitterLesson2019">(<a href="#ref-suttonBitterLesson2019" role="doc-biblioref">Sutton 2019</a>)</span></p>
</blockquote>
<p>The bitter lesson is bitter, because some people really like elegant ideas, and they will not change.</p>
<p>Marvin Minsky insisted for decades that large neural networks do not work in theory (as we saw), so it only <em>appears</em> to work in practice.</p>
<blockquote class="blockquote">
<p>13.5 Why Prove Theorems?</p>
<p><em>Why did you prove all these complicated theorems? Couldn’t you just take a perceptron and see if it can recognize <span class="math inline">\(\psi_{\text {CONNECTED}}\)</span>?</em></p>
<p>No.&nbsp;(1969)</p>
<p><span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, 239</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>I had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks… I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (1981)</p>
<p><span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No.&nbsp;(1988)</p>
<p><span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, xiv</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>In his summary talk at the end of the conference [The AI@50 conference (2006)], Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications.” (2006)</p>
<p><span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 256</a>)</span></p>
</blockquote>
<p>Geoffrey Hinton still liked Boltzmann machines more than backprop, even in 1995:</p>
<blockquote class="blockquote">
<p>That was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation.</p>
<p><span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>Noam Chomsky is totally against statistical language modelling, since at least 1969. See also <span class="citation" data-cites="norvigChomskyTwoCultures2017">(<a href="#ref-norvigChomskyTwoCultures2017" role="doc-biblioref">Norvig 2017</a>)</span>.</p>
<blockquote class="blockquote">
<p>But it must be recognized that the notion of “probability of a sentence” is an entirely useless one, under any known interpretation of this term. (1969)</p>
<p><span class="citation" data-cites="chomskyEmpiricalAssumptionsModern1969">(<a href="#ref-chomskyEmpiricalAssumptionsModern1969" role="doc-biblioref">Chomsky 1969</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>It’s true there’s been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success … which I think is novel in the history of science. It interprets success as approximating unanalyzed data. (2011)</p>
<p><a href="https://languagelog.ldc.upenn.edu/myl/PinkerChomskyMIT.html">Brains, Minds, and Machines symposium 2011, Keynote Panel</a></p>
</blockquote>
<blockquote class="blockquote">
<p>Well, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They’ve achieved zero… GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It’ll use even more energy and achieve exactly nothing, for the same reasons. So there’s nothing to discuss. (2022)</p>
<p><a href="https://whimsical.com/question-1-large-language-models-such-as-gpt-3-DJmQ8R5kD8tqvsXxgCN9vK">Machine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding</a></p>
</blockquote>
<p>On the last one, I suspect he is still fighting the battle against behaviorism, and the pretrained language models like GPT-4 must seem like a kind of neo-behaviorist heresy to him.</p>
<p>Yehoshua Bar-Hillel, speaking in the 1960s, against machine translation – what he called FAHQT (Fully Automatic High-Quality Translation, or as I imagine him saying it, “Machine translation? Ah, FAHQT.”). In short, he argued that statistical language modelling does not work better than manually programming in the rules. Specifically, the Winograd schema challenge (which was known in the 1960s, before Winograd discussed it) requires general world understanding, which is “utterly chimerical and hardly deserves any further discussion’.</p>
<blockquote class="blockquote">
<p>No justification has been given for the implicit belief of the “empiricists” that a grammar satisfactory for MT purposes will be compiled any quicker or more reliably by starting from scratch and “deriving” the rules of grammar from an analysis of a large corpus than by starting from some authoritative grammar and changing it, if necessary, in accordance with analysis of actual texts.</p>
<p>…</p>
<p>Whenever I offered [the Winograd challenge] to one of my colleagues working on MT, their first reaction was: “But why not envisage a system which will put this knowledge at the disposal of the translation machine?” … such a suggestion amounts to, if taken seriously, is the requirement that a translation machine should not only be supplied with a dictionary but also with a universal encyclopedia. This is surely utterly chimerical and hardly deserves any further discussion.</p>
<p><span class="citation" data-cites="bar-hillelPresentStatusAutomatic1960">(<a href="#ref-bar-hillelPresentStatusAutomatic1960" role="doc-biblioref">Bar-Hillel 1960</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>It seems now quite certain to some of us, a small but apparently growing minority, that with all the progress made in hardware (i.e., apparatus), programming techniques and linguistic insight, the quality of fully autonomous mechanical translation, even when restricted to scientific or technological material, will never approach that of qualified human translators and that therefore Machine Translation will only under very exceptional circumstances be able to compete with human translation.</p>
<p><span class="citation" data-cites="bar-hillelFutureMachineTranslation1964">(<a href="#ref-bar-hillelFutureMachineTranslation1964" role="doc-biblioref">Bar-Hillel 1964</a>)</span></p>
</blockquote>
<p>Even Terry Winograd had a tentative guess that the Winograd challenge is too challenging, though it is clearly just a weak guess, not a firm prediction like the previous quotes.</p>
<blockquote class="blockquote">
<p>The limitations on the formalization of contextual meaning make it impossible at present – and conceivably forever – to design computer programs that come close to full mimicry of human language understanding.</p>
<p><span class="citation" data-cites="winogradComputerSoftwareWorking1984">(<a href="#ref-winogradComputerSoftwareWorking1984" role="doc-biblioref">Winograd 1984</a>)</span></p>
</blockquote>


<!-- -->


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bar-hillelPresentStatusAutomatic1960" class="csl-entry" role="listitem">
Bar-Hillel, Yehoshua. 1960. <span>“The Present Status of Automatic Translation of Languages.”</span> <em>Advances in Computers</em> 1: 91–163. <a href="https://www.sciencedirect.com/science/article/pii/S0065245808606075">https://www.sciencedirect.com/science/article/pii/S0065245808606075</a>.
</div>
<div id="ref-bar-hillelFutureMachineTranslation1964" class="csl-entry" role="listitem">
———. 1964. <span>“The Future of Machine Translation.”</span> <em>Hartley Rogers, <span>é</span>diteur, Language and Information–Selected Essays on Their Theory and Application, Logic</em>, 180–84. <a href="https://aclanthology.org/www.mt-archive.info/50/TLS-1962-Bar-Hillel.pdf">https://aclanthology.org/www.mt-archive.info/50/TLS-1962-Bar-Hillel.pdf</a>.
</div>
<div id="ref-bernsteinMarvinMinskyVision1981" class="csl-entry" role="listitem">
Bernstein, Jeremy. 1981. <span>“Marvin <span>Minsky</span>’s <span>Vision</span> of the <span>Future</span>.”</span> <em>The New Yorker</em>, December. <a href="https://www.newyorker.com/magazine/1981/12/14/a-i">https://www.newyorker.com/magazine/1981/12/14/a-i</a>.
</div>
<div id="ref-chomskyEmpiricalAssumptionsModern1969" class="csl-entry" role="listitem">
Chomsky, Noam. 1969. <span>“Some <span>Empirical Assumptions</span> in <span>Modern Philosophy</span> of <span>Language</span>.”</span> In <em>Philosophy, Science, and Method</em>, edited by Ernest Nagel, Sidney Morgenbesser, Patrick Suppes, and Morton White. St. Martin’s Press.
</div>
<div id="ref-denoAnalysisActivityChains1938" class="csl-entry" role="listitem">
de Nó, Rafael Lorente. 1938. <span>“Analysis of the Activity of the Chains of Internuncial Neurons.”</span> <em>Journal of Neurophysiology</em> 1 (3): 207–44. <a href="https://doi.org/10.1152/jn.1938.1.3.207">https://doi.org/10.1152/jn.1938.1.3.207</a>.
</div>
<div id="ref-dertouzosThresholdLogicSynthesis1965" class="csl-entry" role="listitem">
Dertouzos, Michael. 1965. <em>Threshold Logic: <span>A Synthesis Approach</span></em>. <a href="https://mitpress.mit.edu/9780262040099/">https://mitpress.mit.edu/9780262040099/</a>.
</div>
<div id="ref-dudaPatternClassificationScene1973" class="csl-entry" role="listitem">
Duda, Richard O., and Peter E. Hart. 1973. <em>Pattern Classification and Scene Analysis</em>. New York: Wiley.
</div>
<div id="ref-espinosa-sanchezImportanceCajalLorente2023" class="csl-entry" role="listitem">
Espinosa-Sanchez, Juan Manuel, Alex Gomez-Marin, and Fernando de Castro. 2023. <span>“The <span>Importance</span> of <span>Cajal</span>’s and <span>Lorente</span> de <span>N<span>ó</span></span>’s <span>Neuroscience</span> to the <span>Birth</span> of <span>Cybernetics</span>.”</span> <em>The Neuroscientist</em>, July, 10738584231179932. <a href="https://doi.org/10.1177/10738584231179932">https://doi.org/10.1177/10738584231179932</a>.
</div>
<div id="ref-freemanPremisesNeurophysiologicalStudies1984" class="csl-entry" role="listitem">
Freeman, Walter J. 1984. <span>“Premises in Neurophysiological Studies of Learning.”</span> In <em>Neurobiology of Learning and Memory</em>. Guilford Press, New York. <a href="https://web.archive.org/web/20001117004400/http://sulcus.berkeley.edu/FreemanWWW/manuscripts/IC4/84.html">https://web.archive.org/web/20001117004400/http://sulcus.berkeley.edu/FreemanWWW/manuscripts/IC4/84.html</a>.
</div>
<div id="ref-fukushimaVisualFeatureExtraction1969" class="csl-entry" role="listitem">
Fukushima, Kunihiko. 1969. <span>“Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements.”</span> <em>IEEE Transactions on Systems Science and Cybernetics</em> 5 (4): 322–33. <a href="https://ieeexplore.ieee.org/abstract/document/4082265/">https://ieeexplore.ieee.org/abstract/document/4082265/</a>.
</div>
<div id="ref-hardyMathematicianApology1940" class="csl-entry" role="listitem">
Hardy, Godfrey Harold. 1940. <em>A Mathematician’s Apology</em>. Cambridge University Press.
</div>
<div id="ref-hebbOrganizationBehaviorNeuropsychological2002" class="csl-entry" role="listitem">
Hebb, Donald O. 2002. <em>The <span>Organization</span> of <span>Behavior</span> : <span>A Neuropsychological Theory</span></em>. Psychology Press. <a href="https://doi.org/10.4324/9781410612403">https://doi.org/10.4324/9781410612403</a>.
</div>
<div id="ref-hopfieldNeuralNetworksPhysical1982" class="csl-entry" role="listitem">
Hopfield, J J. 1982. <span>“Neural Networks and Physical Systems with Emergent Collective Computational Abilities.”</span> <em>Proceedings of the National Academy of Sciences</em> 79 (8): 2554–58. <a href="https://doi.org/10.1073/pnas.79.8.2554">https://doi.org/10.1073/pnas.79.8.2554</a>.
</div>
<div id="ref-huangDenselyConnectedConvolutional2018" class="csl-entry" role="listitem">
Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2018. <span>“Densely <span>Connected Convolutional Networks</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1608.06993">https://doi.org/10.48550/arXiv.1608.06993</a>.
</div>
<div id="ref-kaplanScalingLawsNeural2020" class="csl-entry" role="listitem">
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. <span>“Scaling <span>Laws</span> for <span>Neural Language Models</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2001.08361">http://arxiv.org/abs/2001.08361</a>.
</div>
<div id="ref-langLearningTellTwo1988" class="csl-entry" role="listitem">
Lang, Kevin J., and M. J. Witbrock. 1988. <span>“Learning to Tell Two Spirals Apart.”</span> In <em>Proceedings of the 1988 Connectionist Models Summer School</em>, 52–59. Morgan Kaufmann. <a href="https://cir.nii.ac.jp/crid/1574231874429435776">https://cir.nii.ac.jp/crid/1574231874429435776</a>.
</div>
<div id="ref-larriva-sahdPredictionsRafaelLorente2014" class="csl-entry" role="listitem">
Larriva-Sahd, Jorge A. 2014. <span>“Some Predictions of <span>Rafael Lorente</span> de <span>N<span>ó</span></span> 80 Years Later.”</span> <em>Frontiers in Neuroanatomy</em> 8 (December). <a href="https://doi.org/10.3389/fnana.2014.00147">https://doi.org/10.3389/fnana.2014.00147</a>.
</div>
<div id="ref-levyGoogleEmployeesInvented2024" class="csl-entry" role="listitem">
Levy, Steven. 2024. <span>“8 <span>Google Employees Invented Modern AI</span>. <span>Here</span>’s the <span>Inside Story</span>.”</span> <em>Wired</em>, March. <a href="https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/">https://www.wired.com/story/eight-google-employees-invented-modern-ai-transformers-paper/</a>.
</div>
<div id="ref-linWhyDoesDeep2017" class="csl-entry" role="listitem">
Lin, Henry W., Max Tegmark, and David Rolnick. 2017. <span>“Why Does Deep and Cheap Learning Work so Well?”</span> <em>Journal of Statistical Physics</em> 168 (6): 1223–47. <a href="https://doi.org/10/gbvgmm">https://doi.org/10/gbvgmm</a>.
</div>
<div id="ref-linnMicrosoftResearchersWin2015" class="csl-entry" role="listitem">
Linn, Allison. 2015. <span>“Microsoft Researchers Win <span>ImageNet</span> Computer Vision Challenge.”</span> <em>The AI Blog</em>. <a href="https://blogs.microsoft.com/ai/microsoft-researchers-win-imagenet-computer-vision-challenge/">https://blogs.microsoft.com/ai/microsoft-researchers-win-imagenet-computer-vision-challenge/</a>.
</div>
<div id="ref-mccullochRecollectionsManySources1974" class="csl-entry" role="listitem">
McCulloch, Warren S. 1974. <span>“Recollections of the Many Sources of Cybernetics.”</span> In <em><span>ASC Forum</span></em>, 6:5–16.
</div>
<div id="ref-mccullochLogicalCalculusIdeas1943" class="csl-entry" role="listitem">
McCulloch, Warren S., and Walter Pitts. 1943. <span>“A Logical Calculus of the Ideas Immanent in Nervous Activity.”</span> <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115–33. <a href="https://doi.org/10/djsbj6">https://doi.org/10/djsbj6</a>.
</div>
<div id="ref-minskyNeuralanalogueCalculatorBased1952" class="csl-entry" role="listitem">
Minsky, Marvin. 1952. <span>“A Neural-Analogue Calculator Based Upon a Probability Model of Reinforcement.”</span> <em>Harvard University Psychological Laboratories, Cambridge, Massachusetts</em>.
</div>
<div id="ref-minskyTheoryNeuralanalogReinforcement1954" class="csl-entry" role="listitem">
———. 1954. <span>“Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model Problem.”</span> PhD thesis, Princeton University. <a href="https://search.proquest.com/openview/a4ee1c0c78c1b940b9ec121f0e89cef8/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y">https://search.proquest.com/openview/a4ee1c0c78c1b940b9ec121f0e89cef8/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a>.
</div>
<div id="ref-minskyPerceptronsIntroductionComputational1988" class="csl-entry" role="listitem">
Minsky, Marvin, and Seymour Papert. 1988. <em>Perceptrons: An Introduction to Computational Geometry</em>. Expanded ed. Cambridge, Mass: MIT Press.
</div>
<div id="ref-murrayStrategyDefeatLuftwaffe1986" class="csl-entry" role="listitem">
Murray, Williamson. 1986. <em>Strategy for <span>Defeat</span>: <span>The Luftwaffe</span> 1933-1945</em>. 2nd edition. Book Sales.
</div>
<div id="ref-nagyNeuralNetworksthenNow1991" class="csl-entry" role="listitem">
Nagy, George. 1991. <span>“Neural Networks-Then and Now.”</span> <em>IEEE Transactions on Neural Networks</em> 2 (2): 316–18. <a href="https://doi.org/10.1109/72.80343">https://doi.org/10.1109/72.80343</a>.
</div>
<div id="ref-nakanoLearningProcessModel1971" class="csl-entry" role="listitem">
Nakano, Kaoru. 1971. <span>“Learning <span>Process</span> in a <span>Model</span> of <span>Associative Memory</span>.”</span> In <em>Pattern <span>Recognition</span> and <span>Machine Learning</span>: <span>Proceedings</span> of the <span>Japan</span>—<span>U</span>.<span>S</span>. <span>Seminar</span> on the <span>Learning Process</span> in <span>Control Systems</span>, Held in <span>Nagoya</span>, <span>Japan August</span> 18–20, 1970</em>, edited by K. S. Fu, 172–86. Boston, MA: Springer US. <a href="https://doi.org/10.1007/978-1-4615-7566-5_15">https://doi.org/10.1007/978-1-4615-7566-5_15</a>.
</div>
<div id="ref-nilssonQuestArtificialIntelligence2009" class="csl-entry" role="listitem">
Nilsson, Nils J. 2009. <em>The <span>Quest</span> for <span>Artificial Intelligence</span></em>. 1st edition. Cambridge ; New York: Cambridge University Press.
</div>
<div id="ref-norbertwienerExtrapolationInterpolationSmoothing1966" class="csl-entry" role="listitem">
Norbert Wiener. 1966. <em>Extrapolation, <span>Interpolation</span>, and <span>Smoothing</span> of <span>Stationary Time Series</span></em>. <a href="http://archive.org/details/extrapolationint0000norb">http://archive.org/details/extrapolationint0000norb</a>.
</div>
<div id="ref-norvigChomskyTwoCultures2017" class="csl-entry" role="listitem">
Norvig, Peter. 2017. <span>“<span>On Chomsky and the Two Cultures of Statistical Learning</span>.”</span> In <em><span>Berechenbarkeit der Welt? Philosophie und Wissenschaft im Zeitalter von Big Data</span></em>, edited by Wolfgang Pietsch, Jörg Wernecke, and Maximilian Ott, 61–83. Wiesbaden: Springer Fachmedien. <a href="https://doi.org/10.1007/978-3-658-12153-2_3">https://doi.org/10.1007/978-3-658-12153-2_3</a>.
</div>
<div id="ref-olazaranHistoricalSociologyNeural1991" class="csl-entry" role="listitem">
Olazaran, Mikel. 1991. <span>“A Historical Sociology of Neural Network Research.”</span> PhD thesis, The University of Edinburgh. <a href="https://era.ed.ac.uk/handle/1842/20075">https://era.ed.ac.uk/handle/1842/20075</a>.
</div>
<div id="ref-ramonycajalHistologieSystemeNerveux1909" class="csl-entry" role="listitem">
Ramón y Cajal, Santiago. 1909. <em><span>Histologie du syst<span>è</span>me nerveux de l’homme et des vert<span>é</span>br<span>é</span>s</span></em>. Vol. II. Paris : A. Maloine. <a href="http://archive.org/details/b2129592x_0002">http://archive.org/details/b2129592x_0002</a>.
</div>
<div id="ref-rosenResearchDevelopmentProgram1965" class="csl-entry" role="listitem">
Rosen, Charles A., Nils J. Nilsson, and Milton B. Adams. 1965. <span>“A Research and Development Program in Applications of Intelligent Automata to Reconnaissance-Phase <span>I</span>.”</span> Proposal for {{Research}} ESU 65-1. Stanford Research Institute.
</div>
<div id="ref-rosenblattPerceptronProbabilisticModel1958" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1958. <span>“The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div>
<div id="ref-rosenblattPerceptualGeneralizationTransformation1960" class="csl-entry" role="listitem">
———. 1960. <span>“Perceptual Generalization over Transformation Groups.”</span> In <em>Self-Organizing <span>Systems</span>: <span>Proceedings</span> of an <span class="nocase">Inter-disciplinary Conference</span>, 5 and 6 <span>May</span>, 1959</em>, 63–96. Museum of Science; Industry, Chicago, Illinois: Pergamon Press. <a href="https://archive.org/details/SelfOrganizingSystems/page/n87/mode/1up">https://archive.org/details/SelfOrganizingSystems/page/n87/mode/1up</a>.
</div>
<div id="ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" class="csl-entry" role="listitem">
———. 1962. <em>Principles of Neurodynamics: <span>Perceptrons</span> and the Theory of Brain Mechanisms</em>. Vol. 55. Spartan books Washington, DC. <a href="https://apps.dtic.mil/sti/citations/AD0256582">https://apps.dtic.mil/sti/citations/AD0256582</a>.
</div>
<div id="ref-rosenfeldTalkingNetsOral2000" class="csl-entry" role="listitem">
Rosenfeld, Edward, and James A. Anderson, eds. 2000. <em>Talking <span>Nets</span>: <span>An Oral History</span> of <span>Neural Networks</span></em>. Reprint edition. The MIT Press.
</div>
<div id="ref-sejnowskiDeepLearningRevolution2018" class="csl-entry" role="listitem">
Sejnowski, Terrence J. 2018. <em>The <span>Deep Learning Revolution</span></em>. Illustrated edition. Cambridge, Massachusetts London, England: The MIT Press.
</div>
<div id="ref-suttonBitterLesson2019" class="csl-entry" role="listitem">
Sutton, Richard. 2019. <span>“The <span>Bitter Lesson</span>.”</span> <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>.
</div>
<div id="ref-tankCollectiveComputationNeuronlike1987" class="csl-entry" role="listitem">
Tank, David W., and John J. Hopfield. 1987. <span>“Collective <span>Computation</span> in <span>Neuronlike Circuits</span>.”</span> <em>Scientific American</em> 257 (6): 104–15. <a href="https://www.jstor.org/stable/24979583">https://www.jstor.org/stable/24979583</a>.
</div>
<div id="ref-werbosNeuralNetworksPath2011" class="csl-entry" role="listitem">
Werbos, Paul. 2011. <span>“Neural Networks as a Path to Self-Awareness.”</span> In <em>The 2011 <span>International Joint Conference</span> on <span>Neural Networks</span></em>, 3264–71. IEEE. <a href="https://ieeexplore.ieee.org/abstract/document/6033654/">https://ieeexplore.ieee.org/abstract/document/6033654/</a>.
</div>
<div id="ref-widrow30YearsAdaptive1990" class="csl-entry" role="listitem">
Widrow, Bernard, and Michael A. Lehr. 1990. <span>“30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation.”</span> <em>Proceedings of the IEEE</em> 78 (9): 1415–42. <a href="https://ieeexplore.ieee.org/abstract/document/58323/">https://ieeexplore.ieee.org/abstract/document/58323/</a>.
</div>
<div id="ref-wienerNorbertWienerLife2017" class="csl-entry" role="listitem">
Wiener, Norbert. 2017. <em>Norbert <span class="nocase">Wiener–a</span> Life in Cybernetics</em>. Cambridge, Massachusetts: The MIT Press.
</div>
<div id="ref-winogradComputerSoftwareWorking1984" class="csl-entry" role="listitem">
Winograd, Terry. 1984. <span>“Computer <span>Software</span> for <span>Working</span> with <span>Language</span>.”</span> <em>Scientific American</em> 251 (3): 130–45. <a href="https://www.jstor.org/stable/24920349">https://www.jstor.org/stable/24920349</a>.
</div>
<div id="ref-yeangFilteringNoiseAntiaircraft2023" class="csl-entry" role="listitem">
Yeang, Chen-Pang. 2023. <span>“Filtering <span>Noise</span> for <span>Antiaircraft Gunfire Control</span>.”</span> In <em>Transforming <span>Noise</span>: <span>A History</span> of <span>Its Science</span> and <span>Technology</span> from <span>Disturbing Sounds</span> to <span>Informational Errors</span>, 1900-1955</em>, edited by Chen-Pang Yeang, 0. Oxford University Press. <a href="https://doi.org/10.1093/oso/9780198887768.003.0011">https://doi.org/10.1093/oso/9780198887768.003.0011</a>.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Notes for a talk on the history of neural networks"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-08"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [history, AI, cybernetics]</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    resources:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - "figure/**"</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - "code/**"</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Notes for a talk on the history of neural networks."</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "log"</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 4</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## What this essay is about</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>This essay is written as a companion piece of a lecture on the history of neural networks. It is not exactly an essay, but more of an extended scrapbook of quotations. The lecture slides are <span class="co">[</span><span class="ot">here</span><span class="co">](code/ML%20history%20presentation.zip)</span>.</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Yuxi on the Wired - Cybernetic Artificial Intelligence</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/cybernetic-artificial-intelligence/)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Yuxi on the Wired - Reading Perceptrons</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/)</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Yuxi on the Wired - The Backstory of Backpropagation</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/)</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Yuxi on the Wired - The Perceptron Controversy</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/)</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Prehistory</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The thunderbolt steers all things. </span><span class="sc">\[</span><span class="at">τὰ δὲ πάντα οἰακίζει κεραυνός</span><span class="sc">\]</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Heraclitus</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="fu">### Santiago Ramón y Cajal</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>Cajal's neural network diagrams are always feedforward. Cajal never depicted feedback except once, in a diagram of a connection in the cerebellum. In fact, according to de No, who was a student of Cajal, Cajal was vehemently *against* feedbacks. More on this later.</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@ramonycajalHistologieSystemeNerveux1909, page 149, figure 103</span><span class="co">]</span>](figure/Cajal_RNN_original.png)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="al">![Colored version highlighting the recurrent loop.](figure/Cajal_RNN.png)</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>The original caption:</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Voie courte des cellules de Golgi. </span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Fig. 103. — Schéma destiné à montrer la marche du courant apporté par les fibres moussues et la part que prennent à ce courant les cellules de Golgi.</span>  </span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A, fibres moussues ; — B, cylindre-axe de Purkinje ; — n, grains ; — b, fibres parallèles; — c, cellule de Golgi ; — d, cellule de Purkinje vue de champ.</span>  </span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>Translation:</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Short pathway of the Golgi cells.</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Fig. 103. — Diagram intended to show the course of the current brought by the mossy fibers and the part played by the Golgi cells in this current.</span>  </span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A, mossy fibers; — B, Purkinje axis-cylinder; — n, grains; — b, parallel fibers; — c, Golgi cell; — d, Purkinje cell seen from the field.</span>  </span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### Rafael Lorente de Nó</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">vestibulo-ocular reflex (VOR)</span><span class="co">](https://en.wikipedia.org/wiki/Vestibulo%E2%80%93ocular_reflex)</span> is a reflex that acts to stabilize gaze during head movement.</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>Lorente de No discovered recurrent circuits in the brain, mainly in the VOR, but also in the cortex <span class="co">[</span><span class="ot">@larriva-sahdPredictionsRafaelLorente2014</span><span class="co">]</span>.</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A recurrent neural network that explains nystagmus. [@denoAnalysisActivityChains1938, figure 13] Original caption: Diagram explaining the production of rhythm during vestibular nystagmus. Fiber f is sup- posed to carry the continuous series of impulses started at the cristae of the semicircular canals which set up the nystagmus. Fibers ft are supposed to be maintaining the tonus of the antagonistic muscle; la, Ib, Za, Zb, 3a, 3b are branches of the axons of cells 1, 2 and 3,4a, 4b, 4d,, 4e are branches of the axon of cell 4; fa, fb, fc are branches of fiber f. II. Diagram of the rhythmic succession of con- tractions and relaxations of the antagonistic muscles during the nystagmus explained by diagram I. Rising of the line indica tes contraction . The interval between turning points a and b is never less than 3 to 4 msec.; -- the interval between 50 msec. long.</span><span class="co">](figure/Lorente%20de%20No_RNN_VOR.png)</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Simplified diagram of the interneurons in the vestibulo-ocular reflex, showing both residual feedforward connections and recurrent connections. [@denoAnalysisActivityChains1938, figure 2] Original caption: Diagram of the pathways connecting the internuncial cells among themselves and with the ocular motoneurons. V, vestibular nerve; 1 to 6, cells in the primary vestibular nuclei; 7, 8, 9, cells in the reticular formation in the medullar (Med.) and pons (P.); 10, 11, 12, cells in the reticular nuclei in the midbrain (M.b.); Oc.n., oculomotor nuclei; f.l.p., fasciculus longitudinalis posterior and similar pathways; i, internuncial pathways; Fl, F2 and Col., position of the stimulating electrodes. The diagrams below indicate the two types of chains formed by internuncial cells; IV, multiple and C, closed chain.</span><span class="co">](figure/Lorente%20de%20No_ResNet_1.png)</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Oculomotor of the rabbit. [@denoAnalysisActivityChains1938, figure 3] Original caption: i1, i2, i3, i4, intemuncial paths. Passage of a synapse means a delay of about 0.6 msec. Note that each fiber has several synaptic knobs on the neuron, an arrangement increasing the possibility of spatial summation.</span><span class="co">](figure/Lorente%20de%20No_ResNet_2.png)</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>Lorente de No actually discovered those during the mid-1920s, but Cajal told him that he shouldn't publish those, because other neuroscientists would think it is crazy, damaging de No's career. So he published in 1934, immediately after Cajal died.</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It is easier to sweep this complexity under the rug, which I do now by resorting to anecdote. Three years ago in a workshop at Irvine I presented some data on the properties of mitral cells in the olfactory bulb, from which I inferred that they formed a mutually excitatory neural population (i.e., one having positive feedback). I cited this his as confirmation of Ramón y Cajal's (1909) hypothesis of "avalanche conduction." in which a weak olfactory stimulus might undergo "amplification" (as we would say now). Rafael Lorente de Nó. in the audience. stated that I was in error, Cajal had in mind feed-forward recruitment of mitral cells and disavowed the notion of feedback. Lorente later recalled (personal communication) that in the mid-1920s he prepared a manuscript on the cytoarchitecture of the cerebral cortex, In which he concluded that feedback relations were a prominent feature. After reading the manuscript. Cajal strongly urged Lorente not to publish it because it would be unacceptable to the scientific community and might blight his career. Out of respect for his mentor. Lorente elected not to publish the material while Cajal lived, when he did publish (Lorente de Nó. 1934), the work established itself as one of the enduring classics in neuroanatomy.   </span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@freemanPremisesNeurophysiologicalStudies1984</span><span class="co">]</span>  </span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>Lorente de No was at the founding of cybernetics, and went to the <span class="co">[</span><span class="ot">Macy conferences</span><span class="co">](https://en.wikipedia.org/wiki/Macy_conferences)</span> a lot. He consistently pointed out that recurrent networks exist in the brain and is possibly responsible for transient memory. He had influenced many of the early cyberneticians, including Hebb, McCulloch, and Pitts. <span class="co">[</span><span class="ot">@espinosa-sanchezImportanceCajalLorente2023</span><span class="co">]</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a><span class="fu">### Donald Hebb</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>Hebbian learning of synapses, also "reverberation". In short, if A fired shortly before B fired, then all A-to-B synapses would increase in strength. It is not just "neurons that fire together wire together", since the neuron before the synapse must fire just before the neuron after the synapse. In modern-day language, it is <span class="co">[</span><span class="ot">spike-timing-dependent plasticity</span><span class="co">](https://en.wikipedia.org/wiki/Spike-timing-dependent_plasticity)</span>.</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>In Hebb's theory, the brain is a large neuron network, with long parallel fibers (like long-range undersea cables) connecting small clusters of neurons ("cell assemblies"). The long parallel fibers are hardwired, but the small clusters are formed by Hebbian learning. The cell assemblies are recurrent, allowing long-time reverberations, allowing things like after-image, imagination, sensory integration, and other things that require the brain to assemble some information together and "keep it in mind" for a while, despite the lack of external stimulus. He also wanted to use this theory for explaining pathologies like hallucination, phantom pain, etc.</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Let us assume that the persistence or repetition of a reverberatory activity (or "trace") tends to induce lasting cellular changes that add to its stability. ... When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.</span>  </span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@hebbOrganizationBehaviorNeuropsychological2002, page 62</span><span class="co">]</span>  </span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; an indefinite reverberation in the structure might be possible, so long as the background activity in other cells in the same gross region remained the same. It would not of course remain the same for long, especially with changes of visual fixa tion; but such considerations make it possible to conceive of "alternating" reverberation which might frequently last for periods of time as great as half a second or a second.</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@hebbOrganizationBehaviorNeuropsychological2002, page 73--74</span><span class="co">]</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The Hebb cell assembly. [@hebbOrganizationBehaviorNeuropsychological2002, page 73, figure 10]</span><span class="co">](figure/Hebb_RNN.png)</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a><span class="al">![Animated version. The "reverberation" is clear in this case.](figure/Hebb_RNN.gif)</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>In proposing the reverberating circuitry, Hebb was influenced by de No <span class="co">[</span><span class="ot">@espinosa-sanchezImportanceCajalLorente2023</span><span class="co">]</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="sc">\[</span><span class="at">The cell assembly theory</span><span class="sc">\]</span><span class="at"> certainly looked improbable to its author--me--when it was first conceived </span><span class="sc">\[</span><span class="at">because it makes the ease of perception of common objects the result of a long process of learning</span><span class="sc">\]</span><span class="at">. The problem of perception remained intractable for about five years (1939 to 1944) and as a result I made no progress in my attempt to understand concepts and thought. It seemed obvious that concepts, like images, must derive from perception, and I could think of no mechanism of perception that corresponded to my preconceptions. In fact, by 1944 I had given up trying to solve the problem. What happened then was that I became aware of some recent work of Lorente de No in conjunction with some observations of Hilgard and Marquis (1940) which led me to think about the problem from a different point of view... The essential basis of an alternative view was provided by Lorente de No, who showed that the cortex is throughout its extent largely composed of enormously complex closed or re-entrant paths, rather than linear connections only between more distant points... When an excitation reaches the cortex, instead of having to be transmitted at once to a motor path, or else die out, it may travel round and round in these closed paths and may continue to do so after the original sensory stimulation has ceased.</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Hebb DO. 1980. Essay on mind. Hillsdale, NJ: Lawrence Erlbaum</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cybernetics</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a><span class="fu">### WWII</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; War is father of all and king of all; and some he manifested as gods, some as men; some he made slaves, some free. </span><span class="sc">\[</span><span class="at">Πόλεμος πάντων μὲν πατήρ ἐστι πάντων δὲ βασιλεύς, καὶ τοὺς μὲν θεοὺς ἔδειξε τοὺς δὲ ἀνθρώπους, τοὺς μὲν δούλους ἐποίησε τοὺς δὲ ἐλευθέρους.</span><span class="sc">\]</span></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Heraclitus</span>  </span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>I have gone a bit overboard with the WWII pictures, but I think war has a high concentration of functionalist beauty. Anything that is optimized to the hilt, like a TSMC fab, or a modern ICBM, is intrinsically beautiful. See <span class="co">[</span><span class="ot">Why Do Hipsters Steal Stuff? · Gwern.net</span><span class="co">](https://gwern.net/larping)</span></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/war_is_the_father_of_all.png)</span></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>For a good tutorial on the problem of land-based anti-aircraft fire control during WWII, see the <span class="co">[</span><span class="ot">1944 United States Army Air Forces film #TF 1-3389, "Flak"</span><span class="co">](https://archive.org/details/TF1-3389Flak)</span>. For another one from the perspective of naval fire control, see the <span class="co">[</span><span class="ot">1953 U.S. Navy training film (MN-6783a) Basic Mechanisms In Fire Control Computers</span><span class="co">](https://www.youtube.com/watch?v=s1i-dnAH9Y4)</span>.</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Damage Assessment Report No. 20. Mosaic map showing target area and extent of [Tokyo bombing](https://en.wikipedia.org/wiki/Bombing_of_Tokyo) caused by the 1945-03-10 raid – Source: U.S. National Archives, Record Group 243, Series 59, Box 6.</span><span class="co">](figure/Tokyo%20bombing%20map_damage%20assessment%20report%20no%2020.jpg)</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">*Cybernetics in Military Affairs*. USSR, Moskva, "[DOSAAF](https://en.wikipedia.org/wiki/DOSAAF)" (M. N. Goncharenko, 1963). The image shows a [Tupolev Tu-123 reconnaissance drone](https://en.wikipedia.org/wiki/Tupolev_Tu-123). Книга М. Н. Гончаренко "Кибернетика в военном деле". СССР, Москва, "ДОСААФ" 1963 год. The original caption says "Puc. 39. Беспилотный разведывательный реактивный самолет AN/VSD-5 многократного использования." \[Fig. 39. AN/VSD-5 reusable unmanned reconnaissance jet aircraft.\]</span><span class="co">](figure/Goncharenko_1963_Tu-123.jpg)</span></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Japanese Mitsubishi A6M Zero drop white phosphorous air-burst bombs on B-24 Bombers over Iwo Jima, 1945-02. Apparently those were used during the Battle of Iwo Jima as a crude anti-bomber attack. The information is scarce, but there is a video [*Pacific Jap Phosphorous Bombs*](https://www.youtube.com/watch?v=lezeyKuWuPQ) demonstrating it.</span><span class="co">](figure/white%20phosphorus%20bomb_Iwo%20Jima%201945.jpg)</span></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Map of the [Kammhuber line](https://en.wikipedia.org/wiki/Kammhuber_Line), a defensive front against British bombers. It consisted of a series of control sectors equipped with radars and searchlights and an associated night fighter. Each sector would direct the night fighter into visual range to target intruding bombers.</span><span class="co">](figure/Kammhuber%20Line%20map.jpg)</span></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a><span class="al">![A map of the Kammhuber line stolen by a spy in 1942.](figure/Kammhuber_Line_Map_-_Agent_Tegal.png)</span></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Scatterplot of photos taken from bombers during [Operation Gomorrah](https://en.wikipedia.org/wiki/Operation_Gomorra) bombing of Hamburg, 1943-07. It was standard practice to take photos from bombers to validate the bombing accuracy, benchmark the loss function, measure the ablation of the target, and plan the next attack.</span><span class="co">](figure/Operation%20Gomorrah_final%20plot.webp)</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Operation Gomorrah with [Avro Lancaster bomber](https://en.wikipedia.org/wiki/Avro_Lancaster), 30/31 January 1943-01-30. The bright sine-like curves are flares, while the smoke and explosions provide diffuse lighting. Source: [C 3371 of the Imperial War Museums.](https://en.wikipedia.org/wiki/File:Attack_on_Hamburg.jpg)</span><span class="co">](figure/Operation%20Gomorrah_sine%20curves.jpeg)</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Multiple explosions/fires in the Sudstadt district of Hannover and a Lancaster silhouetted well below bottom left. Source: [Hannover · IBCC Digital Archive](https://ibccdigitalarchive.lincoln.ac.uk/omeka/collections/document/47703)</span><span class="co">](figure/Hannover_linear%20scatterplot.jpg)</span></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A night [bombing raid on Bremen](https://en.wikipedia.org/wiki/Bombing_of_Bremen_in_World_War_II). A British bomber has been caught in the searchlight cone and heavy anti-aircraft fire is converging on the aircraft. Source: [Australian War Memorial](https://www.awm.gov.au/collection/044856)</span><span class="co">](figure/Bombing%20Bremen_searchlights.webp)</span></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Planning map for the [bombing of Kassel](https://en.wikipedia.org/wiki/Bombing_of_Kassel_in_World_War_II), 1943-10-22--23. [@murrayStrategyDefeatLuftwaffe1986, page 211]</span><span class="co">](figure/Kassel%201943-10-22--23%20Outward%20route.jpg)</span></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">Bombardment of Moscow</span><span class="co">](https://en.wikipedia.org/wiki/Battle_of_Moscow)</span>, 1941-07-26. Photo taken by Margaret Bourke-White.](figure/Moscow%20bombing_1941.jpg)</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a><span class="al">![The same photo of the Bombardment of Moscow, with a scatterplot overlaid to bring out the similarity.](figure/war_curve_Moscow_2.png)</span></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a><span class="al">![The scatterplot itself. The Python code is as follows.](figure/war_curve_Moscow_1.svg)</span></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a seed for reproducibility</span></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the points</span></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>num_points <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>x_points <span class="op">=</span> np.random.normal(<span class="fl">0.5</span>, <span class="fl">0.5</span>, num_points)</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>y_points <span class="op">=</span> np.exp(x_points) <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>, num_points)</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the curves</span></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>num_curves <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>x_curves <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a>r_values <span class="op">=</span> np.random.normal(<span class="dv">1</span>, <span class="fl">0.1</span>, num_curves)</span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, num_curves)</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the points</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>plt.scatter(x_points, y_points, color<span class="op">=</span><span class="st">'white'</span>, s<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the curves</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_curves):</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>plt.plot(x_curves, np.exp(r_values[i] <span class="op">*</span> x_curves) <span class="op">+</span> k_values[i], color<span class="op">=</span><span class="st">'white'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a><span class="co"># Set plot limits and background color</span></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">4</span>)</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> np.zeros((<span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">3</span>), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">256</span>):</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>gradient[:, i, :] <span class="op">=</span> (i,i,i)</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> (<span class="dv">256</span> <span class="op">-</span> gradient) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>plt.imshow(gradient, extent<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">4</span>], aspect<span class="op">=</span><span class="st">'auto'</span>, origin<span class="op">=</span><span class="st">'lower'</span>)</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'war_curve.svg'</span>)</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a><span class="al">![A German instructional poster for training the Luftwaffe. The shapes indicate areas of fire from the 50mm gunner positions aboard the B-17. Also noted are susceptible locations, specifically the fuel and oil tanks on the Flying Fortress. The text is murky but what I can make out are "Darstellung der von den Bordwaffen bestrichenen Räume", "Darstellung des Uberschneidens der Feuerabschnitte", "Viermotoriges Kampfflugzeug Boeing B-17 Flying Fortress". \["Representation of the areas covered by the on-board weapons", "Representation of the overlap of the fire zones", "Four-engine fighter aircraft Boeing B-17 Flying Fortress"\].](figure/B-17.png)</span></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Essentially the same diagram, but for B-24 "Liberator". [Source: Journal of the Second Air Division Association, Newsletter Vol. 18, No.2, 1979-06](https://www.b24.net/2ndADA-Newsletters/1979-Jun.pdf)</span><span class="co">](figure/B-24.png)</span></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Illustration in a training manual for American bomber gunners. This shows how to lead the target by a crude form of linear regression that every gunner performs. Source: [Gunner's Information File - Flexible Gunnery. Air Force Manual No. 20, May 1944](https://digitalcollections.museumofflight.org/items/show/50156)</span><span class="co">](figure/Gunner's%20Information%20File_1944-05_S-18.png)</span></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>Bonus: scaling plot?</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Top image: Learning curve of the production of B-29 airframes at the Boeing Wichita division during WWII. Page 75 from [Source Book of World War II Basic Data - Airframe Industry. Volume 1. Direct Man-Hours - Progress Curves](https://purl.stanford.edu/dg041ny3484). \n Bottom image: Scaling curves for LLM. [@kaplanScalingLawsNeural2020]</span><span class="co">](figure/scaling_law_war_and_peace.jpg)</span></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a><span class="fu">### The problem of fire control</span></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>Wiener treated fire control as a problem of linear regression.</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The linear geometry of missile flight control. Page 21 of [*Standard Fire Control Symbols for Missile Related Quantities*, OP 1700 Volume 3, (1957)](https://maritime.org/doc/symbols/index.php)</span><span class="co">](figure/missile%20control%20linear%20geometry_1957.png)</span></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The curved geometry of missile flight control. Page 53 of [*Standard Fire Control Symbols for Missile Related Quantities*, OP 1700 Volume 3, (1957)](https://maritime.org/doc/symbols/index.php)</span><span class="co">](figure/missile%20control%20curved%20geometry_1957.png)</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The vestibulo-ocular reflex of the fire control. Section 15 of [Fire Control Fundamentals, NAVPERS 91900 (1953), Part G](https://maritime.org/doc/firecontrol/partg.php). Original caption: To measure director elevation in a vertical plane, the optical equipment and radar antenna must be stabilized so that they elevate in a vertical plane. This is accomplished by continuously supplying the director with crosslevel as measured at the stable element. An automatic positioning device continuously rotates the telescopes, rangefinder and radar antenna in response to the crosslevel signal, to keep their axes horizontal. Thus target elevation is always measured in a vertical plane.</span><span class="co">](figure/fire%20control%20VOR.png)</span></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>I think it is no coincidence that negative feedback circuit was first discovered by Lorente de No in the VOR, and constructed during WWII in the artificial-VOR. In both cases, the eyes needs to be stabilized cheaply and fast.</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>Norbert Wiener decided he had to contribute to the war, and since his background was in stochastic process and statistics, he decided to work on aircraft predictor for anti-aircraft fire control. The following quote is from a classified memorandum in 1942. It was connected with sensitive wartime efforts to improve radar communication and became a founding document for cybernetics and information theory.</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This book represents an attempt to unite the theory and practice of two fields of work which are of vital importance in the present emergency, and which have a complete natural methodological unity, but which have up to the present drawn their inspiration from two entirely distinct traditions, and which are widely different in their vocabulary and the training of their personnel. These two fields are those of time series in statistics and of communication engineering.</span>  </span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@norbertwienerExtrapolationInterpolationSmoothing1966, page 1</span><span class="co">]</span>  </span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>This quote from its introduction shows how Wiener thought of statistical communication theory. Prediction of aircraft trajectory is understood as communication: The past trajectory of the aircraft is a noisy channel that is trying to communicate (despite the best efforts of the pilot) the future trajectory of the aircraft.</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>Wiener thought of antiaircraft fire as a problem of feedback control. The loss function is the squared distance between the aircraft and the bullet. The aircraft is killed by minimizing the loss function.</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>The control mechanism is a least-squares predictor. <span class="co">[</span><span class="ot">@yeangFilteringNoiseAntiaircraft2023</span><span class="co">]</span></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; in order to obtain as complete a mathematical treatment as possible of the over-all control problem, it is necessary to assimilate the different parts of the system to a single basis, either human or mechanical. Since our understanding of the mechanical elements of gun pointing appeared to us to be far ahead of our psychological understanding, we chose to try to find a mechanical analogue of the gun pointer and the airplane pilot.</span>  </span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@wienerNorbertWienerLife2017, page 407</span><span class="co">]</span>  </span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a><span class="fu">### Commentary</span></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>Why did it take 50 years for feedback to appear? I don't know, but I have some theories.</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Feedback networks are much harder to solve mathematically.</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Feedback threatens the idea of cause-then-effect.</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Reflexes are simpler to study, so neuroscientists thought that the brain is made of nothing but reflexes.</span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Behaviorism.</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>Also, I think this shows that there *is* progress in theoretical neuroscience.</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a>Freud in the 19th century analogized the brain as a steam engine (thus there is "psychic pressure" that builds up and must be "vented"). In the early 20th century, the brain was often analogized to a telephone system. Later, it was analogized to an artificial neural network. Some commentators have pointed at this, and sneered that, scientists always mistakenly use whatever analogy is fashionable to study what is fundamentally out of their reach.</span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a>What this does not note is that telephone networks are feedforward, while neural networks can be feedback. A telephone network does not work if there is feedback (it would be ringing pretty loudly), but some artificial neural networks work precisely because there is feedback.</span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Simplified diagram of telephone switching. [Source](https://www.calling315.com/basics-of-telephone-exchanges).</span><span class="co">](figure/telephone%20switching.png)</span></span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Crossbar switching in a 1903 patent. [Source](https://en.wikipedia.org/wiki/File:Telephone_switchboard_cross-switching_(Rankin_Kennedy,_Electrical_Installations,_Vol_V,_1903).jpg).</span><span class="co">](figure/manual%20crossbar%20switch_1903.jpg)</span></span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Crossbar switching at a New York telephone exchange center in 1938-05. [Source](https://ethw.org/File:-1_Crossbar_NY_May_1938.jpg).</span><span class="co">](figure/Crossbar_NY_1938-05.jpg)</span></span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a><span class="fu">### McCulloch and Pitts (1943)</span></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>The paper <span class="co">[</span><span class="ot">@mccullochLogicalCalculusIdeas1943</span><span class="co">]</span> is the foundational paper for artificial neural networks. Often cited, rarely read. In modern language, the paper considered neural networks with integer weights, integer biases, 0-1 activation functions, and constant time-delays. The constant time-delay means that the neural network operates in clock-ticks, so that the state of a neuron at time $t$ is decided by the inputs to it at time $t-1$. This is necessary for two things:</span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Formalized Hebbian learning: if A has a potential synapse to B, and A fired at time $t$, then B fired at time $t+1$, then the potential synapse becomes a real synapse.</span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>To make recurrent neural networks work.</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a><span class="al">![A single MP neuron. It has integer weights and biases. A positive weight is symbolized by a flat end, and a negative weight is symbolized by a circular end. The bias (firing threshold) is written on the neuron itself.](figure/MP_neuron.png)</span></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>They proved the following theorems:</span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Any boolean function is implemented by a purely feedforward network ("nets without circles"). Thus, it can be used as the controller in any Turing machine.</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; first, that every net, if furnished with a tape, scanners connected to afferents, and suitable efferents to perform the necessary motor-operations, can compute only such numbers as can a Turing machine; second, that each of the latter numbers can be computed by such a net...</span>  </span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A recurrent network is equivalent to a Turing machine with a finite tape.</span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>A recurrent network can perform Hebbian learning, even if all its weights and biases remain unchanged.</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; at any time the neuron fires, and the axonal terminations are simultaneously excited, they become synapses... capable of exciting the neuron... THEOREM VII. Alterable synapses can be replaced by circles.</span>  </span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>The proof is as shown below:</span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a><span class="al">![The dashed synapse can become a real synapse by Hebbian learning.](figure/MP_recurrent_learning.png)</span>{width=40%}</span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>The paper had two sources: pathological states where the mind folds back onto itself (tinnitus, hallucination, etc), and healthy states where the mind is influenced by its past (memory).</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There is no theory we may hold and no observation we can make that will retain so much as its old defective reference to the facts if the net be altered. Tinnitus, paraesthesias, hallucinations, delusions, confusions and disorientations intervene. Thus empiry confirms that if our nets are undefined, our facts are undefined, and to the "real" we can attribute not so much as one quality or "form". With determination of the net, the unknowable object of knowledge, the "thing in itself," ceases to be unknowable. </span><span class="co">[</span><span class="ot">@mccullochLogicalCalculusIdeas1943</span><span class="co">]</span>  </span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In 1941 I presented my notions on the flow of information through ranks of neurons to Rashevsky's seminar in the Committee on Mathematical Biology of the University of Chicago and met Walter Pitts, who then was about seventeen years old. He was working on a mathematical theory of learning and I was much impressed.</span>  </span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; He was interested in problems of circularity, how to handle regenerative nervous activity in closed loops. I had had to suppose such loops to account for epileptic activity of surgically isolated brain and even of undercut cortex. Lorente de No had shown their significance in vestibular nystagmus. I wanted them to account for causalgia persisting after amputation of a painful limb and even after section of the spinothalamic tract; I wanted them to account for the early stages of memory and conditioning. I wanted them to account for compulsive behavior, for anxiety and for the effects of shock therapy. These appeared to be processes that once started seemed to run on in various ways. Since there obviously were negative feedbacks within the brain, why not regenerative ones? For two years Walter and I worked on these problems whose solution depended upon modular mathematics of which I knew nothing, but Walter did. We needed a rigorous terminology and Walter had it from Carnap, with whom he had been studying. We, I should say Walter Pitts, finally got it in proper form and we published in 1943, A Logical Calculus of the Ideas Immanent in Nervous Activity. H.D. Landahi immediately joined us in a note applying the logical calculus statistically. The crucial third part of our first article is rigorous but opaque and there is an error in subscript. In substance what it proved via its three theorems is that a net made of threshold devices, formal neurons, can compute those and only those numbers that a Turing machine can compute with a finite tape.</span>  </span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@mccullochRecollectionsManySources1974</span><span class="co">]</span>  </span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a><span class="fu">## First neural network period (1950--1970)</span></span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a><span class="fu">### SNARC</span></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a>There were some neural networks in the early 1950s before Rosenblatt. See <span class="co">[</span><span class="ot">@nilssonQuestArtificialIntelligence2009, chapter 4</span><span class="co">]</span> for a review. The most notable one is Minsky's <span class="co">[</span><span class="ot">SNARC</span><span class="co">](https://en.wikipedia.org/wiki/Stochastic_Neural_Analog_Reinforcement_Calculator)</span>. Details are in <span class="co">[</span><span class="ot">Yuxi on the Wired - A Scrapbook of Neural Network Lores</span><span class="co">](https://yuxi-liu-wired.github.io/sketches/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work)</span> and <span class="co">[</span><span class="ot">Yuxi on the Wired - The Perceptron Controversy</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky)</span>.</span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a>Apparently it was a neural network version of <span class="co">[</span><span class="ot">Shannon's robot rat "Theseus"</span><span class="co">](https://en.wikipedia.org/wiki/Claude_Shannon)</span>.</span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>SNARC had 40 neurons, but it was lost. The report <span class="co">[</span><span class="ot">@minskyNeuralanalogueCalculatorBased1952</span><span class="co">]</span> was lost too. I emailed the Harvard library for a copy, but they did not find it.</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="al">![Pictured: Minsky's last remaining neuron. I like to joke that Minsky has very few neurons (see his 1981 comment about unable to afford "thousands or millions of neurons"), and this picture is the only one remaining.](figure/SNARC_neuron.jpg)</span></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a>His PhD thesis <span class="co">[</span><span class="ot">@minskyTheoryNeuralanalogReinforcement1954</span><span class="co">]</span> contained lots and lots of neural networks doing all kinds of Turing complete things. No way it would require *Perceptrons* (1969) to reveal to the world that, surprisingly, single-layered perceptrons can't do XOR. </span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a><span class="fu">### Frank Rosenblatt</span></span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>Started research in 1957 under "Project PARA" ("Perceiving and Recognition Automata"). First published in a report <span class="co">[</span><span class="ot">@rosenblattPerceptronProbabilisticModel1958</span><span class="co">]</span>, later published in a whole book (1961).</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@nagyNeuralNetworksthenNow1991</span><span class="co">]</span> describes some more of Rosenblatt's works.</span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a>Generally, Rosenblatt investigated in two ways: computer simulation (on IBM machines usually) and special hardwares (Perceptron Mark I, Tobermory). After 1965, the computer simulations became much faster than special hardwares. The tragedy of ASIC neuromorphic computing, no doubt...</span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a>Interesting results in the book: (More in <span class="co">[</span><span class="ot">Yuxi on the Wired - The Backstory of Backpropagation</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/index.html#frank-rosenblatt)</span>)</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a>Continuous activation functions (section 10), what he called "transmission functions".</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">First sighting of the Hopfield network. [@rosenblattPerceptualGeneralizationTransformation1960, page 74]</span><span class="co">](figure/Rosenblatt_1960_Hopfield.png)</span></span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a>Recurrent network that can perform a kind of selective visual attention (chapter 21) and memorize output sequences (chapter 22)</span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Hopfield network, again. [@rosenblattPrinciplesNeurodynamicsPerceptrons1962, figure 47]</span><span class="co">](figure/Rosenblatt_1962_Hopfield.png)</span></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Recurrent network that can perform a kind of selective visual attention. [@rosenblattPrinciplesNeurodynamicsPerceptrons1962, figure 63]</span><span class="co">](figure/Rosenblatt_1962_recurrent%20selective%20attention.png)</span></span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a><span class="al">![The first multimodal neural network? Associative learning of image and audio inputs (Figure 58)](figure/Rosenblatt_1962_multimodal.png)</span></span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a>The most interesting example is the "back-propagating error-correction procedure" network with residual connections and weight decay, pictured in Figure 42.</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a><span class="al">![The 4-layered MLP in Rosenblatt. (Figure 42)](figure/Rosenblatt_1962_MLP.png)</span></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>Its structure is <span class="in">`(fixed linear)-(activation)-(residual linear)-activation-linear`</span>:</span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The first layer is fixed (randomly wired).</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The second layer has both a residual connection and a learned connection. The learned weights increase by Hebbian learning, and the weights decay exponentially. Weight decay is a practical necessity because Hebbian learning only increases weights, never decreasing them.</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The third layer is trained by the perceptron learning rule, as usual.</span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>His last great project was Tobermory (1961--1967), for speech recognition. 4 layers with 12,000 weights (magnetic cores). By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines. It occupied an entire room.</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Tobermory schematic. From [System and Circuit Designs for the Tobermory Perceptron: Preliminary Report on Phase I By GEORGE NAGY 1 September, 1963 Prepared Under Contract No. NONR 401 (40) and NSF GP-971](https://apps.dtic.mil/sti/trecms/pdf/AD0607459.pdf)</span><span class="co">](figure/Tobermory.png)</span></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stanford Research Institute</span></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; When I first interviewed for a position at SRI in 1961, I was warned by one researcher there against joining research on neural networks. Such research, he claimed, was “premature,” and my involvement in it could damage my reputation.</span>  </span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@nilssonQuestArtificialIntelligence2009, chapter 24.2</span><span class="co">]</span></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a>The weight matrix of the MINOS II pattern recognition system, consisting of a grid of magnetic cores. The magnetization of a core is proportional to its matrix weight. Matrix weights that are quite literally weighty. The weight matrices are swappable.</span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@rosenResearchDevelopmentProgram1965</span><span class="co">]</span>](figure/MINOS_II.png)</span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a><span class="fu">### ADALINE</span></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>ADALINE (adaptive linear) and MADALINE (many ADALINE).</span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a>The ADALINE is a single perceptron</span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a>\theta\left(\sum_i w_i x_i + b\right)</span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a>and its learning rule is gradient descent on squared error $(\sum_i w_i x_i + b - y)^2$.  </span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/ADALINE.png)</span></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>It used *manual input* by flipping 16 switches *by hand*. The first version ("knobby ADALINE") had weights implemented as rheostats, with knobs turned *by hand*!!! Later versions used memistors that do gradient descent automatically.</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a><span class="al">![Widrow doing brain surgery on ADALINE.](figure/knobby_ADALINE.jpg)</span></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a>They got up to 1000 weights in a 2-layered MADALINE, but the learning rule was really hacky, and it was a deadend. <span class="co">[</span><span class="ot">@widrow30YearsAdaptive1990</span><span class="co">]</span> A video of the MADALINE in action is found in <span class="co">[</span><span class="ot">The LMS algorithm and ADALINE. Part II - ADALINE and memistor ADALINE - YouTube</span><span class="co">](https://www.youtube.com/watch?v=skfNlwEbqck)</span></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a>More information in <span class="co">[</span><span class="ot">Yuxi on the Wired - The Backstory of Backpropagation</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff)</span>.</span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a>I don't know where to put this anecdote, but I want you to read about it anyway.</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called MADALINE I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail.</span>  </span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; B. Widrow, "Adaline: Smarter than Sweet", Stanford Today, Autumn 1963.</span></span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a>See more of these amusing stories at <span class="co">[</span><span class="ot">Yuxi on the Wired - A Scrapbook of Neural Network Lores</span><span class="co">](https://yuxi-liu-wired.github.io/sketches/posts/neural-network-scrapbook/#neural-networks-want-to-work)</span>.</span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a><span class="fu">## Neural network winter (1970s)</span></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a><span class="fu">### The XOR myth</span></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a>The XOR myth can't be true, for the following reasons:</span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>It was widely known that binary perceptron networks are Turing-complete. <span class="co">[</span><span class="ot">@mccullochLogicalCalculusIdeas1943</span><span class="co">]</span> already proved it.</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Minsky wrote his PhD thesis <span class="co">[</span><span class="ot">@minskyTheoryNeuralanalogReinforcement1954</span><span class="co">]</span> describing all kinds of specific neural networks for computing specific boolean functions.</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>MLP was studied as "linear threshold logic" by electric engineers. They wouldn't have been so interested if they couldn't even do XOR. <span class="co">[</span><span class="ot">@dertouzosThresholdLogicSynthesis1965</span><span class="co">]</span></span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>Still, the XOR myth has some power. What exactly is it? To explain this mess, I will draw a flowchart.</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a><span class="al">![XOR flowchart](figure/XOR_problem_flowchart.png)</span></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>Let's follow all the points in the flowchart:</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Can your neural network learn XOR? If you answer "No", then your neural network is patently useless.</span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To learn XOR, the neural network must have at least 2 layers. But how is it made? If you made it manually, then it is not really machine learning, is it?</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If it is learned, then what algorithm?</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The Perceptron learning algorithm only works for featurized linear regression. So where did you get your features? If you designed it manually, by, for example, reading neuroscience papers, then it is not really machine learning, is it? If they are randomly generated, then it will not scale because of combinatorial explosion.</span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If you used a hack like the MADALINE learning rule, or whatever Rosenblatt tried, then you have no doubt noticed how fiddly and unscalable they are, and how dependent they are on the hyperparameters. Each particular problem required a particular setting of hyperparameters.</span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a>After backprop became prevalent, Minsky and Papert wrote their updated Perceptron (1988) with 43 pages to dunk on everything that is the new connectionism. This allows us to add a few more blocks to the XOR myth flowchart:</span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If you use gradient descent, then it would just get stuck in local minima. You can't just draw a scaling plot and hope it will continue, without proving it theoretically!</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In conclusion, there is no general and scalable learning algorithm.</span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a>As an example of the kind of MLP that Minsky approves: a hand-designed deep network <span class="co">[</span><span class="ot">@fukushimaVisualFeatureExtraction1969</span><span class="co">]</span>. Here is someone who actually tried to understand the problem instead of just tossing it to the computer and expect things to magically work.</span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@fukushimaVisualFeatureExtraction1969</span><span class="co">]</span>](figure/Fukushima_1969.png)</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a>Minsky can't afford a few million neurons:</span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that.</span>  </span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span>  </span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a><span class="fu">### Possible reasons for the winter</span></span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a>Minsky and Papert's book. At least, Minsky thought so.</span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There had been several thousand papers published on Perceptrons up to 1969, but our book put a stop to those. It had, in all modesty, some beautiful mathematics in it--it’s really nineteenth-century mathematics.</span>  </span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span>  </span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky's early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community.</span>  </span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Robert Hecht-Nielsen </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a>Lack of funding</span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Many people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. </span></span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Michael A. Arbib </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a>Lack of people</span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>McCulloch (1969) Pitts (1969) Rosenblatt (1971) died</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Widrow cut his losses and looked for applications for a single neuron, revolutionizing adaptive noise filtering.</span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Ted Hoff went to Intel to create microprocessors.</span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>SRI turned to logical AI. Duda and Hart wrote their classic textbook on "pattern classification" which had little neural networks, and half of the book was devoted to "scene analysis". <span class="co">[</span><span class="ot">@dudaPatternClassificationScene1973</span><span class="co">]</span></span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a>And of course, no backpropagation.</span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a><span class="fu">### Minsky and Papert against large neural networks</span></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>See <span class="co">[</span><span class="ot">Yuxi on the Wired - The Perceptron Controversy</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/)</span> for details.</span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### 30 years of people refusing to use gradient descent</span></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>See <span class="co">[</span><span class="ot">Yuxi on the Wired - The Backstory of Backpropagation</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/)</span>.</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>Rosenblatt did not even consider gradient descent. (1960s)</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>Minsky repeatedly shooting down gradient descent. (1960s -- 1990s at least)</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea... It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It's certainly trivial. The idea is how you do gradient descent. I didn't consider it practical.</span>  </span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991, page 249</span><span class="co">]</span>  </span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>Widrow and Hoff could not generalize one-layered gradient descent to two-layered gradient descent and gave up (1960s).</span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic first layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it's very difficult to adapt a hidden layer. ... We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn't that we didn't try. I mean we would have given our eye teeth to come up with something like backprop.</span>  </span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; **you have to have a smooth nonlinearity ... no one knew anything about it at that time.** This was long before Paul Werbos. **Backprop to me is almost miraculous**. </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span>  </span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>The SRI group never worked it out.</span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; During the 1960s, neural net researchers employed various methods for changing a network’s adjustable weights so that the entire network made appropriate output responses to a set of “training” inputs. For example, Frank Rosenblatt at Cornell adjusted weight values in the final layer of what he called the three-layer alpha-perceptron. Bill Ridgway (one of Bernard Widrow’s Stanford students) adjusted weights in the first layer of what he called a MADALINE. We had a similar scheme for adjusting weights in the first layer of the MINOS II and MINOS III neural network machines at SRI. Others used various statistical techniques to set weight values. But what stymied us all was how to change weights in more than one layer of multilayer networks. (I recall Charles Rosen, the leader of our group, sitting in his office with yellow quadrille tablets hand-simulating his ever-inventive schemes for making weight changes; none seemed to work out.)</span>  </span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@nilssonQuestArtificialIntelligence2009, section 29.4</span><span class="co">]</span>  </span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a>Paul Werbos spending 10 years (1972--1982) unable to publish backpropagation, and almost starved.</span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>He developed the backprop in 1971 or 1972 for his PhD thesis, as a mathematical version of Freudian psychoanalysis.</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The model in Figure 1 is actually just a mathematical version of Freud's model of psychodynamics, where the derivative of $R_i$ represents what Freud called the "cathexis" (or affect) or emotional charge or emotional energy attached to the object which $R_i$ represents. In other words, I came up with backpropagation by not just laughing at Freud's "nonscientific" model, but by translating it into mathematics and showing that it works. </span><span class="co">[</span><span class="ot">@werbosNeuralNetworksPath2011</span><span class="co">]</span>  </span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a>The PhD committee was unconvinced, so he had to find a supporting advisor.</span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "But look, the mathematics is straightforward."</span>  </span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "Yeah, yeah, but you know, **we're not convinced it's so straightforward**. You've got to prove some theorems first."</span>  </span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a>So he went to Stephen Grossberg...</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.</span>  </span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a>Then to Marvin Minsky...</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Look, **everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists**. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It's totally crazy. I can't get involved in anything like this.</span>  </span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a>Unable to find a supporting advisor, he faced the committee again. He decided to simplify his work to just backpropagating through piecewise linear activation functions.</span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I handed that to my thesis committee. I had really worked hard to write it up. They said, "Look, this will work, **but this is too trivial** and simple to be worthy of a Harvard Ph.D. thesis."</span>  </span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a>So he lost funding, so he lived in the slums and became malnutritioned.</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... they had discontinued support because they were not interested... There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.</span>  </span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a>Anyway, he did get his PhD thesis and it did contain the backpropagation algorithm, but then promptly got sucked into a decade of misadventures with federal bureaucracy. The net effect is that he only managed to publish the backprop algorithm in 1982.</span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a>Geoffrey Hinton spent several years (1982--1985) refusing to try backpropagation, since he was too invested in the Boltzmann machines.</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I first of all explained to him why it wouldn't work, based on an argument in Rosenblatt's book, which showed that essentially it was an algorithm that couldn't break symmetry. ... the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule ... The next argument I gave him was that it would get stuck in local minima. There was no guarantee you'd find a global optimum. Since you're bound to get stuck in local minima, it wasn't really worth investigating.</span>  </span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a>But it turned out that Boltzmann machines just never worked in practice, and desperate times called for desperate measures (backpropagation). Amazingly, it just worked.</span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.</span>  </span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a>In a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they've solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn't have the right structure of weights. ... I thought, "Oh well, it turns out backpropagation's not that good after all." Then I looked at the error, and the error was zero. I was amazed.  </span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a>He still thinks Boltzmann machines are better though!</span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; That was at the stage when we were just completing the PDP books, so we'd already agreed on what was going to be in the books. The final chapters were being edited. We decided we'd just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn't nearly as satisfying as Boltzmann machines. ... it didn't have the nice probabilistic interpretation.</span>  </span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a><span class="fu">## Some bitter lessons I guess</span></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a><span class="fu">### First attempt</span></span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I hate everything that merely instructs me without increasing or directly quickening my activity.</span>  </span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Goethe, </span><span class="co">[</span><span class="ot">Letter to Friedrich Schiller, 1798-12-19</span><span class="co">](https://www.friedrich-schiller-archiv.de/briefwechsel-von-schiller-und-goethe/1798/552-an-schiller-19-dezember-1798/)</span><span class="at">. Quoted in Nietzsche's *On the Use and Abuse of History for Life*.</span></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We do need history, but quite differently from the jaded idlers in the garden of knowledge, however grandly they may look down on our rude and unpicturesque requirements. In other words, we need it for life and action, not as a convenient way to avoid life and action, or to excuse a selfish life and a cowardly or base action. We would serve history only so far as it serves life; but to value its study beyond a certain point mutilates and degrades life.</span>  </span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Nietzsche, </span><span class="co">[</span><span class="ot">*On the Use and Abuse of History for Life*</span><span class="co">](https://en.wikisource.org/wiki/On_the_Use_and_Abuse_of_History_for_Life)</span><span class="at"> (1874)</span></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a>Nihilism: You can draw lessons from history, but everyone else also can draw the opposite ones, so there is no take-home lesson.</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a>Elitism: First-rate minds make history. Second-rate minds study history. Getting history "right" is for second-rate minds: old professors (that's Schmidhuber) and procrastinating PhD students (that's me). </span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a>Those who don't know their history, repeat it. Those who know their history, are too busy remembering to make it.</span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a>Emotivism: The proper function of history is to fortify your weights and to harden your biases against naysayers, so that you can go forth and make some history.</span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The function of a mathematician is to do something, to prove new theorems, to add to mathematics, and not to talk about what he or other mathematicians have done. Statesmen despise publicists, painters despise art-critics, and physiologists, physicists, or mathematicians have usually similar feelings: there is no scorn more profound, or on the whole more justifiable, than that of the men who make for the men who explain. Exposition, criticism, appreciation, is work for second-rate minds.</span>  </span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@hardyMathematicianApology1940</span><span class="co">]</span>  </span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a><span class="fu">### Second attempt</span></span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a>This is where I was going to end, but GPT told me that I must include *positive* lessons, so I made some up. I fully expect you to ignore my lessons and reinterpret the history I dug up to your lessons.</span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a>To facilitate this freedom of reinterpretation, I will write my weights and biases in bold letters, so that it is not a secret agenda:</span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">center</span><span class="dt">&gt;</span>**VINCIT OMNIA COMPUTATIO**<span class="dt">&lt;/</span><span class="kw">center</span><span class="dt">&gt;</span></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a>With this, you can now subtract my latent vector and add yours, in full Word2Vec fashion.  </span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Nihilism</span></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a>History is useless for doing things, because ideas are cheap and can't be lost. People will keep reinventing the same ideas until it starts working, then it will not be lost again. The idea is a patient seed, waiting for the soil to moisten.</span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a>In particular, it is useless to give correct attribution to "the real inventor" of an idea, because there are so many re-inventors. It is much more interesting to study each re-invention on its own, to find out how they invented. The point is not to respect the dead, but to find out how they did their things, so that we can do more things. It is useless to know who "really" invented backpropagation, but useful to know how each re-inventor managed to invent it in their individual, different <span class="co">[</span><span class="ot">contexts of rediscovery</span><span class="co">](https://plato.stanford.edu/entries/scientific-discovery/#DistBetwContDiscContJust)</span>.</span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a>Those who don't know their history, repeat it. If everyone knows their history, there will be a replication crisis.</span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a>Residual networks were described by Lorente de No, then Rosenblatt. DenseNet <span class="co">[</span><span class="ot">@huangDenselyConnectedConvolutional2018</span><span class="co">]</span> was already in 1988 <span class="co">[</span><span class="ot">@langLearningTellTwo1988</span><span class="co">]</span>. People didn't know it, but this did not matter.</span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Reinventions/rediscoveries of ResNet. Lorente de No (1930s), Rosenblatt (1961), [@langLearningTellTwo1988], ResNet (2015), DenseNet (2016).</span><span class="co">](figure/ResNet_history.png)</span></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a><span class="al">![Reinventions/rediscoveries of RNN. Cajal (1900s), Lorente de No (1930s), McCulloch and Pitts (1943).](figure/RNN_history.png)</span></span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Reinventions/rediscoveries of Hopfield network. [@rosenblattPerceptualGeneralizationTransformation1960], [@nakanoLearningProcessModel1971], [@hopfieldNeuralNetworksPhysical1982]. Note that because Hopfield's paper has no pretty pictures, I got the picture from [@tankCollectiveComputationNeuronlike1987] instead.</span><span class="co">](figure/Hopfield_history.png)</span></span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a>The only place where the lack of a good idea really hurt is backpropagation. But even if they had backpropagation, what could they have made in the 1970s? A personal tragedy for Widrow and Rosenblatt. Hardly a tragedy for the study of neural networks itself. Ted Hoff was right to leave neural networks to develop the microprocessor.</span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Empiricism</span></span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a>Elegant ideas matter little, because there are always opposing elegant ideas. Ideas must be tested by experiments. Benchmarks are the *experimentum crucis*. In mensuris veritas.</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a>30 years of elegant arguments against backpropagation amounted to nothing but jokes for us.</span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>This disjunction between theory and experiments is particularly acute in reinforcement learning. If you have read RL papers, you would notice how often they describe some elegant ideas and giant formulas, then proceeded to talk about the experiments. I always find the elegant ideas suspicious, and only trust the benchmarks. If the idea is really elegant and stands on its own, why all the ponderous benchmarks? Furthermore, it is often the case that the theory works only in the sense of "eventually almost anywhere if the functional space is large enough".</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a>Similar comments apply for the PAC-learning literature, the no-free-lunch literature, and the neural network universality literature. It is surely comforting to know that neural networks are universal function approximators, but so are Turing machines. Neural networks work not because they are universal, but because their inductive biases happen to be very attuned to physically interesting problems. Why is that? That... is unclear, but <span class="co">[</span><span class="ot">@linWhyDoesDeep2017</span><span class="co">]</span> has a guess. Basically, it states that neural networks are particularly good at learning trajectories generated by quadratic Lagrangian mechanical systems.</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a><span class="fu">### The idea follows the compute</span></span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a>It is not just that compute allows good ideas to be filtered out, and that testing ideas requires expensive compute. Sometimes compute is necessary to even approach the good idea. Without compute experiments, one would be left to wander the vast space of possible ideas. This is why ablation studies are so important, though it lacks the pedigreed prestige of theory. Ablations need compute, making it a choice for the *nouveau riche* -- <span class="co">[</span><span class="ot">the GPU-rich</span><span class="co">](https://www.latent.space/p/semianalysis)</span>, that is.</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a>Two examples. One is ResNet</span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For months, they toyed with various ways to add more layers and still get accurate results. After a lot of trial and error, the researchers hit on a system they dubbed "deep residual networks".</span>  </span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@linnMicrosoftResearchersWin2015</span><span class="co">]</span>  </span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a>Another is Transformer</span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There was every possible combination of tricks and modules—which one helps, which doesn’t help. Let’s rip it out. Let’s replace it with this. Why is the model behaving in this counterintuitive way? Oh, it’s because we didn’t remember to do the masking properly. Does it work yet? OK, move on to the next. All of these components of what we now call the transformer were the output of this extremely high-paced, iterative trial and error.</span>  </span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@levyGoogleEmployeesInvented2024</span><span class="co">]</span></span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a><span class="fu">### The bitter lesson</span></span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By convention sweet is sweet, bitter is bitter, hot is hot, cold is cold, color is color; but in truth there are only atoms and the void. </span><span class="sc">\[</span><span class="at">νόμωι (γάρ φησι) γλυκὺ καὶ νόμωι πικρόν, νόμωι θερμόν, νόμωι ψυχρόν, νόμωι χροιή, ἐτεῆι δὲ ἄτομα καὶ κενόν</span><span class="sc">\]</span></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Democritus, quoted in *Sextus Empiricus Against the Mathematicians* VII 135. I like the funny title.</span></span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die and a new generation grows up that is familiar with it ...</span></span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Max Planck, *Scientific Autobiography* (1950), page 33. In simpler terms, "Science progresses one funeral at a time.".</span></span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 1) AI researchers have often tried to build knowledge into their agents, </span></span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 2) this always helps in the short term, and is personally satisfying to the researcher, but </span></span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 3) in the long run it plateaus and even inhibits further progress, and </span></span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. </span></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</span>  </span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@suttonBitterLesson2019</span><span class="co">]</span>  </span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a>The bitter lesson is bitter, because some people really like elegant ideas, and they will not change.</span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a>Marvin Minsky insisted for decades that large neural networks do not work in theory (as we saw), so it only *appears* to work in practice.</span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 13.5 Why Prove Theorems? </span></span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *Why did you prove all these complicated theorems? Couldn't you just take a perceptron and see if it can recognize $\psi_{\text {CONNECTED}}$?*</span></span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; No. (1969)</span></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988, page 239</span><span class="co">]</span>  </span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks... I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (1981)</span>  </span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span>  </span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No. (1988)</span>  </span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988, page xiv</span><span class="co">]</span>  </span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In his summary talk at the end of the conference </span><span class="sc">\[</span><span class="at">The AI\@50 conference (2006)</span><span class="sc">\]</span><span class="at">, Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: "You're not working on the problem of general intelligence. You’re just working on applications." (2006)</span>  </span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, pages 256</span><span class="co">]</span>  </span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a>Geoffrey Hinton still liked Boltzmann machines more than backprop, even in 1995:</span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; That was at the stage when we were just completing the PDP books, so we'd already agreed on what was going to be in the books. The final chapters were being edited. We decided we'd just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn't nearly as satisfying as Boltzmann machines. ... it didn't have the nice probabilistic interpretation.</span>  </span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span>  </span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a>Noam Chomsky is totally against statistical language modelling, since at least 1969. See also <span class="co">[</span><span class="ot">@norvigChomskyTwoCultures2017</span><span class="co">]</span>.</span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; But it must be recognized that the notion of "probability of a sentence" is an entirely useless one, under any known interpretation of this term. (1969)</span>  </span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@chomskyEmpiricalAssumptionsModern1969</span><span class="co">]</span>  </span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data. (2011)</span>  </span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">Brains, Minds, and Machines symposium 2011, Keynote Panel</span><span class="co">](https://languagelog.ldc.upenn.edu/myl/PinkerChomskyMIT.html)</span>  </span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Well, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They've achieved zero... GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It'll use even more energy and achieve exactly nothing, for the same reasons. So there's nothing to discuss. (2022)</span>  </span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">Machine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding</span><span class="co">](https://whimsical.com/question-1-large-language-models-such-as-gpt-3-DJmQ8R5kD8tqvsXxgCN9vK)</span>  </span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a>On the last one, I suspect he is still fighting the battle against behaviorism, and the pretrained language models like GPT-4 must seem like a kind of neo-behaviorist heresy to him.</span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a>Yehoshua Bar-Hillel, speaking in the 1960s, against machine translation -- what he called FAHQT (Fully Automatic High-Quality Translation, or as I imagine him saying it, "Machine translation? Ah, FAHQT."). In short, he argued that statistical language modelling does not work better than manually programming in the rules. Specifically, the Winograd schema challenge (which was known in the 1960s, before Winograd discussed it) requires general world understanding, which is "utterly chimerical and hardly deserves any further discussion'.</span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; No justification has been given for the implicit belief of the “empiricists” that a grammar satisfactory for MT purposes will be compiled any quicker or more reliably by starting from scratch and “deriving” the rules of grammar from an analysis of a large corpus than by starting from some authoritative grammar and changing it, if necessary, in accordance with analysis of actual texts.</span>  </span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Whenever I offered </span><span class="co">[</span><span class="ot">the Winograd challenge</span><span class="co">]</span><span class="at"> to one of my colleagues working on MT, their first reaction was: “But why not envisage a system which will put this knowledge at the disposal of the translation machine?” ... such a suggestion amounts to, if taken seriously, is the requirement that a translation machine should not only be supplied with a dictionary but also with a universal encyclopedia. This is surely utterly chimerical and hardly deserves any further discussion.</span></span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@bar-hillelPresentStatusAutomatic1960</span><span class="co">]</span>  </span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It seems now quite certain to some of us, a small but apparently growing minority, that with all the progress made in hardware (i.e., apparatus), programming techniques and linguistic insight, the quality of fully autonomous mechanical translation, even when restricted to scientific or technological material, will never approach that of qualified human translators and that therefore Machine Translation will only under very exceptional circumstances be able to compete with human translation.</span>  </span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@bar-hillelFutureMachineTranslation1964</span><span class="co">]</span>  </span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>Even Terry Winograd had a tentative guess that the Winograd challenge is too challenging, though it is clearly just a weak guess, not a firm prediction like the previous quotes.</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The limitations on the formalization of contextual meaning make it impossible at present -- and conceivably forever -- to design computer programs that come close to full mimicry of human language understanding.</span>  </span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@winogradComputerSoftwareWorking1984</span><span class="co">]</span>  </span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>