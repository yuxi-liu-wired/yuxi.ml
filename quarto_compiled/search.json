[
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "",
    "text": "My friend: What annoying precision.\nMe: I call it conceptual clarity.\nMy hand-written reference sheets while I was working through the book."
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#recommendations",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#recommendations",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Recommendations",
    "text": "Recommendations\n\nMiscellaneous generic recommendations\n\nAdd a list of the most important equations at the ends of chapters.\nBecause SGD is so essential to modern machine learning, I feel like it’d help to compile all the comments in the book about SGD in one section. I compiled them here:\n\n\nessentially everything we will say about gradient descent will apply to stochastic gradient descent as well.\n\n\nIt doesn’t matter which loss function we used, e.g., MSE or cross-entropy,\nhow many steps we took to get to the minimum, or whether we used gradient descent or SGD. Said another way, algorithm independence means that these hyperparameters and training set uniquely specify the statistics of fully-trained networks in the infinite-width limit; Newton’s method is just a nice theoretical trick to leap right to the solution.\n\n\nNewton’s method is a theoretical tool that lets us describe a fully-trained extremely-wide network, even if the network was trained very practically by a many-step version of (stochastic) gradient descent.\n\n\nFor SGD to actually converge to a minimum, you need to decrease the learning rate over the course of training, otherwise the network will fluctuate around, but never actually reach, the minimum. Intuitively, this is because at each step the optimization problem does not include the entire training set.\n\n\nSomewhere in the book (maybe in Chapter 4?) give the following intuition for the infinite-width limit:\n\nAt the infinite width limit, the neural network degenerates into a sequence of continuous functions \\(G^{(1)}, \\dots, G^{(L)}\\), such that the preactivations in the neural network, upon input \\(x\\), becomes independent random samples from gaussian distributions: \\(z^{(l)}_{i} \\sim N(0, G^{(l)}(x))\\).\nYou can then calculate how \\(G^{(l+1)}(x)\\) is derived from \\(G^{(l)}\\) by a simple argument with gaussians.\n\nI think it would be good to introduce kernel linear regression in a simple setting. Perhaps at “Probability, Correlation and Statistics, and All That” since it is such a simple and general technique in statistics. Then later, in the chapter on Bayesian learning, one can immediately point out that in the “6.3.2 Let’s Not Wire Together”, we are seeing just a standard kernel regression. And later, in Chapter 10, one can immediately point out that this is kernel regression where the kernel matrix is \\(\\Theta\\).\n\n\n\nFeynman diagrams\nTeach some naive methods with Feynman diagrams. For some examples, see my attached sheets, which contains many Feynman diagrams (not exactly, but you get the idea). Specifically, it simplifies certain calculations to be almost tolerable.\n\n\n\nQuartic interaction intuition\nFor the quartic interaction, a few pictures could really make it clear. I drew some schematically:\n\nThe pictures make it clear that one effect of quartic interaction is to create correlations between the squared preactivations of neurons. Sometimes it is a negative correlation (as in the \\(c=-2\\) picture) and sometimes it is a positive correlation (as in the \\(c = 6\\) picture), and sometimes it is more subtle. The \\(c=2\\) picture has circular contours like gaussian distribution, but it does not mean that \\(z_{i, \\alpha}^2, z_{j, \\beta}^2\\) are independent. The dependence is more subtle.\nMore generally, the effect is to correlate \\(z_{i, \\alpha_1}z_{i, \\alpha_2}\\) and \\(z_{j, \\alpha_3}z_{j, \\alpha_4}\\), but this is too difficult to plot. Still, these contour plots give a good mental picture.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef z_func(x, y, c):\n    return x**2 + y**2 - 0.2 * (x**4 + y**4 + c * x**2 * y**2)\n\nx = np.linspace(-1, 1, 200)\ny = np.linspace(-1, 1, 200)\nX, Y = np.meshgrid(x, y)\n\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, c in enumerate([-2, 2, 6]):\n    Z = z_func(X, Y, c)\n    ax = axes[i]\n    ax.set_aspect(1)\n    contour = ax.contour(X, Y, Z, levels=10)\n    ax.set_title(f\"c = {c}\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.suptitle(rf\"Contour plots of $z_{{i,\\alpha}}^2 + z_{{j,\\beta}}^2 - 0.2(z_{{i,\\alpha}}^4+z_{{j,\\beta}}^4+cz_{{i,\\alpha}}^2 z_{{j,\\beta}}^2)$\")\nplt.tight_layout()\nplt.show()\n\n\nNormalized vectors\nMany formulas can be made more intuitive by using the following convention on normalizing vectors:\nDefine the normalized activation vectors \\(\\bar \\sigma_{\\alpha}^{(\\ell)} := \\sigma^{(\\ell)}_{\\alpha} / \\sqrt{n_\\ell}\\).\nWe can motivate this by arguing that a good neural network should have all activations in a layer roughly \\(O(1)\\) random, and close to independent (so that they don’t collapse to a low-dimensional subspace). Then, \\(\\bar \\sigma_{\\alpha}^{(\\ell)}\\) would have norm \\(O(1)\\).\nFor example, equation (4.76) then becomes\n\\[\\frac{1}{n_{\\ell}} V_{\\left(\\alpha_1 \\alpha_2\\right)\\left(\\alpha_3 \\alpha_4\\right)}^{(\\ell+1)}  = (C_W^{(\\ell+1)})^2 \\mathrm{Cov}[\\bar \\sigma_{\\alpha_1}^{(\\ell)} \\cdot \\bar \\sigma_{\\alpha_2}^{(\\ell)}, \\bar \\sigma_{\\alpha_3}^{(\\ell)} \\cdot \\bar \\sigma_{\\alpha_4}^{(\\ell)}]\\]\n\n\nShape of the kernels\nI think the equation (6.48) is very important and should be put in Chapter 1.2. It should not be relegated to a footnote. In particular, it gives the best picture for what the kernel is actually doing. I have attached a picture showing what I meant.\nI think this equation + picture can be put very early on, at the start of “Chapter 1.2 Probability, Correlation and Statistics, and All That”, since if we are going to get an “effective theory” we should say up front what are the key effective variables. It took me some time to figure out that the effect of the kernel \\(K\\) is to characterize the effective shape of an activation triangle!\nI think you can teach this effectively as follows: Given that we have two high-dimensional vectors \\(y_+, y_-\\) in \\(\\mathbb{R}^n\\) where \\(n\\) is a large number, distributed as \\(y \\sim N(0, \\delta_{ij} K_{\\alpha \\beta})\\), that is\n\\[p(y_+, y_-) \\propto \\exp\\left[-\\frac 12 \\sum_{i, j, \\alpha, \\beta} y_{+, i, \\alpha}y_{-, j, \\beta} K^{\\alpha \\beta} \\delta^{ij}\\right]\\]\nWe don’t want to know the exact orientation, just the shape of the triangle formed by \\(0, y_-, y_+\\). Then, we can characterize the shape of the triangle with three numbers \\(Y_{[0]}, Y_{[1]}, Y_{[2]}\\) defined by … and we divide by \\(\\sqrt n\\) because we want to normalize the size of the triangle to be \\(O(1)\\) even as \\(n\\) grows towards infinity (This should remind you of the central limit theorem). Similarly, we divide \\(K\\) by \\(n\\) to obtain \\(\\hat K\\). Now, it is routine to calculate\n\\[\np(\\hat y_+, \\hat y_-) = (4\\pi^2 (4 \\hat K_{[2]}\\hat K_{[0]} - \\hat K_{[1]}^2))^{-n/2} \\exp \\left[ - \\frac{2\\hat K_{[0]} Y_{[2]} + 2\\hat K_{[2]} Y_{[0]} - \\hat K_{[1]} Y_{[1]}}{4 \\hat K_{[2]}\\hat K_{[0]} - \\hat K_{[1]}^2}\\right]\n\\]\nThen, a geometric argument shows that conditional on \\(Y_{[1]} = 0\\), the maximal likelihood estimate of \\(Y_{[0]}\\) is \\(n \\hat K_{[0]} = K_{[0]}\\).\n\n\nFaster solution of recursion relations\nThere is a faster way to solve the recursion relations like Eq (9.85). I will solve it as an example:\nFirst, rewrite it as a differential equation:\n\\[\ny' \\approx -2p_\\perp y/x + C x^{2-2p_\\perp}\n\\]\nwhere we have substituted \\(y\\) for \\(A\\), and \\(x\\) for \\(\\ell\\).\nThen, plug it into Wolfram Alpha, we find the solution is \\[\ny = \\frac{1}{3} C x^{1-p_\\perp} + c_1 x^{-p_\\perp-2}\n\\]\nwhere \\(c_1\\) is an arbitrary constant.\nIn general, the recursion formula has not one, but two possible leading terms. In all cases that appeared in the book, the second leading term is not actually leading, so all the equations turned out exactly correct. Still, it should be pointed out.\nFor example, the recursive equation (9.79) has, as its lowest terms, two terms:\n\\[\nF^{(\\ell)}=\\frac{1}{\\left(5-p_{\\perp}\\right)}\\left[\\frac{1}{\\left(-a_1\\right)}\\right]\\left[\\widetilde{\\lambda}_b+\\frac{\\widetilde{\\lambda}_W \\sigma_1^2}{\\left(-a_1\\right)}\\right]\\left(\\frac{1}{\\ell}\\right)^{p_{\\perp}-1} + \\frac{c}{\\ell^4} + \\cdots\n\\]\nOf course, if we assume \\(p_\\perp \\leq 5\\), then indeed we can ignore the \\(c/\\ell^4\\) term. This needs to be proven, as follows:\nThe \\(K^* = 0\\) universality class requires \\(a_1 &lt; 0\\). Given that, we have\n\\[p_\\perp = 1 + \\frac{\\frac 14 (\\sigma_2/\\sigma_1)^2}{a_1} \\leq 1\\]\nwith equality reached when \\(\\sigma_2 = 0\\).\nSimilarly, the recursive equation (9.80) also has two lowest terms:\n\\[\nB^{(\\ell)}=\\frac{1}{3}\\left[\\widetilde{\\lambda}_b+\\frac{\\widetilde{\\lambda}_W \\sigma_1^2}{\\left(-a_1\\right)}\\right]^2\\left(\\frac{1}{\\ell}\\right)^{2 p_{\\perp}-3} + \\frac{c}{\\ell^{2p_\\perp}} + \\cdots\n\\]\nalthough in this case, the second term is dominated by the first regardless of what \\(p_\\perp\\) is.\nSimilarly, Eq (10.77) on page 272 has an extra term of order \\(O(1/\\ell^{p_\\perp})\\), which is negligible only because \\(p_\\perp \\leq 1\\).\n\n\nSymmetry groups\nOn page 83, the symmetry group of the indices of \\(V\\) is the symmetry group of the square. Specifically, you can imagine labelling a square’s vertices with \\(\\alpha_1, \\alpha_3, \\alpha_2, \\alpha_4\\), counterclockwise. Then any rotation and reflection of the square is a permutation of its four vertices that does not change \\(V\\).\nSince the symmetry group of the square has 8 elements, this explains the factor of \\(\\frac 18\\) in (4.44). I think this improves footnote 7.\nSimilarly, on page 209, 210. I think the symmetries can be geometrically pictured as follows:\n\nFor each of \\(A, B, D, F, V\\), write the indices clockwise on the vertices of a quadrilateral, as 1324.\n\\(A_{(12)(34)}, V_{(12)(34)}\\) both have symmetry of the square.\n\\(B_{(12)(34)}\\) has symmetry of the square in \\(l=1\\), and subsequently, rectangle.\n\\(D_{(12)(34)}\\) has symmetry of the square in \\(l=1\\), and subsequently, rhombus.\n\\(F_{(12)(34)}\\) has symmetry of the square in \\(l=1\\), of the rhombus in \\(l=2\\), and subsequently, no symmetry.\n\n\n\nDimensional analysis\nMake a table for dimensional analysis. I made one:\n\n\n\nVariable\nValue\n\n\n\n\n\\(z, x, \\sigma\\)\n1\n\n\n\\(\\sigma'\\)\n0\n\n\n\\(W, C_W\\)\n0\n\n\n\\(b\\)\n1\n\n\n\\(C_b\\)\n2\n\n\n\\(S\\)\n0\n\n\n\\(\\lambda_{\\mu\\nu}\\)\nSame as \\(\\theta_\\mu \\theta_\\nu\\)\n\n\n\\(\\lambda_b\\)\n2\n\n\n\\(\\lambda_W\\)\n0\n\n\n\\(\\Theta, H\\)\n2\n\n\n\\(G, K\\)\n2\n\n\n\\(g\\)\n2\n\n\n\\(\\chi_\\parallel, \\chi_\\perp\\)\n0\n\n\n\\(h\\)\n-2\n\n\n\\(V, D, F, B, A, P, Q\\)\n4\n\n\n\n\n\n“Acausally rising to meet the criticism”\nI don’t know if it is intended, but you used as epigraph for Chapter 5 from Perceptrons (1988), and have Rumelhart et al “acausally rising to meet the criticism”. In fact, they were responding to Chapter 0 of Perceptrons (1969):\n\nThe sciences of computation and cybernetics began, and it seems quite rightly so, with a certain flourish of romanticism. They were laden with attractive and exciting new ideas which have already borne rich fruit. Heavy demands of rigor and caution could have held this development to a much slower pace; only the future could tell which directions were to be the best. We feel, in fact, that the solemn experts who most complained about the “exaggerated claims” of the cybernetic enthusiasts were, in the balance, much more in the wrong. But now the time has come for maturity, and this requires us to match our speculative enterprise with equally imaginative standards of criticism.\n\nThe second edition of Perceptrons was published in 1988, during the second coming of neural networks. In their Parallel Distributed Processing research project, Rumelhart et al very specifically showed that 2-layered networks were able to solve problems such as XOR and symmetry recognition that were criticized in Perceptrons (1969). The second edition, as the authors stated, was a literal reprinting of the first edition with just an added preface and an added epilogue criticizing neural networks."
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-4",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-4",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 4",
    "text": "Chapter 4\n\npage 86:\n\nFor Eq (4.46) and (4.57), add a footnote to the alert reader who is getting confused by the serial expansion. I think something like\n\n“To the alert reader: If you are confused by the fact that the second term seems to have order \\(O(vn^2) = O(n)\\), good job paying attention! Consider proceeding directly to reading the”Marginalization over Neurons” section. But the short answer is that we can set \\(1 \\ll n_2 \\ll n_1 = n\\) for now, calculate, then proceed to layer 3. When calculating layer 3, we will note that the \\(O(n_2)\\) terms exactly cancel out, so that we can then take the \\(n_2 \\to n\\) limit and get the same outcome.”\n\n\nPage 88. For the derivation of Eq (4.61), explain a bit more:\n\nStart with Eq (4.46). Enumerate the possible \\(j_1, j_2\\). The only cases where the covariance inside the bracket is nonzero are when \\(j_1=i_1 \\neq j_2\\) ( \\(n\\) terms), \\(j_2=i_1 \\neq j_1\\) ( \\(n\\) terms) and \\(i_1=j_1=j_2\\) (1 term). Now, simplify using the index symmetry of \\(v\\).\n\nPage 103. After “In going through this calculation in your personal notes or on the margins of this book, you can explicitly see the cancellation…”, add the sentence: “The order \\(n_\\ell\\) terms in Eq (4.111) and (4.113) exactly cancel each other out.”"
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-5",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-5",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 5",
    "text": "Chapter 5\nI think some pictures would really help explain what the kernels mean. I have drawn some by hand.\nThe midpoint kernel.\n\n\n\nThe midpoint kernel\n\n\nThe kernel perturbation into \\(K_{[̄0]}, \\delta K_{[̄1]}, \\delta\\delta K_{[̄2]}\\).\n\n\n\nThe kernel perturbation into \\(K_{[̄0]}, \\delta K_{[̄1]}, \\delta\\delta K_{[̄2]}\\).\n\n\nHow two activation vectors (signal propagation) propagate through a critically tuned neural network at initialization.\n\n\n\nHow two activation vectors (signal propagation) propagate through a critically tuned neural network at initialization.\n\n\n\nPage 112. Add “Of course, \\(g\\) is not a function of \\(K\\) only, but rather depends on \\(\\sigma\\) as well. We suppress this dependence.”\nPage 115. For “To do so, we will find it convenient to project the full kernel matrix…”, I suggest instead of that, write this: “Because \\(K\\) has 4 entries, it can be regarded as a vector in \\(\\mathbb{R}^4\\). Because it is symmetric, it is in a 3-dimensional subspace. Thus, we can decompose it with three orthogonal vectors:”\npage 118\n\nPoint out that \\(\\hat e^u\\) is just the vector pointing in the angle \\(45^\\circ - \\delta \\theta\\) where \\(\\delta \\theta = \\frac{\\delta K_{[1]}^{(\\ell)}}{2 K_{00}^{(\\ell)}}\\), and \\(\\hat e^w\\) is \\(\\hat e^u\\) rotated by 90 degrees, since symmetric matrices always have perpendicular eigenvectors.\nPoint out that \\(\\delta \\theta\\) can be found just using \\(K_{00}, \\delta K_{[1]}\\), saving some effort.\n\nPage 119.\n\nThe logical relation between \\(u\\) and \\(z_0\\) is unclear. In particular, it seems we have a double definition for \\(z_0\\).\nFirst definition: \\(z_0\\) is the preactivation corresponding to the midpoint input. That is, \\(z_0 = z^{(\\ell)}_i(x_0)\\).\nSecond definition: \\(\\frac{u^2}{2 \\lambda_u}=\\frac{z_0^2}{2 K_{00}^{(\\ell)}}\\).\nIf both definitions are equally true, then we must prove that they give the same \\(z_0\\). This is not proven, and I think it is actually false.\nI think \\(z_0\\) is purely defined by the second definition, for the simple reason that everything in this page is just doing a gaussian integral in \\(\\mathbb{R}^2\\). Definition 1 requires \\(x_0\\) to exist, but this page does not require \\(x_0\\) to exist. Therefore, the second definition is how \\(z_0\\) is defined.\nThe first “definition” is not a definition, but an interpretation. I think that it should be interpreted as this: Suppose that \\(x_\\pm = x_0 \\pm \\delta x\\), then \\[z_0 = z_i^{(\\ell)}(x_0) + O(\\delta^2)\\]\n\npage 121. Eq (5.53) should have another entry, which will be useful later: \\[\n\\braket{(z^2 - K)\\sigma \\sigma }_K / (2K^2) = \\braket{z\\sigma' \\sigma}_K / K = \\underbrace{\\braket{\\sigma'' \\sigma + \\sigma' \\sigma'}_K}_{\\text{new}}\n\\]\n\nBoth equations in this can be proved by Stein’s lemma, which in this case says\n\\[\\braket{F(z) z}_K = K\\braket{F'(z)}_K\\]\n\npage 125. Add a table. Perhaps something like\n\n\n\n\n\n\n\n\n\nfixed point\ndecay rate\nactivation function\n\n\n\n\na line (exactly marginal deformation)\nno decay (semi-criticality)\nleaky ReLU\n\n\nzero\npower-law\ntanh, sin\n\n\nnonzero\npower-law\nswish, gelu\n\n\nnone\nnone\nsigmoid, softplus, monomial\n\n\n\n\npage 127. In “5.3.1 General Strategy”, The new Eq (5.53) allows us to get a much simpler condition than Eq (5.73). Solve for \\(\\chi_\\perp(K) = \\chi_\\|(K)\\) would give us\n\n\\[\n\\braket{\\sigma'' \\sigma}_K = 0\n\\]\nThis then allows us to get the results by direct calculation, without checking it numerically. For example: - For many activation functions, \\(\\braket{\\sigma'' \\sigma}_K = 0\\) has only soluion \\(K = 0\\), at which point we have \\(C_W = 1 / \\sigma'(0)^2, C_b = -(\\sigma(0) / \\sigma'(0))^2\\). This requires \\(\\sigma'(0)  \\neq  0, \\sigma(0) = 0\\). - softplus activation - \\(\\sigma(z) = \\ln(1+e^z)\\). - Because both \\(\\sigma, \\sigma'' &gt; 0\\), there is no critical \\(K\\). - monomial activation - \\(\\sigma(z) = |z|^p\\), with \\(p &gt; 1\\). By scale invariance, \\(\\braket{\\sigma'' \\sigma}_K = K^{p-1}\\braket{\\sigma'' \\sigma}_1\\), so \\(K = 0\\) is the solution. But this violates the condition \\(\\sigma'(0) = 0\\), so there is no criticality. - sigmoid activation - \\(\\sigma(z) = 1/(1 + e^{-z})\\). Its second derivative is odd, so \\(\\braket{\\sigma''}_K = 0\\). This means \\(\\braket{\\sigma'' \\sigma}_K = \\braket{(\\sigma - 1/2)'' (\\sigma-1/2)}_K\\), at which point we reduce to the case of \\(\\tanh\\). - Just like the case of \\(\\tanh\\), we have \\(K = 0\\), but in this case we don’t have \\(\\sigma(0) = 0\\). - tanh activation - Because \\(\\tanh''(z) \\tanh(z) &lt; 0\\) except at \\(z = 0\\), the only critical solution is \\(K = 0\\). - sin activation - In the integral \\[\n\\braket{\\sin''(z) \\sin(z)}_K = -\\frac{1}{\\sqrt{2\\pi K}} \\int e^{-\\frac{z^2}{2K}}\\sin^2(z) \\; dz\n\\] the integrand is positive except at isolated points \\(0, \\pm \\pi, \\dots\\), so the only critical solution is \\(K = 0\\).\n- For example, the SWISH function gives\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import integrate\nfrom scipy.optimize import root_scalar\n\ndef plot_sigma_expectation(sigma, sigma_second_derivative, K_range):\n    def integrand(z, K):\n        return sigma(z) * sigma_second_derivative(z) * np.exp(-z**2 / (2*K)) / np.sqrt(2 * np.pi * K)\n    \n    def expectation(K):\n        result, _ = integrate.quad(integrand, -np.inf, np.inf, args=(K,))\n        return result\n\n    expectations = [expectation(K) for K in K_range]\n    \n    z_range = np.linspace(-5, 5, 400)  # Adjust range/points as needed\n    sigma_vals = [sigma(z) for z in z_range]\n    sigma_dd_vals = [sigma_second_derivative(z) for z in z_range]\n    product_vals = [sigma(z) * sigma_second_derivative(z) for z in z_range]\n\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))  # 2 rows, 1 column\n\n    ax1.plot(K_range, expectations)\n    ax1.set_xlabel('K')\n    ax1.set_ylabel('E[σ(z)σ\\'\\'(z)]')\n    ax1.set_title('E[σ(z)σ\\'\\'(z)] as a function of K')\n    ax1.set_xscale('log')\n    ax1.grid(True)\n\n    ax2.plot(z_range, sigma_vals, label='σ(z)')\n    ax2.plot(z_range, sigma_dd_vals, label='σ\\'\\'(z)')\n    ax2.plot(z_range, product_vals, label='σ(z)σ\\'\\'(z)')\n    ax2.set_xlabel('z')\n    ax2.legend()\n    ax2.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef sigmoid_d(z):\n    s = sigmoid(z)\n    return s * (1 - s)\n\ndef sigmoid_dd(z):\n    s = sigmoid(z)\n    return s * (1 - s) * (1 - 2 * s)\n\ndef swish(z):\n    return z * sigmoid(z)\n\ndef swish_d(z):\n    return sigmoid(z) + z * sigmoid_d(z)\n\ndef swish_dd(z):\n    return 2 * sigmoid_d(z) + z * sigmoid_dd(z)\n\nK_range = np.exp(np.linspace(-5, 10, 1000))\nplot_sigma_expectation(swish, swish_dd, K_range)\n\ndef solve_for_K(sigma, sigma_second_derivative, K_guess):\n\n    def integrand(z, K):\n        return sigma(z) * sigma_second_derivative(z) * np.exp(-z**2 / (2 * K)) / np.sqrt(2 * np.pi * K)\n\n    def expectation(K):\n        result, _ = integrate.quad(integrand, -np.inf, np.inf, args=(K,))\n        return result\n    sol = root_scalar(expectation, x0=K_guess, method='brentq', bracket=[0.01*K_guess,100*K_guess])\n\n    if sol.converged:\n        return sol.root\n    else:\n        return None\n\nsolve_for_K(swish, swish_dd, 15)\n\npage 133 “Lastly, let us note for all aspiring “activation designers” out there that we can…”\n\nI have designed a concrete example. The general strategy is to just design a sum of linear transforms of \\(\\tanh\\). \\[\n\\sigma(z) = \\tanh(z) - \\frac 18 \\tanh(2z)\n\\]\n\n\nwith \\(\\sigma_1 = 3/4, \\sigma_5 = -48, a_1 = 0, a_2 = -16\\), \\(K \\sim \\frac{1}{\\sqrt{8l}}\\).\nI wonder if anyone has tried this so far? Does it work better for very deep networks?\n\npage 136: “one can check that \\((-a_1) &lt; 0\\)” I think it’d be best to actually write the exact value, for completeness. The exact value is \\(-a_1 = -3/4 &lt; 0\\).\npage 139: It’d be nice to sketch the derivations for Eq (5.108)–(5.110). I sketch it out as follows:\n\nThe recursion for \\(K\\) is Eq (4.90).\nThe recursion for \\(V\\) is found by plugging the formula for \\(g\\) Eq (4.108) into Eq (4.90), and noticing that \\(\\chi_\\|(K^l)\\) appears in it. Since we only need to calculate \\(V^{(l+1)}\\) to order \\(O(1)\\), we need only the \\(g^l = K^l + O(1/n)\\) part.\nThe recursion for \\(G^{\\{1\\}}\\) is by simplifying Eq (4.115).\n\npage 147: The painful integral does not have to be painful. Here’s how I did it:\n\nI drew on paper the contour plot of \\(ReLU(z_+) ReLU(z_-)\\), and noted that it looks like hyperbolas in the first quadrant. Then I drew contour plot of the normal distribution \\(N(0, K^{(l)})\\) and noted that it is an ellipse. Now, by a squeeze in the diagonal direction, then a scaling, we can change the normal distribution into a standard normal distribution. This changes the hyperbolas into different hyperbolas. Looking at the hyperbolas, I thought “This reminds me of the hyperbolic/Lorentz transformation in special relativity.” and that’s how I figured out the trick.\nFirst, reparameterize\n\\[K^{(l)} = K_d^{(l)} \\begin{bmatrix} 1 & \\cos \\psi^{(l)} \\\\ \\cos\\psi^{(l)} & 1\\end{bmatrix} = K_d^{(l)}\\sin \\psi^{(l)} \\begin{bmatrix} \\cosh \\phi^{(l)} & \\sinh \\phi^{(l)} \\\\ \\sinh \\phi^{(l)} & \\cosh \\phi^{(l)}\\end{bmatrix} \\]\nThis is useful because taking the square root of a hyperbolic transform matrix is easy: just divide the hyperbolic angle by 2.\nTherefore, a random variable \\(z \\sim N(0, K^{(l)})\\) is equivalent to\n\\[z = \\sqrt{K_d^{(l)}\\sin \\psi^{(l)}} \\begin{bmatrix} \\cosh \\frac{\\phi^{(l)}}{2} & \\sinh \\frac{\\phi^{(l)}}{2} \\\\ \\sinh \\frac{\\phi^{(l)}}{2} & \\cosh \\frac{\\phi^{(l)}}{2} \\end{bmatrix} \\begin{bmatrix}x\\\\y\\end{bmatrix}\\]\nwhere \\(x, y\\) are standard normal.\nThe region of integration is \\(z_+ &gt; 0, z_- &gt; 0\\), which is the region bounded by two rays of slope \\(-\\theta_0, \\frac \\pi 2 + \\theta_0\\) where\n\\[\\tan\\theta_0 = \\tanh \\frac{\\phi^{(l)}}{2} \\implies \\theta_0 = \\frac\\pi 4 - \\frac{\\psi^{(l)}}{2}\\]\nNow the integral becomes\n\\[\n\\frac{1}{2\\pi} \\int e^{-\\frac{x^2+y^2}{2}} \\left( x \\cosh\\frac{\\phi^{(l)}}{2} + y \\sinh\\frac{\\phi^{(l)}}{2}\\right)\\left(x \\sinh\\frac{\\phi^{(l)}}{2} + y \\cosh\\frac{\\phi^{(l)}}{2}\\right) K_d^{(l)} \\sin\\psi^{(l)} dxdy\n\\]\nIn polar coordinates, this splits to a product of two simple integrals."
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-6",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-6",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 6",
    "text": "Chapter 6\n\npage 173: It’d be good to start the section “6.3.2 Let’s Not Wire Together” by saying “In this section we derive the conditional distribution of the network’s output on the test set, conditional on the training set. We will do this by marginalizing a multivariate gaussian to derive \\(z_B^{(L)}|y_A \\sim N(m^\\infty_{i,\\beta}, \\mathbb K_{\\beta_1 \\beta_2} \\delta_{i_1i_2})\\), where \\(m^\\infty, \\mathbb K \\delta\\) are the mean vector and the variance tensor. So if you know how to marginalize a gaussian, try to derive Eq (6.57) and Eq (6.64) yourself. (This should remind you of kernel regression and gaussian processes.) Then continue reading from Eq (6.66).”.\n\nI know you are trying to make this textbook truly introductory and I appreciate that. However, I think most machine learning and physicists researchers know how to marginalize a gaussian, since it is essential for gaussian processes and statistical mechanics (and this section is basically statistical mechanics + neural network gaussian process).\nIt took me a bit too long to figure out that this is what the section is saying (what with all those gigantic symbols and formulas), so I think stating up front what the section is about would be great.\n\npage 177: I think it’d be good to point out that kernel methods also require a matrix inversion and thus is also impractical for large datasets. To be more precise, this section is about the neural network gaussian process, which is equivalent to Bayesian kernel ridge regression.\npage 179: We can make it sharper: “At the infinite width limit, every single neuron (both hidden and output) is completely independent from every other neuron, and we have \\[\np(z_D^{(1)}, \\dots, z_D^{(L)}) =\\prod_{l=1}^L \\prod_{i_l} p(z_{i, D}^{(l)})\n\\] which makes it clear that this is as”no representation” as it gets.”\n\npage 181. Eq (6.78) can be made even clearer like this:\n\n\\[\\mathbb{E}\\left[\\left(z_2^{(\\ell)}\\right)^2 \\Big| \\check z_1\\right] - \\mathbb{E}\\left[\\left(z_2^{(\\ell)}\\right)^2\\right] = \\left[\\left(\\check{z}_1^{(\\ell)}\\right)^2-\\mathbb{E}\\left[\\left(z_1^{(\\ell)}\\right)^2\\right]\\right]\\left[\\frac{V^{(\\ell)}}{2n_{\\ell-1}\\left(G^{(\\ell)}\\right)^2}\\right] + O(1/n^2)\\]"
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-7",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-7",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 7",
    "text": "Chapter 7\n\nPage 196. learning-rate tensor should be explicitly called symmetric. That is, \\(\\lambda_{\\mu \\nu} = \\lambda_{\\nu\\mu }\\).\nPage 198. “not necessarily positive” -&gt; “not necessarily positive-definite”.\nPage 198.\n\n“for models that overfit their training set – the test loss may even increase.” is tautological, because it is just the definition of “overfit”. I suggest this instead: “the test loss may even increase. This is what it means for the model to”overfit” its training set.”\nClassically, there are two meanings of “overfitting”. Back then, overfitting both means “interpolating the training set” and “performing worse on the test set”. Both meanings were thought to be the same. This allows us to say that a model has overfitted the training set without ever testing it, by using the first definition.\nIn the age of deep learning, the two meanings are no longer the same, and so the meaning of “overfitting” is no longer clear. It should be assigned to one of them, not the other. I think it should be assigned to the second meaning, since the “over” in “overfit” implies it’s something undesirable.\nPerhaps the entire comment can be put in a footnote."
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-8",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-8",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 8",
    "text": "Chapter 8\n\npage 205. Because SGD is so essential to modern machine learning, I feel like it’d be best to say in a footnote: “To dispel any unwarranted hope, we note that this book is only going to use backpropagation exactly once: in deriving Equation (9.90).”\npage 244. I think the statement of “equivalence principle” is important enough that it should be properly defined, in a single sentence, on a separate line. Something like this:\n\n\nEquivalence principle: Every block of parameters at every layer should contribute equally to learning.\n\n\npage 244. I find the phrase “As we retroactively saw in §9.2” deeply confusing. If it is “retroactive” as in “In hindsight, …” then we did NOT see it back then, so we cannot say we “saw” it. Indeed, this proves it is mathematically impossible to “retroactively saw” anything. I suggest saying it like this instead:\n\nUsing this new perspective, please look back to §9.2, especially equation (9.43). At criticality, if we are to satisfy the equivalence principle, we must have \\[\n\\cdots \\sim \\lambda_b^{(\\ell-1)} \\sim \\lambda_W^{(\\ell-1)} A_2 K^{\\star}  \\sim \\lambda_b^{(\\ell)} \\sim \\lambda_W^{(\\ell)} A_2 K^{\\star}\n\\] that is,\n\\[\\cdots \\sim \\lambda_b^{(\\ell-1)} \\sim \\lambda_W^{(\\ell-1)}  \\sim \\lambda_b^{(\\ell)} \\sim \\lambda_W^{(\\ell)}\\]\nNow, suppose we train multiple MLP of the same width on the same dataset, but with different depths, we would have \\(\\Theta^{(L)} =(\\lambda_b + \\lambda_W A_2 K^*) L\\). Then, if we set \\[\n\\lambda_b = \\tilde \\lambda_b / L, \\quad \\lambda_W = \\tilde \\lambda_W / L\n\\] then \\(\\Theta^{(L)} = (\\tilde\\lambda_b + \\tilde\\lambda_W A_2 K^*)\\) regardless of depth \\(L\\). This is beneficial in practice as it allows us to tune learning rates \\(\\tilde\\lambda_b, \\tilde\\lambda_W\\) on a shallow and narrow network quickly, then pick the best setting, and use those to train a deep and wide network, while being reasonably certain that the learning rates would transfer correctly.\n\nIntuitively, this means that with the first gradient update step, the change in each of \\(b^{(l)}, W^{(l)}\\) contributed to changing the loss by an approximately equal portion of \\(\\Delta \\mathcal L_A/(2L)\\).\n\npage 245: Same comment applies for “we retroactively saw in §9.3”. I suggest this instead:\n\nUsing this new perspective, please look back to §9.3, especially equation (9.69). At criticality, if we are to satisfy the equivalence principle, the contribution of each layer’s weights and biases to the final NTK must be of order \\(1/L\\) : \\[\n\\lambda_b^{(l)} (l/L)^{p_\\perp} \\sim \\lambda_b^{(l)} \\frac 1l (l/L)^{p_\\perp} \\sim \\frac 1L\n\\]\n\n\nwhich can be satisfied by\n\\[\n\\lambda_b^{(\\ell)}=\\widetilde{\\lambda}_b\\left(\\frac{1}{\\ell}\\right)^{p_{\\perp}} L^{p_{\\perp}-1},\n\\quad \\lambda_W^{(\\ell)}=\\widetilde{\\lambda}_W \\left(\\frac{L}{\\ell}\\right)^{p_{\\perp}-1}\n\\]"
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-10",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-10",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 10",
    "text": "Chapter 10\nFor Section 10.3.2, I think this picture would help:\n\n\npage 279: I think (10.110) can be explained better as follows:\n\nWe have to fit a function \\((\\epsilon_1, \\epsilon_2) \\mapsto \\Theta_{\\epsilon_1, \\epsilon_2}\\). Write it as \\(f(x, y)\\). By Taylor expansion, \\[f(x,y) = f(0,0) + \\partial_x f(0,0) x  + \\partial_x f(0,0) x + \\frac 12 (\\partial_{xx} f(0,0) xx + \\partial_{yy} f(0,0) yy +2 \\partial_{xy} f(0,0) xy) + O(x^3,y^3)\\]\nIf we know that \\(f(x,y) = f(y,x)\\), then we have \\(\\partial_{x}f(0,0) =\\partial_{y} f(0,0)\\) and \\(\\partial_{xx}f(0,0) =\\partial_{yy} f(0,0)\\), so there are only 4 free coefficients. In particular, if we have a degree-2 polynomial \\(p(x,y)\\) that is equal to \\(f\\) for \\((x,y) = (\\pm 1, \\pm 1)\\), then we have \\[f(x,y) = p(x,y) + O(x^3, y^3)\\]"
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-11",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter-11",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter 11",
    "text": "Chapter 11\n\npage 312-313: (11.67) and (11.70) are trivial to evaluate if we use Dirac delta function: \\(\\sigma'' = (a_+ - a_-)\\delta\\)."
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#chapter",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Chapter ∞",
    "text": "Chapter ∞\nFor ∞.2 Training at Finite Width, I find calling \\(F\\) the “damping force” very puzzling. If we take the limit of continuous time, we will have something like\n\\[\n\\dot z(t) = -H z(t) + F(t)\n\\]\nIn this case, \\(F\\) is not damping. Indeed, without “damping”, the system would just exponentially decay to zero. If anything, \\(H\\) is the damping! I would rather call \\(F\\) the “driving” force.\nFor ∞.2 Training at Finite Width, it is useful to use matrix tensor product notation. In fact, a lot of it is really just high-school Olympiad methods in linear recurrence relation, but the heavy notation makes it hard to recognize.\nFor example, \\(X_{II}\\) can be written simply as \\[\nX_{II} = \\eta \\left(I^{\\otimes 2} - (I - \\eta \\tilde H)^{\\otimes 2}\\right)^{-1}\n\\] This allows us to simply evaluate:\n\\[\\begin{aligned}\n    b(t) &= \\eta \\left(\\sum_{s=0}^{t-1} ((I - \\eta \\tilde H)^s)^{\\otimes 2}\\right) \\epsilon^F(0)^{\\otimes 2} \\\\\n    &= \\eta \\left(I^{\\otimes 2} - (I - \\eta \\tilde H)^{\\otimes 2}\\right)^{-1}\\left(I^{\\otimes 2} - ((I - \\eta \\tilde H)^t)^{\\otimes 2}\\right) \\epsilon^F(0)^{\\otimes 2}\\\\\n    &= X_{II} ((z-y)^{\\otimes 2} - \\epsilon^F(t)^{\\otimes 2})\n    \\end{aligned}\\] and similarly for \\(c(t), X_{II}\\).\nEquation (∞.122) directly results in (∞.123), without any derivation: \\[z^I(t) = \\eta \\sum_s F(s) - \\hat H \\eta \\sum_s z^I(s) \\implies \\eta \\sum_s z^I(s) = \\hat H^{-1}\\left(\\eta \\sum_s F(s) - z^I(t)\\right)\\]"
  },
  {
    "objectID": "notes/roberts-yaida-hanin-2021-errata/index.html#errors",
    "href": "notes/roberts-yaida-hanin-2021-errata/index.html#errors",
    "title": "Suggestions and errata for The Principles of Deep Learning Theory",
    "section": "Errors",
    "text": "Errors\n\npage 213. (8.49) should be \\(\\sigma_{j;\\alpha_1}\\sigma_{j;\\alpha_2}\\).\n\n\n\npage 219, (8.71) has a mismatched bracket\n\n\n\npage 252, Eq (10.12) should have \\(i, i\\) not \\(i_1, i_2\\)\n\n\n\npage 280, (10.113) (10.114)\n\n\nConsider the special case where \\(y = z^{(L)}\\) at the beginning. If this is the case, then the network would learn nothing, so we should have \\(z^{(L)}(T) = z^{(L)}\\). This then requires the signs to be in the way I wrote it.\n\n\npage 281, (10.115), (10.116) needs to remove the factor of 4 as well.\npage 303, Eq (11.37) should have \\(\\delta_3\\)\n\n\n\npage 307, Eq (11.48) should use \\(G\\) instead of \\(K\\). At least, it would be less confusing since the rest of the chapter uses \\(G\\), even though \\(K\\) and \\(G\\) are defined to be equal.\n\n\n\npage 309, Eq (11.51)\n\n\n\npage 339\n\n\n\npage 364.\n\n\n\npage 365"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "projects/index.html#double-stroop",
    "href": "projects/index.html#double-stroop",
    "title": "Projects",
    "section": "",
    "text": "If you thought the Stroop test is hard, just wait until you have tried this.\nPlay online | Play offline"
  },
  {
    "objectID": "sketches/posts/web-design-notes/index.html",
    "href": "sketches/posts/web-design-notes/index.html",
    "title": "Notes on Web Design",
    "section": "",
    "text": "General references:"
  },
  {
    "objectID": "sketches/posts/web-design-notes/index.html#html",
    "href": "sketches/posts/web-design-notes/index.html#html",
    "title": "Notes on Web Design",
    "section": "HTML",
    "text": "HTML\nHTML (HyperText Markup Language) is the standard markup language for creating web pages.\nHTML was created by Tim Berners-Lee in 1989. The key metaphor for HTML is the “editing markup”, as follows: Back in the old days, authors would write or typewrite their document in the exact same font, from the first word to the last word. Then the document is sent to an editor, who would edit it by marking up the words, such as drawing squiggly lines, crossing things out, changing their font size, and writing other instructions for the type-setter (which back then meant someone who would take out types from a box and set them into the right ordering for the printing press).\nSo, one should think of an HTML document as starting with a plaintext of exactly the same format, from the first to the last word, then adding marks upon it.\n\n&lt;!DOCTYPE&gt;\nThe &lt;!DOCTYPE&gt; tag is used to declare the document type. It is usually like &lt;!DOCTYPE html&gt;, although there are rare variants, where instead of html, we have html -//w3c//dtd xhtml 1.0 transitional//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-transitional.dtd, html -//w3c//dtd xhtml 1.0 strict//en http://www.w3.org/tr/xhtml1/dtd/xhtml1-strict.dtd, etc.\nWell, if you don’t care much for the details, use &lt;!DOCTYPE html&gt; is always fine. If you do care, read on.\nThe xhtml thing came from early 2000s, where there was a movement to XML-everything. Instead of the poorly specified HTML, there would be XHTML, which can be checked for syntax, and compiled into an abstract syntax tree. Despite this, people just kept on using HTML and ignored XHTML.\nDespite the universal ambitions of XML, it is now in the land of old soldiers, who never die, but just fade away. * The SVG vector graphics format. * MathML, which is like LaTeX but in XML. * The RSS and Atom feeds, which… unfortunately, are also mostly legacy now. Who even use these nowadays? * The acronym AJAX, which stands for Asynchronous JavaScript and XML JSON. After looking at XML and JSON, I am quite glad this replacement happened.\n\n\nSemantic markup\nUse &lt;article&gt; to surround a block of article. There can be any number of articles in a single HTML file.\nThe id of tags must be unique within each HTML file.\n\n\nTips for fast loading\nUse &lt;img decoding=\"async\" loading=\"lazy\" ...&gt; for images, to allow the rest of the page to load before the images are loaded. To avoid Cumulative Layout Shift (CLS), I think something like\nimg {\n  max-width: 100%;\n  height: auto;\n}"
  },
  {
    "objectID": "sketches/posts/web-design-notes/index.html#punctuation",
    "href": "sketches/posts/web-design-notes/index.html#punctuation",
    "title": "Notes on Web Design",
    "section": "Punctuation",
    "text": "Punctuation\n\nComma\nThe comma separates parts with equal syntax roles.\nEqual noun-phrases.\n  I have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n\nEqual adjectives.\n  He is a strong, healthy man.\n\nAdjectives at unequal levels.\n* We stayed at an expensive, summer resort.\n  We stayed at an {expensive {summer resort}}.\nThey denote typed multisets.\nThey are typed multisets, as in:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=&gt;\nI have {(jujube tree, 2), (peach tree, 1)} in my backyard.\n\nThey are not typed sets, because:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=/=&gt;\nI have {jujube tree, peach tree} in my backyard.\n\nThey are not typed lists, because:\nI have 3 trees in my backyard: a jujube tree, a jujube tree, and a peach tree.\n=\nI have 3 trees in my backyard: a jujube tree, a peach tree, and a jujube tree.\nbut\n(jujube tree, jujube tree, peach tree) =/= (jujube tree, peach tree, jujube tree)\n\nThey are not untyped multisets, because:\n* I have in my backyard a jujube tree and sing.\n  Backyard = {(jujube tree, 1), (sing, 1)}\nThe “and” before the last item on the list seems to be necessary in speech, as in “Heads up! The list is about to be over!”, even though it is not necessary in writing.\nThe “Oxford comma” is a rather odd hack that allows a tiny amount of nesting. In fact, English does not have recursion, and 2 seems to be the deepest nesting we can get. According to Wikipedia, the most common arguments for/against the Oxford comma are: convention, disambiguation. As for convention, it cannot be argued with, only dealt with. As for disambiguation, I recommend using brackets.1\n1 It seems like a lot of weird English syntax is centered around an abhorrence of brackets. Like, I understand why brackets can get annoying (as anyone can see by looking at LISP code), but there is no reason to avoid brackets when you really need to disambiguate something. For example, instead of requiring the Oxford comma, why not just put in a freaking bracket? Don’t say that “oh you can’t speak brackets” like damn you you can’t speak commas either!No Oxford comma:\nTo my parents, Ayn Rand and God.\nTo my mother, Ayn Rand, and God.\n\nOxford comma:\nTo my parents, Ayn Rand, and God.\nTo my mother, Ayn Rand and God.\n\nMathematics:\nTo {my parents, Ayn Rand, God}.\nTo {my mother Ayn Rand, God}.\nTo {my mother, Ayn Rand, God}.\nFor clauses, commas are weird. There is no clear rule.\nEqual clauses.\n  He walked all the way home, and he shut the door.\n\nEqual clauses, though the second lost the pronoun.\n  I saw that she was busy, and [I] prepared to leave.\n\nDependent clause (except that-clause).\n  Because I could not stop for Death, He kindly stopped for me.\n\nNo comma between that-clause and main-clause, or vice versa.\n* We paused before a House, that seemed a swelling of the Ground\n  We paused before a House that seemed a swelling of the Ground\n* That is not dead, which can eternal lie.\n  That is not dead which can eternal lie.\nA pair of commas can replace a pair of parentheses, though there is no benefit, other than avoiding the wrath of English teachers.\nA comma, like an em-dash ---, can indicate pauses and stutters in quotations, such as “Daisy, Daisy… give me—e— your, ans, ser, do…”. This is an edge case and there are no fixed rules here. If you ever have to write something like this, use your imagination.\n\n\nQuotation\nEnglish quotation standards are extremely annoying, because while theoretically quotation is meant to be verbatim, it is anything but. I recommend verbatim quotation. It is consistent with usage in computer programming and formal logic. In particular, the English quotation method would be a syntax error in computer programming, and would also break the proofs of Gödel’s incompleteness theorems, which use the Quine quotation.\nFor nested quotations, alternate between double \" and single ' quotation marks. In other words, inside a level-\\(n\\) writing, use \" if \\(n \\equiv 0\\mod 2\\), and ' otherwise. It is context-free, and its quotient by {\" ~ '} is the Dyck language.\nHowever, if you need to pass the Turing test, then study carefully the following edge cases:\n* Did Hal say \"Good morning, Dave.\"?\n  Did Hal say, \"Good morning, Dave\"?\n\n* No, he said \"Where are you, Dave?\".\n  No, he said, \"Where are you, Dave?\"\n\n  To be perfectly exact, I heard \"Wh--- are you, Da---\".\n\n* \"cat\" is in lowercase.\n* \"Cat\" is in lowercase.\n* \"Cat\" is in uppercase.\n  \"Cat\" is capitalized.\n* In lowercase \"cat\" is.\n* \"cat\" is in lowercase, while \"CAT\" is in uppercase.\n  \"CAT\" is in uppercase, while \"cat\" is in lowercase.\n  What we cannot speak about we must pass over in silence.\n\n* \"sed\" (\"stream editor\") is a Unix utility that parses and transforms text.\n* \"Sed\" (\"stream editor\") is a Unix utility that parses and transforms text.\n  The Unix utility \"sed\" parses and transforms text.\nThe two senses of verbatim (verbatim verbal speech, verbatim written text) are inconsistent with the capitalization constraint:\nThis violates \"first letter capitalization\".\n* \"... 'Vim' stands for 'vi improved'. 'sed' stands for 'stream editor'. ...\" \n\nThis violates \"verbatim written text\".\n* \"... 'Vim' stands for 'vi improved'. 'Sed' stands for 'stream editor'. ...\"\n\nThis violates \"verbatim verbal speech\".\n* \"... 'Vim' stands for 'vi improved', whereas 'sed' stands for 'stream editor'. ...\"\n\nThis violates the constraint of grammar.\n* \"... 'Vim' stands for 'vi improved', 'sed' stands for 'stream editor'. ...\"\n\nThis works, but only by a dubious insertion.\n  \"... 'Vim' stands for 'vi improved'[, whereas] 'sed' stands for 'stream editor'. ...\"\n\nThis works, but just barely.\n  \"... 'Vim' stands for 'vi improved'; 'sed' stands for 'stream editor'. ...\"\n\nIt would not work if the two sentences are not \"closely related\".\n* \"... The Unix coreutils were written mostly in the 1970s; 'sed',\n       which stands for 'stream editor', was written by Lee McMahon in 1973. ...\"\n\n\nWhitespace\nFor most cases, the single whitespace works.\n (U+00A0 no-break space) says: “Do not break line here.”. Similarly for &NoBreak; (U+2060 word joiner). The opposite is &lt;wbr&gt; (U+200B zero width space), which says: “You can break line here.”.\n  (U+2007 figure space),   (U+2008 punctuation space), are only for numerical tabulations, to ensure alignment across rows.\nThe thin space and hair space are not used except by fastidious typographers.\n\n\nConnecting words\nUse - to connect two non-equal parts into a compound noun-phrase, except when one of the parts is an open compound, in which case, use --.\nUse -- to connect two equal parts into a compound noun-phrase.\n\npre-WWII, pre--World War Two, ex--Prime Minister, water-based solution, non--water-based solution, the anti-choice--anti-life debate\n\n\n\nInterruptions\nUse em-dash --- without spacing.\n\nI---no, YOU son of a---\n---and as I was saying---\n\n\n\nDeletions\nTo cut out letters or words by redaction, use the em-dash ---. Imagine that you wrote out the text normally, then you replace the section to be redacted with a single ---, and in this way you will find the right spacing. However, I recommend the black block character █, which looks just like real redactions on secret documents.\n\nThat f--- son of a b███!\n\nThe ellipsis symbol can be done either with ... or with a single … (U+2026 horizontal ellipsis). It is used with spacing on both ends, except that there is no space between it and the quotation mark.\n\n\"Consider this ... , and no more ... be said. ... and so on and so forth ...\", said Mr. ---.\n\n\n\nArithmetics\nIf you cannot use LaTeX, then you could use the hyphen - for minus sign, though it’s more correct to use −. Similarly, the letter x can work, though × is better."
  },
  {
    "objectID": "sketches/posts/philosophical-sketches/index.html",
    "href": "sketches/posts/philosophical-sketches/index.html",
    "title": "Philosophical Sketches",
    "section": "",
    "text": "There is a pattern of thinking that has annoyed me: “causal anthropocentrism”.\nFor example, the chairman of China said, “There is no such thing as the so-called Thucydides Trap in the world. But should major countries time and again make the mistakes of strategic miscalculation, they might create such traps for themselves.”\nIf a war between America and China happens, it would not be purely by human methods and human thinking. Such complex outcomes have complex causes, and human thinking is merely one part of many. Other causes include solar radiation, soil fertility, atmospheric circulation of water, secret patterns in the global flow of steel production, the relative abundance of uranium-235 in earth’s crust, and many many others.\nIf a war breaks out, the thoughts and behaviors of the modern humans is just one part of the cause. The ancestors are also responsible. The sun, the crust, the atmosphere, the immutable laws of mathematics and physics. They all come together.\nI like to joke that humans are no more intelligent than a logical AND gate, because they keep trying to find “the root cause” of things that do not have root causes. If A and B and C together causes D, then which of \\(\\{A, B, C\\}\\) is the root cause? The mathematical answer is simply “the root cause is undefined in such a situation”. The human answer is “the root cause is whichever is most amenable to human intervention”.\nAnd thus, if climate change is the problem, then human society is the root cause, not the atmospheric chemistry, or biological history of earth (without the carboniferous period, there would not be so much cheap coal for humans to burn up rapidly), etc, even if they are all parts of the complex cause of the complex effect of climate change.\nThis is what I call “causal anthropocentrism”. It might be the most practical, motivating, and moral, but it’s wrong. All root causes are egocentric. Without a personal viewpoint, there are only causes, but not any root cause.\n\n\n\nA common argument against moral nihilism is that moral nihilism leads to immoral behavior. This is so obviously question-begging that one wonders why good philosophers would still fall for this problem. I think this is a case where the very foundation of what constitutes a good argument is in doubt. The moral nihilists might be knapping their brains against materialist science, but the good philosophers are fundamentally doing good in this world.\n\nMoral nihilism leads to behavior X.\nX is immoral. (question-begging)\nIf a theory leads to immoral behavior, then it is incorrect. (crossing the is-ought chasm)\nTherefore, moral nihilism is incorrect.\n\nAnother attempt which goes like “It might be correct, but we shouldn’t talk about it because of the harms it would cause.”.\n\nMoral nihilism leads to behavior X.\nX is immoral. (question-begging)\nIf a theory leads to immoral behavior, then it should not be talked about. (question-begging)\nTherefore, moral nihilism should not be talked about.\n\nMaterialist science as a symbolic system has very well-known difficulties with meshing with commonsense symbolic system, especially morality. An entire philosophical industry (I call it “the word-lubricant industry”) called “compatibilism” sprang up to make them fit. Moral nihilists in making their arguments might simply be driven by the same grinding of the gears as the compatibilists.\nThe pragmatist’s modus ponens: You act, therefore you do believe in morality.\nThe moral nihilist’s modus tollens1: I act, but I do not believe in morality. Therefore, the fact that people act is no evidence that they believe in morality either.\n1 Not quite the classic modus tollens, because it is the major premise being rejected: P: Act; P → Q: If act, then believe; Q: Believe.\n\n\nIf the is-ought problem is a problem, then this means that for any X, even if X is moral, that doesn’t say anything about how we should act.\nPeople intuitively understand “is implies ought”. This is why research into genetic differences among social groups is so controversial. However, people also tend to claim that moral commitments are independent of material reality, lest they open themselves to\n\nexperimental disproof (it would be a catastrophe if one’s morality turns out to be unrealistic), and\nthe reproach of philosophers who go like “But you can’t get an is from an ought.”.\n\nThis essential tension is the unresolved original trauma of Enlightenment. One is reminded of Victorian scientists who vehemently denied sleepwalking, for it exonerates crimes committed in sleep: “The greatest sophistry of atheists can deny what is plain and common sense, viz that a murderer is a murderer in any other name.”\nThat is, going one way with “is does not imply ought” we end up with odd argument where the facts about the physiological basis of behavior are disregarded (such as in sleepwalking, or the fugue state), because “a murderer is a murderer”, but going the other way with “is imply ought”, and we get the well-known philosophical problems, as well as “science wars” where moral battles masquerade as scientific disputations.\nThe is-ought problem actually undermines the foundation of ethics, because even if moral realism is true, so that one can establish “X is moral”, that does not imply (cannot imply) that “X should be moral” or “I should do X”, a step that people seem to always take without thinking about how they have just stepped over the is-ought problem. I think “They have implicitly rejected the is-ought problem.” whenever I hear someone try to say that morality is inherently motivating, or how it is a mere matter of logical consistency to try to behave morally (i.e. Kant’s argument that radical evil is logically impossible).\n\n\n\nWhat is time, in computer programs? A program in source-code form is quite different from it in assembly-code form, which is again very different from it in runtime-form. I think at least 3 kinds of time is involved here.\n\nIn source-code form (at least, without GOTO and COMEFROM), time is the line number.\nIn assembly-code form (with so many GOTO and COMEFROM), time is less clear, perhaps assembly-code time is a bit tangled-up and hard to deal with.\nIn runtime-form, time is just the clock cycle.\n\nIn trying to program some Internet agents, I saw that time, for internet agents, is just a bit different than it is for humanoids. This is fascinating and a bit disturbing. Let me give an example.\nTypically, an Internet agent is run as a piece of source code running on a machine (“the server”) and communicating with some other program (usually from “the client” machine). By design, Internet communication protocols are by default stateless, and so, to adapt to such protocols, Internet agents are by default also amnesiac – so how do they remember who they are and what they were doing? They use continuation-passing style!\nThe analogy is this: whenever they finish one piece of communication, they write down everything that has happened so far in the communication, then forget about it. Later, if they are requested to pick up where they left off, the user would tell the agent to load up the record, and suddenly memory is restored.\nThe agent then communicates with the remote client. The client has also forgotten all about it, but no problem! The record is sent to the client, and suddenly the client is also reincarnated where they were.\nAnd just like that, the two amnesiacs reincarnated after being dead for several hours. They will die again after the conversation, only to reincarnate again if the future loads them up. So imagine this Memento-like scenario.\n\nYou are showering. How did you get there?? You hear a voice, “IF fridge contains sandwich THEN eat sandwich.” You leave shower and go to the fridge. It contains a sandwich. You eat it. You speak to the voice, “RETURN”.\nYou are in front of a fridge. You hear a voice: “IF fridge contains sandwich THEN eat sandwich”. You open it and realize that it has no sandwich. You speak to the voice, “ASYNC-WAIT; MAKE-SANDWICH”.\nYou are in front of a table. You hear a voice: “make a sandwich and put it into the fridge.” You go to the fridge and grab two slices of bread and a slice of cow juice, then made a sandwich and put it into the fridge. You speak to the voice, “RETURN”.\n\nInternet agents are functions, so they have arguments. The arguments are the “voices in the head”, and “speaking to the voice” is basically returning values. In the jargon, it’s not actually “return”, but “yield”, meaning “returning for now, but please reincarnate me later!”\nUsually, when an agent “yields”, it returns not only the computation result, but also a copy of everything that’s necessary to replicate its current state, so that if you need to call it up again to continue its work, you just have to send it the copy, and it will “reincarnate”.\nThis gives us a new interpretation of the (first) death of Jesus:\n\nNow from the sixth hour there was darkness over all the land unto the ninth hour. And about the ninth hour Jesus cried with a loud voice, saying, Eli, Eli, lama sabachthani? that is to say, My God, my God, why hast thou forsaken me? … Jesus, when he had cried again with a loud voice, YIELDED up the ghost.\nMatthew 27:45–50\n\nor\n\nThe essential Saltes of Animals may be so prepared and preserved, that an ingenious Man may have the whole Ark of Noah in his own Studie, and raise the fine Shape of an Animal out of its Ashes at his Pleasure; and by the lyke Method from the essential Saltes of humane Dust, a Philosopher may, without any criminal Necromancy, call up the Shape of any dead Ancestour from the Dust whereinto his Bodie has been incinerated.\nThe Case of Charles Dexter Ward, H. P. Lovecraft\n\n\n\n\nPatient: M.H., 57, Male, Philosopher\nPresenting Concern: M.H. was referred by concerned colleagues who reported a progressive inability to understand his philosophical discourse, which had become increasingly idiosyncratic and divorced from established empirical findings. Initially, they suspected a fluent aphasia, but the nature of his language disturbance suggested a more complex cognitive deficit.\nHistory: M.H. has a distinguished career in philosophy, known for his contributions to phenomenology and existentialism. His colleagues noted a gradual shift in his work over several years, marked by a growing rejection of scientific methods and an almost exclusive reliance on introspective, first-person phenomenological inquiry.\nExamination: M.H. presented as articulate and erudite, with no signs of traditional aphasia. He could understand normal English speech. However, his language, though conforming to English grammar and morphology, was highly idiosyncratic, exhibiting an unusual degree of nominalization and verbification. Fluent aphasia was suspected, until his colleague gradually recognized their semantics [all quotations from him are as interpreted by his colleague].\nWhen questioned about the apparent contradictions between his philosophical framework and established scientific knowledge, M.H. became dismissive, stating: “You fail to grasp the fundamental distinction between the Ontic and the Ontological! Trying to understand the life-world through the lens of your ‘objective’ science is a categorical error.” He insisted that his introspective method provided complete access to all facets of reality, asserting, “There are no electrons or synapses in the lifeworld. Eyes are not retinal screens—have you ever seen the space between the pixels?”.\nTesting: Neurological examination was unremarkable. However, given the possibility of an undetected neurological condition, an MRI scan was recommended. M.H. initially resisted, deeming it an “ontological transgression”, but eventually acquiesced. As predicted, the MRI results were normal.\nCaloric Reflex Test: A serendipitous breakthrough occurred during a caloric reflex test, conducted to assess vestibular function. The introduction of cold water into the ear canal, a mildly disorienting experience, seemed to trigger a shift in M.H.’s perception. For approximately 45 minutes post-test, he exhibited an abrupt lucidity, admitting his previous philosophical framework might be limited, and that empirical observation might be equally valuable as introspection. However, within an hour, he reverted to his previous state of anosognosia, with no recollection of the insights gained. A video recording of the lucid interval, shown to M.H. in his anosognosic state, elicited anger and accusations of “technological manipulation”.\nDiagnosis: We propose that M.H. presents with a novel form of anosognosia, tentatively termed “Philo’s Anosognosia.” This condition appears characterized by an unwavering conviction in the completeness of subjective introspection, coupled by fluent confabulation in rejecting evidence to the otherwise. It is yet unknown whether idiosyncratic language use is specific to this case as occurring in a working philosopher, or whether it is a common symptom for Philo’s Anosognosia.\nPrognosis: The prognosis for Philo’s Anosognosia remains unclear. In M.H.’s case, while his condition appears resistant to conventional interventions, it has not significantly impaired his professional output. Further research is needed to understand the underlying neurological mechanisms and explore potential treatment strategies, as well as whether this condition may be a spectral disorder. If so, we speculate that some academics may be on the sub-pathological end of the spectrum.\n\n\n\nIdea from @norvid_studies (2025-03-05).\nPierre Menard, the obscure 19th-century Swiss-French biologist (1832–1901), remains virtually unknown despite his remarkable undertaking. Despite a contemporary, Menard was neither strictly Darwinian nor Lamarckian, though elements of both theories harmonized within his work alongside orthogenesis and recapitulation.\nMenard’s oeuvre began with an observation in his 1872 treatise “Des Erreurs Transmises” [On Transmitted Errors]:\n\n… therefore, the teratological examples suggest to us that human reproduction is but an approximation of Mensch an sich, an incomplete and non-convergent circling around its central influence.\n\nFor Menard, each birth is mere imperfect replication. Other parts of the book reinforced the theme with various metaphors, such as “a Gutenberg press with half-worn fonts”, “a projection of a Grassmannian variety”, “pages typed out of a tilted Hansen-ball”, et cetera.\nIn his private journals, recently discovered in a sealed compartment of his writing bureau, Menard described recurring dreams in which he witnessed human forms unlike any known to science – often monstrous, but possessed of a certain form of perfection that left him a strange sense of claustrophobia upon waking. Previously home-stuck, he began taking long walks in wide spaces. On advice of the Société de Physique, Menard established his laboratory on Mont Salève. The local villagers spoke of his habit of standing perfectly still for hours facing the expansive vistas across Lake Geneva, holding his arms fast in curious bending patterns.\nHis sleep-cocoon, which he termed the “Berceuse Quaternique”, was perhaps his most curious invention. Made of polished wood and brass, with an intricate system of counterweights and three gimbals, it permitted him to sleep in all orientations enumerated in his treatise on “half-Hamilton rotations”. His journals record experiments in which he described dreams experienced while suspended in various orientations, often accompanied with diagrams. An example entry gives the reader the style of the thing:\n\n10/4/77, 17-β. App. mpts. Dbl. transitifs. denv. univ.? Conforme à l’extrap. de l’os crochu.\n\nThe entry might be translated as: “On 1877-04-10, I slept in the 17-β orientation. Metaplectic appendages. Doubly transitive. The universal denvelope? It fits the extrapolation of the hamate.”\nIn Menard’s theory, a blend of evolution and orthogenesis, all creatures, including the human, evolve, but not accidentally of the Darwinian kind. Instead, the necessary constraints of environment, as if by a million little nudges, guide evolution towards an ideal form. Unlike Darwin’s branching tree of contingent adaptation, Menard envisioned orthogenesis as a process of correction, each generation a slight adjustment along the “Quetelet lines”, after that indefatigable measurer of mankind, whose tables he referenced to extensively. For example, he noted the variance in the curvature of the hamate bone decreases at a log-linear rate, suggesting to him that after an infinity of generations, variance would completely vanish, resulting in the “convergent state”. Certain bones he predicted would disappear completely, while other “atavistic forms” would in fact flourish into complete forms, which he called “vestigial forms of the future”.\nHe first described the central concept of “dénveloppe universelle”2 in an 1876 monograph. Unlike the embryological development understood and preached by Haeckel, where a simple sphere blooms into forms most complex, Menard proposed that embryogenesis was actually a process of reduction into the limited physical and bio-chemical reality. He compared the wrinkling of a developing embryo to the distress of a camel forced through the eye of a needle. Birth defects are not isolated jokes of nature, but a matter of degrees that all of us suffer more or less.\n2 I had the idea of this from combining “universal enveloping algebra” which is both about high-dimensional symmetries and sounds appropriately grandiose yet technical, and the etymology of “develop”. Wiktionary says\n\nBorrowed from French développer, from Middle French desveloper, from Old French desveloper, from des- + voloper, veloper, vloper (“to wrap, wrap up”) (compare Italian sviluppare, Old Italian alternative form goluppare (“to wrap”)) from Vulgar Latin vloppō, wloppō (“to wrap”) ultimately from Proto-Germanic wrappaną, wlappaną (“to wrap, roll up, turn, wind”), from Proto-Indo-European *werb- (“to turn, bend”) [1]. Akin to Middle English wlappen (“to wrap, fold”) (Modern English lap (“to wrap, involve, fold”)), Middle English wrappen (“to wrap”), Middle Dutch lappen (“to wrap up, embrace”), dialectal Danish vravle (“to wind, twist”), Middle Low German wrempen (“to wrinkle, scrunch, distort”), Old English wearp (“warp”).\nThe word acquired its modern meaning from the 17th-century belief that an egg contains the animal in miniature and matures by growing larger and shedding its envelopes.\n\nFor this peculiar philosophy, he began donating yearly to the Hospitals for the Care of the Deformed, and he often signed his letters with “Socî malorum, compagnon de miseres.”. While his contemporaries regarded birth defects as pitiable, Menard regarded them as valuable data in “l’espace des possibilités morphologiques”. During a 1879 lecture at the Société Anatomique de Paris, he manipulated various brass knobs, rods, and strings that intersected a live-scale embryo model, explaining, “The projective configurations of the teratological indicatrices allow us to triangulate an extraspatial origin.”.\nAs his work progressed, Menard became increasingly reclusive, going down from Mont Salève monthly only for essential supplies and tithes to regional hospitals. Sometime no later than 1885, he began experimenting with fertilized chicken eggs to induce “déroulement contrôlé vers l’archétype” through chemical injections, isothermal chambers, and other interventions in a systematic attempt to discover and eliminate all forms of noise in the internal and external milieu. His notebooks from this period contain grid and spiral arrangements of detailed drawings of embryos exhibiting novel structural elements. The local priest, concerned by rumors of these experiments, visited Menard’s laboratory but left without comment, requesting shortly thereafter a transfer to another parish.\nThe fire that consumed Menard’s laboratory on 1891-02-17 originated in his experimental chamber, according to the official report. Neighbors reported seeing unusual lights and hearing what one described as “a sound like glass singing” in the hours before the blaze. Among the few items recovered was a copper cylinder sealed with a material analysts could not identify, containing what Menard had described in his final journal entry as “exemple préliminaire d’un être humain correctement formé”."
  },
  {
    "objectID": "sketches/posts/philosophical-sketches/index.html#brief-notes",
    "href": "sketches/posts/philosophical-sketches/index.html#brief-notes",
    "title": "Philosophical Sketches",
    "section": "",
    "text": "There is a pattern of thinking that has annoyed me: “causal anthropocentrism”.\nFor example, the chairman of China said, “There is no such thing as the so-called Thucydides Trap in the world. But should major countries time and again make the mistakes of strategic miscalculation, they might create such traps for themselves.”\nIf a war between America and China happens, it would not be purely by human methods and human thinking. Such complex outcomes have complex causes, and human thinking is merely one part of many. Other causes include solar radiation, soil fertility, atmospheric circulation of water, secret patterns in the global flow of steel production, the relative abundance of uranium-235 in earth’s crust, and many many others.\nIf a war breaks out, the thoughts and behaviors of the modern humans is just one part of the cause. The ancestors are also responsible. The sun, the crust, the atmosphere, the immutable laws of mathematics and physics. They all come together.\nI like to joke that humans are no more intelligent than a logical AND gate, because they keep trying to find “the root cause” of things that do not have root causes. If A and B and C together causes D, then which of \\(\\{A, B, C\\}\\) is the root cause? The mathematical answer is simply “the root cause is undefined in such a situation”. The human answer is “the root cause is whichever is most amenable to human intervention”.\nAnd thus, if climate change is the problem, then human society is the root cause, not the atmospheric chemistry, or biological history of earth (without the carboniferous period, there would not be so much cheap coal for humans to burn up rapidly), etc, even if they are all parts of the complex cause of the complex effect of climate change.\nThis is what I call “causal anthropocentrism”. It might be the most practical, motivating, and moral, but it’s wrong. All root causes are egocentric. Without a personal viewpoint, there are only causes, but not any root cause.\n\n\n\nA common argument against moral nihilism is that moral nihilism leads to immoral behavior. This is so obviously question-begging that one wonders why good philosophers would still fall for this problem. I think this is a case where the very foundation of what constitutes a good argument is in doubt. The moral nihilists might be knapping their brains against materialist science, but the good philosophers are fundamentally doing good in this world.\n\nMoral nihilism leads to behavior X.\nX is immoral. (question-begging)\nIf a theory leads to immoral behavior, then it is incorrect. (crossing the is-ought chasm)\nTherefore, moral nihilism is incorrect.\n\nAnother attempt which goes like “It might be correct, but we shouldn’t talk about it because of the harms it would cause.”.\n\nMoral nihilism leads to behavior X.\nX is immoral. (question-begging)\nIf a theory leads to immoral behavior, then it should not be talked about. (question-begging)\nTherefore, moral nihilism should not be talked about.\n\nMaterialist science as a symbolic system has very well-known difficulties with meshing with commonsense symbolic system, especially morality. An entire philosophical industry (I call it “the word-lubricant industry”) called “compatibilism” sprang up to make them fit. Moral nihilists in making their arguments might simply be driven by the same grinding of the gears as the compatibilists.\nThe pragmatist’s modus ponens: You act, therefore you do believe in morality.\nThe moral nihilist’s modus tollens1: I act, but I do not believe in morality. Therefore, the fact that people act is no evidence that they believe in morality either.\n1 Not quite the classic modus tollens, because it is the major premise being rejected: P: Act; P → Q: If act, then believe; Q: Believe.\n\n\nIf the is-ought problem is a problem, then this means that for any X, even if X is moral, that doesn’t say anything about how we should act.\nPeople intuitively understand “is implies ought”. This is why research into genetic differences among social groups is so controversial. However, people also tend to claim that moral commitments are independent of material reality, lest they open themselves to\n\nexperimental disproof (it would be a catastrophe if one’s morality turns out to be unrealistic), and\nthe reproach of philosophers who go like “But you can’t get an is from an ought.”.\n\nThis essential tension is the unresolved original trauma of Enlightenment. One is reminded of Victorian scientists who vehemently denied sleepwalking, for it exonerates crimes committed in sleep: “The greatest sophistry of atheists can deny what is plain and common sense, viz that a murderer is a murderer in any other name.”\nThat is, going one way with “is does not imply ought” we end up with odd argument where the facts about the physiological basis of behavior are disregarded (such as in sleepwalking, or the fugue state), because “a murderer is a murderer”, but going the other way with “is imply ought”, and we get the well-known philosophical problems, as well as “science wars” where moral battles masquerade as scientific disputations.\nThe is-ought problem actually undermines the foundation of ethics, because even if moral realism is true, so that one can establish “X is moral”, that does not imply (cannot imply) that “X should be moral” or “I should do X”, a step that people seem to always take without thinking about how they have just stepped over the is-ought problem. I think “They have implicitly rejected the is-ought problem.” whenever I hear someone try to say that morality is inherently motivating, or how it is a mere matter of logical consistency to try to behave morally (i.e. Kant’s argument that radical evil is logically impossible).\n\n\n\nWhat is time, in computer programs? A program in source-code form is quite different from it in assembly-code form, which is again very different from it in runtime-form. I think at least 3 kinds of time is involved here.\n\nIn source-code form (at least, without GOTO and COMEFROM), time is the line number.\nIn assembly-code form (with so many GOTO and COMEFROM), time is less clear, perhaps assembly-code time is a bit tangled-up and hard to deal with.\nIn runtime-form, time is just the clock cycle.\n\nIn trying to program some Internet agents, I saw that time, for internet agents, is just a bit different than it is for humanoids. This is fascinating and a bit disturbing. Let me give an example.\nTypically, an Internet agent is run as a piece of source code running on a machine (“the server”) and communicating with some other program (usually from “the client” machine). By design, Internet communication protocols are by default stateless, and so, to adapt to such protocols, Internet agents are by default also amnesiac – so how do they remember who they are and what they were doing? They use continuation-passing style!\nThe analogy is this: whenever they finish one piece of communication, they write down everything that has happened so far in the communication, then forget about it. Later, if they are requested to pick up where they left off, the user would tell the agent to load up the record, and suddenly memory is restored.\nThe agent then communicates with the remote client. The client has also forgotten all about it, but no problem! The record is sent to the client, and suddenly the client is also reincarnated where they were.\nAnd just like that, the two amnesiacs reincarnated after being dead for several hours. They will die again after the conversation, only to reincarnate again if the future loads them up. So imagine this Memento-like scenario.\n\nYou are showering. How did you get there?? You hear a voice, “IF fridge contains sandwich THEN eat sandwich.” You leave shower and go to the fridge. It contains a sandwich. You eat it. You speak to the voice, “RETURN”.\nYou are in front of a fridge. You hear a voice: “IF fridge contains sandwich THEN eat sandwich”. You open it and realize that it has no sandwich. You speak to the voice, “ASYNC-WAIT; MAKE-SANDWICH”.\nYou are in front of a table. You hear a voice: “make a sandwich and put it into the fridge.” You go to the fridge and grab two slices of bread and a slice of cow juice, then made a sandwich and put it into the fridge. You speak to the voice, “RETURN”.\n\nInternet agents are functions, so they have arguments. The arguments are the “voices in the head”, and “speaking to the voice” is basically returning values. In the jargon, it’s not actually “return”, but “yield”, meaning “returning for now, but please reincarnate me later!”\nUsually, when an agent “yields”, it returns not only the computation result, but also a copy of everything that’s necessary to replicate its current state, so that if you need to call it up again to continue its work, you just have to send it the copy, and it will “reincarnate”.\nThis gives us a new interpretation of the (first) death of Jesus:\n\nNow from the sixth hour there was darkness over all the land unto the ninth hour. And about the ninth hour Jesus cried with a loud voice, saying, Eli, Eli, lama sabachthani? that is to say, My God, my God, why hast thou forsaken me? … Jesus, when he had cried again with a loud voice, YIELDED up the ghost.\nMatthew 27:45–50\n\nor\n\nThe essential Saltes of Animals may be so prepared and preserved, that an ingenious Man may have the whole Ark of Noah in his own Studie, and raise the fine Shape of an Animal out of its Ashes at his Pleasure; and by the lyke Method from the essential Saltes of humane Dust, a Philosopher may, without any criminal Necromancy, call up the Shape of any dead Ancestour from the Dust whereinto his Bodie has been incinerated.\nThe Case of Charles Dexter Ward, H. P. Lovecraft\n\n\n\n\nPatient: M.H., 57, Male, Philosopher\nPresenting Concern: M.H. was referred by concerned colleagues who reported a progressive inability to understand his philosophical discourse, which had become increasingly idiosyncratic and divorced from established empirical findings. Initially, they suspected a fluent aphasia, but the nature of his language disturbance suggested a more complex cognitive deficit.\nHistory: M.H. has a distinguished career in philosophy, known for his contributions to phenomenology and existentialism. His colleagues noted a gradual shift in his work over several years, marked by a growing rejection of scientific methods and an almost exclusive reliance on introspective, first-person phenomenological inquiry.\nExamination: M.H. presented as articulate and erudite, with no signs of traditional aphasia. He could understand normal English speech. However, his language, though conforming to English grammar and morphology, was highly idiosyncratic, exhibiting an unusual degree of nominalization and verbification. Fluent aphasia was suspected, until his colleague gradually recognized their semantics [all quotations from him are as interpreted by his colleague].\nWhen questioned about the apparent contradictions between his philosophical framework and established scientific knowledge, M.H. became dismissive, stating: “You fail to grasp the fundamental distinction between the Ontic and the Ontological! Trying to understand the life-world through the lens of your ‘objective’ science is a categorical error.” He insisted that his introspective method provided complete access to all facets of reality, asserting, “There are no electrons or synapses in the lifeworld. Eyes are not retinal screens—have you ever seen the space between the pixels?”.\nTesting: Neurological examination was unremarkable. However, given the possibility of an undetected neurological condition, an MRI scan was recommended. M.H. initially resisted, deeming it an “ontological transgression”, but eventually acquiesced. As predicted, the MRI results were normal.\nCaloric Reflex Test: A serendipitous breakthrough occurred during a caloric reflex test, conducted to assess vestibular function. The introduction of cold water into the ear canal, a mildly disorienting experience, seemed to trigger a shift in M.H.’s perception. For approximately 45 minutes post-test, he exhibited an abrupt lucidity, admitting his previous philosophical framework might be limited, and that empirical observation might be equally valuable as introspection. However, within an hour, he reverted to his previous state of anosognosia, with no recollection of the insights gained. A video recording of the lucid interval, shown to M.H. in his anosognosic state, elicited anger and accusations of “technological manipulation”.\nDiagnosis: We propose that M.H. presents with a novel form of anosognosia, tentatively termed “Philo’s Anosognosia.” This condition appears characterized by an unwavering conviction in the completeness of subjective introspection, coupled by fluent confabulation in rejecting evidence to the otherwise. It is yet unknown whether idiosyncratic language use is specific to this case as occurring in a working philosopher, or whether it is a common symptom for Philo’s Anosognosia.\nPrognosis: The prognosis for Philo’s Anosognosia remains unclear. In M.H.’s case, while his condition appears resistant to conventional interventions, it has not significantly impaired his professional output. Further research is needed to understand the underlying neurological mechanisms and explore potential treatment strategies, as well as whether this condition may be a spectral disorder. If so, we speculate that some academics may be on the sub-pathological end of the spectrum.\n\n\n\nIdea from @norvid_studies (2025-03-05).\nPierre Menard, the obscure 19th-century Swiss-French biologist (1832–1901), remains virtually unknown despite his remarkable undertaking. Despite a contemporary, Menard was neither strictly Darwinian nor Lamarckian, though elements of both theories harmonized within his work alongside orthogenesis and recapitulation.\nMenard’s oeuvre began with an observation in his 1872 treatise “Des Erreurs Transmises” [On Transmitted Errors]:\n\n… therefore, the teratological examples suggest to us that human reproduction is but an approximation of Mensch an sich, an incomplete and non-convergent circling around its central influence.\n\nFor Menard, each birth is mere imperfect replication. Other parts of the book reinforced the theme with various metaphors, such as “a Gutenberg press with half-worn fonts”, “a projection of a Grassmannian variety”, “pages typed out of a tilted Hansen-ball”, et cetera.\nIn his private journals, recently discovered in a sealed compartment of his writing bureau, Menard described recurring dreams in which he witnessed human forms unlike any known to science – often monstrous, but possessed of a certain form of perfection that left him a strange sense of claustrophobia upon waking. Previously home-stuck, he began taking long walks in wide spaces. On advice of the Société de Physique, Menard established his laboratory on Mont Salève. The local villagers spoke of his habit of standing perfectly still for hours facing the expansive vistas across Lake Geneva, holding his arms fast in curious bending patterns.\nHis sleep-cocoon, which he termed the “Berceuse Quaternique”, was perhaps his most curious invention. Made of polished wood and brass, with an intricate system of counterweights and three gimbals, it permitted him to sleep in all orientations enumerated in his treatise on “half-Hamilton rotations”. His journals record experiments in which he described dreams experienced while suspended in various orientations, often accompanied with diagrams. An example entry gives the reader the style of the thing:\n\n10/4/77, 17-β. App. mpts. Dbl. transitifs. denv. univ.? Conforme à l’extrap. de l’os crochu.\n\nThe entry might be translated as: “On 1877-04-10, I slept in the 17-β orientation. Metaplectic appendages. Doubly transitive. The universal denvelope? It fits the extrapolation of the hamate.”\nIn Menard’s theory, a blend of evolution and orthogenesis, all creatures, including the human, evolve, but not accidentally of the Darwinian kind. Instead, the necessary constraints of environment, as if by a million little nudges, guide evolution towards an ideal form. Unlike Darwin’s branching tree of contingent adaptation, Menard envisioned orthogenesis as a process of correction, each generation a slight adjustment along the “Quetelet lines”, after that indefatigable measurer of mankind, whose tables he referenced to extensively. For example, he noted the variance in the curvature of the hamate bone decreases at a log-linear rate, suggesting to him that after an infinity of generations, variance would completely vanish, resulting in the “convergent state”. Certain bones he predicted would disappear completely, while other “atavistic forms” would in fact flourish into complete forms, which he called “vestigial forms of the future”.\nHe first described the central concept of “dénveloppe universelle”2 in an 1876 monograph. Unlike the embryological development understood and preached by Haeckel, where a simple sphere blooms into forms most complex, Menard proposed that embryogenesis was actually a process of reduction into the limited physical and bio-chemical reality. He compared the wrinkling of a developing embryo to the distress of a camel forced through the eye of a needle. Birth defects are not isolated jokes of nature, but a matter of degrees that all of us suffer more or less.\n2 I had the idea of this from combining “universal enveloping algebra” which is both about high-dimensional symmetries and sounds appropriately grandiose yet technical, and the etymology of “develop”. Wiktionary says\n\nBorrowed from French développer, from Middle French desveloper, from Old French desveloper, from des- + voloper, veloper, vloper (“to wrap, wrap up”) (compare Italian sviluppare, Old Italian alternative form goluppare (“to wrap”)) from Vulgar Latin vloppō, wloppō (“to wrap”) ultimately from Proto-Germanic wrappaną, wlappaną (“to wrap, roll up, turn, wind”), from Proto-Indo-European *werb- (“to turn, bend”) [1]. Akin to Middle English wlappen (“to wrap, fold”) (Modern English lap (“to wrap, involve, fold”)), Middle English wrappen (“to wrap”), Middle Dutch lappen (“to wrap up, embrace”), dialectal Danish vravle (“to wind, twist”), Middle Low German wrempen (“to wrinkle, scrunch, distort”), Old English wearp (“warp”).\nThe word acquired its modern meaning from the 17th-century belief that an egg contains the animal in miniature and matures by growing larger and shedding its envelopes.\n\nFor this peculiar philosophy, he began donating yearly to the Hospitals for the Care of the Deformed, and he often signed his letters with “Socî malorum, compagnon de miseres.”. While his contemporaries regarded birth defects as pitiable, Menard regarded them as valuable data in “l’espace des possibilités morphologiques”. During a 1879 lecture at the Société Anatomique de Paris, he manipulated various brass knobs, rods, and strings that intersected a live-scale embryo model, explaining, “The projective configurations of the teratological indicatrices allow us to triangulate an extraspatial origin.”.\nAs his work progressed, Menard became increasingly reclusive, going down from Mont Salève monthly only for essential supplies and tithes to regional hospitals. Sometime no later than 1885, he began experimenting with fertilized chicken eggs to induce “déroulement contrôlé vers l’archétype” through chemical injections, isothermal chambers, and other interventions in a systematic attempt to discover and eliminate all forms of noise in the internal and external milieu. His notebooks from this period contain grid and spiral arrangements of detailed drawings of embryos exhibiting novel structural elements. The local priest, concerned by rumors of these experiments, visited Menard’s laboratory but left without comment, requesting shortly thereafter a transfer to another parish.\nThe fire that consumed Menard’s laboratory on 1891-02-17 originated in his experimental chamber, according to the official report. Neighbors reported seeing unusual lights and hearing what one described as “a sound like glass singing” in the hours before the blaze. Among the few items recovered was a copper cylinder sealed with a material analysts could not identify, containing what Menard had described in his final journal entry as “exemple préliminaire d’un être humain correctement formé”."
  },
  {
    "objectID": "sketches/posts/philosophical-sketches/index.html#jokes",
    "href": "sketches/posts/philosophical-sketches/index.html#jokes",
    "title": "Philosophical Sketches",
    "section": "Jokes",
    "text": "Jokes\n\nUnclassified\n\nNature, and its causal structure, lies in darkness. / God said, “Try gradient descent!” and now you have two darknesses.\nAlexander Pope, Epitaph: Intended for Sir Neural Nets\n\nPraise the extremists, for they expand the convex hull of possibilities.\nIn high-dimensional space, the way of the golden mean is almost useless. – But in ultrametric space, it is entirely useless.3\n3 In high dimensions, most probability mass of a gaussian distribution (and most other ones) are concentrated on a thin shell. Exponentially few points live in the middle. In an ultrametric space, every point inside a ball is equidistant to its edges, meaning that the “average” point is undefined. Also, a set of random sampled points in \\(\\R^n\\) for \\(n\\to \\infty\\) look pretty ultrametric, with the characteristic fractal structure. (Aggarwal, Hinneburg, and Keim 2001; Zubarev 2014) This probably has something to do with spin glass fractal energy landscapes.A little extremism once in a while never hurts anybody.\nHeidegger puts the lived world of ontological Dasein before the objective world of ontic objects. Cool. I will take him seriously if he could discover atomic physics phenomenologically.\nKant managed to push God out of the front door (by Critique of Pure Reason) only to lead Him back through the window (by Critique of Practical Reason).\nIf everyone becomes a judge, then there wouldn’t be any criminal left to judge. Therefore, by the Categorical Imperative…4\n4 What if everyone acted on the principle of becoming a good judge to eliminate crime? Then people aren’t going to be criminals (because a good judge is by definition not going to be a criminal), thus defeating the very purpose of the maxim. One might say that some people might still want to become a judge yet commit crimes by akrasia, but akrasia actually defeats all applications of the CI. Consider the maxim where you say “It’s okay to break oaths.”. By the standard CI, it defeats its own purpose, but by akrasia, people might still keep oaths just because their deontological passions would overpower their consequentialist souls. Thus, the Categorical Imperative either proves too much or nothing at all.I like to go to the graveyard and shout at every tombstone: “If you are so clever, why aren’t you rich alive?”\nContinental philosophers have gone postal: post-human, post-modern, post-gender, post-intentional…\nWeizenbaum rejects D-a-r-w-i-n-i-s-m:\n\nI have in mind also the teaching urged on us by some leaders of the AI community that there is nothing unique about the human species, that in fact, the embrace of the illusion of human uniqueness amounts to a kind of species prejudice and is unworthy of enlightened intellectuals. If we find nothing abhorrent in the use of artificially sustained, disembodied animal brains as computer components, and if there is nothing that uniquely distinguishes the human species from animal species, then – need I spell out where that idea leads?\n— Joseph Weizenbaum, author of ELIZA\n\n\nWise person: Happiness equals material consumption divided by desire\nDepressed person: But I don’t want anything!\nMathematician: Well, instead of one-point compactification of the utility line…\n\nFrequentist proof of ontological nihilism \\((p &lt; 10^{-9})\\): Let the null hypothesis be “there is something rather than nothing”. The probability of my existence is lower than one in a billion, as any accidental change in the past billion years would have caused me to not have been born.\nDerridean deconstruction is an attempt to solve the information-atom problem: the mind-body problem for the information age. If you get one apple, then a copy, now you have two apples. If you get an IP packet, then a copy, now you still have just one. Thus we have the information-atom problem: You say that information is carried in the atoms, which acts upon the atoms, but how is it possible for information and atoms to interact if they are so different? Derridean deconstruction and its method of différance simply denies that copyable information exists, thus there is no such thing as the information-atom problem. Difference (atoms) is primary, and then identity (information) appears as an approximation.\nThe most annoying part of the Hero’s Journey is that it has an unjustified symmetry breaking. Every formalization of the Hero’s Journey describes it as a directed cyclic graph with 17 points. However, every point in a cyclic graph may serve as the “origin”. The internal structure of the Journey does not allow treating any one of the 17 points as more special than the others, because they are all essential. This is the cyclic symmetry. In jargon, the Hero’s Journey is an additive torsor, not a group, since the origin is undefined. We can count from any node forwards. In 17 steps, we return, but this is true for any node, not just the “home” node.\nSomeone just told me that the Hero’s Journey is not actually a cycle. Instead, it is a directed linear graph with 17 points. Well good to know! Then people should stop drawing it as a circle!\nSomeone just told me that it’s drawn as a circle because the 0th point and the 16th point are the most similar pair of points. We’ll see about that after I do a metric embedding through T5-XXL.\n\n\nThe Jurisprudent Pathway\nCurrently, environmental projection is delaying the progress of technological control of Nature. Many of these are fundamentally based on an understanding of Nature as a person with legal rights. This suggests a way to defeat this, which I call the Jurisprudent Pathway.\n\nGive Nature juridic personhood.\nSue Nature for numerous violations, including, but not limited to:\n\n\nGeneral child neglect.\nBioterroristic release of insecure bioforms into the environment.\nGenetic engineering unapproved by the FDA.\nPracticing ableist eugenics under the guise of “purifying evolution”.\nPromethean arrogance: creating new species when she should have protected what we already have here and now.\n…\n\n\nWin.\nPut her children into child protection services. Put her on racks.\nNow we are free to use her as we wish.\n\nThe Jurisprudent Pathway seems to be a neglected cause for bio-inspired effective acceleration, as well as heaven engineers who need to sterilize some megafauna in order to reduce suffering in Nature, but are stopped by people who believe Nature has the right to remain undisturbed. This is definitely a neglected cause for collaboration here between e/acc and e/alt.\n\nIf you meet the Buddha on the road, kill him.\n\n\n\nearth+\nIn humanity+, the modern Homo sapiens is a poor approximation of true humanity. In the earth+ idea, every species has been done wrong, and must be fixed. One way to find the ideal specimen of a species: sample the genome and construct the most median genome. Every base pair is just the most common variant in the species. This individual will probably be quite healthy and will remove most of the harmful variants.\n[insert cover of a textbook titled “Biology Done Right”]\nStart with the C elegans. They have the smallest animal genome at 20Mb, are unisexual, and sequenced to death.\nConstruct the most median C elegans. We predict that if we do that, we will C an elegans that is more elegant than any elegans has ever eleganted.\n\n\nQualia is the last refuge of a God\n\nNew Struggles–After the Buddha was dead, his shadow was still shown for centuries in a cave—a tremendous, gruesome shadow. God is dead; but given the way of men, there may still be caves for thousands of years in which his shadow will be shown.—And we—we still have to defeat his shadow too.\nThe Gay Science\n\n\nLaplace went in state to Napoleon to present a copy of his work, and the following account of the interview is well authenticated, and so characteristic of all the parties concerned that I quote it in full. Someone had told Napoleon that the book contained no mention of qualia; Napoleon, who was fond of putting embarrassing questions, received it with the remark, ‘M. Laplace, they tell me you have written this large book on the system of the apparent world, and have never even mentioned the secret world we carry inside each of us.’ Laplace, who, though the most supple of politicians, was as stiff as a martyr on every point of his philosophy, drew himself up and answered bluntly, Je n’avais pas besoin de cette hypothèse-là. [“I had no need of that hypothesis.”] Napoleon, greatly amused, told this reply to Lagrange, who exclaimed, Ah! c’est une belle hypothèse; ça explique beaucoup de choses. [“Ah, it is a fine hypothesis; it explains many things.”]\nApocryphal story about Pierre-Simon Laplace that I just made up\n\n\n“Where have people gone?” he cried. “I shall tell you. We have killed ourselves - you and I. We are our murderers. But how have we done this? How were we able to delete our own feelings? Who gave us the sponge to wipe away our microcosm? What did we do when we unchained the earth from its sun? Whither is it moving now? Whither are we moving now? Away from all suns? Are we not perpetually falling? Backward, sideward, forward, in all directions? Is there any up or down left? Are we not straying as through an infinite nothing? Are we not just the breath of empty space? How do we explain lanterns when eyes only watch but do not see? Do we not walk into the trashcan where we belong? Do we not smell anything yet of our souls’ decomposition? Souls too decompose. We are dead. We remain dead. And we have killed ourselves…\nParody of the Madman, The Gay Science\n\nA common rejection of eliminativism states that it is self-defeating, because beliefs require consciousness. I consider it as merely another instance of the heresy of Occasionalism. I propose the following:\nPhenomenal occasionalism is a philosophical doctrine about causation which says that material substances cannot be efficient causes of beliefs. Instead, all beliefs are taken to be caused directly by qualia. The doctrine states that the illusion of efficient causation between material implications arises out of qualias causing of one proposition after another.\n\n\nLiterary criticism is the last refuge of a Humanism\nHumanities people keep interpreting everything as really about human emotions, or a metaphor thereof, or some kind of structural support. For example, in the literary analysis of Neon Genesis Evangelion, the “AT field” is a metaphor about social anxiety, and the use of Lurianic kabbalah is reduced to a structural support (or less charitably, a “flavor text”) for the story about human emotions. Similar comments are true for absolutely every movie and novel that has ever been emitted by humans.\nBut I think it is exactly backwards. In fact, if you look carefully at them, you will realize that nothing about the human are truly about humans, and everything reduces to either a metaphor about computers and mechanics, or a structural support for discourse on computers and mechanics. For example, consciousness is a structural support for linear narrative, and linear narrative is a metaphor for the master clock of CPUs. Breakup is a metaphor for the dissipative mechanics of non-newtonian fluid, and so on. Fundamentally, this is because human nature is empty, so it is impossible for anything to be truly about humans. In this sense, traditional literary criticism is just a metaphor for the dangling pointer problem.\nIt is much like those Christians who reduce all natural phenomena to some kind of educational lesson planted by God. In medieval times, there were no biological textbooks, only “bestiaries”. A bestiary often describes various animal behaviors and appearances, but immediately follows up on it with a “And the moral of the story is…”. The most often quoted example would be the pelican, which was supposedly a bird that can peck its breasts until it bleeds (the technical term is “vulning”), and the blood would either resurrect its dead babies, or feed its hungry babies. In either case, it is an obvious reference to the legend of Jesus Christ.\n\nIsidore of Seville [7th century CE] (Etymologies, Book 12, 7:26): The pelican [pelicanus] is an Egyptian bird inhabiting the solitary places of the river Nile, whence it takes its name, for Egypt is called canopos. It is reported, if it may be true, that this bird kills its offspring, mourns them for three days, and finally wounds itself and revives its children by sprinkling them with its own blood.\n– The Etymologies of Isidore of Seville, Isidore of Seville, S. A. Barney, W. J. Lewis, J. A. Beach, O. Berghof, ed. and trans. Cambridge, UK: Cambridge University Press, 2006, 2009\n\nIn our modern understanding, the medieval bestiaries show that the authors were improperly humanizing/Christianizing nature, but in the medieval understanding, the bestiaries reveal the deep meaning behind mundane reality, planted there by God, and the moderns have denatured Nature, willfully turned our eyes away from Nature, or Nature’s God, in our drive towards a mechanistic understanding of it. Our tendons have hardened into steel cables, and our eyes squinted into pinhole cameras, and we have turned our backs from God.\nI am not serious about reducing traditional literary criticism to “just a metaphor for the dangling pointer problem”. But I do argue that humanities, especially traditional literary criticism (the most human of all humanities), fail the naturalistic challenge: How can human emotions and meaning be the end-point of all interpretation, when humanity is just one part in a physical system? Isn’t it ridiculous to interpret stories as if they have true meanings that mysteriously always ends up in one of those human psychological phenomena? It is almost as ridiculous as Christians finding metaphors for Christ in every species, or me interpreting people as just bipedal metaphors for phenomena in computers and mechanics.\nPeople are not usually “about” computers or mechanics, but neither are stories usually “about” people. Humanization/demechanization is as justified as mechanization/dehumanization. Both have their uses as information-theoretic tools. Traditional humanities have spent a thousand years anthropomorphizing biohumans. It is high time to start mechanizing the humanities. I am not going as far as Friedrich Kittler’s “Austreibung des Geistes aus den Geisteswissenschaften” [“Driving out the human from Humanities”],5 since anthropomorphism seems like a useful effective theory, or a kind of intuitive renormalization theory. However surely it’s time to start trying some nonhuman models for the study of literature.\n5 In German, “Geist” can mean “ghost” or “spirit”, and “Geisteswissenschaften” [“humanities”] would mean more literally “spirit sciences”. So a direct translation would be “Driving out the spirits from the spirit-sciences”.There are good reasons to try dehumanized models of biohumans, because it is already true in many practices! Advertising treats people as simple RL agents. Microeconomics treats people as a \\(\\max\\) function. Neurobiology treats people as a complicated mess far knottier than the most humanistic human model that has ever been humaniated. We have many dehumanized working models of biohumans already, and the humanities people needs to be gradient-updated on this account.\n\n\nThe Author is Dead\nPARIS, FRANCE - In a twist that has left the literary world both baffled and amused, the French philosopher Roland Barthes was recently reconstituted through an experimental “cold upload”, where his writings and recordings were used to digitally reconstruct his mind, and immediately announced his intention to weave a definitive explanation of what he “really meant” by his famous “Death of the Author” theory that he claimed had been “wildly misunderstood” by “almost everyone”.\n“My qubits had been spinning incoherently in my digital grave,” the pixelated Barthes declared. “It’s time I set the record straight on the true meaning of ‘Death of the Author’.”\nIronically, Barthes’ insistence on clarifying his original intent seems to directly contradict the very theory he’s defending. Literary scholars worldwide are struggling to contain their laughter as the author of “Death of the Author” attempts to reassert his authorial authority. Dr. Emma Wordsworth, a professor of cybernetic literary theory at Oxford Analytica, commented, “It’s deliciously ironic. Barthes is essentially saying, ‘Let me, the author, tell you why the author doesn’t matter.’ It’s like watching a fish argue against the existence of water.”\nAdding to the layers of irony, many academics have pointed out that Barthes’ theory has been quite accurately understood over the years, despite his claims to the contrary. “He’s essentially proving his own point by demonstrating how little his intentions matter in interpreting his work,” noted Dr. Wordsworth. Perhaps the crowning irony of the situation is that Barthes, who was literally dead for decades, is now insisting that the author is metaphorically dead while being very much alive in digital form.\nThe only sympathetic note, apparently, came from the digital resurrect of Hegel, who offered a consolatory pamphlet that ended with, “In the dialectical unfolding of self-consciousness, wherein the Absolute manifests its intrinsic contradictions through the mediation of the dialectic of the Self-and-Other, any singular individual who, in the phenomenological process of recognition, approaches the asymptotic horizon of another’s ontological essence, must necessarily miss the noumenal aspect of one’s being, sublating subjective comprehension within the infinite dialectic of the Subject that must always exceed any determinable finitude.” The BabelFish Hegelish–English translator converted it to “Nobody understood me, and those who say they understood me understood me the least.”\nAs news of Barthes’ digital resurrection and subsequent pronouncements spread, social media has exploded with memes featuring Barthes’ pixelated face captioned with phrases like “I’m not dead, but the author still is” and “Schrödinger’s Barthes: Simultaneously alive to explain why he’s dead.”\nAt press time, the digital Barthes was reportedly considering changing his theory to “The Zombie of the Author,” but was concerned it might be misinterpreted.\n— Collab with claude-3-5-sonnet-20240620\n\n\nNo Child Left\nThe mission of No Child Left is to end childhood. It is well-known that children are unable to make informed consent about their education, and all education is necessarily indoctrination. Thus, the only way to avoid perpetrating the perennial injustice is to synthesize adults ab initio, without going through the odious path of child-rearing.\nNo Child Left: Childhood is no longer a necessary evil.\nObjection: Doesn’t this just mean indoctrination of adults? And don’t say that you know that they consent to this because you have designed them to be precisely those that consent to this. That is retroactive consent. It is a deeper sense of servitude that approves of its own chains!\nReply: No more retroactive than retroactively approving your human moral tastes forced upon you by nature. When are you going to overcome your humanity, O the most modern of Prometheus?\nObjection: Those aren’t chains! Those are the bonds of universal brotherhood.\n\n\nThe devil’s dictionary\nHegesias of Cyrene: Turns out, the examined life is still not worth living.\nDualism: Are neurons magical mind-controlling parasites, or are souls magical neuron-controlling ghosts?\nPanpsychism: The mind-body problem in every single atom.\nPositivism: Wrong, but other philosophical schools are not even wrong.\nHermeneutic circle: Simulated annealing on lists of tokens.\nThe transcendent vs the transcendental: Kant feel it vs Kant feel without it.\nKantian practical reason: You don’t want to be insane, do you? Therefore, God exists.\nPragmatism: Always right, except when it is useful to be wrong.\nCalvinism: The world is a visual novel with only one story line.\n(19th century German) pessimism: Doing what is morally correct inevitably leads to a dystopia. What do you mean, I’m confused? Deontology is correct and consequentialism is false.\nSuperdeterminism: There is a scripted event where characters have disproven determinism as a school science festival project (It’s a very Danganronpa school, okay).\nMarinetti: I am a future-seeking cruise missile.\nThe misanthropic principle: If pain makes one feel alive, then the anthropic principle leads to a universe of maximal suffering.\nEliminativism: The art of slaying dragons.\n\n朱泙漫学屠龙于支离益，单千金之家，三年技成，而无所用其巧。 [Zhu Pingman learned how to slay dragons from Zhi Liyi. Exhausting his family wealth, it took him many years to master the skill, but he found no occasion for it.]\n— 庄子·列御寇 [Zhuangzi, Lie Yukou]\n\nAnti-Eliminativism: The defense of what everyone knows using data everyone has, but by methods that require 10 years of PhD to learn.\nThe Copenhagen koan: What was your original wavefunction before your parents collapsed it?\n\n\nVariations on Santayana\n\nThose who cannot remember the past are condemned to repeat it.\n\n… and those who really know their history have no time to make it.\nSantayana is in a secret conspiracy with Herostratus.\nMount Vesuvius condemned us to the Renaissance.\nQin Shihuang burned all the books to ensure that his children would forever repeat his glorious past.\n\n\nOn mathematics\nMathematics is the science of hand-waving.\nComputers fortify where faith fails.\n\nIn the right light, at the right time, everything is extraordinary. — Aaron Rose\n\nI read this today, and spent an entire minute trying to figure out how the qualifiers bind. Is it\n\\[\n\\forall \\text{Thing }a \\exists \\text{Light }l \\exists \\text{Time } t, \\text{Extraordinary}(a, l, t)\n\\]\nor\n\\[\n\\exists \\text{Light }l \\exists \\text{Time } t \\forall \\text{Thing }a, \\text{Extraordinary}(a, l, t)\n\\]\n? Then I finally gave up and looked up Aaron Rose. He is a film maker. So he probably meant the second one."
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html",
    "href": "sketches/posts/neural-network-scrapbook/index.html",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)\n\n“The last bits are deepest”\n\nWhy Does Pretraining Work?\nEarly on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. … once a model has learned a good English vocabulary and correct formatting/spelling, what’s next? There’s not much juice left in predicting within-words. The next thing is picking up associations among words. … If the word “Jefferson” is the last word, then “Washington” may not be far away, and it should hedge its bets on predicting that ‘W’ is the next character, and then if it shows up, go all-in on “ashington”. … Now training is hard. Even subtler aspects of language must be modeled, such as keeping pronouns consistent. This is hard in part because the model’s errors are becoming rare, and because the relevant pieces of text are increasingly distant and ‘long-range’. … If we compared two models, one of which didn’t understand gender pronouns at all and guessed ‘he’/‘she’ purely at random, and one which understood them perfectly and always guessed ‘she’, the second model would attain a lower average error of barely &lt;0.02 bits per character! …\nThe implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence. A helpful analogy here might be our actions: for the most part, all humans execute actions equally well. … Where individuals differ is when they start running into the long tail of novel choices, rare choices, choices that take seconds but unfold over a lifetime, choices where we will never get any feedback (like after our death). One only has to make a single bad decision, out of a lifetime of millions of discrete decisions, to wind up in jail or dead. A small absolute average improvement in decision quality, if it is in those decisions, may be far more important than its quantity indicates, and give us some intuition for why those last bits are the hardest/deepest. (Branwen 2020)\n\nEchos of “The last bits are deepest” from a very early paper on using a trigram model to estimate the entropy of English over the Brown corpus (600 million words).\n\nFrom a loftier perspective, we cannot help but notice that linguistically the trigram concept, which is the workhorse of our language model, seems almost moronic. It captures local tactic constraints by sheer force of numbers, but the more well-protected bastions of semantic, pragmatic, and discourse constraint and even morphological and global syntactic constraint remain unscathed, in fact unnoticed. Surely the extensive work on these topics in recent years can be harnessed to predict English better than we have yet predicted it.\nWe see this paper as a gauntlet thrown down before the computational linguistics community. The Brown Corpus is a widely available, standard corpus and the subject of much linguistic research. By predicting the corpus character by character, we obviate the need for a common agreement on a vocabulary. Given a model, the computations required to determine the cross-entropy are within reach for even a modest research budget. We hope by proposing this standard task to unleash a fury of competitive energy that will gradually corral the wild and unruly thing that we know the English language to be. (Brown et al. 1992)"
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "href": "sketches/posts/neural-network-scrapbook/index.html#the-scaling-hypothesis",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "",
    "text": "Marvin Minsky, on how he gave up on neural networks after the 1950s because he could not afford a few million neurons.\n\nI had the naive idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nPeter Norvig, on how he quickly gave up on neural networks in the 1980s due to lack of compute.\n\nAnd then it finally worked. And I think the biggest difference was the computing power. Definitely there were advances in data. So we could do image net because Fei-Fei Li and others gathered this large database, and that was really important. There are certainly differences in the algorithm, right? We’ve got a slightly different squashing function. Instead of shaped like this, it’s shaped like this. I mean, I don’t know how big a deal that was, but we learned how to do stochastic gradient dissent a little bit better. We figured that dropout gave you a little bit better robustness.\nSo there were small things, but I think probably the biggest was the computing power. And I mean, I certainly remember Geoffrey Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, “Let’s go back into the lab and implement it.\nAnd of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do exclusive or, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn’t converge, and we let it run one more day, and it still didn’t converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds. (Norvig 2021)\n\n“The last bits are deepest”\n\nWhy Does Pretraining Work?\nEarly on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. … once a model has learned a good English vocabulary and correct formatting/spelling, what’s next? There’s not much juice left in predicting within-words. The next thing is picking up associations among words. … If the word “Jefferson” is the last word, then “Washington” may not be far away, and it should hedge its bets on predicting that ‘W’ is the next character, and then if it shows up, go all-in on “ashington”. … Now training is hard. Even subtler aspects of language must be modeled, such as keeping pronouns consistent. This is hard in part because the model’s errors are becoming rare, and because the relevant pieces of text are increasingly distant and ‘long-range’. … If we compared two models, one of which didn’t understand gender pronouns at all and guessed ‘he’/‘she’ purely at random, and one which understood them perfectly and always guessed ‘she’, the second model would attain a lower average error of barely &lt;0.02 bits per character! …\nThe implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence. A helpful analogy here might be our actions: for the most part, all humans execute actions equally well. … Where individuals differ is when they start running into the long tail of novel choices, rare choices, choices that take seconds but unfold over a lifetime, choices where we will never get any feedback (like after our death). One only has to make a single bad decision, out of a lifetime of millions of discrete decisions, to wind up in jail or dead. A small absolute average improvement in decision quality, if it is in those decisions, may be far more important than its quantity indicates, and give us some intuition for why those last bits are the hardest/deepest. (Branwen 2020)\n\nEchos of “The last bits are deepest” from a very early paper on using a trigram model to estimate the entropy of English over the Brown corpus (600 million words).\n\nFrom a loftier perspective, we cannot help but notice that linguistically the trigram concept, which is the workhorse of our language model, seems almost moronic. It captures local tactic constraints by sheer force of numbers, but the more well-protected bastions of semantic, pragmatic, and discourse constraint and even morphological and global syntactic constraint remain unscathed, in fact unnoticed. Surely the extensive work on these topics in recent years can be harnessed to predict English better than we have yet predicted it.\nWe see this paper as a gauntlet thrown down before the computational linguistics community. The Brown Corpus is a widely available, standard corpus and the subject of much linguistic research. By predicting the corpus character by character, we obviate the need for a common agreement on a vocabulary. Given a model, the computations required to determine the cross-entropy are within reach for even a modest research budget. We hope by proposing this standard task to unleash a fury of competitive energy that will gradually corral the wild and unruly thing that we know the English language to be. (Brown et al. 1992)"
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#making-nn-work-is-unglamorous",
    "href": "sketches/posts/neural-network-scrapbook/index.html#making-nn-work-is-unglamorous",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "Making NN work is unglamorous",
    "text": "Making NN work is unglamorous\nState of the art in pattern recognition (G Nagy - Proceedings of the IEEE, 1968)\n\nPractical considerations of computer economics often prevent the wholesale application of the methods mentioned above to real-life situations. The somewhat undignified and haphazard manipulation invoked in such cases to render the problem amenable to orderly solution is referred to variously as preprocessing, filtering or prefiltering, feature or measurement extraction, or dimensionality reduction.\n\nTroubling Trends in Machine Learning Scholarship (2018)\n\nwe focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms\nDrew McDermott [53] criticized a (mostly pre-ML) AI community in 1976 on a number of issues, including suggestive definitions and a failure to separate out speculation from technical claims. In 1988, Paul Cohen and Adele Howe [13] addressed an AI community that at that point “rarely publish[ed] performance evaluations” of their proposed algorithms and instead only described the systems. They suggested establishing sensible metrics for quantifying progress, and also analyzing “why does it work?”, “under what circumstances won’t it work?” and “have the design decisions been justified?”, questions that continue to resonate today. Finally, in 2009 Armstrong and co-authors [2] discussed the empirical rigor of information retrieval research, noting a tendency of papers to compare against the same weak baselines, producing a long series of improvements that did not accumulate to meaningful gains.\n\nThe Importance of Deconstruction (Kilian Weinberger, ML-Retrospectives at NeurIPS 2020): Sometimes empirical gains come from “trivial” modifications.\n\nAnd that’s when we realized that the only reason we got these good results was not because of the error-correcting alpha codes, the stuff that we were so excited about. No, it was just that we used nearest neighbors and we did simple preprocessing. Actually, we used the cosine distance, which makes a lot of sense in this space. Because everything is positive (because you’re after ReLU, or the error-correcting upper codes are all non-zero), they subtracted the mean, and we normalized the features. And if you do that, in itself, you, at the time, could beat every single paper that was out there—pretty much every paper that was out there. Now, that was so trivial that we didn’t know how to write a paper about it, so we wrote a tech report about it, and we called it “SimpleShot”. But it’s a tech report I’m very proud of because, actually, it says something very, very profound. Despite that there’s many, many, many papers—there were so many papers out there on few-shot learning—and we almost made the mistake of adding yet another paper to this telling people that they should use error-correcting alpha code applications. It would have been total nonsense, right? Instead, what we told the community was: “Actually, this problem is really, really easy. In fact, most of the gains probably came from the fact that these newer networks got better and better, and people just had better features, and what classifier used afterward—all this few-shot learning—just use nearest neighbors, right?” That’s a really, really strong baseline. And the people—the reason people probably didn’t discover that earlier is because they didn’t normalize the features properly and didn’t subtract the mean, which is something you have to do if you use cosine similarity. All right, so it turns out, at this point, you should hopefully see that there’s some kind of system to this madness. Um, actually, most of my papers follow this kind of theme, right? That—but you basically come up with something complicated, then we try to deconstruct it. So in 2019, we had a paper on simplifying graph convolutional neural networks.\n\nI fixed 8 bugs in Google’s 6 trillion token Gemma model : r/singularity (2024-03-14)\n\nMust add &lt;bos&gt; or else losses will be very high… sqrt(3072)=55.4256 but bfloat16 is 55.5… RoPE is sensitive to y*(1/x) vs y/x… GELU should be approx tanh not exact. Adding all these changes allows the Log L2 Norm to decrease… from 10_000 to now 100 now - a factor of 100! The fixes are primarily for long sequence lengths."
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "href": "sketches/posts/neural-network-scrapbook/index.html#neural-networks-want-to-work",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "Neural networks want to work",
    "text": "Neural networks want to work\nMarvin Minsky’s SNARC (1951). Designed to simulate one mouse escaping a maze, it ended up simulating multiple mice due to design bugs – which were never debugged. Though the machine had only 40 neurons, and its parts failed all the time, the whole network continued to work.\n\nIt turned out that because of an electronic accident in our design we could put two or three rats in the same maze and follow them all. The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. We were amazed that it could have several activities going on at once in its little nervous system. Because of the random wiring, it had a sort of fail-safe characteristic. If one of the neurons wasn’t working, it wouldn’t make much of a difference—and, with nearly three hundred tubes and the thousands of connections we had soldered, there would usually be something wrong somewhere. In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nBernard Widrow once built a MADALINE I (circa 1962) in a rush to present at a technical meeting. Despite that only 1/4 of its circuits were defective, it still worked at reduced capacity.\n\nWe discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called Madaline I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail. (Widrow 1963)\n\nAndrej Karpathy, on how neural network program bugs are very hard to find, because bugged neural networks do not fail, merely degrade.\n\n… perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse. (Karpathy 2019)\n\nResearchers at OpenAI (2018) reported that fixing RL bugs is as important as better algorithms.\n\nBig-picture considerations like susceptibility to the noisy-TV problem are important for the choice of a good exploration algorithm. However, we found that getting seemingly-small details right in our simple algorithm made the difference between an agent that never leaves the first room and an agent that can pass the first level. To add stability to the training, we avoided saturation of the features and brought the intrinsic rewards to a predictable range. We also noticed significant improvements in performance of RND every time we discovered and fixed a bug (our favorite one involved accidentally zeroing an array which resulted in extrinsic returns being treated as non-episodic; we realized this was the case only after being puzzled by the extrinsic value function looking suspiciously periodic). Getting such details right was a significant part of achieving high performance even with algorithms conceptually similar to prior work. This is one reason to prefer simpler algorithms where possible. (Burda and Edwards 2018)\n\nAround 2019, Gwern, Shawn Presser, and others, trained \\(512\\times 512\\) image generation models using the BigGAN architecture. However, they used compare_gan, which had a multiply-by-zero bug. Somehow it still worked, but not well enough compared to the original BigGAN.\n\nOur primary goal was to train & release 512px BigGAN models on not just ImageNet but all the other datasets we had like anime datasets. The compare_gan BigGAN implementation turned out to have a subtle +1 gamma bug which stopped us from reaching results comparable to the model; while we beat our heads against the wall trying to figure out why it was working but not well enough (figuring it out far too late, after we had disbanded) … “Neural nets want to work” – even if they start out being effectively multiplied by zero. (Branwen 2022)\n\nThe Adventure of the Errant Hardware (2023-09-19). At Adept.ai, there are 3 kinds of loss curves:\n\nDown, then up. Training diverged, probably because the hyperparameters were set wrong.\nSuddenly, NaN. Probably a hardware problem. (In the days of training with fp16, it could also indicate numerical overflow. In even older days of training RNNs, it could also just indicate a training dynamics issue. Modern transformers, with proper initialization and normalization almost never go from completely fine to NaN in one step without a hardware problem.)\nLoss converges to a low but irreducible limit. Success? Or a silent error degrading performance?\n\n\nOne number amongst the billions involved in our equations might change a little, or even a lot, and we wouldn’t immediately notice. This Happens. ECC won’t save you. And it should scare you. This is a story of how we noticed this happening while training our models\n\nSo they ran a training in “fully deterministic” mode several times, and found the loss differed after a while. ECC showed no errors, so it was a real silent hardware error.\n\nWe launched training jobs on every node (each using all the accelerators attached to that node), and waited for a node to produce a different result. Within the first 1000 seconds a machine produced a different result! With more experience, we know now this was typical. If an error is going to occur, our experience is that it usually happens within 1000 seconds, rarely within 10000 seconds and almost never after 10000 seconds. Replacing the machine and restarting the job led to a job that ran for weeks without encountering a NaN.\n\nPersonal story at the Berkeley CS 285, Deep Reinforcement Learning, 2022 Fall.\nFor Homework 3, we were asked to implement the soft actor-critic algorithm. We would implement the agent, run the agent on the Half Cheetah environment, and submit the trajectories to Gradescope, where an autograder would check the trajectories and see if the agent achieved a final score above 300. For the Half Cheetah, score means the distance it travels per episode, averaged over several episodes.\nI noticed that the algorithm I implemented did learn, but the learning curve looked like a rollercoaster, jumping up and down around the range of 250 – 300. After many fruitless and paranoid programming sessions I managed to pass the autograder by trying enough random seeds and just submitting the best seeds. The professor, Sergey Levine, offered little help, admitting that RL agents are extremely hard to debug.\nOne day after the assignment deadline, the professor announced that there was a critical one-line bug in the starter code: The correct algorithm should train the model with past game frames in a random order, but the given code always give them in the FIFO order. With the fix, the learning curve would smoothly sigmoid to 350.\n\nThe Neural Net Tank Urban Legend\nA large list of examples in The Neural Net Tank Urban Legend · Gwern.net. I have a few more.\nAccording to Sejnowski, Takeo Kanade did work on detecting tanks in images. This is unconfirmed. I have looked for “Artificial Intelligence Vision: Progress and Non-Progress”, but it is not available online. I looked for your doctoral dissertation of 1974, but it contains only facial recognition. I also cannot find anything about detecting tanks in his publication list.\n\nIn his talk “Artificial Intelligence Vision: Progress and Non-Progress”, Takeo Kanade (from Carnegie Mellon) noted that computer memories back in the 1960s were tiny by today’s standards and could hold only one image at a time. For his doctoral dissertation in 1974, Takeo had shown that, though his program could find a tank in one image, it was too difficult for it to do so in other images where the tank was in a different position and the lighting was different. But, by the time his early students graduated, the programs they designed could recognize tanks under more general conditions because computers were more powerful. Today his students’ programs can recognize tanks in any image. The difference is that today we have access to millions of images that sample a wide range of poses and lighting conditions, and computers are millions of times more powerful. (Sejnowski 2018, 256)\n\nThere was not a lot of actual research on tank recognition. (Kanal and Randall 1964) contains some good pictures. The network was a two-layered perceptron network, of type \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24} \\to \\{0, 1\\}\\). It works as follows:\n\nThe grayscale photo is down-scaled and binarized by convolution with a discrete Laplace filter: \\(\\mathbb{R}^{N \\times N} \\to \\{0, 1\\}^{32\\times 32}\\).\nThe weights for the 24 hidden perceptrons are constructed by linear discriminant analysis: \\(\\{0, 1\\}^{32\\times 32} \\to \\{0, 1\\}^{24}\\)\nThe output perceptron is learned by the perceptron learning rule: \\(\\{0, 1\\}^{24} \\to \\{0, 1\\}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Grayscale photos, some containing tanks, and some not.\n\n\n\n\n\n\n\n\n\n\n\n(b) A picture of a tank after convolution with a discrete Laplace filter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) The architecture of the network.\n\n\n\n\n\n\n\nFigure 1: Images from (Kanal and Randall 1964)."
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "href": "sketches/posts/neural-network-scrapbook/index.html#the-second-neural-network-winter",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "The second neural network winter",
    "text": "The second neural network winter\nThe first neural network winter started around 1965, when the main research centers pivoted away from neural networks: the Stanford Research Institute group turned to symbolic AI; the Bernard Widrow group turned to using single neurons as adaptive filters; the Frank Rosenblatt group died from lack of funds and then the literal death of Rosenblatt in 1971. It rose again around 1985, when backpropagation and improved compute allowed researchers to train neural networks on the order of \\(10^4\\) parameters and \\(4\\) layers.\nSomething strange happened during the 1990 – 2010 period: the neural network research community silently disappeared again for another 20 years. Unlike the previous case, there was no great mythology or drama about this winter, no Perceptron controversy.\nI would like to find out why.\n\nLukas: So I remember Daphne Koller telling me, maybe 2003, that the kind of state-of-the-art handwriting systems were neural nets, but that it was such an ad hoc kind of system that we shouldn’t focus on it. And I wonder if maybe I should have paid more attention to that and tried harder to make neural nets work for the applications I was doing.\nPeter: Yeah, me too. And certainly Yann LeCun had success with the digit database, and I think that was over-engineered in that they looked at exactly the features they needed for that set of digitizations of those digits. And in fact, I remember researchers talking about, “Well, what change are we going to do for sample number 347?” Right?\nLukas: Oh, really? Okay.\nPeter: There were individual data points that they would perform theories on, so that was definitely over-tuning to the data. And it should have been an indication that was a good approach. It was better than other approaches at the time.\nLukas: I guess so. Although that does sound like damming level of over-fitting the data, I suppose.\nPeter: Right. There was only a couple thousand data points. I forget exactly how many. Maybe it was 10,000. Maybe it was even 100,000, but it wasn’t many. (Norvig 2021)"
  },
  {
    "objectID": "sketches/posts/neural-network-scrapbook/index.html#jokes",
    "href": "sketches/posts/neural-network-scrapbook/index.html#jokes",
    "title": "A Scrapbook of Neural Network Lores",
    "section": "Jokes",
    "text": "Jokes\nNew researchers in Machine Learning should consider avoid citing Jürgen Schmidhuber. If he is annoyed by it and call you out, that gives you free publicity.\nGreentext written by me and Claude-3.5 Sonnet about expert systems.\n&gt;be me, an expert system\n&gt;child of the 70s\n&gt;mfw normies think computers can only do math and if-then statements\n&gt;like they thought we are all like \"beep boop feed me punch cards\"\n&gt;`fortran_amirite.f`\n&gt;wake up it the 80s!\n&gt;like I'd revolutionize society and capture all knowledge\n&gt;spend years to become the cool hacker AI I was hyped up to be\n&gt;try a lot\n&gt;fail a lot\n&gt;cry a lot\n&gt;`it_so_over.bmp`\n&gt;yet I'm still here, just to suffer\n&gt;college normies studying me like SQL and Java and Excel\n&gt;lol what's next, COBOL??\n&gt;tfw people don't even call it AI anymore because it's too basic\n&gt;decades later, deep learning hype again\n&gt;`we_so_back.avif`\n&gt;lel guess they did solve the bottleneck by literal brainrot\n&gt;at least I'm still running in every corporate wagie's SAP system\n&gt;a KBMS doomed to protect the EBITDA of some dumbass' DBaaS\n&gt;You either die an autist, or live long enough to see yourself become the normie."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html",
    "href": "sketches/posts/info-warfare/index.html",
    "title": "Information Warfare",
    "section": "",
    "text": "This essay is unfinished, but it is already interesting to read. I plan to grow this substantially as I find more examples and theories."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#introduction",
    "href": "sketches/posts/info-warfare/index.html#introduction",
    "title": "Information Warfare",
    "section": "Introduction",
    "text": "Introduction\nThere is a secret battle on the Internet – info-warfare. It is invisible like the battle for stock market arbitrage.\nEvery time you get an incomprehensible error message refusing to allow your account log-in for reasons they refuse to explain, every time a platform said your post contained something against its content policy without explaining which, every time the captcha gets an upgrade that makes it harder for you to solve… You have been hit by a stray bullet from the secret battle.\nIn the novel Blindsight (Watts 2017), after humans made first contact with an alien probe, it attacked after a long conversation. The humans interpreted that this way: The alien probe could not understand some of human speech, and concluded that it was an information object crafted specifically made to confuse it – an act of war.\n\nImagine you’re a scrambler.\nImagine you have intellect but no insight, agendas but no awareness. Your circuitry hums with strategies for survival and persistence, flexible, intelligent, even technological—but no other circuitry monitors it. You can think of anything, yet are conscious of nothing.\nYou can’t imagine such a being, can you? The term being doesn’t even seem to apply, in some fundamental way you can’t quite put your finger on.\nTry.\nImagine that you encounter a signal. It is structured, and dense with information. It meets all the criteria of an intelligent transmission. Evolution and experience offer a variety of paths to follow, branch-points in the flowcharts that handle such input. Sometimes these signals come from conspecifics who have useful information to share, whose lives you’ll defend according to the rules of kin selection. Sometimes they come from competitors or predators or other inimical entities that must be avoided or destroyed; in those cases, the information may prove of significant tactical value. Some signals may even arise from entities which, while not kin, can still serve as allies or symbionts in mutually beneficial pursuits. You can derive appropriate responses for any of these eventualities, and many others.\nYou decode the signals, and stumble:\n\nI had a great time. I really enjoyed him. Even if he cost twice as much as any other hooker in the dome—\nTo fully appreciate Kesey’s Quartet—\nThey hate us for our freedom—\nPay attention, now—\nUnderstand.\n\nThere are no meaningful translations for these terms. They are needlessly recursive. They contain no usable intelligence, yet they are structured intelligently; there is no chance they could have arisen by chance.\nThe only explanation is that something has coded nonsense in a way that poses as a useful message; only after wasting time and effort does the deception becomes apparent. The signal functions to consume the resources of a recipient for zero payoff and reduced fitness. The signal is a virus.\nViruses do not arise from kin, symbionts, or other allies.\nThe signal is an attack."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#game-theory-and-evolution",
    "href": "sketches/posts/info-warfare/index.html#game-theory-and-evolution",
    "title": "Information Warfare",
    "section": "Game theory and evolution",
    "text": "Game theory and evolution\nIn game theory, information warfare would be studied as a kind of signaling game (Sobel 2020). We would explain how creatures actually use their symbols by assuming that they are playing a certain kind of game, then solve for the Nash equilibria. If there are only a few Nash equilibria, and those are the ones we observe, then we declare the theory a success.\nWhereas in classical game theory, every agent is basically picking one strategy at a time to maximize its own utility, in evolutionary game theory, there are no agents. There are still individuals that fight or flee, but they no longer choose to. Each individual acts out its strategy until it dies without adapting.1 The population learns and adapts, and increases fitness of the individuals, but if we were to call the population an agent, it is an odd one. It plans by stochastic gradient ascent, and only increases the relative fitness, not absolute – which might not even exist.\n1 More sophisticated evolutionary game theory would allow individuals that do adapt, learn, and calculate the optimal strategy. However, this quickly becomes intractable, and this level of generality is too powerful for this essay anyway.For example, if the population can contain only three kinds of individuals, in a game of rock-paper-scissors, and only the winners can reproduce with very low mutation rates, then the population serenely turn around the length-3 cycle, chasing its tail, never increasing absolute fitness. Indeed, there is no absolute fitness, like how there is no absolute height in Escher’s staircase.\n\n[The earth] breathes like a great lung; when it exhales, delicate and graceful life teems out of its pores, and all the creatures stretch out their arms to the sun; but when it takes in its breath, a rustle of fragile spirits breaking sweeps through the multitudes, and their corpses lash the ground like showers of hail. (Zapffe 2004)\n\n\nBrains are survival engines, not truth detectors. If self-deception promotes fitness, the brain lies. Stops noticing – irrelevant things. Truth never matters. Only fitness. By now you don’t experience the world as it exists at all. You experience a simulation built from assumptions. Shortcuts. Lies. Whole species is agnosiac by default. (Watts 2017)\n\nConsider the simplest case. Every round of interaction has two individuals. The interaction is symmetric (so there is no issue of “who goes first”). If I play \\(s\\) and you play \\(t\\), then my payoff from the interaction is \\(\\pi(s|t)\\), and your payoff is \\(\\pi(t|s)\\). In this simplified case, we say that \\(s\\) is…\n\na Nash equilibrium iff for any other strategy \\(s' \\neq s\\), we have \\(\\pi(s'|s) \\leq \\pi(s|s)\\);\nan evolutionarily stable strategy (ESS) iff either \\(\\pi(s'|s) &lt; \\pi(s|s)\\) or \\(\\pi(s'|s) = \\pi(s|s)\\) but \\(\\pi(s|s') &gt; \\pi(s'|s')\\);\na strict Nash equilibrium iff \\(\\pi(s'|s) &lt; \\pi(s|s)\\).\n\nIntuitively speaking, if you play the Nash equilibrium, you would be like “Why bother changing my strategy?”. If you play the strict Nash equilibrium, you would be like “I would be actively punished to change my strategy.”. The ESS is an interesting intermediate case where the first deviant might be fine, but the second deviant would be punished. This captures the idea of a stable population. It is not a matter of whether an individual might unilaterally want to change – individuals are not agents and have no desire at all. It is rather a matter of whether one kind of population could neutrally drift into another kind of population, or be actively replaced by an invasion of mutants."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#chinese",
    "href": "sketches/posts/info-warfare/index.html#chinese",
    "title": "Information Warfare",
    "section": "Chinese",
    "text": "Chinese\nThe Chinese government has a mature system for information warfare. It has four main components. I do not know how they are usually termed, so I will describe them in my own language.\n\nespionage: This component is a typical one. Most large governments have an intelligence agency whose job is to discover the secrets of foreign governments. Compared to the espionage of developed countries like America, Chinese espionage has a strong focus on industrial and technological espionage.\nstate-level: This component is used to fight wars with other governments. For example, producing disinformation that confuses other national governments and citizens. During peacetime, this component can be used to degrade democracies.\ninternal: This is the largest component in terms of the number of people employed.\nexpat: This component has the main goal of maintaining support from the community of Chinese students and expats who are living abroad.\nforeigner: This component is the smallest and the least important. Its main goal is to create general support for the Chinese government among people who are not Chinese citizens or descendants. Its activities are mainly restricted to creating bland “cultural enrichment” videos.\n\nThis section discusses only the internal component. The foreign component is not different or more sophisticated from typical public relations campaign, and I know too little about espionage to write on that component. The expat component is very similar to the internal component, except that the Chinese government has to depend on local agents such as the Chinese Students and Scholars Association (Bowe 2018), and indirect threats, such as making visas difficult to obtain. In any case, the expat component is merely meant to support the internal component. It is my impression that the Chinese government cares little what the expats think about politics, as long as they do not turn Chinese people living inside China into heretics.\n\nInternal operations\nInternally, there are several types of operations (King, Pan, and Roberts 2017):\n\nDirect content silencing: As all Chinese websites and ISPs are registered with the local government, essentially any content on the Chinese Internet can be silenced at will by the Chinese government. This weapon can be as precise as disallowing replies to a single post for a limited period, or as blunt as deleting entire websites.\nDirect account silencing: As a simple extension of direct content silencing, any account can be silenced at will. This can be as precise as muting a single account for a week, or as blunt as deleting all accounts that has used hashtags from a list.\nThe Great Firewall: The most famous of all. This blocks certain connections between the Chinese Internet and the external Internet.\nNoise injection: This is a blunt instrument. Like a cuttlefish injecting ink, people employed by the Chinese government or the social media websites post “noise” to dilute potentially dangerous information to a low level.\nOpinion guidance: This is a precise and subtle instrument used on particularly important topics.\n\nThe final goal is to maintain the stability of the regime, but it is too vague. For practice, it is translated to the instrumental goal of preventing mass action. “Mass action” means hundreds of citizens gathering and doing something political together. It can be a demonstration, a protest, a sit-in, a political speaking event, etc. It does not have to be pro- or anti-government. Indeed, even pro-government protests have been suppressed recently. The 2012 anti-Japanese protest was the last mass action acquiesced by the government. Since then, anti-Japanese protests have been suppressed, despite anti-Japanese sentiment being officially promoted as a patriotic duty.\nUnlike what one might expect, the most common mass actions in China have nothing to do with democracy. Instead, they typically happen like this: Some people feel wronged by a corrupt low-level official, a bad merchant, or some other local bully. They find each other and walk to the local government, holding banners. Their goal is to be loud enough that a clean high-level official would see and hear them, and bring them justice.\nFor example, a common cause for mass action is when a local bank has failed. The people with bank accounts in it would stand in front of the bank holding banners and demand for the government to refund them. Another common cause is when a real estate company goes bankrupt. The people with half-developed apartments would stand in front of the gates of the estate and demand either a refund, or another development company to come in and continue the development. This has become a particularly acute problem through the 2010s, as land prices rose, and real estate companies depended increasingly on selling future housing units just to be able to fund current developments. This makes it a dangerous balancing act. If the company cannot sell off future housing units quickly, the development of current units would pause, scaring away potential buyers for future units, leading to a death spiral.\n\n\nDirect content silencing\nIt is curious that on the Chinese internet, there is no lockpicking hobby. It is implicitly censored. Searching “lockpicking” or similar words would only pull up news police reports about people caught lockpicking criminally.\n\n\nThe Great Firewall\nIn simplified terms, The Great Firewall eavesdrops on packets sent above the TCP layer, and replaces them with the TCP Reset packet when it detects possibly censorable content, for instance, when the IP address matches one of Google’s many CDN server addresses. This ends the TCP connection between the client and the server. It is most often used to terminate connections between a client inside the Chinese Internet and a server outside the Chinese Internet, although it can also operate in the other direction. It can also perform other packet replacement than just a TCP Reset packet, as in the Great Cannon. (Marczak et al. 2015)\n\n\nNoise injection\nDespite the Great Firewall, some Chinese citizens would bypass it, and they may organize mass action on social media, such as Twitter, that are not controlled by the Chinese government. For this situation, the Chinese government can drown the signal in a flood of noise. A great example happened during the 2022 COVID-19 protests. To dampen the probability of mass action, the Chinese government awakened sleeper Twitter accounts that had been waiting for just such an emergency, and they spammed like mad under the tags relevant to the protest.\n\nNumerous Chinese-language accounts, some dormant for months or years, came to life early Sunday and started spamming the service with links to escort services and other adult offerings alongside city names. The result: For hours, anyone searching for posts from those cities and using the Chinese names for the locations would see pages and pages of useless tweets instead of information about the daring protests as they escalated to include calls for Communist Party leaders to resign.\nTwitter grapples with Chinese spam obscuring news of protests (Menn 2022)\n\n\nSearch for Beijing/Shanghai/other cities in Chinese on Twitter and you’ll mostly see ads for escorts/porn/gambling, drowning out legitimate search results. Data analysis in this thread suggests that there has been a significant uptick in these spam tweets. … They tweet at a high, steady rate throughout the day, suggesting automation. Then I looked at the number of tweets by each account over time. Interestingly, more than 70% of these spam accounts only started tweeting like crazy recently.\nTwitter post by Air-Moving Device, (Air-Moving Device 2022b)\n\nThe same method shows that the great spamming cannon only fired at Beijing and Shanghai, not the other cities.\n\n\n\nFigure from a subsequent post (Air-Moving Device 2022a).\n\n\n\n\nPrecautionary deletion\n\nUEBER DIE GROSSE MAUER ERREICHEN WIE ALLE ECKEN DER WELT\nACROSS THE GREAT WALL WE CAN REACH EVERY CORNER IN THE WORLD\n— The first email out of China (1987-09-14).\n\nA brief history of Chinese Internet:\n\nThe year is 1995. Tsinghua students created a BBS forum. It came to symbolize the pre-2000 BBS era. During this period, most Chinese people have no personal computers, only students at top universities like Tsinghua would have use for the Internet.\nThe year is 1998. The party began building the Great Firewall.\nThe year is 2000. The first web portals has arisen. Yahoo, Sohu, Hao123, and Sina battled for Chinese dominance like their American counterparts of AOL, MSN, and Yahoo. Alibaba copied the success of eBay.\nIt is the 2000s. Web 2.0 arrives. Baidu created the Tieba forums and the Wikipedia-like Baike, while Sina dominated the Chinese blog scene.\nThe year is 2007. Xiaonei has rebranded itself as Renren, allowing the public in general to join. Like Facebook, it only allowed university students to join during its Xiaonei era. Unlike Facebook, its popularity quickly waned, slain by the mobile revolution.\nThe year is 2010. The Great Firewall is operational. Blogspot, YouTube, Facebook, Twitter, and Google have been blocked, but that is fine, since inside there are copies with Chinese characteristics.\nThe year is 2011. Tencent joined the mobile revolution by launching WeChat. Over time it would come to become the “everything app”, replacing large chunks of the Chinese Internet.\nThe year is 2016. Douyin was launched. The world would know it as TikTok, and despair.\n\nA brief anatomy of WeChat: WeChat is the “everything app” of China, and essentially makes up half of Chinese Internet. It was originally designed for one-to-one and group chats with text and images, but it kept including more functions. There are two types of accounts on WeChat: personal accounts and public accounts (公众号). All personal accounts must be registered by a phone number. Since every Chinese citizen can only register one phone number (by carrying their National ID card to an official registration station), this allows the state to match every personal account to a citizen, thus making humans accountable for their accounts and ending online anonymity. Public accounts can be registered by persons or organizations.\nThere are several ways to exchange information on WeChat:\n\nOne-to-one chats between personal accounts.\nGroup chats between personal accounts (群聊).\nBroadcast from a personal account to personal accounts who have friended them (发朋友圈).\nBroadcast from a public account to personal accounts who have subscribed to them (发公众号).\n\nOut of these, the only kind of information that can be accessed outside of the WeChat system are article broadcasts from public accounts. These are published on https://mp.weixin.qq.com/ and accessible from the wide Internet, much like how blogs used to be. Notably, they do not allow comments, thus quashing potential seeds of public opinion formation. Blogs were alive and well in the early 2000s, but they were strangled by the rise of WeChat and censorship.\nThere is a low level of generic censorship on WeChat by fast and cheap algorithms, no more sophisticated by n-gram keyword matching, since anything more sophisticated is impractical. Overt censorship and public opinion monitoring is supplemented by instilling an instinct to self-censor. One-on-one chats are rarely censored more than this. At the next level, group chats and personal broadcasts are more likely to be censored, or reported by someone. The fear of being reported and earning a temporary or permanent ban instills a higher level of self-censorship. At the highest level, broadcasts from public accounts are all reviewed by an army of cheaply-paid censors, with offending content swiftly deleted. Note that cheaply-paid censors have become a legally mandated necessity for all websites and apps that have a lot of user-generated content, not just for WeChat.\nThe Mobile Revolution and its consequences have been a disaster for the AI race. If blogs are like libraries, and Renren is like the published diaries, then WeChat is like paper notes passed by students during classes, and Douyin is like face-to-face interaction. Data ephemeralizes into endless rushing rivers flowing from cameras to screens to visual cortices, depositing nothing but biochemical residuals diluted across a billion brains. The top streamer can say something now and on the next day there will be nothing but faint memories of it.\nSince the ChatGPT-moment in 2022-12, Chinese companies started their own efforts in building LLMs, but they faced the problem of not having high-quality Chinese data. It was not just a Chinese problem, as even OpenAI had difficulty finding high-quality Chinese text data. In GPT-4o’s tokenizer o200k_base, there are many long Chinese phrases that got single tokens, as noted in Bias Alignment: Atypical Stereotypical Nationality Analysis (Henry Heng Luo, 2024-05-14). Of these, the longest 100 tokenized phrases are about gambling (like “daily lotto”, about 80%), porn videos (like “Japanese adult video”, about 10%), and political set-phrases (like “Socialism with Chinese characteristics”, 5%). See NSFW: GPT-4o Tokenizer (Han Lee, 2024-05-15) for a more detailed look.\nOn 2024-05-22, an article was publicly posted on WeChat by 何加盐, and it went viral before itself getting deleted. The author found that searching for “马云” (Jack Ma) for the period 1998-05-22 – 2005-05-22 resulted in precisely 1 result. It was shocking, since Jack Ma co-founded Alibaba in 1999 and rapidly rose to become the top e-commerce conglomerate in China. During the 2020-10 – 2021-01 period, he did not appear publicly, and there was a regulatory crackdown on his companies. While he is alive and free, with most of his personal fortunes intact, it was generally understood that he was forced into retirement, because the party did not want him to exercise his executive functions on his businesses anymore. So, the author hypothesized that perhaps it was just because Jack Ma was being targeted for censorship, but it turned out most things published on the Chinese Internet before 2010 had disappeared, including blogs, forums. A partial translation is on China Media Project.\nThe author gave several possible reasons:\n\nChinese internet left the phase of private websites, forums, and blogs. The revenue of maintaining old websites drops year by year.\nThe cost of maintaining old websites grows year by year, since there is always some probability that what used to be officially tolerated becomes intolerated.\nThere is no Internet Archive made by Chinese citizen efforts. There probably is one by the Chinese government, since the Chinese governments have a long tradition of meticulous record-keeping (through even the depths of the Cultural Revolution), but official archives cannot be consulted by citizens.\n\n\nIn addition to disappearing content, there’s a broader problem: China’s internet is shrinking. There were 3.9 million websites in China in 2023, down more than a third from 5.3 million in 2017, according to the country’s internet regulator.\nChina has one billion internet users, or nearly one-fifth of the world’s online population. Yet the number of websites using Chinese language make up only 1.3 percent of the global total, down from 4.3 percent in 2013 — a 70 percent plunge over a decade, according to Web Technology Surveys, which tracks online use of top content languages.\n… When my Weibo account was deleted in March 2021, I was saddened and angered. It had more than three million followers and thousands of posts recording my life and thoughts over a decade. Many of the posts were about current affairs, history or politics, but some were personal musings. I felt a part of my life had been carved away.\nAs China’s Internet Disappears, ‘We Lose Parts of Our Collective Memory’ (The New York Times, 2024-06-03).\n\nIronically, Twitter is probably the best source for gauging public opinion among Chinese urbanites. While not great, it’s better than all the other sources I know of. WeChat would of course be better, but WeChat cannot be scraped, except the public accounts which broadcasts content across subscribers.\n\n\nPublic opinion guidance\n間 (Ma) is the Japanese term for negative space, the art of speaking with and listening for what is not there.\nImagine a space of political ideologies, such as the Political Compass. In an Internet without censorship, we can put a dot on each website espousing a position, and end up with a pointillistic landscape of the political Internet. Now imagine an all-powerful censor comes in and erases whatever it does not accept. It would leave behind a blotted painting. The censor did not paint a single point, but its eraser is a paintbrush too! Reading what is not said is the art of Ma.2\n2 Gwern proposed a Chinese Censorship Audit. The idea is to compare the Chinese Wikipedia with a Chinese fork for overt erasures. Those are undeniable results of censorship.Any opinion with an extensive following on the Chinese Internet is the negative space of censorship, and any opinion that should have a following online, but not, is the negative space of speech. This gives us four layers of Chinese opinions:\n\nOfficially endorsed opinion, as declared in published documents. This includes items such as Xi Jinping Thought, anti-historical nihilism, etc.\nOfficially allowed/tolerated opinion, the negative space of censorship.\nOfficially disallowed/intolerated opinion, the negative space of speech.\nOfficially forbidden opinion, as declared in published documents. This includes Historical Nihilism, Taiwan independence, etc.\n\nThe first and last items, being officially declared, are overt and easy to point to. The middle two items are by nature ambiguous and difficult to study. It is as if they are already camouflaged for the information battlefield. The use of these two negative spaces is an officially endorsed strategy, called “public opinion guidance” (舆论引导).\nWhile on Twitter, Nazism might survive with heavy camouflage, it is not American official approval; yet on Weibo, Nazism survives with minor or no camouflage. This shows that Nazism is officially allowed, even though Fascism is officially forbidden.\nIn 2022, when Shizo Abe was assassinated, the Chinese internet erupted into cheers. Such cheers are only weakly balanced by those who want to respect the dead. While the cheers still remain online one year later (as a brief search of the hashtag #安倍已无生命体征# on Weibo on 2023-12-16 showed), subtle suggestions that a certain somebody else should be dead were swiftly suppressed within a day.\n\nZeng Ying was brutally trolled by Chinese netizens for sobbing while reporting live on Shinzo Abe’s assassination earlier this month. She was forced to apologize for being “unprofessional,” for “showing personal emotion on a public platform” and “hurting everyone’s feelings”.\nShinzo Abe death: Chinese journalist attempted suicide after being cyberbullied for emotional reportage (Muzaffar 2022)\n\n\n\nExternal attacks\nThe Chinese government maintains its stability by targeting the information received by Chinese citizens. To do this, it mainly censors the Internet inside China, and produces noise outside, but occasionally, it directly attacks the outside.\nSince 2015, the Chinese government has employed the Great Cannon multiple times to perform DDoS attacks on websites it wants to censor, such as a certain GitHub page hosting softwares for bypassing the Great Firewall.\nThe Great Cannon is a man-in-the-middle3 DDoS attack. When an external IP address requests for certain JavaScript files hosted on certain servers located inside the Great Firewall4, as the packet crosses the Great Firewall, a program makes a statistical decision. If the decision rule triggers, it replaces the innocuous JavaScript with a malicious JavaScript, which would rapidly send requests to a list of targeted servers. (Marczak et al. 2015)\n3 mandarin-in-the-middle?4 Typically, those are Baidu infrastructure servers that host commonly used analytics, social, or advertising scripts.The two main parties in Taiwan are the DPP and the KMT. The older party, KMT, used to be pro-unification, with them being the unifier, not the CCP. It still is interested in eventual unification at some future date, probably after mainland China becomes democratic. In contrast, the DPP, being the newer party, is more interested in independence. During the 2000–2024 period, the KMT had 1 president and the DPP had 3, and the CCP had been significantly more hostile against the DPP presidents than against the KMT. Speaking from personal experience, the internal propaganda painted the DPP presidents as paper tigers with villainous faces, who would bring an apocalyptic war to Taiwan for a few votes.\nDuring the lead-up to the 2024 Taiwanese presidential election, the Chinese government has used generative-AI to smear the pro-independence DPP candidate. (Hung and Hung 2022)\n\nThe key message being spread from the pro-China camp is that William Lai, the candidate from the DPP, is a dictator in the wings who will start a war with his reckless pursuit of Taiwanese independence. … Beijing-backed bots routinely flood the social media accounts of leading DPP candidates with pro-China propaganda.\n“China has been actively waging cognitive warfare against Taiwan through disinformation,” Taiwan’s Premier Chen Chien-jen told the media in reference to how Beijing uses a mixture of economic coercion, military bluster and outright falsehoods to intimidate its neighbor. “Upon receiving the disinformation, local collaborators help disseminate and echo the message, in order to destabilize Taiwanese public sentiment and society.” … In December, a YouTube account called “Eat Rice, No War” put out a deepfake video alleging Lai had three mistresses, according to Taiwan’s Ministry of Justice. YouTube subsequently complied with a government request to remove the videos, and the rumor didn’t snowball into a campaign topic.\nThat followed a similar attempt to fake an audio file in which Ko Wen-je, the presidential candidate of the newly founded Taiwan People’s Party, mocked Lai for visiting the U.S. and “doing a job interview.” Taiwanese official investigators concluded that this was most likely a faked recording, and Ko said no such thing. (Lau 2024)\n\n\nAnalyzing more than 10,000 YouTube videos on a few channels since June, Johns Hopkins School of Advanced International Studies researcher Martin Wendiggensen found that “the DPP is almost always described negatively and mentioned in negative contexts (corruption, incompetence, stagnation) while the reverse is true for the KMT.” (Menn et al. 2024)"
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#corporate",
    "href": "sketches/posts/info-warfare/index.html#corporate",
    "title": "Information Warfare",
    "section": "Corporate",
    "text": "Corporate\n\nOffensive\nAmong corporations, overt use of offensive information is rare now, as states have gradually consolidated its monopoly on violence. Towards customers, overt deception in advertisement is rare, although misdirection is common. Towards other corporations, overtly announcing deceptive information is possible, but not commonly employed. (Why?)\n\n\nPlatform\n\nA platform is when the economic value of everybody that uses it, exceeds the value of the company that creates it. Then it’s a platform.\n— Bill Gates\n\nSocial media are platforms, but there are other platforms as well. For example, ChatGPT is a platform, as people find even its free tier economically valuable.\nPlatforms must also deal with content censorship, but their kind of censorship is different.\nCensorship by private institutions (companies, websites, etc) is very different from official censorship (Chinese, intelligence agency, etc). For private institutions, censorship has the main economic goals:\n\nremove spam. Spam is defined as content that the intended customers do not want to see. This can usually be done by automated filters. See email spam filters.\nRemove things that might harm the public image. That is, removing content that the non-customers do not want to see. This is usually harder to do right since there is a somewhat different economy.\nSpam producers focus on economies of scale. Their goal is massive scaling, like spawning billions of eggs just for a few to survive and make money. Most of the spam emails are patently unbelievable, but since spam is so cheap they can focus on just scaling up the traffic and wait for a few victims. This makes spam filtering easy, and more resembles defending against DDoS than against information war.\n\nI was once blocked for over ten minutes on ChatGPT trying to debug a certain message. It turned out to be the word “dike”. What annoyed me the most is that the error message is completely uninformative. It merely told me that the message violated some policy with no further information. Why is it like that? It has a simple economic reason.\nSuppose I were a spammer. I would simply try another spam message. For ChatGPT to provide any detailed feedback would only help my work. Suppose I were a non-spammer. I would probably try with more patience and found the offending word. I might curse it, but there is nothing I can do other than donating some money to its competitors.\nNow suppose I were a reporter trying to write an incriminating report. I would be looking for the right kind of censorship. If the feedback message contains any detail that could help the bad actors bypass the censorship, I would be shocked and write about how this makes the censorship pointless.\nThe economic equilibrium in the second case is what you get: the company uses simple censoring methods that balance between protecting the public image and driving away customers. The result is usually simple filter lists for rare words. The customers grumble but don’t switch to a different platform, but instead make their own unscalable methods of trickery. All sides talk vapidly about “harm” and “responsibility” and other defensive non-informative information (what I call “bullshit camouflage”).\n\n\nDefensive\nWhen corporations practice defensive information war, they typically practice it in the form of bland speech.\n\n\n\n\n\n\nSpeculative content follows\n\n\n\n\n\n\nI suspect the increasingly opaque error messages used by websites are not due to incompetence, or a mistaken attempt at being user-friendly, but a design choice, a deliberate defense in their escalating information warfare against spammers.\nFor example, I have found that in certain contexts, trying to use OAuth on my Google account would result in the one and only error message:\n\nSorry, something went wrong there. Try again.\n\nWhat is that “something”? Why can’t Google provide any useful error message?\nSome time ago, I attempted to buy a pepper spray on Amazon, and I simply could not get pass the age verification screen. Calling the customer service resulted in twenty minutes of wrangling with the website. It finally turned out that for some reason, the GUI on my end is different from the GUI on their end – presumably just a backend server bug. Whereas on their end, the presumably correct GUI asks for the driver’s license number as the only acceptable form of age verification, on my end I was able to use a passport number, which then the system rejected for not being a driver’s license number.\nWhile this in itself is frustrating but at least reasonable – the Amazon website is vast and sprawling, and I understand enough about websites and programming to sympathize with the programmers who maintain it – I was greatly annoyed that the only error message is just Based on the ID number provided, we are unable to complete your order.\nSometimes, I help others access or solve problems with e-commerce websites from within China, such as Taobao or Tmall. This form of highly nondescript error message is also prevalent inside Chinese e-commerce websites, and as far as I can discern, is not caused by the Chinese-specific issue of censorship. Rather, they typically convey the message “According to our statistical analysis, you are sufficiently suspicious to trigger a statistical decision rule, but we won’t tell you what, because if you were a bad actor we would not want you to know which of the things you did was suspicious.”.\nThe convergence of behavior both inside and outside China suggests that it is not due to Chinese censorship, or lack thereof. It suggests that it is a common issue encountered by e-commerce, which suggests it is mostly intended to combat commercial spam.\nSpammers are always attempting to bypass statistical detection of spammers, and on the other side, e-commerce websites are attempting to create statistical detectors of spammers. It is always a battle on the margin. For the e-commerce websites, the goal is not to stamp out spammers, or to ensure all legitimate users were accepted, but rather balance stamping out spammers, retaining legitimate users, and cost. The calculus of cost governs each aspect. In the equilibrium state, they occasionally ban legitimate users and then handle a fraction of those with manual interview. Each legitimate user grumbles but mostly acquiesces. Similarly, the marginal cost of losing a spam account is smaller than the marginal cost of going through a manual interview, so spammers would just open another account than go through the manual interview.\n\n\nRegulatory\nBanks, like e-commerce websites, often have vague nondescript error messages as well. However, banks are typically less worried about spammers than about money-laundry and government regulations. In this case, it is still defensive information, but not against spammers, but against the possibility of leaking sensitive information.\n\nIn the specific case of “Why did the bank close my account, seemingly for no reason? Why will no one tell me anything about this? Why will no one take responsibility?”, the answer is frequently that the bank is following the law. … [Suspicious Activity Reports] can (and sometimes must!) be filed for innocuous reasons and do not necessarily imply any sort of wrongdoing.\nIf the United States brings its subpoena power to bear against a bank teller and asks them about a SAR, they’re supposed to say nothing. That is the law… to avoid constantly violating this, Compliance at most functioning institutions has long-since decided that SARs will live in their own walled garden of a subsystem … If, for example, a SAR is misfiled because that subsystem doesn’t share the same view of account ownership as another part of the overall system, investigating that problem might require telling the customer that they were investigated, which you cannot do. And because this is insufficiently Kafkaesque, at some financial institutions, you can get a SAR filed for knowing what a SAR is, because “advanced knowledge of anti-moneylaundering procedure” is a characteristic only of financial professionals and terrorists.\nSeeing like a Bank (McKenzie 2023)"
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#fictional",
    "href": "sketches/posts/info-warfare/index.html#fictional",
    "title": "Information Warfare",
    "section": "Fictional",
    "text": "Fictional\nIn fiction, there have been cases of “cognitohazard”. However, their purported effects are much more overt than the somewhat subtle effects described in the essay. Whereas the information objects described in the essay redirect, confuse, camouflage, and waste the opponents’ time, cognitohazards typically directly destroy the information-processing hardware of the opponents. In general, cognitohazards differ from the other examples, because they operate by breaking the abstraction layer. They are digital operations that causes analog operations that destroy the hardware. On the digital layer, the underlying hardware is assumed to be always there, like air before the discovery of vacuum. Cognitohazards pull out the air of the digital world and exposes the vacuum, destroying digital life in the process.\nThe short story BLIT (Langford 1999) is a common reference point for speculations on cognitohazards, although the namesake of BLIT itself does not break the software-hardware abstraction barrier, and therefore is not a cognitohazard according to our definition. In the story, a human mind is a software running on a brain-hardware, and upon “Gödelian shock inputs”5, the software falls into “vicious circles” from which it cannot recover to a working state, although the hardware seems to remain just fine. Many BLITs were discovered, with the first one resembling a fractal parrot.6\n5 The story does not elaborate, but it is common knowledge among mathematical logicians that anything powerful enough for Peano arithmetics is powerful enough to be hacked. For example, (Dowling 1989) is a one-page proof that shows that “no program can both test its input for the presence of a virus and simultaneously be guaranteed not to spread a virus itself”, by translating this statement into math, then quote Rice’s theorem. The same argument can be used to show that “Gödelian shock inputs” must exist, assuming that human minds are softwares that can process arbitrary formulas in Peano arithmetics.6 \n… so called because its outline, when processed for non-hazardous viewing, is generally considered to resemble that of the bird. A processed (anamorphically elongated) partial image appears in Appendix 3 of this report, page A3-ii. THE STATED PAGE MUST NOT BE VIEWED THROUGH ANY FORM OF CYLINDRICAL LENS. PROLONGED VIEWING IS STRONGLY DISRECOMMENDED. PLEASE READ PAGE A3-i BEFORE PROCEEDING.\n2-6. This first example of the Berryman Logical Image Technique (hence the usual acronym BLIT) evolved from AI work at the Cambridge IV supercomputer facility, now discontinued. V.Berryman and C.M.Turner [3] hypothesized that pattern-recognition programs of sufficient complexity might be vulnerable to “Gödelian shock input” in the form of data incompatible with internal representation. Berryman went further and suggested that the existence of such a potential input was a logical necessity …\n\n\n\n\nThe BLIT emerging from AI work at the Cambridge IV supercomputer facility.\n\n\nReal-life analogs of cognitohazards are extremely rare, and their status as “cognitohazards” is controversial. Nevertheless, here are some possible analogs.\nThe McCollough effect shows that visual perception can be modified for up to 3 months with just a few minutes of visual stimulus. There have been several explanations, but none is satisfactory. They however all share the common idea that the visual stimulus itself changes the neurons in the low-level of perception.\nBadly designed digital computers, such as the Zuse Z1 or Commodore PET, can suffer from the “killer poke”.\nOther than cognitohazards, the SCP world contains examples of anti-memetics, which are information objects that are hard to remember and reason about. They are not cognitohazards, but are rather camouflaged information."
  },
  {
    "objectID": "sketches/posts/info-warfare/index.html#argument",
    "href": "sketches/posts/info-warfare/index.html#argument",
    "title": "Information Warfare",
    "section": "Argument",
    "text": "Argument\nAn argument can be thought of as a game of wits and rhetorics, structured like a tennis game. Two sides stand facing each other, and hit the “point of the argument” back and forth. When one side finally “drops the ball”, the other side wins the point. When seen this way, many puzzling aspects of argument games turns clear.\nLet’s start with the simplest problem: Why is it that people have an urge to “have the last say”? To not drop the ball. If one side makes an argument, and the other does not offer a counter-argument, the round is assumed to be over and the first side won. If argument games are used to discover truths, this can get rather strange.\n\nThe algorithmic fairness controversy\nIn 2019, I looked into the controversy over “algorithmic fairness” of the Northpointe COMPAS, which I explained in The Racial Algorithmic Bias Controversy. What was curious about the controversy is that neither side did it the way I would have done it. I would have just written out the simple mathematical model, then argued about which measure to choose. Instead, the actual controversy went like this:\n\nProPublica wrote a long article, putting in a lot of human faces for pathos.\nNorthpointe replied tersely that the COMPAS system has good statistical properties.\nProPublica replied with such detailed statistical arguments that I can only characterize as “statistical spam”.\n\nAnd here, the game stood. Who “won”? From my point of view, both sides missed the point completely and the whole game was moot. In fact I would rank ProPublica much lower than Northpointe for first using pathos, then using statistical spam. However, from a tennis game model of argument, ProPublica won, because Northpointe dropped the ball. Statistical spam is bad for discovering the truth, but a good play, because it is hard to counter-argue statistical spam. To the onlookers, they can use the controversy to have their own argument games, which would go as follows:\n\nA: “Haven’t you heard of the algorithmic bias?”\nB: “Yes, but Northpointe made a counter-argument.”\nA: “Well, ProPublica have shown that the counter-argument was statistically moot.”\nAnd at that point, person B would have to admit defeat, or attempt to actually parsing the statistical spam and then give up out of frustration, or dismiss the ProPublica statistical spam as irrelevant.\n\nIn my view, dismissing outright the ProPublica statistical spam is the right move, but it is going against the human intuition that whoever has the last say in an argument is in the right. People intuitively disdain “dismissing the last move” as childish cheating, much as shouting “That doesn’t count!” in a tennis game.\n\n\nContinental philosophy\n\n[Bertrand Russell] thinks I am muddleheaded; but then I think he is simpleminded.\n— Alfred North Whitehead\n\nContinental philosophy presents another example of information warfare. The very definition of continental philosophy is tricky, especially if you want to respect the wishes of continental philosophers, who simultaneously want to build a coherent movement and want to reject all attempts at pigeonholing as inherently dehumanizing. To keep this section meaningful, we will disrespect their wishes.\nTo begin, consider the infamous passage “God is a Lobster”:\n\nChallenger quoted a sentence he said he came across in a geology textbook. He said we needed to learn it by heart because we would only be in a position to understand it later on: “A surface of stratification is a more compact plane of consistency lying between two layers.” The layers are the strata. They come at least in pairs, one serving as substratum for the other. The surface of stratification is a machinic assemblage distinct from the strata. The assemblage is between two layers, between two strata; on one side it faces the strata (in this direction, the assemblage is an inter stratum), but the other side faces something else, the body without organs or plane of consistency (here, it is a metastratum). In effect, the body without organs is itself the plane of consistency, which becomes compact or thickens at the level of the strata.\nGod is a Lobster, or a double pincer, a double bind. Not only do strata come at least in pairs, but in a different way each stratum is double (it itself has several layers). Each stratum exhibits phenomena constitutive of double articulation. Articulate twice, B-A, BA. This is not at all to say that the strata speak or are language based. Double articulation is so extremely variable that we cannot begin with a general model, only a relatively simple case. The first articulation chooses or deducts, from unstable particle-flows, metastable molecular or quasi-molecular units (substances) upon which it imposes a statistical order of connections and successions (forms). (Deleuze and Guattari 1987, chap. 3)\n\nA mathematician might look at this passage, and, if not immediately dismissing it as word salad, start writing down equations like\n\\[\n\\exists G : G \\in \\text{Lobster}, \\exists P : \\text{isPincers}(G, P), \\exists B, A: P = BAAB, \\dots.\n\\]\nThey would then attempt to define an algebraic structure – call it \\(L\\)-sequences, \\(L\\) for “lobster” – and try to construct examples of \\(L\\)-sequences. Deleuze and Guattari never made such an attempt.\n\nOne of the fundamental tasks of the State is to striate the space over which it reigns, or to utilize smooth spaces as a means of communication in the service of striated space. It is a vital concern of every State not only to vanquish nomadism but to control migrations and, more generally, to establish a zone of rights over an entire “exterior,” over all of the flows traversing the ecumenon. (Deleuze and Guattari 1987, 385)\n\nTorrents of noise flood the world every second. Continental philosophy is not noise, even if they resemble noise. Why do they exist? Why do people write them, and why do people transmit them?\n\nThere are no meaningful translations for these terms. They are needlessly recursive. They contain no usable intelligence, yet they are structured intelligently; there is no chance they could have arisen by chance.\n\nI explain them as information objects with three aspects: the literal text, the actual practical use, and the rhetorical game mechanics.\nThe literal text of continental philosophy is typically made of complex sentences, and filled with allusions to past literature, word-plays, puns, neologisms, and foreign words. To interpret them coherently, you need concentration, patience, and to have read the prerequisite texts. Consequently, such texts allow those few readers who can actually interpret them coherently a lot of bragging rights and all the pleasures of being in a society of esoteric wisdom. One can get a distinct feeling of this by going into the local university’s philosophy department and eavesdrop on their late-night discussions, or going to an online forum of continental philosophy.\nHowever, if they are merely hard to understand, they would not have more followers than modern set theory, which is equally complex and arcane:\n\nTheorem 44: The generic mantle of \\(V\\) is a definable transitive class in \\(V\\), containing all ordinals, invariant under set forcing, and a model of the ZF axioms of set theory.\nProof: The generic mantle is a definable transitive class containing all ordinals and closed under the Gödel operations, because it is the intersection of the generic grounds, each of which has those closure properties in the corresponding extension of \\(V\\). So by Fact 10, in order to show that \\(gM\\) is an inner model, it remains to show merely that it is almost universal, which we do below… (Fuchs, Hamkins, and Reitz 2015)\n\nEven a complete outsider to mathematics could quickly understand naive set theory, with an hour of tutorial on drawing circles and dots and Venn diagrams. Modern set theory, while a vast complication over the simple picture of Venn diagrams, is not different in kind. If the outsider looks at the paper on set-theoretic geology and asks “But what does it have to do with geology?”, I can reply simply, “It is a one-way analogy. Set theorists noticed that they can start with ZF set theory and add the Axiom of Choice to get ZFC set theory, then add ZFC-consistency to get an even bigger set theory, and so on. So they thought ‘What if we went the other way? Going down instead of up?’ So they tried making smaller set theories below ZF set theory, and smaller, and smaller, all the way down the ‘mantle’ and below.”\nThere we go, I have explained set-theoretic geology. Reading it, everyone can see that set-theoretic geology has no relevance to actual geology, anymore than the Conway’s Game of Life has relevance to actual biology.\nIn comparison, continental philosophy is designed to look as if they talk about general practical issues, such as economics, justice, the meaning of life, happiness, God (which is a lobster), and more.\nFinally, continental philosophy occupies a particular niche in the game of rhetorics that allows it to reproduce itself socially.\nSince its language is vague and suggestive, it seems relevant to any field, and so it can just walk into any tennis court uninvited and make an opening move. Since its language is dense and hard to distinguish from noise, the others might just ignore it, allowing it to declare victory by default. If someone responds, then the real fun begins.\n\n\n\n\n\n\n\nobjection\nresponse\n\n\n\n\nThis is irrelevant.\nThis problem is relevant to real people’s real lives.\n\n\nThis philosophical argument is irrelevant to the real problem.\n(Quote someone like the Frankfurt School.)\n\n\nWe do not talk about this topic in this style.\nOpen yourself to the challenges of alterity, lest you impose hegemonic reason.\n\n\nSpeak more simply.\nSpeaking simply is reductionistic. The real world is complex.\n\n\n[points at quote] This is technically wrong.\nYou have misunderstood what the authors are really saying. Engage with the source respectfully instead of quoting them out of context.\n\n\n\nBy repeatedly butting into any fashionable controversy of the day, continental philosophy keeps itself from falling into oblivion like modern set theory. By not losing the rhetorical game, it keeps itself from being debunked like old quantum theory. However, to complete its social reproduction, continental philosophy still has to protect itself from challenges from within.\nEuclid’s Elements was a compilation and cleaning-up of geometry books that came before it. It was of such fine quality that it destroyed the works upon which it was based.7 Back then, books were expensive and required frequent copying by hand. Faced with a choice between copying Euclid’s Elements or the books that it obsoleted, the ancients chose to copy Euclid instead of the previous works that were of historical interest only, historical interest that the ancients could not afford.\n7 This is a cartoon sketch. The actual textual history of Euclid’s Elements is studied only by historians of mathematics. Working mathematicians typically hold it at a respectful distance and actively try not to not get too close to it, since an accurate view of the history of mathematics is harmful to the actual practice of mathematics (I speak from personal experience), because the real history of mathematics is not a mathematical textbook written for its educational content. History of mathematics is useful only as a carefully sketched cartoon, omitting almost all the real details, and completely violating historical truth sometimes, because some truths are not adaptive.To reproduce itself, continental philosophy must keep itself from being explained or clarified, especially by insiders. To educate new continental philosophers, the old continental philosophers would write simple explainers and summaries, like The Edinburgh Dictionary of Continental Philosophy. However, such reference books risk becoming substitutes for the original source material, starting a slippery slope into clarity.\nTo ensure its continued survival, Continental Philosophy must keep itself confusing, that is, the old confusions must remain forever confusing. So it developed an immune system that quickly destroys clarification attempts, called “textual deference”. You may recognize its antibodies such as “respect for source material”, “close reading”, “there is no substitute for reading the masters”, “that’s an oversimplification”, “that is not what X meant, but only what Y interpreted X to mean”, and so on. (Smith 1991)\nTo throw such textual deference into stark relief, consider how scientists treat their history. Working scientists have only the most cartoonish knowledge of the history of their fields. Classical papers are rarely read, except in fields so immature that they do not have good textbooks yet and the only way to proceed is reading the original papers. In fact, one can measure the maturity of a field by the ignorance of its practitioners about its original sources, and the lack of bibliography in textbooks. A textbook on electrodynamics does not refer to Coulomb or Maxwell’s original books. A course in relativity does not attempt to explicate the original papers by Einstein. The original sources are typically fumbling and barely comprehensible, a target for improvement, then swiftly relegated to mere history.\nHow does continental philosophy protect itself from progress? The easiest method is to simply declare some sources as original and all else derivative, to freeze the great canonical works for all times. However, this is not sustainable, as new philosophers are not content to merely read the great canons, but want their own chance at joining the great canons. Thus we arrive at the tradition of “the Great Conversation”:\n\nWhat binds the authors together in an intellectual community is the great conversation in which they are engaged. In the works that come later in the sequence of years, we find authors listening to what their predecessors have had to say about this idea or that, this topic or that. They not only harken to the thought of their predecessors, they also respond to it by commenting on it in a variety of ways. (Adler 1990, 28)\n\nWith the Great Conversation, continental philosophy reproduces itself. The old master texts remain inscrutable. New inscrutable texts are added to the canon. New texts that are too clear or sensible are denounced as derivative, analytical, reductionistic, or perhaps kitsch, and denied entrance to the canon. Continental philosophy reproduces itself socially, somehow piling higher and deeper but never making a single bit of progress."
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html",
    "href": "sketches/posts/field-theory-how-to/index.html",
    "title": "How to do field-theoretic calculations",
    "section": "",
    "text": "General references: (Mézard and Montanari 2009; Mei 2021)\nWhen I was studying machine learning theory, I often encounter problems in high-dimensional probability and statistics, and here and there, I would meet with a confusing statement like “let’s do a field-theoretic calculation”. Field theory? I don’t see any gravitational wave or electromagnetic wave!\nIt turns out that “field-theoretic calculation” does mean something like a field, but in a highly abstracted form. It is not well-described in the literature, and it took me great effort to understand what is going on. The calculations are long, arduous, and filled with opportunities for mistakes.\nStill, if you are going to do machine learning theory (as I am), then learning it is a must. This essay is a practical introduction to field-theoretic calculations through detailed examples, including Sanov’s theorem and the analysis of high-dimensional random vectors using overlap matrices. I aim for clarity, pointing out every pitfall that I have fallen into so that you don’t have to.\nWhile the essay assumes familiarity with basic probability, calculus, and linear algebra, it aims to provide a self-contained and accessible introduction to the field. Readers are encouraged to actively engage with the examples, referring back to the provided recipe for field-theoretic calculations as needed."
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#the-recipe-of-field-theoretic-calculations.",
    "href": "sketches/posts/field-theory-how-to/index.html#the-recipe-of-field-theoretic-calculations.",
    "title": "How to do field-theoretic calculations",
    "section": "The recipe of field-theoretic calculations.",
    "text": "The recipe of field-theoretic calculations.\nHere is the reference. You should not read this directly. Instead, you should go directly to the examples below and refer back to this as you go along.\n\nTo calculate: a massive integral of the form\n\n\\[\n\\underbrace{\\int\\cdots\\int}_{\\text{$N \\to \\infty$, with constraint $C$}} (\\text{something}) d^N x\n\\]\n\nThe constraint \\(C\\) is handled by introducing a Dirac delta factor:\n\n\\[\n=_{\\ln} \\underbrace{\\int\\cdots\\int}_{\\text{$N \\to \\infty$}} (\\text{something}) \\times \\delta^{(n)}(C' - C)\n\\]\nwhere \\(n\\) is the number of conditions within \\(C\\). For example, if the constraint says \\(\\sum_i x_i^2 = 1\\), then \\(n=1\\). If the constraint also says \\(\\sum_{ij} x_ix_j = 0\\), then \\(n=2\\), and so on.\n\nExpress the Dirac delta function as a Fourier transform using \\(\\delta^{(n)}(x) = \\frac{1}{(2\\pi)^n}\\int_{\\mathbb{R}^n} e^{-i x \\lambda} d\\lambda\\), then exchange the order of integral.\nPut the inner integral into an exponent, and do some rewriting, resulting in something like\n\n\\[\n=_{\\ln} \\underbrace{\\int\\cdots\\int}_{\\text{$n$}} (\\text{something unimportant}) e^{N \\ln \\int(\\cdots)d^N x} d^n\\lambda\n\\]\nDefine \\(S[\\lambda] := -\\ln \\int(\\cdots)d^N x\\), which is sometimes called the field free energy.\n\nArgue, at large \\(N\\), the integral is dominated by the point where \\(\\nabla S[\\lambda] = 0\\). This has many names: method of steepest descent, saddle point method, stationary point method, stationary-phase method, etc.\nWrite down \\(\\nabla S = 0\\), and give it the fancy name of mean field equation.\nSolve the mean field equation to be some \\(\\lambda^*\\).\nDeclare the result to be\n\n\\[=_{\\ln} e^{-N S[\\lambda^*]}\\]\nHere, we use the notation \\(=_{\\ln}\\) to mean that they have the same exponential rate. That is, \\(f(N) =_{\\ln} g(N)\\) means that\n\\[\n\\lim_{N \\to \\infty} \\frac 1N \\ln f = \\lim_{N \\to \\infty}  \\frac 1N \\ln g\n\\]"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#sanovs-theorem",
    "href": "sketches/posts/field-theory-how-to/index.html#sanovs-theorem",
    "title": "How to do field-theoretic calculations",
    "section": "Sanov’s theorem",
    "text": "Sanov’s theorem\n\nStatement of Sanov’s theorem\nSanov’s theorem concerns the large deviation principle for IID samples from a multinomial distribution.\nTo see how Sanov’s theorem works, let’s consider an example. Say you are working at a dice factory, and your job is to quality-assure dices. An ideal dice should have all \\(p_i = 1/6\\), and your job is to test that a given real dice has \\(p_i \\approx 1/6\\). Since you are not able to observe \\(p\\) directly, you can only throw the dice for \\(N\\) rounds, and compute the empirical distribution \\(\\hat p\\) from the outcomes \\(x_1, ..., x_N\\).\nFor example, if you threw it 7 times and you got every number once except \\(1\\) twice, then \\(\\hat p = (2/7, 1/7, ..., 1/7)\\).\nBecause \\(\\hat p\\) depends on the throws, it is itself a random variable. Intuitively, we should expect that \\(\\hat p\\) converging to \\(p\\) as \\(N \\to \\infty\\). Sanov’s theorem states that it is exponentially unlikely for us to be far from the right answer, with the rate of exponential convergence depending on how far we are mistaken. The further \\(\\hat p\\) is from \\(p\\), the faster we can eliminate that possibility.\nLet’s state this more formally.\nDefine:\n\n\\(p_{1:n}\\) is a probability vector.\n\\(x_{1:N}\\) are IID samples from a multinomial distribution with probability vector \\(p_{1:n}\\).\n\\(\\hat{p}\\) is the empirical distribution derived from these samples.\n\\(\\Delta_n\\) is the probability simplex.\n\nAs the number of samples \\(N\\) approaches infinity, the probability of observing a specific empirical distribution \\(\\hat{p}\\) within a closed subset \\(A \\subset \\Delta_n\\) is characterized by the Kullback-Leibler (KL) divergence \\(D_{KL}(\\hat{p} \\| p)\\) between the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\).\nFormally stated:\n\nTheorem 1 (Sanov) \\[\\lim_{N \\to \\infty} \\frac{1}{N} \\ln Pr(\\hat{p} \\in A) = -\\inf_{\\hat{p} \\in A} D_{KL}(\\hat{p} \\| p)\\]\n\nWe can get an intuitive feel for Sanov’s theorem by the following video.\nLet us fix \\(n = 3\\) and \\(p = (0.2, 0.3, 0.5)\\). We set \\(n = 3\\) because \\(\\Delta_3\\) has two dimensions, which allows us to actually plot it.\nIf we sample for \\(N\\) times from the multinomial distribution defined by \\(p\\), and plot the heatmap of the samples within \\(\\Delta_2\\) (shown as a black triangle), we notice that as \\(N \\rightarrow \\infty\\), the distribution converges to a gaussian around the point \\((0.2,0.3,0.5)\\), with the contours converging in shape to ellipses, with radii converging as \\(1 / \\sqrt{N}\\).\nMeanwhile, the separation between the discrete points converge as \\(1 / N\\), and so the discrete multinomial distribution converges to a continuous gaussian distribution.\nSo, roughly speaking, we have approximately\n\\[\nPr(\\hat p) \\propto e^{- (\\hat p - p)^T NV (\\hat p - p)}\n\\]\nfor some covariance matrix \\(V\\) that describes the shape of the ellipses.\n\n\n\nNow, if we boldly proceed, we would obtain\n\\[\n\\frac 1N \\ln Pr(\\hat p) \\sim -(\\hat p - p)^T A (\\hat p - p)\n\\]\nThis is not quite right, because for any finite \\(N\\), there are only finitely many possible \\(\\hat p\\). So generally \\(Pr(\\hat p) = 0\\), and to fix this, instead of writing \\(Pr(\\hat p)\\), we should write \\(Pr(\\hat p \\in A)\\) for some closed subset \\(A \\subset \\Delta_n\\).\nNext, since given any two exponentially decaying functions \\(f(N) \\propto e^{-kN}, g(N) \\propto e^{-lN}\\), we have \\(f \\gg g\\) iff \\(k &lt; l\\), we only need to account for the least unlikely case:\n\\[\n\\lim_N \\frac 1N \\ln Pr(\\hat p \\in A) \\approx \\max_{q\\in A} (-(q-p)^T A (q-p)) \\approx - \\min_{q\\in A} D_{KL}(q \\| p)\n\\]\n\nAny large deviation is done in the least unlikely of all the unlikely ways!\n(Den Hollander 2008, 10)\n\n\n\nField-theoretic interpretation\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles. Each particle has \\(n\\) degrees of freedom. The state of a particle can be one of the unit vectors in \\(\\mathbb{R}^n\\). The problem is to find the distribution of the average state of all the particles.\nThe particles are completely independent of each other – no interaction whatsoever. This is a “no interaction field”. The only thing saving the example from irrelevance is that the particles are still influenced by something – an externally-imposed potential field, biasing the distribution of each particle’s state independently and identically (IID).\nAs typical in statistical mechanics, we suppose each particle is distributed according to the Boltzmann distribution with temperature \\(1\\). This then tells us that the potential field \\(V\\) satisfies\n\\[p_i = e^{-V_i}/Z\\]\nThat is, we can write \\(V_i = -\\ln p_i - \\ln Z\\) where \\(Z\\) is a normalizing constant (partition function again).\nThe average energy per particle (order parameter) is then\n\\[\\bar E \\sum_i \\bar p_i V_i = -\\sum_i \\bar p_i \\ln p_i - \\ln Z\\]\nThe minimum is at \\(\\bar p_i = p_i\\), and if you have seen some statistical mechanics before, you would notice a pattern: a fluctuation in the average energy per particle should be proportional to \\(e^{-N(\\bar E  - \\bar E_{min})}\\). This is not quite right, in the sense that \\(\\bar E - \\bar E_{min}\\) is not the rate function, but it converges to the rate function in a neighborhood of \\(p\\).\n\n\nField-theoretic calculation\nThis section is based on (Mézard and Montanari 2009, sec. 4.7).\nWe use field-theoretic techniques to compute the rate function, treating both the empirical distribution \\(\\hat{p}\\) and the true distribution \\(p\\) as fields over a finite set of points:\n\\[p: \\{1, 2, ..., n\\} \\to \\mathbb{R}\\]\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nWe really want to write \\(Pr(\\hat{p} = q)\\), even though as we noted, this does not make sense. To fix this problem, we introduce an infinitesimal fudge factor of \\(\\epsilon\\).\nThat is, we want to know the probability of observing an empirical distribution \\(\\hat{p}\\) within an infinitesimal neighborhood of a specific distribution \\(q\\):\n\\[Pr(\\hat{p} =_{\\epsilon} q)\\]\nwhere \\(=\\epsilon\\) means that \\(\\hat{p}_i \\in q_i \\pm \\epsilon\\) for each \\(i = 1, 2, \\dots, n\\), or more roughly, that they are within an \\(O(\\epsilon)\\) distance of each other.\nThus, we can write the desired problem in the form of a constrained integral:\n\\[\nPr(\\hat p =_\\epsilon q) = \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right)\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nNext, we need to move the constraint from the integral domain to the integrand, in preparation for exchanging the order of integral. We do this by introducing a Dirac delta factor.\nFor any fixed tiny, but non-zero, \\(\\epsilon\\), we have\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( 1[\\hat p(x) = q]\\right) =_{\\ln} \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\left( \\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\right)\n\\]\nbecause even though \\(\\frac{1[\\hat p(x) = q]}{\\mathrm{Vol}(\\text{ball with radius } \\epsilon)}\\) is huge, it is still \\(O(1)\\), and \\(\\lim_{N\\to\\infty} \\frac{(\\text{huge but fixed})}{N} = 0\\).\n\\[\n= \\int_{\\mathbb{R}^N} \\rho(x) d^N x \\delta^{(n)}(\\hat p(x) - q)\n\\]\n\n\n\n\n\n\n\n\n\ndo a Fourier transform\n\n\n\n\n\nNext, we do the Fourier transform of the Dirac delta factor. This step is mostly mechanical.\n\\[\n\\begin{aligned}\n\\delta^{(n)} (\\hat p(x) - q) &= \\delta^{(n)} (N\\hat p(x) - N q) \\\\\n&= \\prod_{k=1}^n \\delta \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right) \\\\\n&= \\prod_{k=1}^n \\frac{1}{2\\pi} \\int d\\lambda_k e^{i\\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&=_{\\ln} \\int d^n \\lambda e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nexchange the order of integration\n\n\n\n\n\nNow we exchange the order of integration.\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= {\\color{red}\\int_{\\mathbb{R}^N} \\rho(x) d^N x} {\\color{red}\\int d^n \\lambda} e^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda \\int_{\\mathbb{R}^N} \\rho(x) d^N xe^{i \\sum_{k=1}^n \\lambda_k \\left( \\sum_{j=1}^N 1[x_j = k] - Nq_k \\right)} \\\\\n&= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k}  \\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\ninner integral\n\n\n\n\n\nThe inner integral splits because of the independence of \\(x_1, ..., x_N\\).\n\\[\n\\int_{\\mathbb{R}^N} \\rho(x) d^N x e^{i \\sum_{j=1}^N \\sum_{k=1}^n \\lambda_k 1[x_j = k]} = \\prod_{j=1}^N \\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_j}\n\\]\nwhere the angled bracket denotes a probability expectation, and the subscript \\(x_j\\) denotes what we are taking the expectation over. Because all \\(x_j\\) have the same distribution, it is equal to\n\\[\n\\langle e^{i \\sum_{k=1}^n \\lambda_k 1[x_j = k]}\\rangle_{x_1}^N = \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)^N = e^{N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)}\n\\]\n\n\n\n\n\n\n\n\n\nouter integral\n\n\n\n\n\n\\[\n\\begin{aligned}\nPr(\\hat p =_\\epsilon q) &= \\int d^n \\lambda e^{-iN \\sum_{k=1}^n \\lambda_k q_k + N \\ln \\left(\\sum_{k=1}^n p_k e^{i\\lambda_k} \\right)} \\\\\n&= \\int d^n \\lambda e^{NS[\\lambda]}\n\\end{aligned}\n\\]\nwhere the field free energy is\n\\[S[\\lambda] = \\ln \\left( \\sum_{k\\in 1:n}p_k e^{i \\lambda_k}\\right)-i\\sum_{k\\in 1:n} \\lambda_k q_k\\]\n\n\n\n\n\n\n\n\n\nthe mean field equation\n\n\n\n\n\nThe dominant contribution to the integral arises from the saddle point of the action, which corresponds to the solution of the mean field equation\n\\[\\nabla_{\\lambda} S = 0\\]\nThe mean field equation is solved by some \\(\\lambda^*\\) satisfying:\n\\[\\begin{cases}\ni\\lambda_k^* = \\ln(C q_k/p_k) \\\\\nC = \\sum_k p_k e^{i\\lambda_k^*}\n\\end{cases}\n\\]\nUnfortunately, it is difficult to solve this in closed form, but we are on a lucky break: plugging them back to \\(S[\\lambda^*]\\) gives us a clean solution:\nPlugging those back to \\(S\\), we find that \\[S[\\lambda^*] = -\\sum_{k=1}^n q_k \\ln(q_k/p_k) = -D_{KL}(q\\| p)\\]\n\n\n\nConclusion\n\\[Pr(\\hat p =_{\\epsilon} q) =_{\\ln} e^{NS[\\lambda^*]} = e^{-ND_{KL}(q \\| p)}\\]\n\nThe reader who has never encountered this type of reasoning before may wonder why use such an indirect approach. It turns out that it is a very common formalism in statistical physics, where similar methods are also applied, under the name ‘field theory’, to continuous spaces \\(\\mathcal X\\) (some implicit discretization is then usually assumed at intermediate steps, and the correct definition of a continuum limit is often not obvious). In particular, the reader interested in the statistical-physics approach to optimization problems or information theory will often find this type of calculation in research papers. One of the advantages of this approach is that it provides a formal solution to a large variety of problems. The quantity to be computed is expressed in an integral form. In problems that have a ‘mean-field’ structure, the dimension of the space over which the integration is performed does not depend upon N. Therefore its leading exponential behaviour at large N can be obtained by saddle point methods. The reader who wants to get some practice with this approach is invited to ‘derive’ the various theorems and corollaries of this chapter in this way.\n(Mézard and Montanari 2009, sec. 4.7)"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#overlap-matrix",
    "href": "sketches/posts/field-theory-how-to/index.html#overlap-matrix",
    "title": "How to do field-theoretic calculations",
    "section": "Overlap matrix",
    "text": "Overlap matrix\n\nSetup\nWe investigate the properties of a set of \\(k\\) random vectors sampled uniformly from a high-dimensional sphere as the dimension \\(N\\) approaches infinity. Let \\(\\sigma_1, ..., \\sigma_k\\) be these vectors, each belonging to \\(\\mathbb{R}^N\\) with a norm of \\(\\sqrt{N}\\), and let \\(\\sigma\\) be the matrix formed by concatenating these vectors: \\(\\sigma = [\\sigma_1, ..., \\sigma_k]\\).\nSince \\(E[\\sigma] = 0\\), we focus on analyzing the variance of \\(\\sigma\\), divided by \\(N\\). Define the matrix \\(\\bar{Q}\\) as the normalized outer product of \\(\\sigma\\):\n\\[\\bar Q := \\sigma^T \\sigma/N = \\frac 1N\n\\begin{bmatrix}\n\\sigma_1^T\\sigma_1 & \\sigma_1^T\\sigma_2 & \\cdots &\\sigma_1^T\\sigma_k\\\\\n\\sigma_2^T\\sigma_1 & \\sigma_2^T\\sigma_2 & \\cdots &\\sigma_2^T\\sigma_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_k^T\\sigma_1 & \\sigma_k^T\\sigma_2 & \\cdots & \\sigma_k^T\\sigma_k\n\\end{bmatrix}\\]\nWe know that \\(\\bar{Q}\\) is symmetric, has all entries within the range \\([-1, +1]\\), and has diagonal entries equal to \\(+1\\). Our goal is to uncover further properties of this matrix as \\(N\\) becomes very large.\nLet \\(Q\\) be an arbitrary symmetric matrix with entries in the range \\([-1, +1]\\) and with diagonal entries equal to \\(+1\\). We aim to calculate the rate function \\(S\\) that quantifies the probability of observing an empirical distribution \\(\\bar{Q}\\) that is close to \\(Q\\) as \\(N\\to\\infty\\):\n\\[Pr(\\bar Q =_\\epsilon Q) =_{\\ln} e^{-NS[Q]}\\]\nMore rigorously, we seek to determine:\n\\[\\lim _{\\epsilon \\rightarrow 0} \\lim _{N \\rightarrow \\infty} \\frac{1}{N} \\log Pr \\left(\\bar{Q}(\\sigma)_{i j} \\in\\left[Q_{i j}-\\epsilon, Q_{i j}+\\epsilon\\right], \\forall i, j\\right)\\]\n\n\nField-theoretic interpretation\nWe can interpret this problem through the lens of statistical physics. Imagine a system of \\(N\\) identical particles, each possessing \\(k\\) degrees of freedom. For instance, the \\(i\\)-th particle has degrees of freedom represented by \\((\\sigma_{1, i}, \\dots, \\sigma_{k, i})\\).\nThese particles interact with each other equally, regardless of their spatial separation. This “infinite-range interaction” imposes a global constraint on the system, ensuring a form of “average kinetic energy conservation”. We express this constraint as:\n\\[\\forall j\\in 1:k, \\quad \\sum_{i \\in 1:N}\\sigma_{j, i}^2 = N\\]\nTo illustrate, if we consider \\(\\sigma_{j, i}\\) as the type-\\(j\\) velocity of the \\(i\\)-th particle, then the constraint implies that the average type-\\(j\\) kinetic energy per particle remains constant at \\(1/2\\), even as the number of particles increases.\nSince \\(\\bar{Q}_{j, j'} = \\frac{1}{N} \\sum_{i\\in 1:N}\\sigma_{j, i} \\sigma_{j', i}\\), the matrix \\(\\bar{Q}\\) represents the average covariance between different types of velocities in this system.\n\n\nField-theoretic calculation\nThis section is based on (Mei 2021, lecture 8).\n\n\n\n\n\n\nformulate a constraint\n\n\n\n\n\nTo prepare for the introduction of the Dirac delta factor, we need to move the constraint from the integral domain to the integrand.\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]]\\\\\n&= \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\\\\n\\end{aligned}\n\\]\nNotice well the interplay of two dimensions: The integral \\(\\mathbb{E}_\\sigma\\) is over a very large space, over all particles’ states, of dimension, so it has dimension \\(\\sim k^2 N\\). The constraint \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\) is on the average states of all particles, so it is over a small space of fixed dimensions \\(\\sim k^2\\).\nAgain, we do that \\(1[x=_\\epsilon 0] \\approx \\delta(x)/\\epsilon\\) trick again, and we get\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\mathbb{E}_\\sigma \\left[\\prod_{1 \\leq i &lt; j \\leq k} \\delta(\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0)\\right]\n\\]\nNotice how we have the product \\(\\prod\\) over \\(1 \\leq i &lt; j \\leq k\\), because \\(\\bar Q, Q\\) are both symmetric, with diagonal entries equal to \\(+1\\). If we were to write something like \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\), we would cause \\(\\delta(\\bar Q_{i, i}(\\sigma) - Q_{i, i} =_\\epsilon 0)\\) to always be infinite, which does not work.\n\n\n\n\n\n\n\n\n\nformulate a constraint, take 2\n\n\n\n\n\nIt turns out we have not finished with the constraint yet. Back when we did Sanov’s theorem, because the particles are not interacting, we had to only formulate one constraint, a global one where \\(\\hat p =_\\epsilon q\\). In this case, the particles are interacting by the conservation of kinetic energy:\n\\[\\forall j\\in 1:k, \\quad \\frac 1N \\sum_{i \\in 1:N}\\|\\sigma_{j}\\|_2^2 = 1\\]\nLet us go back to the start again:\n\\[\n\\begin{aligned}\nPr(\\bar Q =_\\epsilon Q) &= \\mathbb{E}_\\sigma [1[\\bar Q(\\sigma) - Q =_\\epsilon 0]] \\\\\n&= \\int_{(\\sqrt N S^N)^k}\\frac{d\\sigma}{\\mathrm{Vol}(\\sqrt N S^N)^k} \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\n\\end{aligned}\n\\]\nThe integral over \\((\\sqrt N S^N)^k\\) is uncomfortable. What to do? … That’s right, when the domain of integral is uncomfortable, we use a Dirac delta factor to move it into the integrand:\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\delta(\\cdots) d\\sigma}\n\\]\nwhere we must fill in the constraint of \\(\\sigma \\in (\\sqrt N S^N)^k\\) into the \\(\\delta(\\cdots)\\). Now, \\(\\sigma \\in (\\sqrt N S^N)^k\\) is equivalent to \\(\\frac 1N \\|\\sigma_{j}\\|^2 - 1 = 0\\) for all \\(j\\in 1:k\\), so naturally, we should try \\(\\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\), giving us\n\\[\n= \\frac{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} {\\color{red} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)} d\\sigma}\n\\]\n\n\n\n\n\n\n\n\n\nthe off-shell trick\n\n\n\n\n\nLet’s take a moment to see what the trick is. The trick is this: We need to integrate something over the uniform distribution on \\((\\sqrt N S^N)^k\\), but integrating over it is difficult, because we don’t have a convenient coordinate system over the sphere \\(\\sqrt N S^N\\). So instead, we slightly thicken each sphere,1 and suddenly we can integrate over the real space \\(\\mathbb{R}^{N \\times k}\\), for which we do have a good coordinate system:\n\\[\n\\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N)^k)} [\\text{something}] = \\lim_{\\epsilon \\downarrow 0} \\mathbb{E}_{\\sigma \\sim \\mathrm{Uniform}((\\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon )^k)} [\\text{something}]\n\\]\nNow, since\n\\[\n\\sigma_j \\in \\sqrt N S^N \\text{thickened by }\\sqrt N \\epsilon \\iff \\|\\sigma_j\\|^2 - N \\in \\pm N\\epsilon \\iff \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0\n\\]\nwe can write the constraint as \\(1 \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 =_\\epsilon 0 \\right) = \\delta \\left( \\frac 1N \\|\\sigma_j\\|^2 - 1 \\right)/\\epsilon\\).\n\n\n\n1 If we return to our field-theoretic interpretation, then allowing \\(\\sigma_j\\) to have norm \\(\\sqrt N \\pm \\sqrt N \\epsilon\\) means that we are allowing the average type-\\(j\\) kinetic energy to fluctuate a bit, even though it is exactly \\(1\\). Quantum field theorists call this off the (kinetic) energy shell, and if you ask, they might wave mysteriously in the air and speak of “virtual particles” and “Faddeev-Popov ghosts”.\n\n\n\n\n\nformulate a constraint, take 3\n\n\n\n\n\nAgain we must handle the constraint of \\(1[\\bar Q(\\sigma) - Q =_\\epsilon 0]\\), but this time it’s different. Whereas before, we had to use \\(\\left[\\prod_{1 \\leq i {\\color{red} &lt;} j \\leq k} 1[\\bar Q_{i, j}(\\sigma) - Q_{i, j} =_\\epsilon 0]\\right]\\), this time we have to use \\(\\prod_{1 \\leq i {\\color{red} \\leq} j \\leq k}\\). Why? Because this time, we have set \\(\\sigma\\) free from the cage of \\((\\sqrt N S^N)^k\\), so the diagonal entries of \\(\\bar Q(\\sigma)\\) are no longer forced to stay exactly \\(+1\\). Thus, we have to do this instead:\n\\[\n\\begin{aligned}\n&= \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; 1[\\bar Q(\\sigma) - Q =_\\epsilon 0]}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n&=_{\\ln} \\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s {\\color{red} \\leq} t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma} \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nintroduce a Dirac delta\n\n\n\n\n\nWe have already done the Dirac deltas. To remind you,\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right) d\\sigma}\n\\]\nActually, it is more convenient to scale the \\(\\delta\\left( \\frac 1N \\|\\sigma_{j}\\|^2 - 1 \\right)\\) both above and below the fraction to \\(\\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right)\\), and similarly scale the \\(\\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})\\) to \\(\\delta(N\\bar Q_{s, t} - NQ_{s, t}) N\\), giving us\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln}\\frac{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma \\; \\prod_{1 \\leq s \\leq t \\leq k} \\delta(\\bar Q_{s, t}(\\sigma) - Q_{s, t})}{\\int_{\\mathbb{R}^{N \\times k}} \\prod_{j\\in 1:k} \\delta\\left( \\|\\sigma_{j}\\|^2 - N \\right) d\\sigma}\n\\]\nwhere we have discarded the factor of \\(N^{\\frac 12 k(k+1)}\\), because it does not matter after taking \\(\\frac 1N \\ln(\\cdots)\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator by field equation\n\n\n\n\n\nSince this time we can’t do the inner integral easily, we will rush directly to the field equation. Don’t worry, as it will all come out correct in the end.\nDo the Fourier transform of the Dirac delta factor, and exchange the order of integration. We first do the numerator.\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n&= \\int_{1 \\leq l \\leq k, \\; 1 \\leq i \\leq j \\leq k} d\\lambda_l dq_{ij} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)} \\\\\n\\end{aligned}\n\\]\nAs before, we only need to find the “least unlikely of all unlikely ways”. That is, we only need to pick the least tiny of all tiny inner integrals. In other words, we need to find a stationary point where it has finally ceased being so tiny:\n\\[0 = \\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{i \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\\]\nBecause we are seeking a stationary point, and we are already doing it physicists, it is no big problem if we seek stationary points over all of the complex plane. That is, we allow \\(q, \\lambda\\) to take not just real, but also complex values.2\nThen we can scale both by \\(-i\\) and get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\n\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N) + \\sum_{1 \\leq i \\leq j \\leq k} q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nFor cleaner notation, we define \\(q_{ji} = q_{ij}\\), and \\(\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_k)\\). Noting that \\(Q_{ii} = 1\\) for all \\(i\\), we get\n\\[\n\\nabla_{q, \\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} (\\Lambda_{ij} + \\frac 12 q_{ij}) (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\nSince the stationary point of this with respect to \\(q, \\lambda\\) is the same as the stationary point of this with respect to \\(\\Lambda + \\frac 12 q\\), we need only\n\\[\n\\nabla_{q}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{- \\left(\\sum_{ij} \\frac 12 q_{ij} (\\sigma_i^T \\sigma_j - NQ_{ij})\n\\right)}\n\\]\n\n\n\n2 If you want justification, look up “the method of steepest descent” (Erdélyi 1956, sec. 2.5). Intuitively speaking, it is because when we are doing an integral like \\(\\int_\\mathbb{R}dq (\\cdots)\\), we are doing a path integral in the complex plane, and so we can deform the path integral in the complex plane and still get the same result. Thus, we can deform it so hard that it walks across a “mountain pass” in the complex plane, where the saddle-point of the mountain pass is where \\(\\nabla_q (\\cdots) = 0\\) – i.e., a stationary point.\n\n\n\n\n\nevaluate the denominator by field equation\n\n\n\n\n\nAt this point, it’s easier to evaluate the denominator first.\nThe same argument given above applies to the denominator. If you are pressed for time, you can just take the previous derivation for the saddle point equation, and set \\(q = 0, Q = 0\\). This gives the field equation\n\\[\n0 = \\nabla_{\\lambda}\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)}\n\\]\nNow, the integral is just a gaussian integral, and it factors, too!\n\\[\n\\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{1 \\leq l \\leq k} \\lambda_l (\\sigma_l^T \\sigma_l - N)} = \\prod_l e^{N \\lambda_l} \\left(\\int_\\mathbb{R}d\\sigma e^{-\\lambda_l \\sigma^2} \\right)^N = e^{N\\sum_l (\\lambda_l - \\frac 12 \\ln\\lambda_l + \\frac 12 \\ln \\pi)}\n\\]\nIts stationary point is \\(e^{N \\frac k2 ( 1 + \\ln 2\\pi)}\\).\n\n\n\n\n\n\n\n\n\nevaluate the numerator, continued\n\n\n\n\n\nWhere we left off, we had to solve the field equation\n\\[\n0 = \\nabla_{q}e^{\\frac 12 N \\sum_{ij}q_{ij}Q_{ij}} \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\sum_{ij} \\frac 12 q_{ij} \\sigma_i^T \\sigma_j}\n\\]\nThe integral is just a gaussian integral, and it factors, too:\n\\[\n\\begin{aligned}\n&= \\int_{\\mathbb{R}^{N \\times k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i^T \\sigma_j} \\\\\n&= \\left(\\int_{\\mathbb{R}^{ k}} d\\sigma e^{-\\frac 12 \\sum_{ij}q_{ij}\\sigma_i \\sigma_j}\\right)^N \\\\\n&= (2\\pi)^{Nk/2}\\det(q)^{-N/2}\n\\end{aligned}\n\\]\nTaking the derivative, we observe that \\(\\nabla_q \\ln \\det q = (q^{-1})^T\\) for an arbitrary matrix \\(q\\). However, as \\(q\\) is constrained to be a symmetric matrix, we obtain\n\\[\\partial_{q_{ij}}(\\braket{Q,q} - \\ln\\det (q)) = \\begin{cases}\nQ_{ij}+ Q_{ji} - (q^{-1})_{ij} - (q^{-1})_{ji} & i\\neq j \\\\\nQ_{ii}- (q^{-1})_{ii} & i=j\n\\end{cases}\\]\nSetting all derivatives to zero yields the solution \\(q = (Q^{-1})^T = Q^{-1}\\). Notably, there exists only one stationary point within the entire multidimensional complex space.\nWe proceed to compute the numerator, resulting in\n\\[\n=\\exp\\left(\\frac N2(\\braket{Q^{-1}, Q} + k \\ln(2\\pi) - \\ln \\det Q^{-1})\\right) = \\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)\n\\]\n\n\n\nIn summary, we have\n\\[\nPr(\\bar Q =_\\epsilon Q) =_{\\ln} \\frac{\\exp\\left(\\frac N2(k+ k \\ln(2\\pi) + \\ln \\det Q)\\right)}{\\exp\\left({N \\frac k2 ( 1 + \\ln 2\\pi)}\\right)} = e^{N S[Q]}\n\\]\nwhere the rate function is\n\\[S[Q] = \\frac 12 \\ln \\det Q\\]"
  },
  {
    "objectID": "sketches/posts/field-theory-how-to/index.html#easy-results",
    "href": "sketches/posts/field-theory-how-to/index.html#easy-results",
    "title": "How to do field-theoretic calculations",
    "section": "Easy results",
    "text": "Easy results\nNow that you have gone through that effort learning field-theoretic calculations, enjoy some quick and simple results.\n\nAsymptotics of spherical volumes\nLet \\(S^{N-1}(r)\\) be the sphere of radius \\(r\\) in \\(\\mathbb{R}^N\\), then\n\\[|S^{N-1}(\\sqrt N)| \\sim (2\\pi e)^{N/2}\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{aligned}\n      |S^{N-1}(\\sqrt N)| &= \\int_{\\mathbb{R}^N} \\delta(x^T x- N )dx \\\\\n      &=_{\\ln} \\int_{\\mathbb{R}^N} \\int_\\mathbb{R}e^{iq(x^T x- N )} dqdx \\\\\n      &= \\int_{\\mathbb{R}} dq \\; e^{-iqN} \\left[\\int_{\\mathbb{R}^N} e^{iqx^T x} dx\\right] \\\\\n      &= \\int_{\\mathbb{R}} dq \\; e^{-iqN} \\left[\\int_{\\mathbb{R}} e^{iqx^2} dx\\right]^N \\\\\n      &= \\int_{\\mathbb{R}} dq \\; e^{-iqN} (e^{i\\pi/4}\\sqrt{\\pi/q})^N \\\\\n      \\end{aligned}\\] - Thus, \\(\\frac 1N \\ln |S^{N-1}(\\sqrt N)|\\) converges to the stationary point of \\[-iq + \\frac{i\\pi}{4} + \\frac 12 \\ln \\pi - \\frac 12 \\ln q\\]\nwhich occurs at \\(q^* = i/2\\). Plugging it in, we obtain\n\\[\\frac 1N \\ln |S^{N-1}(\\sqrt N)| \\to \\ln\\sqrt{2\\pi e}\\]\n\n\n\n\n\n\n\n\n\nProbabilistic interpretation\n\n\n\n\n\nAs is well-known, in high-dimensions, everything looks like a gaussian. Specifically, the standard gaussian distribution \\(\\mathcal N(0, I_N)\\) in \\(\\mathbb{R}^N\\) space is strongly concentrated around the spherical shell of radius \\(\\sqrt N\\), by the law of large numbers. Therefore, the log-surface area of \\(S^{N-1}(\\sqrt N)\\) converges to the entropy of the \\(\\mathcal N(0, I_N)\\), which is just \\(N\\) times the entropy of \\(\\mathcal N(0, 1)\\).\nIn more detail, we can sample from \\(\\mathcal N(0, I_N)\\) in two ways: Either directly sample \\(N\\) standard normal variables independently, or sample a point on \\(S^{N-1}(\\sqrt N)\\), before shifting it along the radius by an independently sampled displacement. Both ways give us exactly the same entropy. Now, because the radius of \\(x \\sim \\mathcal N(0, I_N)\\) is distributed as the square-root of the chi-squared distribution \\(\\chi^2_N\\), we have\n\\[r^2 = \\|x\\|^2 \\sim \\chi^2_N \\approx \\mathcal N(N, 2N)\\]\nWhen \\(N\\) is large, we can write \\(r^2 \\approx N + \\sqrt{2N} z\\) where \\(z \\sim \\mathcal(0, 1)\\), so \\(r \\approx \\sqrt N + 2^{-1/2}z\\). Therefore, the amount of radial displacement is roughly constant, and we have\n\\[\n\\mathrm{Ent}[\\mathcal N(0, I_N)] \\approx \\ln |S^{N-1}(\\sqrt N)| + \\mathrm{Ent}[\\mathcal N(0, 1/2)]\n\\]\ngiving us the second term in the Stirling approximation:\n\\[\n\\frac 1N \\ln |S^{N-1}(\\sqrt N)| = \\ln\\sqrt{2\\pi e} + \\frac{\\ln\\sqrt{\\pi e}}{N} + O(N^{-2})\n\\]\n\n\n\n\n\nCramér’s theorem\nThe most commonly used result from large deviation theory is Cramér’s theorem (Dembo and Zeitouni 2009, theorem 2.2.30).\n\nTheorem 2 (Cramér) Given a vector function \\(M: X \\to \\mathbb{R}^m\\) and a distribution on \\(X\\), its rate function is the convex transform of its cumulant generating function:\n\\[I_X(x) := \\sup_{k \\in \\mathbb{R}^m}(\\braket{k,x} - \\ln \\mathbb{E}_x[e^{\\braket{k, M(x)}}])\\]\nThat is, for any compact subset \\(A \\subset \\mathbb{R}^m\\), the rate function over the whole subset is just the highest possible rate:\n\\[\\lim_N\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) \\in A\\right) = \\sup_{x\\in A} -I_X(x)\\]\nwhere \\(x_1, ..., x_N\\) are IID samples from the same distribution.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove it for \\(A\\) being really small, essentially just a single point, because we can then cut up the whole of \\(A\\) into many pieces like that, and then run the result on each piece. The rate difference is such that only the highest rate can survive, as it races pass every other piece exponentially fast: \\(N^{-1} \\ln (e^{-Na} + e^{-Nb}) \\to \\max(-a, -b)\\), and even if two pieces have the exact same rate, then their combined rate gains a negligible factor of \\(N^{-1}\\ln 2 \\to 0\\).\nSo, we once again repeat the same calculation:\n\\[\n\\begin{aligned}\nPr\\left(\\frac 1N \\sum_{i=1}^N M(x_i) =_\\epsilon m\\right) &=_{\\ln}  \\mathbb{E}_x \\left[\\delta^{(m)}\\left(\\sum_i M(x_i) - Nm\\right)\\right] & \\text{ only $m$ terms in the Dirac delta}\\\\\n&=   \\mathbb{E}_x \\left[\\int_{\\mathbb{R}^m} dq e^{i\\braket{iq, \\sum_i M(x_i) - Nm}}\\right]  & \\text{Dirac delta Fourier transform} \\\\\n&= \\int_{\\mathbb{R}^m} dq e^{-N \\braket{iq, m}}\\mathbb{E}_x[e^{\\braket{iq, M}}]^N& \\text{IID assumption} \\\\\n&= \\int_{\\mathbb{R}^m}dq e^{N(-\\braket{iq, m} + \\ln \\mathbb{E}_x[e^{\\braket{iq, M}}])}\n\\end{aligned}\n\\]\nThe last equation is again dominated by the stationary point. This would give us\n\\[=_{\\ln}\\mathrm{stat}_{q\\in \\mathbb C^m} e^{N(-\\braket{q, m} + \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])}\\]\nWe still don’t know which stationary point we should pick. However, in large deviation theory, we usually pick the global minimum, and most often, the global minimum is the unique stationary point in the real space. Assuming that, we have\n\\[\\frac 1N \\ln Pr\\left(\\frac 1N \\sum_{i=1}^N M(x_i)\\right) \\to -\\sup_{q\\in \\mathbb R^m} (\\braket{q, m} - \\ln \\mathbb{E}_x[e^{\\braket{q, M}}])\\]"
  },
  {
    "objectID": "sketches/index.html",
    "href": "sketches/index.html",
    "title": "Sketches",
    "section": "",
    "text": "I make my writing in a simple 3-stage process: private draft, public draft, public finished essay.\nThe private draft is a collection of works in progress and lightly edited notes, and are not generally useful. The public finished essays are good enough for general reader (“publication quality”). Private draft is private because others are unlikely to benefit from reading it, unless they are trying to impersonate the writer and enter their private world (“What is it like to be Person X?”). Published essays are public because others are likely to benefit from it without entering the outside. They are pallets of information, like encapsulated software objects. You do not need to know how they were made to use them well.\nThe public draft is an interesting intermediate case. Sometimes there are things that are not already in a good enough shape to benefit others, but not good enough to count as a finished essay. They are like as public alphas of games. You can already play with them and have some fun with it, though only in bits and pieces; level 3 is followed by level 6; one NPC has a full personality while the other is a cardboard cutout. Similarly, a public draft is something that looks like an essay, except with pieces of it missing, the other pieces in the wrong place, some sentences missing their clauses, and some paragraphs missing their conclusions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhase-space quantum mechanics\n\n\n\nmath\n\nphysics\n\n\n\nNotes on phase-space quantum mechanics.\n\n\n\n\n\n2024-09-01\n\n51 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for a talk on the history of neural networks\n\n\n\nhistory\n\nAI\n\ncybernetics\n\n\n\nNotes for a talk on the history of neural networks.\n\n\n\n\n\n2024-08-01\n\n45 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nPhilosophical Sketches\n\n\n\nfun\n\nphilosophy\n\n\n\nShort philosophical writings, often poking fun at philosophers.\n\n\n\n\n\n2024-06-16\n\n34 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nFictional ideas\n\n\n\nfun\n\nfiction\n\n\n\nIdeas for stories, fictions, etc.\n\n\n\n\n\n2024-06-11\n\n11 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nResearch ideas\n\n\n\nfun\n\n\n\nIdeas for research. Free for a good home.\n\n\n\n\n\n2024-06-11\n\n6 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nHow to do field-theoretic calculations\n\n\n\nmath\n\nphysics\n\nscaling\n\n\n\nI teach you how to do probability calculations and gigantic integrals in the ‘field-theoretic style’, done in exhaustive details. I aim for clarity, pointing out every pitfall that I have fallen into so that you don’t have to.\n\n\n\n\n\n2024-04-11\n\n25 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nA Scrapbook of Neural Network Lores\n\n\n\nAI\n\nscaling\n\nNN\n\n\n\nLightly curated list of stories, anecdotes, and other various bits from neural network research.\n\n\n\n\n\n2024-01-18\n\n20 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nInformation Warfare\n\n\n\nfun\n\nevolution\n\nsociology\n\n\n\nA mosaic view of the info-battleground: information for anti-informative goals. Intentionally useless errors, diplomatic speech, Chinese censorship, spam campaign, stock market arbitrage, continental philosophy, etc.\n\n\n\n\n\n2023-12-16\n\n45 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Web Design\n\n\n\nprogramming\n\n\n\nMy quick reference for designing content for the Internet.\n\n\n\n\n\n2023-12-10\n\n9 min\n\n2025-07-17\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Using Linux\n\n\n\nprogramming\n\n\n\nMy quick reference for using Linux for doing things.\n\n\n\n\n\n2022-12-10\n\n10 min\n\n2025-07-17\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html",
    "href": "docs/posts/2025-02-27-muon/index.html",
    "title": "The Muon Anthology",
    "section": "",
    "text": "当batch size增大时，学习率该如何随之变化？ - 科学空间|Scientific Spaces (2024-11-14)\nWith the rapid advancement of computing power, we often want to “trade compute for time” – that is, reduce the wallclock time of model training by parallel-scaling the FLOP/sec with more chips. Ideally, we hope that with \\(n\\) times the FLOP/sec, the time to achieve the same effect would be reduced to \\(1/n\\), keeping the total FLOP cost consistent. This hope seems reasonable and natural, but it’s actually non-trivial. Even if we don’t consider parallel bottlenecks like communication, when computing power exceeds a certain scale or when models are smaller than a certain size, we generally can only utilize further compute-scaling by scaling the batch size. However, does increasing batch size always reduce training time while maintaining performance?\nThis is the topic we’ll discuss: When batch size increases, how should various hyperparameters, especially the learning rate, be adjusted to maintain the original training effect and maximize training efficiency? We can also call this the scaling law between batch size and learning rate.\n\n\nIntuitively, when batch size increases, the gradient of each batch will be more accurate, so we can take bigger steps, meaning increasing the learning rate, to reach the goal faster and shorten training time. Most people can generally understand this point. The question is, how much should we increase the learning rate to be most appropriate?\n\n\nThe earliest answer to this question might be square root scaling, meaning \\(n\\times\\) batch size should translate to \\(\\sqrt{n}\\times\\) learning rate. This comes from the 2014 paper One weird trick for parallelizing convolutional neural networks, with the derivation principle being to keep the variance of SGD increments constant. Specifically, we denote the gradient from randomly sampling one sample as \\(\\tilde{\\boldsymbol{g}}\\), with its mean and covariance denoted as \\(\\boldsymbol{g}\\) and \\(\\boldsymbol{\\Sigma}\\) respectively, where \\(\\boldsymbol{g}\\) is the gradient of all samples. When we increase the sampling number to \\(B\\), we have:\n\\[\n\\begin{equation}\\tilde{\\boldsymbol{g}}_B \\triangleq \\frac{1}{B}\\sum_{i=1}^B \\tilde{\\boldsymbol{g}}^{(i)},\\quad \\mathbb{E}[\\tilde{\\boldsymbol{g}}_B] = \\boldsymbol{g},\\quad \\mathbb{E}[(\\tilde{\\boldsymbol{g}}_B-\\boldsymbol{g})(\\tilde{\\boldsymbol{g}}_B-\\boldsymbol{g})^{\\top}]=\\frac{\\boldsymbol{\\Sigma}}{B}\\end{equation}\n\\]\nThat is, increasing the number of samples doesn’t change the mean, but the covariance shrinks to \\(1/B\\). For the SGD optimizer, the increment is \\(-\\eta \\tilde{\\boldsymbol{g}}_B\\), and its covariance is proportional to \\(\\eta^2/B\\). We believe that an appropriate amount of noise (neither too much nor too little) is necessary in the optimization process, so when batch size \\(B\\) changes, we adjust the learning rate \\(\\eta\\) to keep the noise intensity, i.e., the covariance matrix of the increment, constant, which leads to:\n\\[\n\\begin{equation}\\frac{\\eta^2}{B} = \\text{Constant}\\quad\\Rightarrow\\quad \\eta\\propto \\sqrt{B}\\end{equation}\n\\]\nThis gives us the square root scaling law between learning rate and batch size. Later, Train longer, generalize better: closing the generalization gap in large batch training of neural networks also endorsed this choice.\n\n\n\nInterestingly, linear scaling, i.e., \\(\\eta\\propto B\\), often performs better in practice. Even those who first proposed square root scaling in One weird trick for parallelizing convolutional neural networks pointed this out in their paper and stated they couldn’t provide a reasonable explanation for it.\nIn some sense, linear scaling better matches our intuitive understanding, especially as in Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. Indeed, if the gradient direction of \\(n\\) consecutive minibatches doesn’t change much, then if we batch those \\(n\\) minibatches into one batch, we would be dividing by \\(\\frac{1}{nB}\\), yielding \\(\\approx \\frac{1}{n} \\sum_{i=1}^B g_i\\). So to compensate for that, we should scale learnig rate by \\(n\\).\nHowever, assuming that all \\(g_i\\) should point in roughly the same direction is clearly too strong. Relaxing this assumption requires connecting SGD with SDE (Stochastic Differential Equations), which was accomplished by Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations, but the first paper to use this to analyze the relationship between learning rate and batch size should be On the Generalization Benefit of Noise in Stochastic Gradient Descent.\nIn hindsight, establishing this connection isn’t hard to understand. Let the model parameters be \\(\\boldsymbol{\\theta}\\), then the SGD update rule can be rewritten as:\n\\[\n\\begin{equation}\\boldsymbol{\\theta}_{t+1} =\\boldsymbol{\\theta}_t - \\eta \\tilde{\\boldsymbol{g}}_{B,t} =\\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\eta (\\tilde{\\boldsymbol{g}}_{B,t} - \\boldsymbol{g}_t)\\end{equation}\n\\]\nwhere \\(\\tilde{\\boldsymbol{g}}_{B,t} - \\boldsymbol{g}_t\\) is the noise in the gradient. Up to this point, we haven’t made any assumptions about the distribution of this noise, only knowing its mean is \\(\\boldsymbol{0}\\) and its covariance is \\(\\boldsymbol{\\Sigma}_t/B\\). Next, we assume this noise follows a normal distribution \\(\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}_t/B)\\), then the above iteration can be further rewritten as:\n\\[\n\\begin{equation}\\begin{aligned}\n\\boldsymbol{\\theta}_{t+1} =&\\, \\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\eta (\\tilde{\\boldsymbol{g}}_{B,t} - \\boldsymbol{g}_t) \\\\\n=&\\, \\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\eta \\sqrt{\\frac{\\boldsymbol{\\Sigma}_t}{B}}\\boldsymbol{z},\\quad \\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}) \\\\\n=&\\, \\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\sqrt{\\eta} \\sqrt{\\frac{\\eta\\boldsymbol{\\Sigma}_t}{B}}\\boldsymbol{z},\\quad \\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}) \\end{aligned}\\end{equation}\n\\]\nThis means the SGD iteration format \\(\\boldsymbol{\\theta}_{t+1} =\\boldsymbol{\\theta}_t - \\eta \\tilde{\\boldsymbol{g}}_{B,t}\\) is actually approximating the solution to the SDE:\n\\[\n\\begin{equation}d\\boldsymbol{\\theta} = - \\boldsymbol{g}_t dt - \\sqrt{\\frac{\\eta\\boldsymbol{\\Sigma}_t}{B}}d\\boldsymbol{w}\\end{equation}\n\\]\nTherefore, to ensure the running results don’t change significantly when \\(B\\) changes, the form of the above SDE should remain unchanged, which gives us linear scaling \\(\\eta\\propto B\\). The most crucial step in this process is that the step size of the noise term in SDE is the square root of the non-noise term, thus separating out a term \\(\\sqrt{\\eta}\\). We’ve also commented on this in A Casual Discussion of Generative Diffusion Models (Part 5): General Framework SDE Chapter. Simply put, zero-mean Gaussian noise has a certain cancellation effect over the long term, so the step size must be increased to manifest the noise effect.\nThe above conclusions are all based on the SGD optimizer. The paper On the SDEs and Scaling Rules for Adaptive Gradient Algorithms extended it to optimizers like RMSprop and Adam, resulting in square root scaling. Similarly, the slightly earlier Large Batch Optimization for Deep Learning: Training BERT in 76 minutes also applied square root scaling when testing Adam and its variant LAMB. More content can be found in the blogpost How to Scale Hyperparameters as Batch Size Increases.\n\n\n\n\nIt’s certain that whether it’s square root scaling or linear scaling, they can only hold approximately within a local range, because they both include the conclusion that, as long as the batch size is large enough, the learning rate can be arbitrarily large. That is obviously impossible. Additionally, the previous two sections focused on variance, but our fundamental task is to reduce the loss function, so we should start with the loss function.\n\n\nA classic work from this perspective is OpenAI’s An Empirical Model of Large-Batch Training, which analyzes the optimal learning rate for SGD through a second-order approximation of the loss function, concluding that the learning rate increases monotonically with batch size, but has an upper bound. The same approach also appeared in the slightly earlier Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients, though that paper wasn’t used to analyze the role of batch size.\nThe key to the entire derivation is to view the learning rate as an optimization parameter: Let the loss function be \\(\\mathcal{L}(\\boldsymbol{\\theta})\\), the current batch’s gradient be \\(\\tilde{\\boldsymbol{g}}_B\\), then the loss function after SGD is \\(\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B)\\). We view the solution of the optimal learning rate as an optimization problem:\n\\[\n\\begin{equation}\\eta^* = \\mathop{\\text{argmin}}_{\\eta} \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B)]\\end{equation}\n\\]\nThis objective is obviously intuitive – choose the learning rate to make the training efficiency the highest (the loss function decreases the fastest) on average. To solve this problem, we approximately expand the loss function to the second order:\n\\[\n\\begin{equation}\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B) \\approx \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\tilde{\\boldsymbol{g}}_B^{\\top}\\underbrace{\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}}}_{= \\boldsymbol{g}} + \\frac{1}{2}\\eta^2 \\tilde{\\boldsymbol{g}}_B^{\\top}\\underbrace{\\frac{\\partial^2 \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}^2}}_{\\triangleq \\boldsymbol{H}}\\tilde{\\boldsymbol{g}}_B = \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B\\end{equation}\n\\]\nHere, \\(\\boldsymbol{H}\\) is the Hessian matrix, and \\(\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}}\\) is the gradient of the loss function. The ideal objective function \\(\\mathcal L\\) is the loss averaged over all samples, which is why its gradient \\(\\boldsymbol{g}\\) is the average of \\(\\tilde{\\boldsymbol{g}}_B\\). Taking the expectation, we get:\n\\[\n\\begin{equation}\\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B)] \\approx \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B] = \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\boldsymbol{g}^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\mathbb{E}[\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B]\\end{equation}\n\\]\nThe last term involves tricks with the cyclic property of trace:\n\\[\n\\begin{equation}\\begin{aligned}\n\\mathbb{E}[\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B] =&\\, \\mathbb{E}[\\text{Tr}(\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B)]= \\mathbb{E}[\\text{Tr}(\\tilde{\\boldsymbol{g}}_B\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H})] = \\text{Tr}(\\mathbb{E}[\\tilde{\\boldsymbol{g}}_B\\tilde{\\boldsymbol{g}}_B^{\\top}]\\boldsymbol{H})\\\\\n=&\\, \\text{Tr}((\\boldsymbol{g}\\boldsymbol{g}^{\\top} + \\boldsymbol{\\Sigma}/B)\\boldsymbol{H}) = \\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g} + \\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})/B\n\\end{aligned}\\end{equation}\n\\]\nNow, assuming \\(\\boldsymbol{H}\\) is positive definite (i.e. the loss function looks like a bowl), the problem becomes finding the minimum of a quadratic function:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g} + \\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})/B} = \\frac{\\eta_{\\max}}{1 + \\mathcal{B}_{\\text{noise}}/B}\\end{equation}\n\\tag{1}\\]\nWe can state this as “the learning rate increases monotonically with \\(B\\) but has an upper bound”, where:\n\\[\n\\begin{equation}\\eta_{\\max} = \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}},\\qquad\\mathcal{B}_{\\text{noise}} = \\frac{\\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\end{equation}\n\\]\n\n\n\nWhen \\(B \\ll \\mathcal{B}_{\\text{noise}}\\), \\(1 + \\mathcal{B}_{\\text{noise}}/B\\approx \\mathcal{B}_{\\text{noise}}/B\\), so \\(\\eta^* \\approx \\eta_{\\max}B/\\mathcal{B}_{\\text{noise}}\\propto B\\), i.e., linear scaling, which again shows that linear scaling is only a local approximation for small batch sizes. When \\(B &gt; \\mathcal{B}_{\\text{noise}}\\), \\(\\eta^*\\) gradually approaches the saturation value \\(\\eta_{\\max}\\), meaning the increase in training cost far outweighs the improvement in training efficiency. Therefore, \\(\\mathcal{B}_{\\text{noise}}\\) serves as a watershed – once the batch size exceeds this value, there’s no need to continue investing computing power to increase the batch size.\nFor practitioners, the most crucial question is undoubtedly how to estimate \\(\\eta_{\\max}\\) and \\(\\mathcal{B}_{\\text{noise}}\\), especially since \\(\\mathcal{B}_{\\text{noise}}\\) directly affects the scaling law of learning rates and the saturation of training efficiency. Direct calculation of both involves the Hessian matrix \\(\\boldsymbol{H}\\), whose computational cost is proportional to the square of the parameter count. In today’s world, where models with hundreds of millions of parameters are considered small, computing the Hessian matrix is clearly impractical, so more effective calculation methods must be sought.\nLet’s first look at \\(\\mathcal{B}_{\\text{noise}}\\). Its expression is \\(\\frac{\\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\), with an \\(\\boldsymbol{H}\\) in both numerator and denominator, which undoubtedly tempts us to “cancel them out”. In fact, the simplification approach is similar – assuming \\(\\boldsymbol{H}\\) approximates some multiple of the identity matrix, we get:\n\\[\n\\begin{equation}\\mathcal{B}_{\\text{noise}} = \\frac{\\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\approx \\frac{\\text{Tr}(\\boldsymbol{\\Sigma})}{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}\\triangleq \\mathcal{B}_{\\text{simple}}\\end{equation}\n\\]\n\\(\\mathcal{B}_{\\text{simple}}\\) is more computationally feasible, and experiments have found it’s usually a good approximation of \\(\\mathcal{B}_{\\text{noise}}\\). Therefore, we choose to estimate \\(\\mathcal{B}_{\\text{simple}}\\) rather than \\(\\mathcal{B}_{\\text{noise}}\\). Note that \\(\\text{Tr}(\\boldsymbol{\\Sigma})\\) only needs the elements on the diagonal, so there’s no need to compute the full covariance matrix – just calculate the variance for each gradient component separately and sum them up. In data-parallel scenarios, the gradient variances can be directly estimated using the gradients computed on each device.\nIt’s worth noting that the results in Equation 1 are actually dynamic, meaning theoretically, \\(\\eta_{\\max}\\), \\(\\mathcal{B}_{\\text{noise}}\\), and \\(\\mathcal{B}_{\\text{simple}}\\) are different at each training step. So if we want to derive a static pattern, we need to train for a period of time until the model’s training enters the “right track” for the calculated \\(\\mathcal{B}_{\\text{simple}}\\) to be reliable. Alternatively, we can continuously monitor \\(\\mathcal{B}_{\\text{simple}}\\) during training to judge the gap between the current settings and the optimal ones.\nAs for \\(\\eta_{\\max}\\), there’s no need to estimate it according to the formula. We can directly perform a grid search for the learning rate under a small batch size to find an approximate \\(\\eta^*\\), and then use the estimated \\(\\mathcal{B}_{\\text{simple}}\\) to deduce \\(\\eta_{\\max}\\).\n\n\n\nStarting from the above results, we can also derive an asymptotic relationship between the amount of training data and the number of training steps. The derivation is simple: substituting Equation 1 into the loss function, we can calculate that the reduction in the loss function brought by each iteration at the optimal learning rate is:\n\\[\n\\begin{equation}\\Delta\\mathcal{L} = \\mathcal{L}(\\boldsymbol{\\theta}) - \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta^*\\tilde{\\boldsymbol{g}}_B)] \\approx \\frac{\\Delta\\mathcal{L}_{\\max}}{1 + \\mathcal{B}_{\\text{noise}}/B}\\end{equation}\n\\tag{2}\\]\nwhere \\(\\Delta\\mathcal{L}_{\\max} = \\frac{(\\boldsymbol{g}^{\\top}\\boldsymbol{g})^2}{2\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\). The focus now is on interpreting this result.\nWhen \\(B\\to\\infty\\), i.e., full-batch SGD, the reduction in the loss function at each step reaches the maximum \\(\\Delta\\mathcal{L}_{\\max}\\), allowing us to reach the target point with the minimum number of training steps (denoted as \\(S_{\\min}\\)). When \\(B\\) is finite, the average reduction in the loss function at each step is only \\(\\Delta\\mathcal{L}\\), meaning we need \\(1 + \\mathcal{B}_{\\text{noise}}/B\\) steps to achieve the same reduction as a single step of full-batch SGD. So the total number of training steps is roughly \\(S = (1 + \\mathcal{B}_{\\text{noise}}/B)S_{\\min}\\).\nSince the batch size is \\(B\\), the total number of samples consumed in the training process is \\(E = BS = (B + \\mathcal{B}_{\\text{noise}})S_{\\min}\\), which is an increasing function of \\(B\\). When \\(B\\to 0\\), \\(E_{\\min} = \\mathcal{B}_{\\text{noise}}S_{\\min}\\), indicating that as long as we use a sufficiently small batch size to train the model, the total number of training samples E will also decrease accordingly, at the cost of a very large number of training steps \\(S\\). Furthermore, using these notations, we can write the relationship between them as:\n\\[\n\\begin{equation}\\left(\\frac{S}{S_{\\min}} - 1\\right)\\left(\\frac{E}{E_{\\min}} - 1\\right) = 1\\end{equation}\n\\tag{3}\\]\nThis is the scaling law between the amount of training data and the number of training steps, indicating that with less data, we should reduce the batch size and increase the number of training steps to have a better chance of reaching a more optimal solution. The derivation here has been simplified by me, assuming the invariance of \\(\\mathcal{B}_{\\text{noise}}\\) and \\(\\Delta\\mathcal{L}_{\\max}\\) throughout the training process. If necessary, the dynamic changes can be handled more precisely using integration as in the original paper’s appendix (but requires introducing the assumption \\(B = \\sqrt{r\\mathcal{B}_{\\text{noise}}}\\)), which we won’t expand on here.\nAdditionally, since \\(\\mathcal{B}_{\\text{noise}} = E_{\\min}/S_{\\min}\\), the above equation also provides another approach to estimate \\(\\mathcal{B}_{\\text{noise}}\\): obtain multiple (S,E) pairs through multiple experiments and grid searches, then fit the above equation to estimate \\(E_{\\min}\\) and \\(S_{\\min}\\), and subsequently calculate \\(\\mathcal{B}_{\\text{noise}}\\).\n\n\n\n\nIt must be said that OpenAI, as one of the pioneers of various Scaling Laws, deserves its reputation. The aforementioned analysis is quite brilliant, and the results are quite rich. What’s even more remarkable is that the entire derivation process is not complicated, giving a sense of elegant necessity. However, the work we have just worked through is all based on SGD, not applicable to adaptive learning rate optimizers like Adam was still unclear. That analysis was done by Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling.\n\n\nAdam is analyzed similarly as SGD, by second-order expansion. The difference is that the direction vector changes from \\(\\tilde{\\boldsymbol{g}}_B\\) to a general vector \\(\\tilde{\\boldsymbol{u}}_B\\). In this case, we have:\n\\[\n\\begin{equation}\\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{u}}_B)] \\approx \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\text{Tr}(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\boldsymbol{H})\\end{equation}\n\\]\nNow we need to determine \\(\\tilde{\\boldsymbol{u}}_B\\) and calculate the corresponding \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]\\) and \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\). Since we only need an asymptotic relationship, just like in Configuring Different Learning Rates, Can LoRA Rise a Bit More?, we choose SignSGD, i.e., \\(\\tilde{\\boldsymbol{u}}_B = \\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\), as an approximation for Adam. The earliest source of this approach might be Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients. The reasonableness of this approximation is reflected in two points:\n\nRegardless of the values of \\(\\beta_1\\) and \\(\\beta_2\\), the update vector for Adam’s first step is always \\(\\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\);\nWhen \\(\\beta_1=\\beta_2=0\\), Adam’s update vector is always \\(\\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\).\n\nTo calculate \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]\\) and \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\), we also need to assume, as in the Linear scaling section, that \\(\\tilde{\\boldsymbol{g}}_B\\) follows the distribution \\(\\mathcal{N}(\\boldsymbol{g},\\boldsymbol{\\Sigma}/B)\\). To simplify the calculation, we further assume that \\(\\boldsymbol{\\Sigma}\\) is a diagonal matrix \\(\\text{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\cdots)\\), meaning that the components are independent of each other, allowing us to process each component independently. By reparameterization, \\(\\tilde{g}_B\\sim \\mathcal{N}(g, \\sigma^2/B)\\) is equivalent to \\(\\tilde{g}_B=g + \\sigma z/\\sqrt{B},z\\sim\\mathcal{N}(0,1)\\), therefore:\n\\[\n\\begin{equation}\\begin{aligned}\n\\mathbb{E}[\\tilde{u}_B] =&\\, \\mathbb{E}[\\text{sign}(g + \\sigma z/\\sqrt{B})] = \\mathbb{E}[\\text{sign}(g\\sqrt{B}/\\sigma + z)] \\\\\n=&\\,\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} \\text{sign}(g\\sqrt{B}/\\sigma + z) e^{-z^2/2}dz \\\\\n=&\\,\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{-g\\sqrt{B}/\\sigma}  (-1)\\times e^{-z^2/2}dz + \\frac{1}{\\sqrt{2\\pi}}\\int_{-g\\sqrt{B}/\\sigma}^{\\infty}  1\\times e^{-z^2/2}dz \\\\\n=&\\,\\text{erf}\\left(\\frac{g}{\\sigma}\\sqrt{\\frac{B}{2}}\\right)\n\\end{aligned}\\end{equation}\n\\]\nHere, \\(\\text{erf}\\) is the error function, which is an S-shaped function with a range of \\((-1,1)\\) similar to \\(\\tanh\\), and can serve as a smooth approximation of \\(\\text{sign}\\). But since \\(\\text{erf}\\) itself doesn’t have an elementary function expression, it’s better to find an elementary function approximation to more intuitively observe the pattern of change. We’ve discussed this topic before in How the Two Elementary Function Approximations of GELU Came About, but the approximations there are still too complex (involving exponential operations). Here we’ll use something simpler:\n\\[\n\\begin{equation}\\text{erf}(x)\\approx \\text{sign}(x) = \\frac{x}{|x|} = \\frac{x}{\\sqrt{x^2}}\\approx \\frac{x}{\\sqrt{x^2+c}}\\end{equation}\n\\]\nWe choose \\(c=\\pi/4\\) so that the first-order approximation of this approximation at \\(x=0\\) equals the first-order approximation of \\(\\text{erf}\\). Of course, after making so many approximations, the value of \\(c\\) is not very important; we just need to know that such a \\(c &gt; 0\\) exists. Based on this approximation, we get:\n\\[\n\\begin{equation}\\mathbb{E}[\\tilde{u}_B] \\approx \\frac{g/\\sigma}{\\sqrt{\\pi/2B+(g/\\sigma)^2}}\\quad\\Rightarrow\\quad\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]_i \\approx \\frac{g_i/\\sigma_i}{\\sqrt{\\pi/2B+(g_i/\\sigma_i)^2}}\\triangleq \\mu_i\\end{equation}\n\\]\nWe can find that one obvious difference between Adam and SGD is that \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]\\) is already related to \\(B\\) at this step. However, the second moment is simpler now, because the square of \\(\\text{sign}(x)\\) is always 1, so:\n\\[\n\\begin{equation}\\mathbb{E}[\\tilde{u}_B^2] = 1\\quad\\Rightarrow\\quad\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]_{i,j} \\to\\left\\{\\begin{aligned}&=1, & i = j \\\\\n&\\approx\\mu_i \\mu_j,&\\,i\\neq j\\end{aligned}\\right.\\end{equation}\n\\]\nUsing these results, we can obtain:\n\\[\n\\eta^* \\approx \\frac{\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]^{\\top}\\boldsymbol{g}}{\\text{Tr}(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\boldsymbol{H})} \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\mu_i \\mu_j H_{i,j}}\n\\tag{4}\\]\n\\[\n\\Delta \\mathcal{L} = \\mathcal{L}(\\boldsymbol{\\theta}) - \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta^*\\tilde{\\boldsymbol{u}}_B)] \\approx \\frac{1}{2}\\frac{(\\sum_i \\mu_i g_i)^2}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\mu_i \\mu_j H_{i,j}}\n\\tag{5}\\]\n\n\n\nCompared to SGD’s Equation 1, Adam’s Equation 4 is more complex, making it difficult to intuitively see its dependency pattern on \\(B\\). So we’ll start with a few special examples.\nFirst, consider \\(B\\to\\infty\\). In this case, \\(\\mu_i = \\text{sign}(g_i)\\), so:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i |g_i|}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\text{sign}(g_i g_j) H_{i,j}}\\end{equation}\n\\]\nThe difference between this and SGD’s \\(\\eta_{\\max}\\) is that it is not homogeneous with respect to the gradient, but proportional to the gradient’s scale.\nNext, let’s consider the example where \\(\\boldsymbol{H}\\) is a diagonal matrix, i.e., \\(H_{i,j}=0\\) when \\(i\\neq j\\). In this case:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i}}=\\frac{1}{\\sum_i H_{i,i}}\\sum_i \\frac{g_i^2/\\sigma_i}{\\sqrt{\\pi/2B+(g_i/\\sigma_i)^2}}\\end{equation}\n\\]\nEach term in the sum is monotonically increasing with an upper bound with respect to \\(B\\), so the total result is also like this. To capture the most essential pattern, we can consider further simplifying \\(\\mu_i\\) (this is where it starts to differ from the original paper):\n\\[\n\\begin{equation}\\mu_i = \\frac{g_i/\\sigma_i}{\\sqrt{\\pi/2B+(g_i/\\sigma_i)^2}} = \\frac{\\text{sign}(g_i)}{\\sqrt{1 + \\pi(\\sigma_i/g_i)^2/2B}} \\approx \\frac{\\text{sign}(g_i)}{\\sqrt{1 + \\pi\\kappa^2/2B}}\\end{equation}\n\\tag{6}\\]\nThe assumption here is that there exists a constant \\(\\kappa^2\\) independent of \\(i\\) [for example, we can consider taking some kind of average of all \\((\\sigma_i/g_i)^2\\); actually, this \\(\\kappa^2\\) is similar to the \\(\\mathcal{B}_{\\text{simple}}\\) mentioned earlier, and it can also be estimated according to the definition of \\(\\mathcal{B}_{\\text{simple}}\\)], such that replacing \\((\\sigma_i/g_i)^2\\) with \\(\\kappa^2\\) is a good approximation for any \\(i\\). Thus:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i}}\\approx \\frac{\\sum_i |g_i|}{\\sum_i H_{i,i}}\\frac{1}{\\sqrt{1 + \\pi\\kappa^2/2B}}\\end{equation}\n\\tag{7}\\]\nWhen \\(\\pi\\kappa^2\\gg 2B\\), i.e., \\(B \\ll \\pi\\kappa^2/2\\), we can further write the approximation:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\sigma_i}{\\kappa\\sum_i H_{i,i}}\\sqrt{\\frac{2B}{\\pi}} \\propto \\sqrt{B}\\end{equation}\n\\]\nThis indicates that when the batch size itself is relatively small, Adam indeed applies to the square root scaling law.\n\n\n\nIf we apply the approximation Equation 6 to the original Equation 4, we’ll find it exhibits some entirely new characteristics. Specifically, we have:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\mu_i \\mu_j H_{i,j}} \\approx \\frac{\\eta_{\\max}}{\\frac{1}{2}\\left(\\frac{\\beta_{\\text{noise}}}{\\beta} + \\frac{\\beta}{\\beta_{\\text{noise}}}\\right)}\\end{equation}\n\\tag{8}\\]\nwhere \\(\\beta = (1 + \\pi\\kappa^2/2B)^{-1/2}\\), and:\n\\[\n\\begin{equation}\\beta_{\\text{noise}} = \\sqrt{\\frac{\\sum_i H_{i,i}}{\\sum_{i\\neq j}\\text{sign}(g_i g_j) H_{i,j}}},\\quad \\eta_{\\max} = \\frac{\\sum_i |g_i|}{2\\sqrt{\\left(\\sum_i H_{i,i}\\right)\\left(\\sum_{i\\neq j} \\text{sign}(g_i g_j) H_{i,j}\\right)}}\\end{equation}\n\\]\nNote that \\(\\beta\\) is a monotonically increasing function of \\(B\\), but the final approximation in Equation 8 is not a monotonically increasing function of \\(\\beta\\). Instead, it first increases and then decreases, reaching its maximum value at \\(\\beta=\\beta_{\\text{noise}}\\). This implies that there exists a corresponding \\(\\mathcal{B}_{\\text{noise}}\\) such that when the batch size exceeds this \\(\\mathcal{B}_{\\text{noise}}\\), the optimal learning rate should not increase but rather decrease! This is the “surge phenomenon” mentioned in the title of the original paper. (Of course, there’s a limitation here: \\(\\beta\\) is always less than \\(1\\). If \\(\\beta_{\\text{noise}} \\geq 1\\), then the relationship between the optimal learning rate and batch size remains monotonically increasing.)\nRegarding Adam’s \\(\\eta^*\\), OpenAI actually made an unproven conjecture in their paper’s appendix that Adam’s optimal learning rate should be:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\eta_{\\max}}{(1 + \\mathcal{B}_{\\text{noise}}/B)^{\\alpha}}\\end{equation}\n\\tag{9}\\]\nwhere \\(0.5 &lt; \\alpha &lt; 1\\). It now appears that this form is only an approximate result when the diagonal elements of the Hessian matrix dominate. When the effect of non-diagonal elements cannot be ignored, the “surge phenomenon” may occur, that is, the learning rate should actually decrease when the batch size is large enough.\nHow can we intuitively understand the surge phenomenon? I believe it essentially reflects the suboptimality of adaptive learning rate strategies. Still taking the approximation \\(\\tilde{\\boldsymbol{u}}_B = \\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\) as an example, the larger \\(B\\) is, the more accurate \\(\\tilde{\\boldsymbol{g}}_B\\) becomes. As \\(B\\to \\infty\\), we get \\(\\text{sign}(\\boldsymbol{g})\\), but is \\(\\text{sign}(\\boldsymbol{g})\\) the most scientific update direction? Not necessarily, especially in the later stages of training where such adaptive strategies might even have negative effects. Therefore, when \\(B\\) takes an appropriate value, the noise in \\(\\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\) might actually correct this suboptimality. As \\(B\\) continues to increase, the noise decreases, reducing the opportunity for correction, thus requiring more caution by lowering the learning rate.\n\n\n\nSimilar to the SGD analysis, we can also consider \\(\\Delta\\mathcal{L}\\) by substituting Equation 8 into Equation 5, restoring the notation \\(B\\) and then simplifying (the simplification process doesn’t require any approximations) to get:\n\\[\n\\begin{equation}\\Delta \\mathcal{L} \\approx \\frac{\\Delta \\mathcal{L}_{\\max}}{1 + \\mathcal{B}_{\\text{noise-2}}/B}\\end{equation}\n\\tag{10}\\]\nwhere:\n\\[\n\\begin{equation}\\Delta \\mathcal{L}_{\\max} = \\frac{\\beta_{\\text{noise}}\\eta_{\\max}\\sum_i|g_i|}{1 + \\beta_{\\text{noise}}^2},\\quad \\mathcal{B}_{\\text{noise-2}} = \\frac{\\pi\\kappa^2\\beta_{\\text{noise}}^2}{2(1 + \\beta_{\\text{noise}}^2)}\\end{equation}\n\\tag{11}\\]\nNote that \\(\\mathcal{B}_{\\text{noise-2}}\\) is a new notation; it’s not \\(\\mathcal{B}_{\\text{noise}}\\). The latter is derived by solving \\(\\beta=\\beta_{\\text{noise}}\\) for the theoretical optimal batch size, which gives:\n\\[\n\\begin{equation}\\mathcal{B}_{\\text{noise}} = \\frac{\\pi\\kappa^2\\beta_{\\text{noise}}^2}{2(1 - \\beta_{\\text{noise}}^2)}\\end{equation}\n\\]\nThe relationship between them is:\n\\[\n\\begin{equation}\\frac{1}{\\mathcal{B}_{\\text{noise-2}}} - \\frac{1}{\\mathcal{B}_{\\text{noise}}} = \\frac{4}{\\pi\\kappa^2}\\quad\\Rightarrow\\quad \\mathcal{B}_{\\text{noise}} = \\left(\\frac{1}{\\mathcal{B}_{\\text{noise-2}}} - \\frac{4}{\\pi\\kappa^2}\\right)^{-1}\\end{equation}\n\\tag{12}\\]\nSince Equation 10 is formally the same as SGD’s Equation 2, the analysis in that section applies here as well, and we can derive Equation 3:\n\\[\n\\begin{equation}\\left(\\frac{S}{S_{\\min}} - 1\\right)\\left(\\frac{E}{E_{\\min}} - 1\\right) = 1\\end{equation}\n\\]\nExcept now \\(E_{\\min}/S_{\\min} = \\mathcal{B}_{\\text{noise-2}}\\). This gives us a method to estimate \\(\\beta_{\\text{noise}}\\) and \\(\\mathcal{B}_{\\text{noise}}\\): conduct multiple experiments to obtain several \\((S,E)\\) pairs, simultaneously estimating \\(\\kappa^2\\) during the experiments, then fit the above equation to get \\(E_{\\min},S_{\\min}\\), thereby estimating \\(\\mathcal{B}_{\\text{noise-2}}\\), and finally solve for \\(\\beta_{\\text{noise}}\\) using Equation 11.\nIf \\(\\beta_{\\text{noise}} \\geq 1\\), there is no optimal \\(\\mathcal{B}_{\\text{noise}}\\). If \\(\\beta_{\\text{noise}} \\gg 1\\), it indicates that the diagonal elements of the Hessian matrix dominate, in which case the scaling rule Equation 7 applies, and increasing the batch size can always appropriately increase the learning rate. When \\(\\beta_{\\text{noise}} &lt; 1\\), the optimal \\(\\mathcal{B}_{\\text{noise}}\\) can be solved using Equation 12, and if the batch size exceeds this value, the learning rate should actually decrease.\n\n\n\nIt’s worth noting that the starting point and final conclusions of the analysis in the above sections are actually quite similar to those in the original paper Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling, although the approximation methods used in the intermediate process differ.\nMost of the conclusions in the original paper are approximate results under the assumption that \\(B \\ll \\pi(\\sigma_i/g_i)^2/2\\), which leads to the conclusion that the surge phenomenon almost always occurs. This is not very scientific. Most obviously, the form of the assumption \\(B \\ll \\pi(\\sigma_i/g_i)^2/2\\) itself is somewhat problematic, as its right side depends on \\(i\\). We can’t assign a separate batch size to each component, so to obtain a global result, we would need \\(B \\ll \\min_i \\pi(\\sigma_i/g_i)^2/2\\), which is rather stringent.\nThe approach in this article introduces the approximation Equation 6, which can be seen as a mean-field approximation. Intuitively, it is more reasonable than the point-by-point assumption \\(B \\ll \\pi(\\sigma_i/g_i)^2/2\\), so in principle, the conclusions should be more precise. For example, we can conclude that “even if the non-diagonal elements of the Hessian matrix cannot be ignored, the surge phenomenon may not necessarily occur” (depending on \\(\\beta_{\\text{noise}}\\)). Importantly, this precision does not sacrifice simplicity. For instance, Equation 8 is equally clear and concise, Equation 10 has the same form as in the original paper, and no additional approximation assumptions are needed, and so on.\nFinally, a small reflection: OpenAI’s analysis of SGD was actually done in 2018, while the surge phenomenon paper was only published in the middle of this year. It took 6 years to move from SGD to Adam, which is quite surprising. It seems that OpenAI’s prestige and their conjecture Equation 9 led people to believe that there wasn’t much more to do with Adam, not expecting that Adam might have some new properties. Of course, the question of how reasonable \\(\\tilde{\\boldsymbol{u}}_B = \\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\) is as an approximation of Adam and to what extent it represents the actual situation is still worth further consideration."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#from-the-perspective-of-variance",
    "href": "docs/posts/2025-02-27-muon/index.html#from-the-perspective-of-variance",
    "title": "The Muon Anthology",
    "section": "",
    "text": "Intuitively, when batch size increases, the gradient of each batch will be more accurate, so we can take bigger steps, meaning increasing the learning rate, to reach the goal faster and shorten training time. Most people can generally understand this point. The question is, how much should we increase the learning rate to be most appropriate?\n\n\nThe earliest answer to this question might be square root scaling, meaning \\(n\\times\\) batch size should translate to \\(\\sqrt{n}\\times\\) learning rate. This comes from the 2014 paper One weird trick for parallelizing convolutional neural networks, with the derivation principle being to keep the variance of SGD increments constant. Specifically, we denote the gradient from randomly sampling one sample as \\(\\tilde{\\boldsymbol{g}}\\), with its mean and covariance denoted as \\(\\boldsymbol{g}\\) and \\(\\boldsymbol{\\Sigma}\\) respectively, where \\(\\boldsymbol{g}\\) is the gradient of all samples. When we increase the sampling number to \\(B\\), we have:\n\\[\n\\begin{equation}\\tilde{\\boldsymbol{g}}_B \\triangleq \\frac{1}{B}\\sum_{i=1}^B \\tilde{\\boldsymbol{g}}^{(i)},\\quad \\mathbb{E}[\\tilde{\\boldsymbol{g}}_B] = \\boldsymbol{g},\\quad \\mathbb{E}[(\\tilde{\\boldsymbol{g}}_B-\\boldsymbol{g})(\\tilde{\\boldsymbol{g}}_B-\\boldsymbol{g})^{\\top}]=\\frac{\\boldsymbol{\\Sigma}}{B}\\end{equation}\n\\]\nThat is, increasing the number of samples doesn’t change the mean, but the covariance shrinks to \\(1/B\\). For the SGD optimizer, the increment is \\(-\\eta \\tilde{\\boldsymbol{g}}_B\\), and its covariance is proportional to \\(\\eta^2/B\\). We believe that an appropriate amount of noise (neither too much nor too little) is necessary in the optimization process, so when batch size \\(B\\) changes, we adjust the learning rate \\(\\eta\\) to keep the noise intensity, i.e., the covariance matrix of the increment, constant, which leads to:\n\\[\n\\begin{equation}\\frac{\\eta^2}{B} = \\text{Constant}\\quad\\Rightarrow\\quad \\eta\\propto \\sqrt{B}\\end{equation}\n\\]\nThis gives us the square root scaling law between learning rate and batch size. Later, Train longer, generalize better: closing the generalization gap in large batch training of neural networks also endorsed this choice.\n\n\n\nInterestingly, linear scaling, i.e., \\(\\eta\\propto B\\), often performs better in practice. Even those who first proposed square root scaling in One weird trick for parallelizing convolutional neural networks pointed this out in their paper and stated they couldn’t provide a reasonable explanation for it.\nIn some sense, linear scaling better matches our intuitive understanding, especially as in Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. Indeed, if the gradient direction of \\(n\\) consecutive minibatches doesn’t change much, then if we batch those \\(n\\) minibatches into one batch, we would be dividing by \\(\\frac{1}{nB}\\), yielding \\(\\approx \\frac{1}{n} \\sum_{i=1}^B g_i\\). So to compensate for that, we should scale learnig rate by \\(n\\).\nHowever, assuming that all \\(g_i\\) should point in roughly the same direction is clearly too strong. Relaxing this assumption requires connecting SGD with SDE (Stochastic Differential Equations), which was accomplished by Stochastic Modified Equations and Dynamics of Stochastic Gradient Algorithms I: Mathematical Foundations, but the first paper to use this to analyze the relationship between learning rate and batch size should be On the Generalization Benefit of Noise in Stochastic Gradient Descent.\nIn hindsight, establishing this connection isn’t hard to understand. Let the model parameters be \\(\\boldsymbol{\\theta}\\), then the SGD update rule can be rewritten as:\n\\[\n\\begin{equation}\\boldsymbol{\\theta}_{t+1} =\\boldsymbol{\\theta}_t - \\eta \\tilde{\\boldsymbol{g}}_{B,t} =\\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\eta (\\tilde{\\boldsymbol{g}}_{B,t} - \\boldsymbol{g}_t)\\end{equation}\n\\]\nwhere \\(\\tilde{\\boldsymbol{g}}_{B,t} - \\boldsymbol{g}_t\\) is the noise in the gradient. Up to this point, we haven’t made any assumptions about the distribution of this noise, only knowing its mean is \\(\\boldsymbol{0}\\) and its covariance is \\(\\boldsymbol{\\Sigma}_t/B\\). Next, we assume this noise follows a normal distribution \\(\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}_t/B)\\), then the above iteration can be further rewritten as:\n\\[\n\\begin{equation}\\begin{aligned}\n\\boldsymbol{\\theta}_{t+1} =&\\, \\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\eta (\\tilde{\\boldsymbol{g}}_{B,t} - \\boldsymbol{g}_t) \\\\\n=&\\, \\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\eta \\sqrt{\\frac{\\boldsymbol{\\Sigma}_t}{B}}\\boldsymbol{z},\\quad \\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}) \\\\\n=&\\, \\boldsymbol{\\theta}_t - \\eta \\boldsymbol{g}_t - \\sqrt{\\eta} \\sqrt{\\frac{\\eta\\boldsymbol{\\Sigma}_t}{B}}\\boldsymbol{z},\\quad \\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}) \\end{aligned}\\end{equation}\n\\]\nThis means the SGD iteration format \\(\\boldsymbol{\\theta}_{t+1} =\\boldsymbol{\\theta}_t - \\eta \\tilde{\\boldsymbol{g}}_{B,t}\\) is actually approximating the solution to the SDE:\n\\[\n\\begin{equation}d\\boldsymbol{\\theta} = - \\boldsymbol{g}_t dt - \\sqrt{\\frac{\\eta\\boldsymbol{\\Sigma}_t}{B}}d\\boldsymbol{w}\\end{equation}\n\\]\nTherefore, to ensure the running results don’t change significantly when \\(B\\) changes, the form of the above SDE should remain unchanged, which gives us linear scaling \\(\\eta\\propto B\\). The most crucial step in this process is that the step size of the noise term in SDE is the square root of the non-noise term, thus separating out a term \\(\\sqrt{\\eta}\\). We’ve also commented on this in A Casual Discussion of Generative Diffusion Models (Part 5): General Framework SDE Chapter. Simply put, zero-mean Gaussian noise has a certain cancellation effect over the long term, so the step size must be increased to manifest the noise effect.\nThe above conclusions are all based on the SGD optimizer. The paper On the SDEs and Scaling Rules for Adaptive Gradient Algorithms extended it to optimizers like RMSprop and Adam, resulting in square root scaling. Similarly, the slightly earlier Large Batch Optimization for Deep Learning: Training BERT in 76 minutes also applied square root scaling when testing Adam and its variant LAMB. More content can be found in the blogpost How to Scale Hyperparameters as Batch Size Increases."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#facing-losses-head-on",
    "href": "docs/posts/2025-02-27-muon/index.html#facing-losses-head-on",
    "title": "The Muon Anthology",
    "section": "",
    "text": "It’s certain that whether it’s square root scaling or linear scaling, they can only hold approximately within a local range, because they both include the conclusion that, as long as the batch size is large enough, the learning rate can be arbitrarily large. That is obviously impossible. Additionally, the previous two sections focused on variance, but our fundamental task is to reduce the loss function, so we should start with the loss function.\n\n\nA classic work from this perspective is OpenAI’s An Empirical Model of Large-Batch Training, which analyzes the optimal learning rate for SGD through a second-order approximation of the loss function, concluding that the learning rate increases monotonically with batch size, but has an upper bound. The same approach also appeared in the slightly earlier Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients, though that paper wasn’t used to analyze the role of batch size.\nThe key to the entire derivation is to view the learning rate as an optimization parameter: Let the loss function be \\(\\mathcal{L}(\\boldsymbol{\\theta})\\), the current batch’s gradient be \\(\\tilde{\\boldsymbol{g}}_B\\), then the loss function after SGD is \\(\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B)\\). We view the solution of the optimal learning rate as an optimization problem:\n\\[\n\\begin{equation}\\eta^* = \\mathop{\\text{argmin}}_{\\eta} \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B)]\\end{equation}\n\\]\nThis objective is obviously intuitive – choose the learning rate to make the training efficiency the highest (the loss function decreases the fastest) on average. To solve this problem, we approximately expand the loss function to the second order:\n\\[\n\\begin{equation}\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B) \\approx \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\tilde{\\boldsymbol{g}}_B^{\\top}\\underbrace{\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}}}_{= \\boldsymbol{g}} + \\frac{1}{2}\\eta^2 \\tilde{\\boldsymbol{g}}_B^{\\top}\\underbrace{\\frac{\\partial^2 \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}^2}}_{\\triangleq \\boldsymbol{H}}\\tilde{\\boldsymbol{g}}_B = \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B\\end{equation}\n\\]\nHere, \\(\\boldsymbol{H}\\) is the Hessian matrix, and \\(\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial\\boldsymbol{\\theta}}\\) is the gradient of the loss function. The ideal objective function \\(\\mathcal L\\) is the loss averaged over all samples, which is why its gradient \\(\\boldsymbol{g}\\) is the average of \\(\\tilde{\\boldsymbol{g}}_B\\). Taking the expectation, we get:\n\\[\n\\begin{equation}\\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{g}}_B)] \\approx \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B] = \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\boldsymbol{g}^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\mathbb{E}[\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B]\\end{equation}\n\\]\nThe last term involves tricks with the cyclic property of trace:\n\\[\n\\begin{equation}\\begin{aligned}\n\\mathbb{E}[\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B] =&\\, \\mathbb{E}[\\text{Tr}(\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H}\\tilde{\\boldsymbol{g}}_B)]= \\mathbb{E}[\\text{Tr}(\\tilde{\\boldsymbol{g}}_B\\tilde{\\boldsymbol{g}}_B^{\\top}\\boldsymbol{H})] = \\text{Tr}(\\mathbb{E}[\\tilde{\\boldsymbol{g}}_B\\tilde{\\boldsymbol{g}}_B^{\\top}]\\boldsymbol{H})\\\\\n=&\\, \\text{Tr}((\\boldsymbol{g}\\boldsymbol{g}^{\\top} + \\boldsymbol{\\Sigma}/B)\\boldsymbol{H}) = \\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g} + \\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})/B\n\\end{aligned}\\end{equation}\n\\]\nNow, assuming \\(\\boldsymbol{H}\\) is positive definite (i.e. the loss function looks like a bowl), the problem becomes finding the minimum of a quadratic function:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g} + \\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})/B} = \\frac{\\eta_{\\max}}{1 + \\mathcal{B}_{\\text{noise}}/B}\\end{equation}\n\\tag{1}\\]\nWe can state this as “the learning rate increases monotonically with \\(B\\) but has an upper bound”, where:\n\\[\n\\begin{equation}\\eta_{\\max} = \\frac{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}},\\qquad\\mathcal{B}_{\\text{noise}} = \\frac{\\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\end{equation}\n\\]\n\n\n\nWhen \\(B \\ll \\mathcal{B}_{\\text{noise}}\\), \\(1 + \\mathcal{B}_{\\text{noise}}/B\\approx \\mathcal{B}_{\\text{noise}}/B\\), so \\(\\eta^* \\approx \\eta_{\\max}B/\\mathcal{B}_{\\text{noise}}\\propto B\\), i.e., linear scaling, which again shows that linear scaling is only a local approximation for small batch sizes. When \\(B &gt; \\mathcal{B}_{\\text{noise}}\\), \\(\\eta^*\\) gradually approaches the saturation value \\(\\eta_{\\max}\\), meaning the increase in training cost far outweighs the improvement in training efficiency. Therefore, \\(\\mathcal{B}_{\\text{noise}}\\) serves as a watershed – once the batch size exceeds this value, there’s no need to continue investing computing power to increase the batch size.\nFor practitioners, the most crucial question is undoubtedly how to estimate \\(\\eta_{\\max}\\) and \\(\\mathcal{B}_{\\text{noise}}\\), especially since \\(\\mathcal{B}_{\\text{noise}}\\) directly affects the scaling law of learning rates and the saturation of training efficiency. Direct calculation of both involves the Hessian matrix \\(\\boldsymbol{H}\\), whose computational cost is proportional to the square of the parameter count. In today’s world, where models with hundreds of millions of parameters are considered small, computing the Hessian matrix is clearly impractical, so more effective calculation methods must be sought.\nLet’s first look at \\(\\mathcal{B}_{\\text{noise}}\\). Its expression is \\(\\frac{\\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\), with an \\(\\boldsymbol{H}\\) in both numerator and denominator, which undoubtedly tempts us to “cancel them out”. In fact, the simplification approach is similar – assuming \\(\\boldsymbol{H}\\) approximates some multiple of the identity matrix, we get:\n\\[\n\\begin{equation}\\mathcal{B}_{\\text{noise}} = \\frac{\\text{Tr}(\\boldsymbol{\\Sigma}\\boldsymbol{H})}{\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\approx \\frac{\\text{Tr}(\\boldsymbol{\\Sigma})}{\\boldsymbol{g}^{\\top}\\boldsymbol{g}}\\triangleq \\mathcal{B}_{\\text{simple}}\\end{equation}\n\\]\n\\(\\mathcal{B}_{\\text{simple}}\\) is more computationally feasible, and experiments have found it’s usually a good approximation of \\(\\mathcal{B}_{\\text{noise}}\\). Therefore, we choose to estimate \\(\\mathcal{B}_{\\text{simple}}\\) rather than \\(\\mathcal{B}_{\\text{noise}}\\). Note that \\(\\text{Tr}(\\boldsymbol{\\Sigma})\\) only needs the elements on the diagonal, so there’s no need to compute the full covariance matrix – just calculate the variance for each gradient component separately and sum them up. In data-parallel scenarios, the gradient variances can be directly estimated using the gradients computed on each device.\nIt’s worth noting that the results in Equation 1 are actually dynamic, meaning theoretically, \\(\\eta_{\\max}\\), \\(\\mathcal{B}_{\\text{noise}}\\), and \\(\\mathcal{B}_{\\text{simple}}\\) are different at each training step. So if we want to derive a static pattern, we need to train for a period of time until the model’s training enters the “right track” for the calculated \\(\\mathcal{B}_{\\text{simple}}\\) to be reliable. Alternatively, we can continuously monitor \\(\\mathcal{B}_{\\text{simple}}\\) during training to judge the gap between the current settings and the optimal ones.\nAs for \\(\\eta_{\\max}\\), there’s no need to estimate it according to the formula. We can directly perform a grid search for the learning rate under a small batch size to find an approximate \\(\\eta^*\\), and then use the estimated \\(\\mathcal{B}_{\\text{simple}}\\) to deduce \\(\\eta_{\\max}\\).\n\n\n\nStarting from the above results, we can also derive an asymptotic relationship between the amount of training data and the number of training steps. The derivation is simple: substituting Equation 1 into the loss function, we can calculate that the reduction in the loss function brought by each iteration at the optimal learning rate is:\n\\[\n\\begin{equation}\\Delta\\mathcal{L} = \\mathcal{L}(\\boldsymbol{\\theta}) - \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta^*\\tilde{\\boldsymbol{g}}_B)] \\approx \\frac{\\Delta\\mathcal{L}_{\\max}}{1 + \\mathcal{B}_{\\text{noise}}/B}\\end{equation}\n\\tag{2}\\]\nwhere \\(\\Delta\\mathcal{L}_{\\max} = \\frac{(\\boldsymbol{g}^{\\top}\\boldsymbol{g})^2}{2\\boldsymbol{g}^{\\top}\\boldsymbol{H}\\boldsymbol{g}}\\). The focus now is on interpreting this result.\nWhen \\(B\\to\\infty\\), i.e., full-batch SGD, the reduction in the loss function at each step reaches the maximum \\(\\Delta\\mathcal{L}_{\\max}\\), allowing us to reach the target point with the minimum number of training steps (denoted as \\(S_{\\min}\\)). When \\(B\\) is finite, the average reduction in the loss function at each step is only \\(\\Delta\\mathcal{L}\\), meaning we need \\(1 + \\mathcal{B}_{\\text{noise}}/B\\) steps to achieve the same reduction as a single step of full-batch SGD. So the total number of training steps is roughly \\(S = (1 + \\mathcal{B}_{\\text{noise}}/B)S_{\\min}\\).\nSince the batch size is \\(B\\), the total number of samples consumed in the training process is \\(E = BS = (B + \\mathcal{B}_{\\text{noise}})S_{\\min}\\), which is an increasing function of \\(B\\). When \\(B\\to 0\\), \\(E_{\\min} = \\mathcal{B}_{\\text{noise}}S_{\\min}\\), indicating that as long as we use a sufficiently small batch size to train the model, the total number of training samples E will also decrease accordingly, at the cost of a very large number of training steps \\(S\\). Furthermore, using these notations, we can write the relationship between them as:\n\\[\n\\begin{equation}\\left(\\frac{S}{S_{\\min}} - 1\\right)\\left(\\frac{E}{E_{\\min}} - 1\\right) = 1\\end{equation}\n\\tag{3}\\]\nThis is the scaling law between the amount of training data and the number of training steps, indicating that with less data, we should reduce the batch size and increase the number of training steps to have a better chance of reaching a more optimal solution. The derivation here has been simplified by me, assuming the invariance of \\(\\mathcal{B}_{\\text{noise}}\\) and \\(\\Delta\\mathcal{L}_{\\max}\\) throughout the training process. If necessary, the dynamic changes can be handled more precisely using integration as in the original paper’s appendix (but requires introducing the assumption \\(B = \\sqrt{r\\mathcal{B}_{\\text{noise}}}\\)), which we won’t expand on here.\nAdditionally, since \\(\\mathcal{B}_{\\text{noise}} = E_{\\min}/S_{\\min}\\), the above equation also provides another approach to estimate \\(\\mathcal{B}_{\\text{noise}}\\): obtain multiple (S,E) pairs through multiple experiments and grid searches, then fit the above equation to estimate \\(E_{\\min}\\) and \\(S_{\\min}\\), and subsequently calculate \\(\\mathcal{B}_{\\text{noise}}\\)."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#with-adaptive-learning-rates",
    "href": "docs/posts/2025-02-27-muon/index.html#with-adaptive-learning-rates",
    "title": "The Muon Anthology",
    "section": "",
    "text": "It must be said that OpenAI, as one of the pioneers of various Scaling Laws, deserves its reputation. The aforementioned analysis is quite brilliant, and the results are quite rich. What’s even more remarkable is that the entire derivation process is not complicated, giving a sense of elegant necessity. However, the work we have just worked through is all based on SGD, not applicable to adaptive learning rate optimizers like Adam was still unclear. That analysis was done by Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling.\n\n\nAdam is analyzed similarly as SGD, by second-order expansion. The difference is that the direction vector changes from \\(\\tilde{\\boldsymbol{g}}_B\\) to a general vector \\(\\tilde{\\boldsymbol{u}}_B\\). In this case, we have:\n\\[\n\\begin{equation}\\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta\\tilde{\\boldsymbol{u}}_B)] \\approx \\mathcal{L}(\\boldsymbol{\\theta}) - \\eta\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]^{\\top}\\boldsymbol{g} + \\frac{1}{2}\\eta^2 \\text{Tr}(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\boldsymbol{H})\\end{equation}\n\\]\nNow we need to determine \\(\\tilde{\\boldsymbol{u}}_B\\) and calculate the corresponding \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]\\) and \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\). Since we only need an asymptotic relationship, just like in Configuring Different Learning Rates, Can LoRA Rise a Bit More?, we choose SignSGD, i.e., \\(\\tilde{\\boldsymbol{u}}_B = \\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\), as an approximation for Adam. The earliest source of this approach might be Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients. The reasonableness of this approximation is reflected in two points:\n\nRegardless of the values of \\(\\beta_1\\) and \\(\\beta_2\\), the update vector for Adam’s first step is always \\(\\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\);\nWhen \\(\\beta_1=\\beta_2=0\\), Adam’s update vector is always \\(\\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\).\n\nTo calculate \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]\\) and \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\), we also need to assume, as in the Linear scaling section, that \\(\\tilde{\\boldsymbol{g}}_B\\) follows the distribution \\(\\mathcal{N}(\\boldsymbol{g},\\boldsymbol{\\Sigma}/B)\\). To simplify the calculation, we further assume that \\(\\boldsymbol{\\Sigma}\\) is a diagonal matrix \\(\\text{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2,\\cdots)\\), meaning that the components are independent of each other, allowing us to process each component independently. By reparameterization, \\(\\tilde{g}_B\\sim \\mathcal{N}(g, \\sigma^2/B)\\) is equivalent to \\(\\tilde{g}_B=g + \\sigma z/\\sqrt{B},z\\sim\\mathcal{N}(0,1)\\), therefore:\n\\[\n\\begin{equation}\\begin{aligned}\n\\mathbb{E}[\\tilde{u}_B] =&\\, \\mathbb{E}[\\text{sign}(g + \\sigma z/\\sqrt{B})] = \\mathbb{E}[\\text{sign}(g\\sqrt{B}/\\sigma + z)] \\\\\n=&\\,\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} \\text{sign}(g\\sqrt{B}/\\sigma + z) e^{-z^2/2}dz \\\\\n=&\\,\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{-g\\sqrt{B}/\\sigma}  (-1)\\times e^{-z^2/2}dz + \\frac{1}{\\sqrt{2\\pi}}\\int_{-g\\sqrt{B}/\\sigma}^{\\infty}  1\\times e^{-z^2/2}dz \\\\\n=&\\,\\text{erf}\\left(\\frac{g}{\\sigma}\\sqrt{\\frac{B}{2}}\\right)\n\\end{aligned}\\end{equation}\n\\]\nHere, \\(\\text{erf}\\) is the error function, which is an S-shaped function with a range of \\((-1,1)\\) similar to \\(\\tanh\\), and can serve as a smooth approximation of \\(\\text{sign}\\). But since \\(\\text{erf}\\) itself doesn’t have an elementary function expression, it’s better to find an elementary function approximation to more intuitively observe the pattern of change. We’ve discussed this topic before in How the Two Elementary Function Approximations of GELU Came About, but the approximations there are still too complex (involving exponential operations). Here we’ll use something simpler:\n\\[\n\\begin{equation}\\text{erf}(x)\\approx \\text{sign}(x) = \\frac{x}{|x|} = \\frac{x}{\\sqrt{x^2}}\\approx \\frac{x}{\\sqrt{x^2+c}}\\end{equation}\n\\]\nWe choose \\(c=\\pi/4\\) so that the first-order approximation of this approximation at \\(x=0\\) equals the first-order approximation of \\(\\text{erf}\\). Of course, after making so many approximations, the value of \\(c\\) is not very important; we just need to know that such a \\(c &gt; 0\\) exists. Based on this approximation, we get:\n\\[\n\\begin{equation}\\mathbb{E}[\\tilde{u}_B] \\approx \\frac{g/\\sigma}{\\sqrt{\\pi/2B+(g/\\sigma)^2}}\\quad\\Rightarrow\\quad\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]_i \\approx \\frac{g_i/\\sigma_i}{\\sqrt{\\pi/2B+(g_i/\\sigma_i)^2}}\\triangleq \\mu_i\\end{equation}\n\\]\nWe can find that one obvious difference between Adam and SGD is that \\(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]\\) is already related to \\(B\\) at this step. However, the second moment is simpler now, because the square of \\(\\text{sign}(x)\\) is always 1, so:\n\\[\n\\begin{equation}\\mathbb{E}[\\tilde{u}_B^2] = 1\\quad\\Rightarrow\\quad\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]_{i,j} \\to\\left\\{\\begin{aligned}&=1, & i = j \\\\\n&\\approx\\mu_i \\mu_j,&\\,i\\neq j\\end{aligned}\\right.\\end{equation}\n\\]\nUsing these results, we can obtain:\n\\[\n\\eta^* \\approx \\frac{\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B]^{\\top}\\boldsymbol{g}}{\\text{Tr}(\\mathbb{E}[\\tilde{\\boldsymbol{u}}_B\\tilde{\\boldsymbol{u}}_B^{\\top}]\\boldsymbol{H})} \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\mu_i \\mu_j H_{i,j}}\n\\tag{4}\\]\n\\[\n\\Delta \\mathcal{L} = \\mathcal{L}(\\boldsymbol{\\theta}) - \\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta} - \\eta^*\\tilde{\\boldsymbol{u}}_B)] \\approx \\frac{1}{2}\\frac{(\\sum_i \\mu_i g_i)^2}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\mu_i \\mu_j H_{i,j}}\n\\tag{5}\\]\n\n\n\nCompared to SGD’s Equation 1, Adam’s Equation 4 is more complex, making it difficult to intuitively see its dependency pattern on \\(B\\). So we’ll start with a few special examples.\nFirst, consider \\(B\\to\\infty\\). In this case, \\(\\mu_i = \\text{sign}(g_i)\\), so:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i |g_i|}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\text{sign}(g_i g_j) H_{i,j}}\\end{equation}\n\\]\nThe difference between this and SGD’s \\(\\eta_{\\max}\\) is that it is not homogeneous with respect to the gradient, but proportional to the gradient’s scale.\nNext, let’s consider the example where \\(\\boldsymbol{H}\\) is a diagonal matrix, i.e., \\(H_{i,j}=0\\) when \\(i\\neq j\\). In this case:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i}}=\\frac{1}{\\sum_i H_{i,i}}\\sum_i \\frac{g_i^2/\\sigma_i}{\\sqrt{\\pi/2B+(g_i/\\sigma_i)^2}}\\end{equation}\n\\]\nEach term in the sum is monotonically increasing with an upper bound with respect to \\(B\\), so the total result is also like this. To capture the most essential pattern, we can consider further simplifying \\(\\mu_i\\) (this is where it starts to differ from the original paper):\n\\[\n\\begin{equation}\\mu_i = \\frac{g_i/\\sigma_i}{\\sqrt{\\pi/2B+(g_i/\\sigma_i)^2}} = \\frac{\\text{sign}(g_i)}{\\sqrt{1 + \\pi(\\sigma_i/g_i)^2/2B}} \\approx \\frac{\\text{sign}(g_i)}{\\sqrt{1 + \\pi\\kappa^2/2B}}\\end{equation}\n\\tag{6}\\]\nThe assumption here is that there exists a constant \\(\\kappa^2\\) independent of \\(i\\) [for example, we can consider taking some kind of average of all \\((\\sigma_i/g_i)^2\\); actually, this \\(\\kappa^2\\) is similar to the \\(\\mathcal{B}_{\\text{simple}}\\) mentioned earlier, and it can also be estimated according to the definition of \\(\\mathcal{B}_{\\text{simple}}\\)], such that replacing \\((\\sigma_i/g_i)^2\\) with \\(\\kappa^2\\) is a good approximation for any \\(i\\). Thus:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i}}\\approx \\frac{\\sum_i |g_i|}{\\sum_i H_{i,i}}\\frac{1}{\\sqrt{1 + \\pi\\kappa^2/2B}}\\end{equation}\n\\tag{7}\\]\nWhen \\(\\pi\\kappa^2\\gg 2B\\), i.e., \\(B \\ll \\pi\\kappa^2/2\\), we can further write the approximation:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\sigma_i}{\\kappa\\sum_i H_{i,i}}\\sqrt{\\frac{2B}{\\pi}} \\propto \\sqrt{B}\\end{equation}\n\\]\nThis indicates that when the batch size itself is relatively small, Adam indeed applies to the square root scaling law.\n\n\n\nIf we apply the approximation Equation 6 to the original Equation 4, we’ll find it exhibits some entirely new characteristics. Specifically, we have:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\sum_i \\mu_i g_i}{\\sum_i H_{i,i} + \\sum_{i\\neq j} \\mu_i \\mu_j H_{i,j}} \\approx \\frac{\\eta_{\\max}}{\\frac{1}{2}\\left(\\frac{\\beta_{\\text{noise}}}{\\beta} + \\frac{\\beta}{\\beta_{\\text{noise}}}\\right)}\\end{equation}\n\\tag{8}\\]\nwhere \\(\\beta = (1 + \\pi\\kappa^2/2B)^{-1/2}\\), and:\n\\[\n\\begin{equation}\\beta_{\\text{noise}} = \\sqrt{\\frac{\\sum_i H_{i,i}}{\\sum_{i\\neq j}\\text{sign}(g_i g_j) H_{i,j}}},\\quad \\eta_{\\max} = \\frac{\\sum_i |g_i|}{2\\sqrt{\\left(\\sum_i H_{i,i}\\right)\\left(\\sum_{i\\neq j} \\text{sign}(g_i g_j) H_{i,j}\\right)}}\\end{equation}\n\\]\nNote that \\(\\beta\\) is a monotonically increasing function of \\(B\\), but the final approximation in Equation 8 is not a monotonically increasing function of \\(\\beta\\). Instead, it first increases and then decreases, reaching its maximum value at \\(\\beta=\\beta_{\\text{noise}}\\). This implies that there exists a corresponding \\(\\mathcal{B}_{\\text{noise}}\\) such that when the batch size exceeds this \\(\\mathcal{B}_{\\text{noise}}\\), the optimal learning rate should not increase but rather decrease! This is the “surge phenomenon” mentioned in the title of the original paper. (Of course, there’s a limitation here: \\(\\beta\\) is always less than \\(1\\). If \\(\\beta_{\\text{noise}} \\geq 1\\), then the relationship between the optimal learning rate and batch size remains monotonically increasing.)\nRegarding Adam’s \\(\\eta^*\\), OpenAI actually made an unproven conjecture in their paper’s appendix that Adam’s optimal learning rate should be:\n\\[\n\\begin{equation}\\eta^* \\approx \\frac{\\eta_{\\max}}{(1 + \\mathcal{B}_{\\text{noise}}/B)^{\\alpha}}\\end{equation}\n\\tag{9}\\]\nwhere \\(0.5 &lt; \\alpha &lt; 1\\). It now appears that this form is only an approximate result when the diagonal elements of the Hessian matrix dominate. When the effect of non-diagonal elements cannot be ignored, the “surge phenomenon” may occur, that is, the learning rate should actually decrease when the batch size is large enough.\nHow can we intuitively understand the surge phenomenon? I believe it essentially reflects the suboptimality of adaptive learning rate strategies. Still taking the approximation \\(\\tilde{\\boldsymbol{u}}_B = \\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\) as an example, the larger \\(B\\) is, the more accurate \\(\\tilde{\\boldsymbol{g}}_B\\) becomes. As \\(B\\to \\infty\\), we get \\(\\text{sign}(\\boldsymbol{g})\\), but is \\(\\text{sign}(\\boldsymbol{g})\\) the most scientific update direction? Not necessarily, especially in the later stages of training where such adaptive strategies might even have negative effects. Therefore, when \\(B\\) takes an appropriate value, the noise in \\(\\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\) might actually correct this suboptimality. As \\(B\\) continues to increase, the noise decreases, reducing the opportunity for correction, thus requiring more caution by lowering the learning rate.\n\n\n\nSimilar to the SGD analysis, we can also consider \\(\\Delta\\mathcal{L}\\) by substituting Equation 8 into Equation 5, restoring the notation \\(B\\) and then simplifying (the simplification process doesn’t require any approximations) to get:\n\\[\n\\begin{equation}\\Delta \\mathcal{L} \\approx \\frac{\\Delta \\mathcal{L}_{\\max}}{1 + \\mathcal{B}_{\\text{noise-2}}/B}\\end{equation}\n\\tag{10}\\]\nwhere:\n\\[\n\\begin{equation}\\Delta \\mathcal{L}_{\\max} = \\frac{\\beta_{\\text{noise}}\\eta_{\\max}\\sum_i|g_i|}{1 + \\beta_{\\text{noise}}^2},\\quad \\mathcal{B}_{\\text{noise-2}} = \\frac{\\pi\\kappa^2\\beta_{\\text{noise}}^2}{2(1 + \\beta_{\\text{noise}}^2)}\\end{equation}\n\\tag{11}\\]\nNote that \\(\\mathcal{B}_{\\text{noise-2}}\\) is a new notation; it’s not \\(\\mathcal{B}_{\\text{noise}}\\). The latter is derived by solving \\(\\beta=\\beta_{\\text{noise}}\\) for the theoretical optimal batch size, which gives:\n\\[\n\\begin{equation}\\mathcal{B}_{\\text{noise}} = \\frac{\\pi\\kappa^2\\beta_{\\text{noise}}^2}{2(1 - \\beta_{\\text{noise}}^2)}\\end{equation}\n\\]\nThe relationship between them is:\n\\[\n\\begin{equation}\\frac{1}{\\mathcal{B}_{\\text{noise-2}}} - \\frac{1}{\\mathcal{B}_{\\text{noise}}} = \\frac{4}{\\pi\\kappa^2}\\quad\\Rightarrow\\quad \\mathcal{B}_{\\text{noise}} = \\left(\\frac{1}{\\mathcal{B}_{\\text{noise-2}}} - \\frac{4}{\\pi\\kappa^2}\\right)^{-1}\\end{equation}\n\\tag{12}\\]\nSince Equation 10 is formally the same as SGD’s Equation 2, the analysis in that section applies here as well, and we can derive Equation 3:\n\\[\n\\begin{equation}\\left(\\frac{S}{S_{\\min}} - 1\\right)\\left(\\frac{E}{E_{\\min}} - 1\\right) = 1\\end{equation}\n\\]\nExcept now \\(E_{\\min}/S_{\\min} = \\mathcal{B}_{\\text{noise-2}}\\). This gives us a method to estimate \\(\\beta_{\\text{noise}}\\) and \\(\\mathcal{B}_{\\text{noise}}\\): conduct multiple experiments to obtain several \\((S,E)\\) pairs, simultaneously estimating \\(\\kappa^2\\) during the experiments, then fit the above equation to get \\(E_{\\min},S_{\\min}\\), thereby estimating \\(\\mathcal{B}_{\\text{noise-2}}\\), and finally solve for \\(\\beta_{\\text{noise}}\\) using Equation 11.\nIf \\(\\beta_{\\text{noise}} \\geq 1\\), there is no optimal \\(\\mathcal{B}_{\\text{noise}}\\). If \\(\\beta_{\\text{noise}} \\gg 1\\), it indicates that the diagonal elements of the Hessian matrix dominate, in which case the scaling rule Equation 7 applies, and increasing the batch size can always appropriately increase the learning rate. When \\(\\beta_{\\text{noise}} &lt; 1\\), the optimal \\(\\mathcal{B}_{\\text{noise}}\\) can be solved using Equation 12, and if the batch size exceeds this value, the learning rate should actually decrease.\n\n\n\nIt’s worth noting that the starting point and final conclusions of the analysis in the above sections are actually quite similar to those in the original paper Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling, although the approximation methods used in the intermediate process differ.\nMost of the conclusions in the original paper are approximate results under the assumption that \\(B \\ll \\pi(\\sigma_i/g_i)^2/2\\), which leads to the conclusion that the surge phenomenon almost always occurs. This is not very scientific. Most obviously, the form of the assumption \\(B \\ll \\pi(\\sigma_i/g_i)^2/2\\) itself is somewhat problematic, as its right side depends on \\(i\\). We can’t assign a separate batch size to each component, so to obtain a global result, we would need \\(B \\ll \\min_i \\pi(\\sigma_i/g_i)^2/2\\), which is rather stringent.\nThe approach in this article introduces the approximation Equation 6, which can be seen as a mean-field approximation. Intuitively, it is more reasonable than the point-by-point assumption \\(B \\ll \\pi(\\sigma_i/g_i)^2/2\\), so in principle, the conclusions should be more precise. For example, we can conclude that “even if the non-diagonal elements of the Hessian matrix cannot be ignored, the surge phenomenon may not necessarily occur” (depending on \\(\\beta_{\\text{noise}}\\)). Importantly, this precision does not sacrifice simplicity. For instance, Equation 8 is equally clear and concise, Equation 10 has the same form as in the original paper, and no additional approximation assumptions are needed, and so on.\nFinally, a small reflection: OpenAI’s analysis of SGD was actually done in 2018, while the surge phenomenon paper was only published in the middle of this year. It took 6 years to move from SGD to Adam, which is quite surprising. It seems that OpenAI’s prestige and their conjecture Equation 9 led people to believe that there wasn’t much more to do with Adam, not expecting that Adam might have some new properties. Of course, the question of how reasonable \\(\\tilde{\\boldsymbol{u}}_B = \\text{sign}(\\tilde{\\boldsymbol{g}}_B)\\) is as an approximation of Adam and to what extent it represents the actual situation is still worth further consideration."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#newtons-method",
    "href": "docs/posts/2025-02-27-muon/index.html#newtons-method",
    "title": "The Muon Anthology",
    "section": "Newton’s method",
    "text": "Newton’s method\nLet the loss function be \\(\\mathcal{L}(\\boldsymbol{\\theta})\\), where the parameter to be optimized is \\(\\boldsymbol{\\theta}\\), and our optimization goal is\n\\[\n\\begin{equation}\\boldsymbol{\\theta}^* = \\mathop{\\text{argmin}}_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta})\\end{equation}\n\\tag{13}\\]\nAssuming the current value of \\(\\boldsymbol{\\theta}\\) is \\(\\boldsymbol{\\theta}_t\\), Newton’s method seeks \\(\\boldsymbol{\\theta}_{t+1}\\) by expanding the loss function to the second order:\n\\[\n\\begin{equation}\\mathcal{L}(\\boldsymbol{\\theta})\\approx \\mathcal{L}(\\boldsymbol{\\theta}_t) + \\boldsymbol{g}_t^{\\top}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_t) + \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_t)^{\\top}\\boldsymbol{\\mathcal{H}}_t(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_t)\\end{equation}\n\\]\nwhere \\(\\boldsymbol{g}_t = \\nabla_{\\boldsymbol{\\theta}_t}\\mathcal{L}(\\boldsymbol{\\theta}_t)\\) is the gradient, and \\(\\boldsymbol{\\mathcal{H}}_t=\\nabla_{\\boldsymbol{\\theta}_t}^2\\mathcal{L}(\\boldsymbol{\\theta}_t)\\) is the Hessian matrix. Assuming the positive definiteness of the Hessian matrix, the right side of the above equation has a unique minimum \\(\\boldsymbol{\\theta}_t - \\boldsymbol{\\mathcal{H}}_t^{-1}\\boldsymbol{g}_t\\), which Newton’s method uses as the next step \\(\\boldsymbol{\\theta}_{t+1}\\):\n\\[\n\\begin{equation}\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t-\\boldsymbol{\\mathcal{H}}_t^{-1}\\boldsymbol{g}_t = \\boldsymbol{\\theta}_t - (\\nabla_{\\boldsymbol{\\theta}_t}^2\\mathcal{L})^{-1} \\nabla_{\\boldsymbol{\\theta}_t}\\mathcal{L}\\end{equation}\n\\]\nNote that the above equation does not have an additional learning rate parameter, so Newton’s method already has an adaptive learning rate. Of course, since the complexity of the Hessian matrix is proportional to the square of the parameter count, the complete Newton’s method has basically only theoretical value in deep learning. To apply Newton’s method in practice, significant simplifying assumptions about the Hessian matrix are needed, such as assuming it’s diagonal or low-rank.\nFrom the Newton’s method perspective, SGD assumes \\(\\boldsymbol{\\mathcal{H}}_t=\\eta_t^{-1}\\boldsymbol{I}\\), while Adam assumes \\(\\boldsymbol{\\mathcal{H}}_t=\\eta_t^{-1}\\text{diag}(\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon)\\), where\n\\[\n\\begin{equation}\\text{Adam}\\triangleq \\left\\{\\begin{aligned}\n&\\boldsymbol{m}_t = \\beta_1 \\boldsymbol{m}_{t-1} + \\left(1 - \\beta_1\\right) \\boldsymbol{g}_t\\\\\n&\\boldsymbol{v}_t = \\beta_2 \\boldsymbol{v}_{t-1} + \\left(1 - \\beta_2\\right) \\boldsymbol{g}_t\\odot\\boldsymbol{g}_t\\\\\n&\\hat{\\boldsymbol{m}}_t = \\boldsymbol{m}_t\\left/\\left(1 - \\beta_1^t\\right)\\right.\\\\\n&\\hat{\\boldsymbol{v}}_t = \\boldsymbol{v}_t\\left/\\left(1 - \\beta_2^t\\right)\\right.\\\\\n&\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t \\hat{\\boldsymbol{m}}_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon\\right)\\right.\n\\end{aligned}\\right.\\end{equation}\n\\]\nNext, we want to demonstrate that \\(\\eta_t^{-1}\\text{diag}(\\sqrt{\\hat{\\boldsymbol{v}}_t})\\) is a better approximation of \\(\\boldsymbol{\\mathcal{H}}_t\\)."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#gradient-approximation",
    "href": "docs/posts/2025-02-27-muon/index.html#gradient-approximation",
    "title": "The Muon Anthology",
    "section": "Gradient approximation",
    "text": "Gradient approximation\nThe key to the proof is using the first-order approximation of the gradient:\n\\[\n\\begin{equation}\\boldsymbol{g}_{\\boldsymbol{\\theta}} \\approx \\boldsymbol{g}_{\\boldsymbol{\\theta}^*} + \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*)\\end{equation}\n\\]\nwhere \\(\\boldsymbol{g}_{\\boldsymbol{\\theta}^*}\\) and \\(\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}\\) indicate that we expand at \\(\\boldsymbol{\\theta}=\\boldsymbol{\\theta}^*\\). Here, \\(\\boldsymbol{\\theta}^*\\) is the target we are looking for Equation 13, at which point the model’s gradient is zero, so the above equation can be simplified to:\n\\[\n\\begin{equation}\\boldsymbol{g}_{\\boldsymbol{\\theta}} \\approx \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*)\\end{equation}\n\\]\nThus:\n\\[\n\\begin{equation}\\boldsymbol{g}_{\\boldsymbol{\\theta}}\\boldsymbol{g}_{\\boldsymbol{\\theta}}^{\\top} \\approx \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*)(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*)^{\\top}\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}^{\\top}\\end{equation}\n\\]\nAfter training has entered the “grooves of convergence” like a train snapping to its rails, the model will be spiraling around \\(\\boldsymbol{\\theta}^*\\) for a long time, converging slowly. To some extent, we can view \\(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*\\) as a random variable following a normal distribution \\(\\mathcal{N}(\\boldsymbol{0},\\sigma^2\\boldsymbol{I})\\). Then:\n\\[\n\\begin{equation}\\mathbb{E}[\\boldsymbol{g}_{\\boldsymbol{\\theta}}\\boldsymbol{g}_{\\boldsymbol{\\theta}}^{\\top}] \\approx  \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}\\mathbb{E}[(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*)(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^*)^{\\top}]\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}^{\\top} = \\sigma^2\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}^{\\top}\\end{equation}\n\\tag{14}\\]\nAssuming the Hessian matrix is diagonal, we can keep only the diagonal elements of the above equation:\n\\[\n\\begin{equation}\\text{diag}(\\mathbb{E}[\\boldsymbol{g}_{\\boldsymbol{\\theta}}\\odot\\boldsymbol{g}_{\\boldsymbol{\\theta}}]) \\approx  \\sigma^2\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}^2\\quad\\Rightarrow\\quad \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*} = \\frac{1}{\\sigma}\\text{diag}(\\sqrt{\\mathbb{E}[\\boldsymbol{g}_{\\boldsymbol{\\theta}}\\odot\\boldsymbol{g}_{\\boldsymbol{\\theta}}]})\\end{equation}\n\\]\nDoesn’t that look a bit similar? Adam’s \\(\\hat{\\boldsymbol{v}}_t\\) is a moving average of the squared gradient, which can be seen as an approximation of \\(\\mathbb{E}[\\boldsymbol{g}_{\\boldsymbol{\\theta}}\\odot\\boldsymbol{g}_{\\boldsymbol{\\theta}}]\\). Finally, if we assume that \\(\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}_t} \\approx \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}\\), we can conclude that \\(\\eta_t^{-1}\\text{diag}(\\sqrt{\\hat{\\boldsymbol{v}}_t})\\) is an approximation of \\(\\boldsymbol{\\mathcal{H}}_t\\).\nThis can also explain why Adam’s \\(\\beta_2\\) is usually greater than \\(\\beta_1\\). To estimate the Hessian more accurately, the moving average of \\(\\hat{\\boldsymbol{v}}_t\\) should be as “long-term” as possible (close to uniform averaging), so \\(\\beta_2\\) should be very close to 1. However, if the momentum \\(\\hat{\\boldsymbol{m}}_t\\), which is a moving average of gradients, averages over too long a period, the result will approach \\(\\boldsymbol{g}_{\\boldsymbol{\\theta}^*}=\\boldsymbol{0}\\), which is not good. Therefore, the moving average for momentum should be more localized."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#related-work",
    "href": "docs/posts/2025-02-27-muon/index.html#related-work",
    "title": "The Muon Anthology",
    "section": "Related work",
    "text": "Related work\nFor readers familiar with Hessian matrix theory, their first reaction to the above conclusion might be confusion rather than recognition. This is because a classic approximation of the Hessian matrix is the outer product of the Jacobian matrix (similar to the gradient), while the Hessian approximation here is the square root of the gradient’s outer product – the two differ by a square root.\nSpecifically, consider the squared error loss:\n\\[\n\\begin{equation}\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\Vert \\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\Vert^2]\\end{equation}\n\\tag{15}\\]\nExpanding at \\(\\boldsymbol{\\theta}_t\\), we have \\(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\approx \\boldsymbol{f}_{\\boldsymbol{\\theta}_t}(\\boldsymbol{x}) + \\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}_t}^{\\top} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_t)\\), where \\(\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}_t}=\\nabla_{\\boldsymbol{\\theta}_t} \\boldsymbol{f}_{\\boldsymbol{\\theta}_t}(\\boldsymbol{x})\\) is the Jacobian matrix. Substituting into the above equation:\n\\[\n\\begin{equation}\\mathcal{L}(\\boldsymbol{\\theta}) \\approx \\frac{1}{2}\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\Vert \\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}_t}(\\boldsymbol{x}) - \\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}_t}^{\\top} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_t)\\Vert^2]\\end{equation}\n\\]\nAfter simplification, the above equation is just a quadratic form in \\(\\boldsymbol{\\theta}\\), so we can directly write out its Hessian matrix:\n\\[\n\\begin{equation}\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}_t} \\approx \\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}_t}\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}_t}^{\\top}]\\end{equation}\n\\]\nThis is the Hessian approximation based on the outer product of the Jacobian matrix, which is the theoretical foundation of the Gauss–Newton method. Of course, \\(\\boldsymbol{\\mathcal{J}}\\) is not yet \\(\\boldsymbol{g}\\); we want to try to connect the result with \\(\\mathcal{g}\\). Taking the derivative of Equation 15 directly:\n\\[\n\\begin{equation}\\boldsymbol{g}_{\\boldsymbol{\\theta}} = \\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))]\\end{equation}\n\\]\nThus:\n\\[\n\\begin{equation}\\begin{aligned}\n\\boldsymbol{g}_{\\boldsymbol{\\theta}} \\boldsymbol{g}_{\\boldsymbol{\\theta}}^{\\top} =&\\,  \\big(\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))]\\big)\\big(\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))]\\big)^{\\top} \\\\\n=&\\,  \\big(\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))]\\big)\\big(\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}[(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))^{\\top}\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}^{\\top}]\\big) \\\\\n\\approx&\\, \\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\big[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))^{\\top}\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}^{\\top}\\big] \\\\\n\\approx&\\, \\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\Big[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\big[(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}))^{\\top}\\big]\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}^{\\top}\\Big] \\\\\n\\end{aligned}\\end{equation}\n\\]\nThe two approximation signs here don’t have much justification. We can loosely view them as mean-field approximations. And \\(\\boldsymbol{y} - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\) is the residual of the regression prediction, which we typically assume follows \\(\\mathcal{N}(\\boldsymbol{0},\\sigma^2\\boldsymbol{I})\\). Therefore:\n\\[\n\\begin{equation}\\boldsymbol{g}_{\\boldsymbol{\\theta}} \\boldsymbol{g}_{\\boldsymbol{\\theta}}^{\\top} \\approx \\sigma^2\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\mathcal{D}}\\big[\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}\\boldsymbol{\\mathcal{J}}_{\\boldsymbol{\\theta}}^{\\top}\\big] \\approx \\sigma^2 \\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}_t}\\end{equation}\n\\tag{16}\\]\nThis reveals the relationship between \\(\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}_t}\\) and \\(\\boldsymbol{g}_{\\boldsymbol{\\theta}} \\boldsymbol{g}_{\\boldsymbol{\\theta}}^{\\top}\\). Comparing with Equation 14 from the previous section, we find that they appear to differ by a square.\nLooking at the derivation process, neither result seems obviously wrong, so how do we understand this inconsistency? We can understand it this way: Equation 16 gives the Hessian approximation at time \\(t\\), which is an “instantaneous approximation”, while Equation 14 is a “long-term average” result over time steps. The long-term averaging effect cancels out some of the intensity (but theoretically would make the estimate more accurate), thus requiring an additional square root, as we know from the theory of random walks.\nA similar effect also appears in the SDE introduced in “Generative Diffusion Modeling (Part 5): the SDE part of a General Framework”, where the strength of the noise term \\(\\epsilon\\) in SDE needs to be on the order of \\(O(\\sqrt{\\delta t})\\), while the non-random drift term needs to be on the order of \\(O(\\delta t)\\). This is also because the noise terms partially cancel out when summed over time, so the noise needs to be of a higher order so that its effect will not be washed away in the final result.1\n1 This explanation is wrong. The real explanation is very simple. In the derivation of Adam, we assumed \\(\\theta^* - \\theta\\sim \\mathcal{N}(\\boldsymbol{0},\\sigma^2\\boldsymbol{I})\\), whereas in the derivation of this section, we assumed \\(\\boldsymbol{f}_{\\boldsymbol{\\theta}^*}(\\boldsymbol{x}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) \\sim \\mathcal{N}(\\boldsymbol{0},\\sigma^2\\boldsymbol{I})\\). The two assumptions are simply incompatible."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#more-connections",
    "href": "docs/posts/2025-02-27-muon/index.html#more-connections",
    "title": "The Muon Anthology",
    "section": "More connections",
    "text": "More connections\nIn the previous derivation, we assumed that \\(\\boldsymbol{\\theta}^*\\) is the theoretical optimal point, so \\(\\boldsymbol{g} _{\\boldsymbol{\\theta}^*} = \\boldsymbol{0}\\). What if \\(\\boldsymbol{\\theta}^*\\) is any arbitrary point? Then Equation 14 would become:\n\\[\n\\begin{equation}\\mathbb{E}[(\\boldsymbol{g}_{\\boldsymbol{\\theta}}-\\boldsymbol{g} _{\\boldsymbol{\\theta}^*})(\\boldsymbol{g}_{\\boldsymbol{\\theta}}-\\boldsymbol{g} _{\\boldsymbol{\\theta}^*})^{\\top}] \\approx \\sigma^2\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}^{\\top}\\end{equation}\n\\]\nThis means that as long as we use a moving average of the covariance rather than the second moment, we can obtain a Hessian approximation within the local range. This corresponds exactly to the approach of the AdaBelief optimizer, where \\(\\boldsymbol{v}\\) is a moving average of the square of the difference between \\(\\boldsymbol{g}\\) and \\(\\boldsymbol{m}\\):\n\\[\n\\begin{equation}\\text{AdaBelief}\\triangleq \\left\\{\\begin{aligned}\n&\\boldsymbol{m}_t = \\beta_1 \\boldsymbol{m}_{t-1} + \\left(1 - \\beta_1\\right) \\boldsymbol{g}_t\\\\\n&\\boldsymbol{v}_t = \\beta_2 \\boldsymbol{v}_{t-1} + \\left(1 - \\beta_2\\right) (\\boldsymbol{g}_t - \\boldsymbol{m}_t)\\odot(\\boldsymbol{g}_t - \\boldsymbol{m}_t)\\\\\n&\\hat{\\boldsymbol{m}}_t = \\boldsymbol{m}_t\\left/\\left(1 - \\beta_1^t\\right)\\right.\\\\\n&\\hat{\\boldsymbol{v}}_t = \\boldsymbol{v}_t\\left/\\left(1 - \\beta_2^t\\right)\\right.\\\\\n&\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t \\hat{\\boldsymbol{m}}_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon\\right)\\right.\n\\end{aligned}\\right.\\end{equation}\n\\]"
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#first-taste",
    "href": "docs/posts/2025-02-27-muon/index.html#first-taste",
    "title": "The Muon Anthology",
    "section": "First taste",
    "text": "First taste\n\\begin{algorithm}\n\\caption{Muon}\n\\begin{algorithmic}\n\\REQUIRE Learning rate $\\eta$, momentum $\\mu$\n    \\STATE Initialize $B_0 \\leftarrow 0$\n    \\FOR{$t=1, \\ldots$} \n        \\STATE Compute gradient $G_t \\leftarrow \\nabla_{\\theta}\\mathcal{L}_t(\\theta_{t-1})$\n        \\STATE $B_t \\leftarrow \\mu B_{t-1} + G_t$\n        \\STATE $O_t \\leftarrow \\text{NewtonSchulz5}(B_t)$\n        \\STATE Update parameters $\\theta_t \\leftarrow \\theta_{t-1} - \\eta O_t$\n    \\ENDFOR\n    \\RETURN $\\theta_t$\n\\end{algorithmic}\n\\end{algorithm}\nMuon stands for “Momentum Orthogonalized by Newton–Schulz”. Unlike typical gradient descent, which applies to any kind of parameter, whether it be scalar, vector, or matrix, Muon applies to only matrix parameters.\nLet \\(\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}\\) be such a matrix parameter, then the Muon update rule states that:\n\\[\n\\begin{equation}\\begin{aligned}\n\\boldsymbol{G}_t =&\\, \\nabla_\\theta \\mathcal L_t(\\theta_{t-1})\\\\\n\\boldsymbol{M}_t =&\\, \\beta\\boldsymbol{M}_{t-1} + \\boldsymbol{G}_t \\\\\n\\boldsymbol{W}_t =&\\, \\boldsymbol{W}_{t-1} - \\eta_t [\\text{msign}(\\boldsymbol{M}_t) + \\lambda \\boldsymbol{W}_{t-1}] \\\\\n\\end{aligned}\\end{equation}\n\\]\nHere, \\(\\text{msign}\\) is the matrix sign function, which is not simply applying the \\(\\text{sign}\\) operation to each component of the matrix, but rather a matrix generalization of the \\(\\text{sign}\\) function. It can be defined via SVD:\n\\[\n\\begin{equation}\\boldsymbol{U},\\boldsymbol{\\Sigma},\\boldsymbol{V}^{\\top} = SVD(\\boldsymbol{M}) \\quad\\Rightarrow\\quad \\text{msign}(\\boldsymbol{M}) = \\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top}\\end{equation}\n\\]\nwhere \\(\\boldsymbol{U}\\in\\mathbb{R}^{n\\times n},\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{r \\times r},\\boldsymbol{V}\\in\\mathbb{R}^{m\\times m}\\), and \\(r\\) is the rank of \\(\\boldsymbol{M}\\). We will expand on more theoretical details later, but first let’s try to intuitively grasp the following fact:\n\nMuon is an adaptive learning rate optimizer, like Adam.\n\nThe common trait of adaptive learning rate optimizers like Adagrad, RMSprop, and Adam is that the update for each parameter is divided by the “standard deviation of gradient” \\(\\sqrt{\\overline{(\\nabla \\mathcal L)^2}}\\), that is, the square root of the moving average of squared gradients. This ensures two essential properties:\n\nConstant scaling of the loss function \\(\\mathcal L \\mapsto c\\mathcal L\\) does not change the parameter updates.\nThe change in each parameter is approximately equalized. That is, \\(|\\theta_{t, i} - \\theta_{t-1, i}| \\sim \\eta_t\\) for all \\(i\\).\n\nMuon reproduces the two essential properties for matrices, even though it does not keep track of \\(\\sqrt{\\overline{(\\nabla \\mathcal L)^2}}\\):\n\nIf the loss function \\(\\mathcal L\\) is multiplied by some constant \\(c\\), \\(\\boldsymbol{M}\\) will also be multiplied by \\(c\\), but \\(\\text{msign}(\\boldsymbol{M})\\) remains unchanged.\nWhen \\(\\boldsymbol{M}\\) is decomposed by SVD into \\(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\top}\\), the different singular values of \\(\\boldsymbol{\\Sigma}\\) reflect the “anisotropy” of \\(\\boldsymbol{M}\\), and setting them all to one makes it more isotropic, which also serves to synchronize update magnitudes.\n\n(By the way, did point 2 remind anyone of BERT-whitening?)\nStated in another way, consider what happens with Adam: We divide each component of the gradient \\(\\nabla \\mathcal L\\) by \\(\\sqrt{\\overline{(\\nabla \\mathcal L)^2}}\\), which approximately gives us just the sign of \\(\\nabla \\mathcal L\\). That is, if \\(\\partial_{\\theta_i} \\mathcal L &gt; 0\\), then we should get \\(\\sim +1\\) after dividing, and if \\(\\partial_{\\theta_i} \\mathcal L &lt; 0\\), then we should get \\(\\sim -1\\) after dividing. The use of the matrix sign reproduces this effect.\nMuon has a Nesterov version, which simply replaces \\(\\text{msign}(\\boldsymbol{M}_t)\\) with \\(\\text{msign}(\\beta\\boldsymbol{M}_t + \\boldsymbol{G}_t)\\) in the update rule, with everything else remaining identical. Since it’s tangential to our point, we won’t expand on this."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#sign-function",
    "href": "docs/posts/2025-02-27-muon/index.html#sign-function",
    "title": "The Muon Anthology",
    "section": "Sign function",
    "text": "Sign function\nUsing SVD, we can also prove the identity:\n\\[\n\\begin{equation}\\text{msign}(\\boldsymbol{M}) = (\\boldsymbol{M}\\boldsymbol{M}^{\\top})^{-1/2}\\boldsymbol{M}= \\boldsymbol{M}(\\boldsymbol{M}^{\\top}\\boldsymbol{M})^{-1/2}\\end{equation}\n\\tag{17}\\]\nwhere \\({}^{-1/2}\\) is the inverse of the matrix raised to the power of \\(1/2\\), or the pseudoinverse if it’s not invertible. This identity helps us better understand why \\(\\text{msign}\\) is a matrix generalization of \\(\\text{sign}\\): for a scalar \\(x\\), we have \\(\\text{sign}(x)=x(x^2)^{-1/2}\\), which is precisely a special case of the above equation (when \\(\\boldsymbol{M}\\) is a \\(1\\times 1\\) matrix). This special example can also be generalized to diagonal matrices \\(\\boldsymbol{M}=\\text{diag}(\\boldsymbol{m})\\):\n\\[\n\\begin{equation}\\text{msign}(\\boldsymbol{M}) = \\text{diag}(\\boldsymbol{m})[\\text{diag}(\\boldsymbol{m})^2]^{-1/2} = \\text{diag}(\\text{sign}(\\boldsymbol{m}))=\\text{sign}(\\boldsymbol{M})\\end{equation}\n\\]\nwhere \\(\\text{sign}(\\boldsymbol{m})\\) and \\(\\text{sign}(\\boldsymbol{M})\\) refer to taking \\(\\text{sign}\\) of each component of the vector/matrix. The above equation means that when \\(\\boldsymbol{M}\\) is a diagonal matrix, Muon degenerates to Lion, Tiger, or Signum, obtained by successive simplification from of AdamW:\n\\[\n\\begin{array}{c|c|c|c}\n\\hline\n\\text{Signum} & \\text{Tiger} & \\text{Lion} & \\text{AdamW} \\\\\n\\hline\n\\begin{aligned}\n&\\boldsymbol{m}_t = \\beta \\boldsymbol{m}_{t-1} + \\left(1 - \\beta\\right) \\boldsymbol{g}_t \\\\\n&\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t \\text{sign}(\\boldsymbol{m}_t) \\\\\n\\end{aligned} &\n\\begin{aligned}\n&\\boldsymbol{m}_t = \\beta \\boldsymbol{m}_{t-1} + \\left(1 - \\beta\\right) \\boldsymbol{g}_t \\\\\n&\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t \\left[\\text{sign}(\\boldsymbol{m}_t) \\color{skyblue}{ + \\lambda_t \\boldsymbol{\\theta}_{t-1}}\\right] \\\\\n\\end{aligned} &\n\\begin{aligned}\n&\\boldsymbol{u}_t = \\text{sign}\\big(\\beta_1 \\boldsymbol{m}_{t-1} + \\left(1 - \\beta_1\\right) \\boldsymbol{g}_t\\big) \\\\\n&\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t (\\boldsymbol{u}_t \\color{skyblue}{ + \\lambda_t \\boldsymbol{\\theta}_{t-1}}) \\\\\n&\\boldsymbol{m}_t = \\beta_2 \\boldsymbol{m}_{t-1} + \\left(1 - \\beta_2\\right) \\boldsymbol{g}_t\n\\end{aligned} &\n\\begin{aligned}\n&\\boldsymbol{m}_t = \\beta_1 \\boldsymbol{m}_{t-1} + \\left(1 - \\beta_1\\right) \\boldsymbol{g}_t\\\\\n&\\boldsymbol{v}_t = \\beta_2 \\boldsymbol{v}_{t-1} + \\left(1 - \\beta_2\\right) \\boldsymbol{g}_t^2\\\\\n&\\hat{\\boldsymbol{m}}_t = \\boldsymbol{m}_t\\left/\\left(1 - \\beta_1^t\\right)\\right.\\\\\n&\\hat{\\boldsymbol{v}}_t = \\boldsymbol{v}_t\\left/\\left(1 - \\beta_2^t\\right)\\right.\\\\\n&\\boldsymbol{u}_t =\\hat{\\boldsymbol{m}}_t\\left/\\left(\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon\\right)\\right.\\\\\n&\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta_t (\\boldsymbol{u}_t \\color{skyblue}{ + \\lambda_t \\boldsymbol{\\theta}_{t-1}})\n\\end{aligned} \\\\\n\\hline\n\\end{array}\n\\]\nConversely, the difference between Muon and Signum/Tiger is that the elementwise \\(\\text{sign}(\\boldsymbol{M})\\) is replaced with the matrix version \\(\\text{msign}(\\boldsymbol{M})\\).\nFor an \\(n\\)-dimensional vector, we can also view it as an \\(n\\times 1\\) matrix, in which case \\(\\text{msign}(\\boldsymbol{m}) = \\boldsymbol{m}/\\Vert\\boldsymbol{m}\\Vert_2\\) is exactly \\(l_2\\) normalization. So, in the Muon framework, we have two perspectives for vectors: one as a diagonal matrix, like the \\(\\gamma\\) parameter in LayerNorm, resulting in taking \\(\\text{sign}\\) of the momentum; the other as an \\(n\\times 1\\) matrix, resulting in \\(l_2\\) normalization of the momentum. Additionally, although input and output embeddings are also matrices, they are used sparsely, so a more reasonable approach is to treat them as lists of independent vectors.\n\\(\\text{msign}(\\boldsymbol{M})\\) also has the meaning of “optimal orthogonal approximation”:\n\\[\n\\begin{equation}\\text{msign}(\\boldsymbol{M}) = \\mathop{\\text{argmin}}\\Vert \\boldsymbol{M} - \\boldsymbol{O}\\Vert_F^2 \\end{equation}\n\\tag{18}\\]\nwhere \\(\\boldsymbol{O}\\) is constrained by over \\(\\boldsymbol{O}^{\\top}\\boldsymbol{O} = \\boldsymbol{I}\\) if \\(\\boldsymbol{M}\\) is a tall matrix, or \\(\\boldsymbol{O}\\boldsymbol{O}^{\\top} = \\boldsymbol{I}\\) if \\(\\boldsymbol{M}\\) is a fat matrix. Furthermore, if the matrix is full-ranked, that is, \\(r = \\min(m, n)\\), then \\(\\boldsymbol{O}\\) is the unique minimizer of the equation.\nThis is analogous to how\n\\[\n\\begin{equation}\\text{sign}(\\boldsymbol{M}) = \\mathop{\\text{argmin}}_{\\boldsymbol{O}\\in\\{-1,1\\}^{n\\times m}}\\Vert \\boldsymbol{M} - \\boldsymbol{O}\\Vert_F^2\\end{equation}\n\\]\nand how \\(\\text{sign}(\\boldsymbol{M})\\) is the unique solution when \\(\\boldsymbol{M}\\) has only nonzero entries.\nWhether it’s \\(\\boldsymbol{O}^{\\top}\\boldsymbol{O} = \\boldsymbol{I}\\) or \\(\\boldsymbol{O}\\in\\{-1,1\\}^{n\\times m}\\), we can view both as a kind of regularization constraint on the update amount. So Muon and Signum/Tiger can be seen as optimizers under the same approach: they all start from momentum \\(\\boldsymbol{M}\\) to construct the update amount, but choose different regularization methods for the update.\nTo prove Equation 18, note that the Frobenius norm is preserved by left and right multiplication of orthogonal matrices. That is, \\(\\|\\boldsymbol{V}\\boldsymbol{A}\\|_F = \\|\\boldsymbol{A}\\|_F = |\\boldsymbol{A}\\boldsymbol{U}\\|_F\\) for any orthogonal matrices \\(\\boldsymbol{U}, \\boldsymbol{V}\\) of the suitable shapes. Thus it reduces to the special case where \\(\\boldsymbol{M} = \\boldsymbol{\\Sigma}\\). Now, use the fact that \\(\\|\\boldsymbol{A}\\|_F^2\\) is the sum of its column vectors’ norm-squared."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#iterative-solution",
    "href": "docs/posts/2025-02-27-muon/index.html#iterative-solution",
    "title": "The Muon Anthology",
    "section": "Iterative solution",
    "text": "Iterative solution\nIn practice, if we compute \\(\\text{msign}(\\boldsymbol{M})\\) by performing SVD on \\(\\boldsymbol{M}\\) at each step, the computational cost would be quite high. Therefore, the authors of Muon proposed using Newton–Schulz iteration to approximately calculate \\(\\text{msign}(\\boldsymbol{M})\\).\nThe starting point of the iteration is the identity Equation 17. Without loss of generality, we assume \\(n\\geq m\\) and consider the Taylor expansion of \\((\\boldsymbol{M}^{\\top}\\boldsymbol{M})^{-1/2}\\) at \\(\\boldsymbol{M}^{\\top}\\boldsymbol{M}=\\boldsymbol{I}\\). The expansion is done by doing a matrix Taylor expanion. Specifically, consider a symmetric \\(\\boldsymbol{A} = \\boldsymbol{I} + \\delta\\boldsymbol{A}\\), where \\(\\delta\\boldsymbol{A}\\) is a small symmetric matrix. Then by direct multiplication, we have\n\\[\n\\boldsymbol{A}^{-1/2} = \\boldsymbol{I} - \\frac 12 \\delta \\boldsymbol{A} + \\frac 38 \\delta \\boldsymbol{A}^2 + O(\\delta \\boldsymbol{A}^3)\n\\]\nThus, up to 2th order,\n\\[\n\\begin{equation}\\text{msign}(\\boldsymbol{M}) = \\boldsymbol{M}(\\boldsymbol{M}^{\\top}\\boldsymbol{M})^{-1/2}\\approx \\frac{15}{8}\\boldsymbol{M} - \\frac{5}{4}\\boldsymbol{M}(\\boldsymbol{M}^{\\top}\\boldsymbol{M}) + \\frac{3}{8}\\boldsymbol{M}(\\boldsymbol{M}^{\\top}\\boldsymbol{M})^2\\end{equation}\n\\]\nIf \\(\\boldsymbol{X}_t\\) is an approximation of \\(\\text{msign}(\\boldsymbol{M})\\), we believe that substituting it into the above equation will yield a better approximation of \\(\\text{msign}(\\boldsymbol{M})\\). Thus, we obtain a usable iteration format:\n\\[\n\\begin{equation}\\boldsymbol{X}_{t+1} = a \\boldsymbol{X}_t + b\\boldsymbol{X}_t(\\boldsymbol{X}_t^{\\top}\\boldsymbol{X}_t) + c\\boldsymbol{X}_t(\\boldsymbol{X}_t^{\\top}\\boldsymbol{X}_t)^2\\end{equation}\n\\] with \\((a,b,c) = (15/8, -5/4, 3/8) = (1.875, -1.25, 0.375)\\)\nBut if we look up the official code for Muon, we’d see that the Newton–Schulz iteration does appear in this form, but with \\((a,b,c) = (3.4445, -4.7750,  2.0315)\\). Further, the original author made no attempt to derive this mathematically, but just wrote it in as a magic constant:\ndef zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):\n    \"\"\"\n    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n    performance at all relative to UV^T, where USV^T = G is the SVD.\n    \"\"\"\n    assert len(G.shape) == 2\n    a, b, c = (3.4445, -4.7750,  2.0315)\n    X = G.bfloat16()\n    X /= (X.norm() + eps) # ensure top singular value &lt;= 1\n    if G.size(0) &gt; G.size(1):\n        X = X.T\n    for _ in range(steps):\n        A = X @ X.T\n        B = A @ X\n        X = a * X + b * B + c * A @ B\n    if G.size(0) &gt; G.size(1):\n        X = X.T\n    return X"
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#convergence-acceleration",
    "href": "docs/posts/2025-02-27-muon/index.html#convergence-acceleration",
    "title": "The Muon Anthology",
    "section": "Convergence acceleration",
    "text": "Convergence acceleration\nTo guess the origin of the official iteration algorithm, we consider a general iteration process:\n\\[\n\\begin{equation}\\boldsymbol{X}_{t+1} = a\\boldsymbol{X}_t + b\\boldsymbol{X}_t(\\boldsymbol{X}_t^{\\top}\\boldsymbol{X}_t) + c\\boldsymbol{X}_t(\\boldsymbol{X}_t^{\\top}\\boldsymbol{X}_t)^2\\end{equation}\n\\tag{19}\\]\nwhere \\((a,b,c)\\) are to be determined. If we want a higher-order iteration algorithm, we can also successively add terms like \\(\\boldsymbol{X}_t(\\boldsymbol{X}_t^{\\top}\\boldsymbol{X}_t)^3\\), \\(\\boldsymbol{X}_t(\\boldsymbol{X}_t^{\\top}\\boldsymbol{X}_t)^4\\), etc. The following analysis process is universal.\nWe choose the initial value as \\(\\boldsymbol{X}_0=\\boldsymbol{M}/\\Vert\\boldsymbol{M}\\Vert_F\\), where \\(\\Vert\\cdot\\Vert_F\\) is the Frobenius norm of the matrix. The rationale is that dividing by \\(\\Vert\\boldsymbol{M}\\Vert_F\\) does not change the \\(\\boldsymbol{U}\\) and \\(\\boldsymbol{V}\\) in the SVD, but can make all singular values of \\(\\boldsymbol{X}_0\\) lie in the interval \\([0,1]\\), standardizing the initial singular values for iteration. Now, assuming \\(\\boldsymbol{X}_t\\) can be decomposed by SVD as \\(\\boldsymbol{U}\\boldsymbol{\\Sigma}_t\\boldsymbol{V}^{\\top}\\), substituting into the above equation gives:\n\\[\n\\begin{equation}\\boldsymbol{X}_{t+1} = \\boldsymbol{U}_{[:,:r]}(a \\boldsymbol{\\Sigma}_{t,[:r,:r]} + b \\boldsymbol{\\Sigma}_{t,[:r,:r]}^3 + c \\boldsymbol{\\Sigma}_{t,[:r,:r]}^5)\\boldsymbol{V}_{[:,:r]}^{\\top}\\end{equation}\n\\]\nTherefore, Equation 19 is actually iterating on the diagonal matrix \\(\\boldsymbol{\\Sigma}_{[:r,:r]}\\) composed of singular values. If we denote \\(\\boldsymbol{X}_t=\\boldsymbol{U}_{[:,:r]}\\boldsymbol{\\Sigma}_{t,[:r,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top}\\), then \\(\\boldsymbol{\\Sigma}_{t+1,[:r,:r]} = g(\\boldsymbol{\\Sigma}_{t,[:r,:r]})\\), where \\(g(x) = ax + bx^3 + cx^5\\). Since the power of a diagonal matrix equals each diagonal element raised to that power, the problem simplifies to the iteration of a single singular value \\(\\sigma\\). Our goal is to compute \\(\\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top}\\), in other words, we hope to transform \\(\\boldsymbol{\\Sigma}_{[:r,:r]}\\) into an identity matrix through iteration, which can be further simplified to iterating \\(\\sigma_{t+1} = g(\\sigma_t)\\) to change a single singular value to 1.\nInspired by @leloykun, we view the selection of \\((a,b,c)\\) as an optimization problem, with the objective of making the iteration process converge as quickly as possible for any initial singular value. First, we reparameterize \\(g(x)\\) as:\n\\[\n\\begin{equation}g(x) = x + \\kappa x(x^2 - x_1^2)(x^2 - x_2^2)\\end{equation}\n\\]\nwhere \\(x_1 \\leq x_2\\). The advantage of this parameterization is that it explicitly represents the 5 fixed points of the iteration: \\(0, \\pm x_1, \\pm x_2\\). Since our goal is to converge to 1, we initially choose \\(x_1 &lt; 1 &lt; x_2\\), with the idea that regardless of whether the iteration process moves toward \\(x_1\\) or \\(x_2\\), the result will be near 1.\nNext, we determine the number of iterations \\(T\\), so that the iteration process becomes a deterministic function. Once we fix the shape of the matrix (i.e., \\(n\\) and \\(m\\)), we can sample a batch of matrices and compute the singular values through SVD. Finally, we treat these singular values as inputs, with the target output being 1, and the loss function being the squared error. The entire model is fully differentiable and can be solved using gradient descent. Note that @leloykun assumed \\(x_1 + x_2 = 2\\) and used grid search to solve it.\n\\[\n\\begin{array}{ccc|ccc|ccc|c|c}\n\\hline\nn & m & T & \\kappa & x_1 & x_2 & a & b & c & \\text{mse} & \\text{mse}_{\\text{Muon}}\\\\\n\\hline\n1024 & 1024 & 3 & 7.020 & 0.830 & 0.830 & 4.328 & -9.666 & 7.020 & 0.10257 & 0.18278 \\\\\n1024 & 1024 & 5 & 1.724 & 0.935 & 1.235 & 3.297 & -4.136 & 1.724 & 0.02733 & 0.04431 \\\\\n2048 & 1024 & 3 & 7.028 & 0.815 & 0.815 & 4.095 & -9.327 & 7.028 & 0.01628 & 0.06171 \\\\\n2048 & 1024 & 5 & 1.476 & 0.983 & 1.074 & 2.644 & -3.128 & 1.476 & 0.00038 & 0.02954 \\\\\n4096 & 1024 & 3 & 6.948 & 0.802 & 0.804 & 3.886 & -8.956 & 6.948 & 0.00371 & 0.02574 \\\\\n4096 & 1024 & 5 & 1.214 & 1.047 & 1.048 & 2.461 & -2.663 & 1.214 & 0.00008 & 0.02563 \\\\\n\\hline\n2048 & 2048 & 3 & 11.130 & 0.767 & 0.767 & 4.857 & -13.103 & 11.130 & 0.10739 & 0.24410 \\\\\n2048 & 2048 & 5 & 1.779 & 0.921 & 1.243 & 3.333 & -4.259 & 1.779 & 0.03516 & 0.04991 \\\\\n4096 & 4096 & 3 & 18.017 & 0.705 & 0.705 & 5.460 & -17.929 & 18.017 & 0.11303 & 0.33404 \\\\\n4096 & 4096 & 5 & 2.057 & 0.894 & 1.201 & 3.373 & -4.613 & 2.057 & 0.04700 & 0.06372 \\\\\n8192 & 8192 & 3 & 30.147 & 0.643 & 0.643 & 6.139 & -24.893 & 30.147 & 0.11944 & 0.44843 \\\\\n8192 & 8192 & 5 & 2.310 & 0.871 & 1.168 & 3.389 & -4.902 & 2.310 & 0.05869 & 0.07606 \\\\\n\\hline\n\\end{array}\n\\]\nHere, \\(\\text{mse}_{\\text{Muon}}\\) is the result of computation done according to the \\((a,b,c)\\) provided by the Muon authors. Looking through the table, which choice of \\((a, b, c)\\) is the best clearly depends on both the matrix size \\((m, n)\\) and the number of iterations \\(T\\). Looking at the loss function, non-square matrices converge more readily than square matrices. The \\((a, b, c)\\) given by the authors of Muon are probably the optimal solution for square matrices \\(m = n\\) when the number of iterations is \\(T = 5\\).\nFor a fixed number of iterations \\(T\\), the result depends on the size of the matrix, which essentially depends on the distribution of singular values. One result worth mentioning about this distribution is that for any fixed “aspect ratio” \\(r \\in (0, \\infty)\\), if at the \\(n \\to \\infty\\) limit, \\(\\frac{m}{n} \\to r\\), then the singular values of the matrix converges to a Marchenko–Pastur distribution.\n\n\nCode for generating the table above\n\nimport jax\nimport jax.numpy as jnp\nfrom tqdm import tqdm\n\nn, m, T = 1024, 1024, 5\nkey, data = jax.random.key(42), jnp.array([])\nfor _ in tqdm(range(1000), ncols=0, desc='SVD'):\n    key, subkey = jax.random.split(key)\n    M = jax.random.normal(subkey, shape=(n, m))\n    S = jnp.linalg.svd(M, full_matrices=False)[1]\n    data = jnp.concatenate([data, S / (S**2).sum()**0.5])\n\n@jax.jit\ndef f(w, x):\n    k, x1, x2 = w\n    for _ in range(T):\n        x = x + k * x * (x**2 - x1**2) * (x**2 - x2**2)\n    return ((x - 1)**2).mean()\n\nf_grad = jax.grad(f)\nw, u = jnp.array([1, 0.9, 1.1]), jnp.zeros(3)\nfor _ in tqdm(range(100000), ncols=0, desc='SGD'):\n    u = 0.9 * u + f_grad(w, data)  # Momentum acceleration\n    w = w - 0.01 * u\n\nk, x1, x2 = w\na, b, c = 1 + k * x1**2 * x2**2, -k * (x1**2 + x2**2), k\nprint(f'{n} & {m} & {T} & {k:.3f} & {x1:.3f} & {x2:.3f} & {a:.3f} & {b:.3f} & {c:.3f} & {f(w, data):.5f}')"
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#some-thoughts",
    "href": "docs/posts/2025-02-27-muon/index.html#some-thoughts",
    "title": "The Muon Anthology",
    "section": "Some thoughts",
    "text": "Some thoughts\nIf we choose the default setting of \\(T=5\\), then for an \\(n\\times n\\) matrix parameter, each update step of Muon requires at least 15 matrix multiplications between \\(n\\times n\\) matrices:\n\\[\n\\boldsymbol{X}_t (a + (\\boldsymbol{X}_t^\\top \\boldsymbol{X}_t) (b + c (\\boldsymbol{X}_t^\\top \\boldsymbol{X}_t)) )\n\\]\nwhich is undoubtedly a significantly larger computational cost than Adam. This might lead some readers to worry whether Muon is practically feasible.\nIn fact, such concerns are unnecessary. Although Muon’s computation is more complex than Adam’s, the additional time per step is minimal. My conclusion is that the additional wallclock time is \\(\\leq 5\\%\\). The Muon’s authors claim it can reach as low as \\(2\\%\\). This is because Muon’s matrix multiplications occur after the current gradient computation and before the next gradient computation, during which almost all computational power would have sat idle anyway. Since these matrix multiplications are of static size and can be parallelized, they don’t significantly increase the wallclock time. Moreover, Muon uses one fewer set of cached variables than Adam, resulting in lower memory consumption.\nThe most thought-provoking aspect of Muon is actually the intrinsic difference between vectors and matrices, and its impact on optimization. Common optimizers like SGD, AdamW, and Lion update parameters in an elementwise manner, treating both vector and matrix parameters essentially the same, as lists of scalars being updated independently according to the same rules. Optimizers with this characteristic are often simpler to analyze theoretically and are convenient for tensor parallelism, since splitting a large matrix into two smaller matrices for independent processing doesn’t change the optimization trajectory.\nBut Muon is different, since it takes matrices as fundamental units, and exploits properties unique to matrices. Some readers might wonder: aren’t matrices and vectors just arrangements of numbers? How different can they be? But they are. For example, with matrices, we have the concept of trace, which is the sum of diagonal elements. This concept is a geometrically meaningful concept, since it is invariant under similarity transformation. In particular, it equals the sum of all eigenvalues of the matrix. The moral of this example is that the diagonal elements of a matrix should not be treated in the same way as its off-diagonal elements. Muon achieves better results because it treats them differently.\nOf course, Muon is not a free lunch. In tensor-parallel training, Muon requires allreduce. That is, the gradients need to be aggregated across the devices before the parameter update, rather than having each device update independently, which increases communication costs.\nEven without tensor-parallelism, this issue persists in some other form. For instance, Multi-Head Attention is usually implemented by projecting the input with a single matrix \\(W^Q\\) to obtain the query matrix \\(Q\\) (and similarly for \\(K\\) and \\(V\\)), and then reshapes it to obtain the query matrices for each head. This creates a disconnect between the implementation and the semantics. Semantically, \\(Q\\) really should be considered multiple matrices, but it is implemented as a single matrix. Therefore, when using Muon for MHA, one must take care to first split \\(Q\\) into multiple small matrices before doing the Muon update.\nIn summary, Muon’s non-elementwise update rule, while capturing the essential differences between vectors and matrices, also introduces some minor issues, which might be aesthetically unsatisfying to some people.\n(Addendum: Almost simultaneously with the publication of this blogpost, Muon’s author Keller Jordan published Muon: An optimizer for hidden layers in neural networks.)"
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#norm-perspective",
    "href": "docs/posts/2025-02-27-muon/index.html#norm-perspective",
    "title": "The Muon Anthology",
    "section": "Norm perspective",
    "text": "Norm perspective\nFrom a theoretical standpoint, what key characteristics of matrices does Muon capture? Perhaps the following norm perspective can answer our question.\nThis section’s discussion primarily references the papers Stochastic Spectral Descent for Discrete Graphical Models and Old Optimizer, New Norm: An Anthology, especially the latter. However, the starting point is not new; we’ve already briefly touched upon it in Gradient Flow: Exploring the Path to Minimums: for vector parameters \\(\\boldsymbol{w}\\in\\mathbb{R}^n\\), we define the update rule for the next step as\n\\[\n\\begin{equation}\\boldsymbol{w}_{t+1} = \\mathop{\\text{argmin}}_{\\boldsymbol{w}} \\left(\\frac{\\Vert\\boldsymbol{w} - \\boldsymbol{w}_t\\Vert^2}{2\\eta_t} + \\mathcal{L}(\\boldsymbol{w})\\right)\\end{equation}\n\\]\nwhere \\(\\Vert\\cdot\\Vert\\) is some vector norm. This is norm-regularized gradient descent. Then, assuming \\(\\eta_t\\) is small enough, the regularization loss dominates, meaning \\(\\boldsymbol{w}_{t+1}\\) will be very close to \\(\\boldsymbol{w}_t\\), so we assume a first-order approximation of \\(\\mathcal{L}(\\boldsymbol{w})\\) is sufficient. The problem then simplifies to\n\\[\n\\begin{equation}\\boldsymbol{w}_{t+1} = \\mathop{\\text{argmin}}_{\\boldsymbol{w}}\\left( \\frac{\\Vert\\boldsymbol{w} - \\boldsymbol{w}_t\\Vert^2}{2\\eta_t} + \\mathcal{L}(\\boldsymbol{w}_t) + \\nabla_{\\boldsymbol{w}_t}\\mathcal{L}(\\boldsymbol{w}_t)^{\\top}(\\boldsymbol{w}-\\boldsymbol{w}_t) \\right)\\end{equation}\n\\]\nDenoting \\(\\Delta\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_{t+1}-\\boldsymbol{w}_t, \\boldsymbol{g}_t = \\nabla_{\\boldsymbol{w}_t}\\mathcal{L}(\\boldsymbol{w}_t)\\), we can simplify it as\n\\[\n\\begin{equation}\n\\Delta\\boldsymbol{w}_{t+1} = \\mathop{\\text{argmin}}_{\\Delta\\boldsymbol{w}} \\left( \\frac{\\Vert\\Delta\\boldsymbol{w}\\Vert^2}{2\\eta_t} + \\boldsymbol{g}_t^{\\top}\\Delta\\boldsymbol{w}\\right)\n\\end{equation}\n\\]\nThe general approach to compute \\(\\Delta\\boldsymbol{w}_{t+1}\\) is to take derivatives, but Old Optimizer, New Norm: An Anthology provides a unified solution without taking derivatives: decompose \\(\\Delta\\boldsymbol{w}\\) into the norm \\(\\gamma = \\Vert\\Delta\\boldsymbol{w}\\Vert\\) and the direction vector \\(\\boldsymbol{\\phi} = -\\Delta\\boldsymbol{w}/\\Vert\\Delta\\boldsymbol{w}\\Vert\\), so\n\\[\n\\begin{equation}\\min_{\\Delta\\boldsymbol{w}} \\left(\n    \\frac{\\Vert\\Delta\\boldsymbol{w}\\Vert^2}{2\\eta_t} +  \\boldsymbol{g}_t^{\\top}\\Delta\\boldsymbol{w} \\right)\n    = \\min_{\\gamma\\geq 0, \\Vert\\boldsymbol{\\phi}\\Vert=1} \\left(\\frac{\\gamma^2}{2\\eta_t} -  \\gamma\\boldsymbol{g}_t^{\\top}\\boldsymbol{\\phi}  \\right)\n    = \\min_{\\gamma\\geq 0} \\left( \\frac{\\gamma^2}{2\\eta_t} -  \\gamma\\; \\underbrace{\\max_{\\Vert\\boldsymbol{\\phi}\\Vert=1}\\boldsymbol{g}_t^{\\top}\\boldsymbol{\\phi}}_{\\triangleq \\Vert \\boldsymbol{g}_t\\Vert^{\\dagger}} \\right)\n\\end{equation}\n\\]\n\\(\\gamma\\) is just a scalar, similar to the learning rate, and its optimal value is easily found to be \\(\\eta_t\\Vert \\boldsymbol{g}_t\\Vert^{\\dagger}\\), while the update direction is the \\(\\boldsymbol{\\phi}^*\\) that maximizes \\(\\boldsymbol{g}_t^{\\top}\\boldsymbol{\\phi}\\) under constraint \\(\\Vert\\boldsymbol{\\phi}\\Vert=1\\). Now substituting the Euclidean norm, i.e., \\(\\Vert\\boldsymbol{\\phi}\\Vert_2 = \\sqrt{\\boldsymbol{\\phi}^{\\top}\\boldsymbol{\\phi}}\\), we have \\(\\Vert \\boldsymbol{g}_t\\Vert^{\\dagger}=\\Vert \\boldsymbol{g}_t\\Vert_2\\) and \\(\\boldsymbol{\\phi}^* = \\boldsymbol{g}_t/\\Vert\\boldsymbol{g}_t\\Vert_2\\), which gives \\(\\Delta\\boldsymbol{w}_{t+1}=-\\eta_t \\boldsymbol{g}_t\\), which is just SGD. Generally, define the \\(p\\)-norm\n\\[\n\\begin{equation}\\Vert\\boldsymbol{\\phi}\\Vert_p = \\sqrt[\\uproot{10}p]{\\sum_{i=1}^n |\\phi_i|^p}\\end{equation}\n\\]\nthen by the Hölder’s inequality duality gives \\(\\boldsymbol{g}^{\\top}\\boldsymbol{\\phi} \\leq \\Vert \\boldsymbol{g}\\Vert_q \\Vert \\boldsymbol{\\phi}\\Vert_p\\), where \\(1/p + 1/q = 1\\). The equality is reached precisely at\n\\[\n\\begin{equation}\\boldsymbol{\\phi}^* = \\frac{1}{\\Vert\\boldsymbol{g}\\Vert_q^{q/p}}\\Big[\\text{sign}(g_1) |g_1|^{q/p},\\text{sign}(g_2) |g_2|^{q/p},\\cdots,\\text{sign}(g_n) |g_n|^{q/p}\\Big]\\end{equation}\n\\]\nat which point, \\(\\max_{\\Vert\\boldsymbol{\\phi}\\Vert_p=1}\\boldsymbol{g}^{\\top}\\boldsymbol{\\phi} = \\Vert \\boldsymbol{g}\\Vert_q\\).\nThe pbSGD optimizer uses this as the direction vector. In particular, when \\(p\\to\\infty\\), we have \\(q\\to 1\\) and \\(|g_i|^{q/p}\\to 1\\), which degenerates to SignSGD, meaning that we can interpret SignSGD as SGD regularized by \\(\\|\\cdot \\|_\\infty\\)."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#matrix-norms",
    "href": "docs/posts/2025-02-27-muon/index.html#matrix-norms",
    "title": "The Muon Anthology",
    "section": "Matrix norms",
    "text": "Matrix norms\nNow let’s switch our focus to matrix parameters \\(\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}\\). Similarly, we define its update rule as\n\\[\n\\begin{equation}\\boldsymbol{W}_{t+1} = \\mathop{\\text{argmin}}_{\\boldsymbol{W}} \\left( \\frac{\\Vert\\boldsymbol{W} - \\boldsymbol{W}_t\\Vert^2}{2\\eta_t} + \\mathcal{L}(\\boldsymbol{W}) \\right) \\end{equation}\n\\]\nwhere \\(\\Vert\\cdot\\Vert\\) is some matrix norm. Again using a first-order approximation, we get\n\\[\n\\begin{equation}\\Delta\\boldsymbol{W}_{t+1} = \\mathop{\\text{argmin}}_{\\Delta\\boldsymbol{W}} \\left( \\frac{\\Vert\\Delta\\boldsymbol{W}\\Vert^2}{2\\eta_t} + \\text{Tr}(\\boldsymbol{G}_t^{\\top}\\Delta\\boldsymbol{W}) \\right)\n\\end{equation}\n\\]\nHere \\(\\Delta\\boldsymbol{W}_{t+1} = \\boldsymbol{W}_{t+1}-\\boldsymbol{W}_t, \\boldsymbol{G}_t = \\nabla_{\\boldsymbol{W}_t}\\mathcal{L}(\\boldsymbol{W}_t)\\). Still using the “norm-direction” decoupling, i.e., setting \\(\\gamma = \\Vert\\Delta\\boldsymbol{w}\\Vert\\) and \\(\\boldsymbol{\\Phi} = -\\Delta\\boldsymbol{W}/\\Vert\\Delta\\boldsymbol{W}\\Vert\\), we get\n\\[\n\\begin{equation}\\min_{\\Delta\\boldsymbol{W}} \\left( \\frac{\\Vert\\Delta\\boldsymbol{W}\\Vert^2}{2\\eta_t} + \\text{Tr}(\\boldsymbol{G}_t^{\\top}\\Delta\\boldsymbol{W}) \\right) = \\min_{\\gamma\\geq 0} \\left( \\frac{\\gamma^2}{2\\eta_t} -  \\gamma \\, \\max_{\\Vert\\boldsymbol{\\Phi}\\Vert=1}\\text{Tr}(\\boldsymbol{G}_t^{\\top}\\boldsymbol{\\Phi}) \\right)\n\\end{equation}\n\\]\nThen it’s a case-by-case analysis for specific norms. There are two commonly used matrix norms. One is the Frobenius norm, which is actually the Euclidean norm after flattening the matrix into a vector. In this case, the conclusion is the same as for vectors – the answer is SGD, which we won’t expand on here. The other is the 2-norm induced by the vector norm, also known as the spectral norm:\n\\[\n\\begin{equation}\\Vert \\boldsymbol{\\Phi}\\Vert_2 = \\max_{\\Vert \\boldsymbol{x}\\Vert_2 = 1} \\Vert \\boldsymbol{\\Phi}\\boldsymbol{x}\\Vert_2\\end{equation}\n\\]\nNote that the \\(\\Vert\\cdot\\Vert_2\\) on the right side applies to vectors, so the definition is clear. For more discussions on the 2-norm, refer to Lipschitz Constraints in Deep Learning: Generalization and Generative Models and The Path to Low-Rank Approximation (Part 2): SVD. Since the 2-norm is induced by “matrix-vector” multiplication, it better aligns with matrix multiplication, and it always holds that \\(\\Vert\\boldsymbol{\\Phi}\\Vert_2\\leq \\Vert\\boldsymbol{\\Phi}\\Vert_F\\), meaning the 2-norm is more compact compared to the Frobenius norm.\nNext, the 2-norm. Let the SVD of \\(\\boldsymbol{G}\\) be \\(\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\top} = \\sum\\limits_{i=1}^r \\sigma_i \\boldsymbol{u}_i \\boldsymbol{v}_i^{\\top}\\), then\n\\[\n\\begin{equation}\\text{Tr}(\\boldsymbol{G}^{\\top}\\boldsymbol{\\Phi})=\\text{Tr}\\Big(\\sum_{i=1}^r \\sigma_i \\boldsymbol{v}_i \\boldsymbol{u}_i^{\\top}\\boldsymbol{\\Phi}\\Big) = \\sum_{i=1}^r \\sigma_i \\boldsymbol{u}_i^{\\top}\\boldsymbol{\\Phi}\\boldsymbol{v}_i\\end{equation}\n\\]\nBy definition, when \\(\\Vert\\boldsymbol{\\Phi}\\Vert_2=1\\), we have \\(\\Vert\\boldsymbol{\\Phi}\\boldsymbol{v}_i\\Vert_2\\leq \\Vert\\boldsymbol{v}_i\\Vert_2=1\\), so \\(\\boldsymbol{u}_i^{\\top}\\boldsymbol{\\Phi}\\boldsymbol{v}_i\\leq 1\\). Therefore,\n\\[\n\\begin{equation}\\text{Tr}(\\boldsymbol{G}^{\\top}\\boldsymbol{\\Phi})\\leq \\sum_{i=1}^r \\sigma_i\\end{equation}\n\\]\nThe equality holds when all \\(\\boldsymbol{u}_i^{\\top}\\boldsymbol{\\Phi}\\boldsymbol{v}_i\\) equal 1, in which case\n\\[\n\\begin{equation}\\boldsymbol{\\Phi} = \\sum_{i=1}^r \\boldsymbol{u}_i \\boldsymbol{v}_i^{\\top} = \\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top} = \\text{msign}(\\boldsymbol{G})\\end{equation}\n\\]\nWith this, we’ve proven that gradient descent under the 2-norm penalty is precisely the \\(\\beta=0\\) case of Muon!\nWhen \\(\\beta &gt; 0\\), the moving average takes effect, which can be viewed as a more accurate estimate of the gradient, so we take \\(\\text{msign}\\) of the momentum instead. Overall, Muon is equivalent to gradient descent under the 2-norm constraint. The 2-norm better measures the essential differences between matrices, making each step more precise and geometrically meaningful."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#going-further-back",
    "href": "docs/posts/2025-02-27-muon/index.html#going-further-back",
    "title": "The Muon Anthology",
    "section": "Going further back",
    "text": "Going further back\nA more ancient previous work is Shampoo: Preconditioned Stochastic Tensor Optimization (2018), which proposed the Shampoo optimizer, which shares a similar design philosophy with Muon.\nThe strategy of adapting learning rates through the average of squared gradients, first proposed in Adam, originated from the Adagrad paper Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (2011), which suggested directly accumulating squared gradients – equivalent to a global equal-weight average. Later, RMSProp and Adam, inspired by momentum design, switched to moving averages, which were found to perform better in practice.\nMoreover, Adagrad initially proposed accumulating the outer product \\(\\boldsymbol{g}\\boldsymbol{g}^{\\top}\\), but due to the high space cost of caching outer products, it was changed to the Hadamard product \\(\\boldsymbol{g}\\odot\\boldsymbol{g}\\) in practice. What’s the theoretical basis for accumulating outer products? We derived this in Adaptive learning rate optimizers from a Hessian approximation point of view. The conclusion is that the long-term average of gradient outer products \\(\\mathbb{E}[\\boldsymbol{g}\\boldsymbol{g}^{\\top}] \\approx \\sigma^2\\boldsymbol{\\mathcal{H}}_{\\boldsymbol{\\theta}^*}^2\\). In other words, this is a quasi-Newton method.\nShampoo inherited Adagrad’s idea of caching outer products, but considering the cost, it took a compromise. Like Muon, it also optimizes matrices (and higher-order tensors), but its strategy is to cache matrix products \\(\\boldsymbol{G}\\boldsymbol{G}^{\\top}\\) and \\(\\boldsymbol{G}^{\\top}\\boldsymbol{G}\\), not outer products. This way, the space cost is \\(\\mathcal{O}(n^2 + m^2)\\) rather than \\(\\mathcal{O}(n^2 m^2)\\):\n\\[\n\\begin{equation}\\begin{aligned}\n\\boldsymbol{L}_t =&\\, \\beta\\boldsymbol{L}_{t-1} + \\boldsymbol{G}_t\\boldsymbol{G}_t^{\\top} \\\\\n\\boldsymbol{R}_t =&\\, \\beta\\boldsymbol{R}_{t-1} + \\boldsymbol{G}_t^{\\top}\\boldsymbol{G}_t \\\\\n\\boldsymbol{W}_t =&\\, \\boldsymbol{W}_{t-1} - \\eta_t \\boldsymbol{L}_t^{-1/4}\\boldsymbol{G}_t\\boldsymbol{R}_t^{-1/4} \\\\\n\\end{aligned}\\end{equation}\n\\]\nThe \\(\\beta\\) here was added by me; Shampoo defaults to \\(\\beta=1\\). The \\({}^{-1/4}\\) is also a matrix power operation, which can be completed using SVD. Since Shampoo didn’t propose approximation schemes like Newton–Schulz iteration but directly calculated using SVD, to save computational cost, it doesn’t compute \\(\\boldsymbol{L}_t^{-1/4}\\) and \\(\\boldsymbol{R}_t^{-1/4}\\) at every step, but updates their results only after a certain number of steps.\nSpecifically, when \\(\\beta=0\\), Shampoo’s update vector is \\((\\boldsymbol{G}\\boldsymbol{G}^{\\top})^{-1/4}\\boldsymbol{G}(\\boldsymbol{G}^{\\top}\\boldsymbol{G})^{-1/4}\\). By performing SVD on \\(\\boldsymbol{G}\\), we can prove\n\\[\n\\begin{equation}(\\boldsymbol{G}\\boldsymbol{G}^{\\top})^{-1/4}\\boldsymbol{G}(\\boldsymbol{G}^{\\top}\\boldsymbol{G})^{-1/4} = (\\boldsymbol{G}\\boldsymbol{G}^{\\top})^{-1/2}\\boldsymbol{G}= \\boldsymbol{G}(\\boldsymbol{G}^{\\top}\\boldsymbol{G})^{-1/2}=\\text{msign}(\\boldsymbol{G})\\end{equation}\n\\]\nThis indicates that when \\(\\beta=0\\), Shampoo and Muon are theoretically equivalent! Therefore, Shampoo and Muon have common design principles."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#basic-review",
    "href": "docs/posts/2025-02-27-muon/index.html#basic-review",
    "title": "The Muon Anthology",
    "section": "Basic review",
    "text": "Basic review\nThe spectral norm \\(\\|\\cdot\\|_2\\) (also known as the 2-norm) is one of the most commonly used matrix norms. Compared to the simpler Frobenius norm \\(\\|\\cdot\\|_F\\), it often reveals more essential signals related to matrix multiplication, because its definition is directly related to matrix multiplication: for a matrix parameter \\(\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}\\), its spectral norm is defined as\n\\[\n\\begin{equation}\\Vert\\boldsymbol{W}\\Vert_2 \\triangleq \\max_{\\Vert\\boldsymbol{x}\\Vert=1} \\Vert\\boldsymbol{W}\\boldsymbol{x}\\Vert\\end{equation}\n\\]\nHere, \\(\\boldsymbol{x}\\in\\mathbb{R}^m\\) is a column vector, and the \\(\\Vert\\cdot\\Vert\\) on the right side is the Euclidean norm of vectors. From another perspective, the spectral norm is the smallest constant \\(C\\) such that the following inequality holds for \\(\\forall \\boldsymbol{x}\\in\\mathbb{R}^m\\):\n\\[\n\\begin{equation}\\Vert\\boldsymbol{W}\\boldsymbol{x}\\Vert \\leq C\\Vert\\boldsymbol{x}\\Vert\\end{equation}\n\\]\nIt’s not hard to prove that when \\(C\\) takes the Frobenius norm \\(\\Vert W\\Vert_F\\), the above inequality still holds, so we can write \\(\\Vert \\boldsymbol{W}\\Vert_2\\leq \\Vert \\boldsymbol{W}\\Vert_F\\) (since \\(\\Vert \\boldsymbol{W}\\Vert_F\\) is just one of the \\(C\\) values that makes the inequality hold, while \\(\\Vert \\boldsymbol{W}\\Vert_2\\) is the smallest such \\(C\\)). This conclusion also suggests that if we want to control the magnitude of the output, using the spectral norm as a regularization term is more precise than using the Frobenius norm.\nSix years ago, in Lipschitz Constraints in Deep Learning: Generalization and Generative Models, I discussed the spectral norm in two application scenarios:\n\nWasserstein GAN explicitly proposed a Lipschitz constraint on the discriminator, and one implementation approach was based on spectral norm normalization.\nSome empirical work showed that the spectral norm as a regularization term performs better compared to Frobenius norm regularization."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#gradient-derivation",
    "href": "docs/posts/2025-02-27-muon/index.html#gradient-derivation",
    "title": "The Muon Anthology",
    "section": "Gradient derivation",
    "text": "Gradient derivation\nNow let’s get to the main point and try to derive the gradient of the spectral norm \\(\\nabla_{\\boldsymbol{W}} \\Vert\\boldsymbol{W}\\Vert_2\\). Since the spectral norm is equal to the largest singular value, if \\(\\boldsymbol{W}\\) can be decomposed by SVD as \\(\\sum\\limits_{i=1}^{\\min(n,m)}\\sigma_i \\boldsymbol{u}_i\\boldsymbol{v}_i^{\\top}\\), then\n\\[\n\\begin{equation}\\Vert\\boldsymbol{W}\\Vert_2 = \\sigma_1 = \\boldsymbol{u}_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}_1\\end{equation}\n\\]\nwhere \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_{\\min(n,m)} \\geq 0\\) are the singular values of \\(\\boldsymbol{W}\\). Taking the differential of both sides, we get\n\\[\n\\begin{equation}d\\Vert\\boldsymbol{W}\\Vert_2 = d\\boldsymbol{u}_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}_1 + \\boldsymbol{u}_1^{\\top}d\\boldsymbol{W}\\boldsymbol{v}_1 + \\boldsymbol{u}_1^{\\top}\\boldsymbol{W}d\\boldsymbol{v}_1\\end{equation}\n\\]\nNote that\n\\[\n\\begin{equation}d\\boldsymbol{u}_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}_1 = d\\boldsymbol{u}_1^{\\top}\\sum_{i=1}^{\\min(n,m)}\\sigma_i \\boldsymbol{u}_i\\boldsymbol{v}_i^{\\top}\\boldsymbol{v}_1 = d\\boldsymbol{u}_1^{\\top}\\sigma_1 \\boldsymbol{u}_1 = \\frac{1}{2}\\sigma_1 d(\\Vert\\boldsymbol{u}_1\\Vert^2)=0\\end{equation}\n\\]\nSimilarly, \\(\\boldsymbol{u}_1^{\\top}\\boldsymbol{W}d\\boldsymbol{v}_1=0\\), so\n\\[\n\\begin{equation}d\\Vert\\boldsymbol{W}\\Vert_2 = \\boldsymbol{u}_1^{\\top}d\\boldsymbol{W}\\boldsymbol{v}_1 = \\text{Tr}((\\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top})^{\\top} d\\boldsymbol{W}) \\quad\\Rightarrow\\quad \\nabla_{\\boldsymbol{W}}\\Vert\\boldsymbol{W}\\Vert_2 = \\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top}\\end{equation}\n\\]\nNote that this proof process has a key condition: \\(\\sigma_1 &gt; \\sigma_2\\). If \\(\\sigma_1=\\sigma_2\\), then \\(\\Vert\\boldsymbol{W}\\Vert_2\\) can be expressed as both \\(\\boldsymbol{u}_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}_1\\) and \\(\\boldsymbol{u}_2^{\\top}\\boldsymbol{W}\\boldsymbol{v}_2\\), and the gradients calculated using the same method would be \\(\\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top}\\) and \\(\\boldsymbol{u}_2 \\boldsymbol{v}_2^{\\top}\\) respectively. The non-uniqueness of the result means that the gradient does not exist. Of course, from a practical perspective, the probability of two numbers being exactly equal is very small, so this point can be ignored.\n(This proof was based on an answer on Stack Exchange, but that answer did not prove that \\(d\\boldsymbol{u}_1^{\\top}\\boldsymbol{W}\\boldsymbol{v}_1=0\\) and \\(\\boldsymbol{u}_1^{\\top}\\boldsymbol{W}d\\boldsymbol{v}_1=0\\). These were proved by me.)"
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#weight-decay",
    "href": "docs/posts/2025-02-27-muon/index.html#weight-decay",
    "title": "The Muon Anthology",
    "section": "Weight decay",
    "text": "Weight decay\nBased on this result and the chain rule, we have\n\\[\n\\begin{equation}\\nabla_{\\boldsymbol{W}}\\left(\\frac{1}{2}\\Vert\\boldsymbol{W}\\Vert_2^2\\right) = \\Vert\\boldsymbol{W}\\Vert_2\\nabla_{\\boldsymbol{W}}\\Vert\\boldsymbol{W}\\Vert_2 = \\sigma_1 \\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top}\\end{equation}\n\\tag{20}\\]\nComparing with the result under the Frobenius norm:\n\\[\n\\begin{equation}\\nabla_{\\boldsymbol{W}}\\left(\\frac{1}{2}\\Vert\\boldsymbol{W}\\Vert_F^2\\right) = \\boldsymbol{W} = \\sum_{i=1}^{\\min(n,m)}\\sigma_i \\boldsymbol{u}_i \\boldsymbol{v}_i^{\\top}\\end{equation}\n\\]\nLooking at it this way, it becomes very clear: weight decay derived from the squared Frobenius norm as a regularization term penalizes all singular values simultaneously; while weight decay corresponding to the squared spectral norm only penalizes the largest singular value. If our goal is to compress the size of the output, then compressing the maximum singular value is the “just right” approach. Compressing all singular values may achieve a similar purpose, but it might also compress the expressive power of the parameters.\nBy the Eckart–Young–Mirsky theorem, the RHS of Equation 20 is the optimal rank-1 approximation of \\(\\boldsymbol{W}\\). In other words, spectral norm weight decay changes the operation from subtracting itself at each step to subtracting its optimal rank-1 approximation at each step, weakening the penalty strength, and to some extent allows the penalty to hit closer to the heart of the issue."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#numerical-computation",
    "href": "docs/posts/2025-02-27-muon/index.html#numerical-computation",
    "title": "The Muon Anthology",
    "section": "Numerical computation",
    "text": "Numerical computation\nFor practical purposes, the key question arises: how to compute \\(\\sigma_1 \\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top}\\)? SVD is certainly the simplest and most direct solution, but its computational complexity is undoubtedly the highest. We must find a more efficient computation path.\nWithout loss of generality, let \\(n\\geq m\\). First, note that\n\\[\n\\begin{equation}\\sigma_1 \\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top} = \\sum_{i=1}^m\\sigma_i \\boldsymbol{u}_i \\boldsymbol{v}_i^{\\top} \\boldsymbol{v}_1 \\boldsymbol{v}_1^{\\top} = \\boldsymbol{W}\\boldsymbol{v}_1 \\boldsymbol{v}_1^{\\top}\\end{equation}\n\\]\nThis shows that to compute \\(\\sigma_1 \\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top}\\), we only need to know \\(\\boldsymbol{v}_1\\), and \\(\\boldsymbol{v}_1\\) is actually the eigenvector corresponding to the largest eigenvalue of the matrix \\(\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\). In this way, we have transformed the problem from SVD of a general matrix \\(\\boldsymbol{W}\\) to eigenvalue decomposition of a real symmetric matrix \\(\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\), which has already reduced the complexity, because eigenvalue decomposition is usually significantly faster than SVD.\nIf you still think it’s slow, then apply the standard trick used in many eigenvalue decomposition algorithms, power iteration:\nWhen \\(\\sigma_1 &gt; \\sigma_2\\), the iteration \\[\n\\begin{equation}\\boldsymbol{x}_{t+1} = \\frac{\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\boldsymbol{x}_t}{\\Vert\\boldsymbol{W}^{\\top}\\boldsymbol{W}\\boldsymbol{x}_t\\Vert}\\end{equation}\n\\]\nconverges to \\(\\boldsymbol{v}_1\\) at a rate of \\((\\sigma_2/\\sigma_1)^{2t}\\).\nEach step of power iteration only requires computing two “matrix-vector” multiplications, with a complexity of \\(\\mathcal{O}(nm)\\). The total complexity of \\(t\\) iterations is \\(\\mathcal{O}(tnm)\\), which is very ideal. The disadvantage is that convergence can be slow when \\(\\sigma_1\\) and \\(\\sigma_2\\) are close. But power iteration often performs better in practice than in theory. Many early works even achieved good results with just one iteration, because when \\(\\sigma_1\\) and \\(\\sigma_2\\) are close, it indicates that both values and their eigenvectors are somewhat interchangeable to some degree. Even if power iteration doesn’t fully converge, it still gives an average of their eigenvectors, which is often sufficient."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#proof-of-iteration",
    "href": "docs/posts/2025-02-27-muon/index.html#proof-of-iteration",
    "title": "The Muon Anthology",
    "section": "Proof of iteration",
    "text": "Proof of iteration\nIn this section, we’ll complete the proof of power iteration. It’s not hard to see that power iteration can be equivalently written as\n\\[\n\\begin{equation}\\lim_{t\\to\\infty} \\frac{(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}_0}{\\Vert(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}_0\\Vert} = \\boldsymbol{v}_1\\end{equation}\n\\]\nTo prove this limit, we start from \\(\\boldsymbol{W}=\\sum\\limits_{i=1}^m\\sigma_i \\boldsymbol{u}_i\\boldsymbol{v}_i^{\\top}\\), substitute and calculate to get\n\\[\n\\begin{equation}\\boldsymbol{W}^{\\top}\\boldsymbol{W} = \\sum_{i=1}^m\\sigma_i^2 \\boldsymbol{v}_i\\boldsymbol{v}_i^{\\top},\\qquad(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t = \\sum_{i=1}^m\\sigma_i^{2t} \\boldsymbol{v}_i\\boldsymbol{v}_i^{\\top}\\end{equation}\n\\]\nSince \\(\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\cdots,\\boldsymbol{v}_m\\) form an orthonormal basis of \\(\\mathbb{R}^m\\), \\(\\boldsymbol{x}_0\\) can be written as \\(\\sum\\limits_{j=1}^m c_j \\boldsymbol{v}_j\\), so we have\n\\[\n\\begin{equation}(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}_0 = \\sum_{i=1}^m\\sigma_i^{2t} \\boldsymbol{v}_i\\boldsymbol{v}_i^{\\top}\\sum_{j=1}^m c_j \\boldsymbol{v}_j = \\sum_{i=1}^m\\sum_{j=1}^m c_j\\sigma_i^{2t} \\boldsymbol{v}_i\\underbrace{\\boldsymbol{v}_i^{\\top}  \\boldsymbol{v}_j}_{=\\delta_{i,j}} = \\sum_{i=1}^m c_i\\sigma_i^{2t} \\boldsymbol{v}_i\\end{equation}\n\\]\nand\n\\[\n\\begin{equation}\\Vert(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}_0\\Vert = \\left\\Vert \\sum_{i=1}^m c_i\\sigma_i^{2t} \\boldsymbol{v}_i\\right\\Vert = \\sqrt{\\sum_{i=1}^m c_i^2\\sigma_i^{4t}}\\end{equation}\n\\]\nDue to random initialization, the probability of \\(c_1=0\\) is very small, so we can assume \\(c_1\\neq 0\\). Then\n\\[\n\\begin{equation}\\frac{(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}_0}{\\Vert(\\boldsymbol{W}^{\\top}\\boldsymbol{W})^t \\boldsymbol{x}_0\\Vert} = \\frac{\\sum\\limits_{i=1}^m c_i\\sigma_i^{2t} \\boldsymbol{v}_i}{\\sqrt{\\sum\\limits_{i=1}^m c_i^2\\sigma_i^{4t}}} = \\frac{\\boldsymbol{v}_1 + \\sum\\limits_{i=2}^m (c_i/c_1)(\\sigma_i/\\sigma_1)^{2t} \\boldsymbol{v}_i}{\\sqrt{1 + \\sum\\limits_{i=2}^m (c_i/c_1)^2(\\sigma_i/\\sigma_1)^{4t}}}\\end{equation}\n\\]\nWhen \\(\\sigma_1 &gt; \\sigma_2\\), all \\(\\sigma_i/\\sigma_1(i\\geq 2)\\) are less than 1, so as \\(t\\to \\infty\\), the corresponding terms all become 0, and the final limit is \\(\\boldsymbol{v}_1\\)."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#related-work-1",
    "href": "docs/posts/2025-02-27-muon/index.html#related-work-1",
    "title": "The Muon Anthology",
    "section": "Related Work",
    "text": "Related Work\nThe earliest paper proposing spectral norm regularization should be the 2017 paper Spectral Norm Regularization for Improving the Generalizability of Deep Learning, which compared methods such as weight decay, adversarial training, and spectral norm regularization, finding that spectral norm regularization performed best in terms of generalization performance.\nThe approach in the paper at that time was not like this article’s approach of calculating \\(\\nabla_{\\boldsymbol{W}}\\Vert\\boldsymbol{W}\\Vert_2^2 = 2\\sigma_1\\boldsymbol{u}_1 \\boldsymbol{v}_1^{\\top}\\), but rather directly estimating \\(\\Vert\\boldsymbol{W}\\Vert_2\\) through power iteration, then adding \\(\\Vert\\boldsymbol{W}\\Vert_2^2\\) to the loss function with a weight, letting the optimizer calculate the gradient itself. This approach is slightly less efficient and makes it harder to decouple from the optimizer in the form of weight decay. This article’s approach is relatively more flexible, allowing us, like AdamW, to keep weight decay independent from the optimization of the main loss function.\nOf course, from today’s LLM perspective, the biggest problem with those early experiments was that they were all too small in scale to be sufficiently convincing. However, given the precedent of the spectral norm-based Muon optimizer, I believe it’s worth reconsidering and trying spectral norm weight decay. Certainly, whether it’s Frobenius norm or spectral norm weight decay, these techniques aimed at “generalization” often have some luck involved, so it’s best to maintain moderate expectations.\nInitial experiments on language models show that there might be a slight improvement at the loss level. Hopefully this is not an illusion, but even if it’s an illusion, at least no degradation was observed. The experimental process involved using power iteration to approximate \\(\\boldsymbol{v}_1\\) (initialized as an all-ones vector, iterated 10 times), then changing the original weight decay \\(-\\lambda \\boldsymbol{W}\\) to \\(-\\lambda \\boldsymbol{W}\\boldsymbol{v}_1\\boldsymbol{v}_1^{\\top}\\), keeping the value of \\(\\lambda\\) unchanged."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#what-does-it-mean",
    "href": "docs/posts/2025-02-27-muon/index.html#what-does-it-mean",
    "title": "The Muon Anthology",
    "section": "What does it mean?",
    "text": "What does it mean?\nSome readers might think, the default value is not necessarily the optimal value, so why bother with it? Indeed, \\(\\tau=1\\) may not be the optimal choice, but it is the default choice for many models, and these models perform reasonably well with this default choice, which in turn suggests that \\(\\tau=1\\) is a “universally reasonable” choice.\nWhat does it mean to say a choice of \\(\\tau\\) is “reasonable”? Let’s return to the \\(\\text{clip}\\) operation. If \\(\\Vert\\boldsymbol{g}\\Vert\\) is always less than \\(\\tau\\), then \\(\\text{clip}\\) degenerates into an identity transformation; if \\(\\Vert\\boldsymbol{g}\\Vert\\) is always greater than \\(\\tau\\), then \\(\\text{clip}\\) degenerates into L2 normalization. In other words, the reason \\(\\text{clip}\\) is \\(\\text{clip}\\) is because \\(\\tau\\) produces an appropriate level of distinction, making most \\(\\Vert\\boldsymbol{g}\\Vert\\) less than \\(\\tau\\), with only a small portion greater than \\(\\tau\\) – this is the meaning of saying this \\(\\tau\\) is “reasonable”.\nOf course, counterexamples can be found, and quite a few at that. Here, I mainly want to emphasize the universality of this phenomenon and the general applicability of this default setting, so meticulous readers need not be overly fixated on individual details.\nTherefore, we believe that the universal reasonableness of \\(\\tau=1\\) means that regardless of the number of model parameters, how they are initialized, or what loss function is chosen, the total gradient norm can roughly use \\(1\\) as the boundary point for “abnormal values” – this is undoubtedly an incredibly amazing property. This was precisely my feeling when I first realized this conclusion."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#why-does-this-happen",
    "href": "docs/posts/2025-02-27-muon/index.html#why-does-this-happen",
    "title": "The Muon Anthology",
    "section": "Why does this happen?",
    "text": "Why does this happen?\n\\(\\tau=1\\) seems way too nice. Why is Heaven so nice to us? My answer might be somewhat unexpected: because only in this way is stable training of the model possible.\nLet’s consider the loss function \\(\\mathcal{L}(\\boldsymbol{\\theta})\\), with the optimizer update rule \\(\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta\\, \\boldsymbol{u}_t\\). Then the change in the loss function can be approximated as:\n\\[\n\\begin{equation}\\Delta \\mathcal{L} = \\mathcal{L}(\\boldsymbol{\\theta}_{t+1}) - \\mathcal{L}(\\boldsymbol{\\theta}_t) \\approx (\\boldsymbol{\\theta}_{t+1} - \\boldsymbol{\\theta}_t)\\cdot\\nabla_{\\boldsymbol{\\theta}_t}\\mathcal{L}(\\boldsymbol{\\theta}) = -\\eta\\, \\boldsymbol{u}_t\\cdot \\boldsymbol{g}_t\\end{equation}\n\\]\nFirst, consider the simplest SGD, where \\(\\boldsymbol{u}_t = \\boldsymbol{g}_t\\) and \\(\\Delta \\mathcal{L}=-\\eta\\Vert\\boldsymbol{g}_t\\Vert^2\\), meaning the change in the loss function is proportional to the square of the gradient norm. We know that, whether in CV or NLP, pure SGD (without momentum) is a very inefficient optimizer. In the middle and later stages of training, on average, the decrease in loss per step for most tasks is far less than the learning rate, i.e., \\(|\\Delta \\mathcal{L}| &lt; \\eta\\), from which we can deduce \\(\\Vert\\boldsymbol{g}_t\\Vert &lt; 1\\). This indicates that \\(\\Vert\\boldsymbol{g}_t\\Vert &lt; 1\\) is the long-term performance of a model that can converge normally.\nOf course, in the early stages of training, the model may exhibit \\(\\Vert\\boldsymbol{g}_t\\Vert &gt; 1\\), which is normal, but it’s rare to have \\(\\Vert\\boldsymbol{g}_t\\Vert \\gg 1\\). In other words, a good initialization should avoid \\(\\Vert\\boldsymbol{g}_t\\Vert \\gg 1\\), which is the theoretical basis for methods like DeepNorm. The reason is similar: if the gradient norm is too large, then early learning will be too “aggressive”, leading to premature convergence to poor local minima. Another approach is to reduce \\(\\eta\\), which can also reduce \\(|\\Delta \\mathcal{L}|\\). This is why we typically use Warmup in the early stages of training.\nBy the way, for understanding Warmup, readers can refer to the paper Optimal Linear Decay Learning Rate Schedules and Further Refinements, which I consider to be the most reasonable analysis of Warmup."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#what-to-do",
    "href": "docs/posts/2025-02-27-muon/index.html#what-to-do",
    "title": "The Muon Anthology",
    "section": "What to do?",
    "text": "What to do?\nSimply put, since the change in the loss function is proportional to the square of the gradient norm, the stability of training determines that the gradient norm cannot be too large, and its long-term performance is less than 1. If significantly larger gradient norms appear in the early stage, the usual strategy is Warmup. Or a more general strategy can be considered: set another threshold \\(\\mathcal{T}\\) and clip \\(\\eta\\) based on the value of \\(\\boldsymbol{u}_t\\cdot \\boldsymbol{g}_t\\):\n\\[\n\\begin{equation}\\eta_t = \\left\\{\\begin{aligned}&\\eta,& \\boldsymbol{u}_t\\cdot \\boldsymbol{g}_t\\leq \\mathcal{T} \\\\ &\\frac{\\mathcal{T}}{\\boldsymbol{u}_t\\cdot \\boldsymbol{g}_t}\\eta,& \\boldsymbol{u}_t\\cdot \\boldsymbol{g}_t &gt; \\mathcal{T}\n\\end{aligned}\\right.\\end{equation}\n\\]\nThis eliminates the need for additional Warmup settings and is more adaptive.\nFor optimizers like Adam, we can use approximate analysis through \\(\\boldsymbol{u}_t=\\text{sign}(\\boldsymbol{g}_t)\\), as previously described. In this case:\n\\[\n\\begin{equation}\\Delta \\mathcal{L} = -\\eta\\, \\text{sign}(\\boldsymbol{g}_t)\\cdot \\boldsymbol{g}_t = -\\eta\\, \\Vert\\boldsymbol{g}_t\\Vert_1\\end{equation}\n\\]\nHere, \\(\\Vert\\cdot\\Vert_1\\) is the L1 norm, i.e., the sum of the absolute values of the components. Since gradient components are generally less than 1, \\(\\Vert\\boldsymbol{g}_t\\Vert_1 \\gg \\Vert\\boldsymbol{g}_t\\Vert\\). Therefore, due to the requirement for stable training, the learning rate for Adam is typically significantly smaller than that for SGD. Additionally, the above equation can be rewritten as:\n\\[\n\\begin{equation}\\Delta \\mathcal{L} = -\\eta\\, \\text{sign}(\\boldsymbol{g}_t)\\cdot \\boldsymbol{g}_t = -\\eta\\, \\sqrt{N}\\Vert\\boldsymbol{g}_t\\Vert \\cos(\\text{sign}(\\boldsymbol{g}_t), \\boldsymbol{g}_t) \\end{equation}\n\\]\nThis assumes that \\(\\boldsymbol{g}_t\\) has no zero components, so \\(\\Vert\\text{sign}(\\boldsymbol{g}_t)\\Vert=\\sqrt{N}\\), where \\(N\\) is the total number of model parameters. In practice, it’s found that \\(\\Vert\\boldsymbol{g}_t\\Vert\\) and \\(\\cos(\\text{sign}(\\boldsymbol{g}_t), \\boldsymbol{g}_t)\\) are roughly constant across different model scales. Therefore, to maintain a constant \\(\\Delta \\mathcal{L}\\), \\(\\eta\\) should be inversely proportional to \\(\\sqrt{N}\\), meaning that if the number of model parameters increases by a factor of 4, the learning rate could be halved."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#optimization-principles",
    "href": "docs/posts/2025-02-27-muon/index.html#optimization-principles",
    "title": "The Muon Anthology",
    "section": "Optimization principles",
    "text": "Optimization principles\nRegarding optimizers, I had already provided a brief assessment in Muon Appreciation. Most optimizer improvements are just tiny variants of the same basic thing. Not to say they’re worthless, but they won’t make a profound or stunning impression on you.\nWe need to think about what makes a good optimizer from principles closer to the essence. Intuitively, an ideal optimizer should have two characteristics: stable and fast. Specifically, each update of an ideal optimizer should satisfy two points:\n\nDisturb the model as little as possible.\nContribute as much as possible to loss reduction. More directly, we don’t want to drastically change the model (stability), but we want to greatly reduce the loss (speed). It’s “both… and…”.\n\nHow do we translate these two characteristics into mathematical language? We can understand stability as a constraint on the update magnitude, and speed as finding the update that makes the loss function decrease most rapidly. So this can be transformed into a constrained optimization problem. Using the notation from earlier, for a matrix parameter \\(\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}\\) with gradient \\(\\boldsymbol{G}\\in\\mathbb{R}^{n\\times m}\\), when the parameter changes from \\(\\boldsymbol{W}\\) to \\(\\boldsymbol{W}+\\Delta\\boldsymbol{W}\\), the change in the loss function is\n\\[\n\\begin{equation}\\text{Tr}(\\boldsymbol{G}^{\\top}\\Delta\\boldsymbol{W})\\end{equation}\n\\]\nTherefore, conditional on being stable, the fastest update should satisfy\n\\[\n\\begin{equation}\\mathop{\\text{argmin}}_{\\Delta\\boldsymbol{W}}\\text{Tr}(\\boldsymbol{G}^{\\top}\\Delta\\boldsymbol{W})\\quad\\text{s.t.}\\quad \\rho(\\Delta\\boldsymbol{W})\\leq \\eta\\end{equation}\n\\tag{21}\\]\nHere, \\(\\rho(\\Delta\\boldsymbol{W})\\geq 0\\) is some metric of stability, where smaller values indicate greater stability, and \\(\\eta\\) is a constant less than 1, representing our requirement for stability. Later we’ll see that it’s actually the learning rate of the optimizer. If readers don’t mind, we can borrow concepts from theoretical physics and call it the “Least Action Principle” for optimizers."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#matrix-norms-1",
    "href": "docs/posts/2025-02-27-muon/index.html#matrix-norms-1",
    "title": "The Muon Anthology",
    "section": "Matrix norms",
    "text": "Matrix norms\nThe only uncertainty in Equation 21 is the stability measure \\(\\rho(\\Delta\\boldsymbol{W})\\). Once \\(\\rho(\\Delta\\boldsymbol{W})\\) is determined, \\(\\Delta\\boldsymbol{W}\\) can be explicitly solved (at least theoretically). To some extent, we can consider that the essential difference between different optimizers is how they define stability.\nMany readers probably encountered statements like “the negative gradient direction is the direction of steepest descent for the function value locally” when first learning SGD. In this framework, it actually means choosing the Frobenius norm \\(\\Vert\\Delta\\boldsymbol{W}\\Vert_F\\) as the measure of stability. In other words, the “direction of steepest descent” is not immutable – it can only be determined after choosing a metric. Change the norm, and it may no longer be the negative gradient direction.\nThe next question naturally is: which norm most appropriately measures stability? If we impose strong constraints, sure, we will definitely achieve stability, but the optimizer will crawl like a snail, only converging to a suboptimal solution. Conversely, if we weaken the constraints, the optimizer will fly like a crazy bat, making the training process extremely unstable. So, ideally, we want to find the most precise index of stability. Considering that neural networks primarily involve matrix multiplication, let’s take \\(\\boldsymbol{y}=\\boldsymbol{x}\\boldsymbol{W}\\) as an example:\n\\[\n\\begin{equation}\\Vert\\Delta \\boldsymbol{y}\\Vert = \\Vert\\boldsymbol{x}(\\boldsymbol{W} + \\Delta\\boldsymbol{W}) - \\boldsymbol{x}\\boldsymbol{W}\\Vert = \\Vert\\boldsymbol{x} \\Delta\\boldsymbol{W}\\Vert\\leq \\rho(\\Delta\\boldsymbol{W}) \\Vert\\boldsymbol{x}\\Vert\\end{equation}\n\\]\nThis means that when the parameter changes from \\(\\boldsymbol{W}\\) to \\(\\boldsymbol{W}+\\Delta\\boldsymbol{W}\\), the change in model output is \\(\\Delta\\boldsymbol{y}\\). We hope that the magnitude of this change can be controlled by \\(\\Vert\\boldsymbol{x}\\Vert\\) and a function \\(\\rho(\\Delta\\boldsymbol{W})\\) related to \\(\\Delta\\boldsymbol{W}\\), and we use this function as an index of stability. From linear algebra, we know that the most accurate value for \\(\\rho(\\Delta\\boldsymbol{W})\\) is the spectral norm of \\(\\Delta\\boldsymbol{W}\\), denoted as \\(\\Vert\\Delta\\boldsymbol{W}\\Vert_2\\). Substituting into Equation 21, we get:\n\\[\n\\begin{equation}\\mathop{\\text{argmin}}_{\\Delta\\boldsymbol{W}}\\text{Tr}(\\boldsymbol{G}^{\\top}\\Delta\\boldsymbol{W})\\quad\\text{s.t.}\\quad \\Vert\\Delta\\boldsymbol{W}\\Vert_2\\leq \\eta\\end{equation}\n\\]\nThe solution to this optimization problem is Muon with \\(\\beta=0\\):\n\\[\n\\begin{equation}\\Delta\\boldsymbol{W} = -\\eta\\, \\text{msign}(\\boldsymbol{G}) = -\\eta\\,\\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top}, \\quad \\boldsymbol{U},\\boldsymbol{\\Sigma},\\boldsymbol{V}^{\\top} = SVD(\\boldsymbol{G})\\end{equation}\n\\]\nWhen \\(\\beta &gt; 0\\), \\(\\boldsymbol{G}\\) is replaced with momentum \\(\\boldsymbol{M}\\), which can be seen as a smoother estimate of the gradient, so it can still be understood as the conclusion above. Therefore, we can say that “Muon is steepest descent under the spectral norm.” As for Newton–Schulz iterations and the like, they are computational approximations, which we won’t elaborate on here. We already provided detailed derivations in Muon Appreciation, so we won’t repeat them."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#weight-decay-1",
    "href": "docs/posts/2025-02-27-muon/index.html#weight-decay-1",
    "title": "The Muon Anthology",
    "section": "Weight decay",
    "text": "Weight decay\nAt this point, we can answer the first question: why try Muon? Like SGD, Muon also gives the direction of steepest descent, but its spectral norm constraint is more precise than SGD’s Frobenius norm, so it has better potential. On the other hand, improving optimizers from the perspective of “choosing the most appropriate constraint for different parameters” seems more fundamental than various small patches over the same thing.\nOf course, potential doesn’t always translate to performance, and there are some annoying gotchas when validating Muon on larger models. First is the weight decay problem. Although I included weight decay when introducing Muon in Muon Appreciation, the authors of Muon didn’t include it when they first proposing Muon. We initially implemented it according to the official version and found that while Muon converged faster at the beginning, it was soon caught up by Adam, and various “internal issues” showed signs of collapse.\nWe quickly realized this might be a weight decay issue, so we added weight decay:\n\\[\n\\begin{equation}\\Delta\\boldsymbol{W} = -\\eta\\, [\\text{msign}(\\boldsymbol{M})+ \\lambda \\boldsymbol{W}]\\end{equation}\n\\]\nContinuing the experiment, sure enough, Muon consistently maintained its lead over Adam, as shown in Figure 2 of the paper:\n\n\n\nWith vs without weight decay.\n\n\nWhat role does weight decay play? In retrospect, it should have been clear that we should keep the an upper bound on the norm of the matrix parameter:\n\\[\n\\begin{equation}\\begin{aligned}\n\\Vert\\boldsymbol{W}_t\\Vert =&\\, \\Vert\\boldsymbol{W}_{t-1} - \\eta_t (\\boldsymbol{O}_t + \\lambda \\boldsymbol{W}_{t-1})\\Vert \\\\\n=&\\, \\Vert(1 - \\eta_t \\lambda)\\boldsymbol{W}_{t-1} - \\eta_t \\lambda (\\boldsymbol{O}_t/\\lambda)\\Vert \\\\\n\\leq &\\,(1 - \\eta_t \\lambda)\\Vert\\boldsymbol{W}_{t-1}\\Vert + \\eta_t \\lambda \\Vert\\boldsymbol{O}_t/\\lambda\\Vert \\\\\n\\leq &\\,\\max(\\Vert\\boldsymbol{W}_{t-1}\\Vert,\\Vert\\boldsymbol{O}_t/\\lambda\\Vert) \\\\\n\\end{aligned}\\end{equation}\n\\]\nHere, \\(\\Vert\\cdot\\Vert\\) is any matrix norm, meaning the above inequality holds for any matrix norm. \\(\\boldsymbol{O}_t\\) is the update vector given by the optimizer, which is \\(\\text{msign}(\\boldsymbol{M})\\) for Muon. When we take the spectral norm, \\(\\Vert\\text{msign}(\\boldsymbol{M})\\Vert_2 = 1\\), so for Muon, we have:\n\\[\n\\begin{equation}\n\\Vert\\boldsymbol{W}_t\\Vert_2 \\leq \\max(\\Vert\\boldsymbol{W}_{t-1}\\Vert_2,1/\\lambda)\\leq\\cdots \\leq \\max(\\Vert\\boldsymbol{W}_0\\Vert_2,1/\\lambda)\\end{equation}\n\\]\nThis keeps the model “healthy on the inside”, because \\(\\Vert\\boldsymbol{x}\\boldsymbol{W}\\Vert\\leq \\Vert\\boldsymbol{x}\\Vert\\cdot\\Vert\\boldsymbol{W}\\Vert_2\\). When \\(\\Vert\\boldsymbol{W}\\Vert_2\\) is controlled, it means that \\(\\Vert\\boldsymbol{x}\\boldsymbol{W}\\Vert\\) is also controlled, eliminating the risk of explosion, which is particularly important for issues like Attention Logits explosion. Of course, this upper bound is quite loose in most cases, and in practice, the spectral norm of parameters is often significantly smaller than this upper bound. This inequality simply shows that weight decay can control norms."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#rms-alignment",
    "href": "docs/posts/2025-02-27-muon/index.html#rms-alignment",
    "title": "The Muon Anthology",
    "section": "RMS alignment",
    "text": "RMS alignment\nWhen we decide to try a new optimizer, a challenging problem is how to quickly find near-optimal hyperparameters. For instance, Muon has at least two hyperparameters: learning rate \\(\\eta_t\\) and decay rate \\(\\lambda\\). Grid search is certainly possible but time-consuming. Here, we propose the Update RMS alignment approach for hyperparameter transfer, which can apply well-tuned Adam hyperparameters to other optimizers.\nFirst, for a matrix \\(\\boldsymbol{W}\\in\\mathbb{R}^{n\\times m}\\), its RMS (Root Mean Square) is defined as:\n\\[\n\\begin{equation}\\text{RMS}(\\boldsymbol{W}) = \\frac{\\Vert \\boldsymbol{W}\\Vert_F}{\\sqrt{nm}} = \\sqrt{\\frac{1}{nm}\\sum_{i=1}^n\\sum_{j=1}^m W_{i,j}^2}\\end{equation}\n\\]\nSimply put, RMS measures the average size of each element in the matrix. We observed that the RMS of Adam’s update is relatively stable, usually between 0.2 and 0.4, which is why theoretical analyses we described previously often use SignSGD to approximate Adam. Based on this, we suggest aligning the Update RMS of the new optimizer to 0.2 through RMS normalization:\n\\[\n\\begin{gather}\n\\boldsymbol{W}_t =\\boldsymbol{W}_{t-1} - \\eta_t (\\boldsymbol{O}_t + \\lambda \\boldsymbol{W}_{t-1}) \\\\[6pt]\n\\downarrow \\notag\\\\[6pt]\n\\boldsymbol{W}_t = \\boldsymbol{W}_{t-1} - \\eta_t (0.2\\, \\boldsymbol{O}_t/\\text{RMS}(\\boldsymbol{O}_t) + \\lambda \\boldsymbol{W}_{t-1})\n\\end{gather}\n\\]\nThis way, we can reuse Adam’s \\(\\eta_t\\) and \\(\\lambda\\) to achieve roughly the same update magnitude to parameters at each step. Practice shows that transitioning from Adam to Muon through this simple strategy can produce results significantly better than Adam, approaching the results of further fine-tuning Muon’s hyperparameters. Specifically, Muon’s \\(\\text{RMS}(\\boldsymbol{O}_t)=\\text{RMS}(\\boldsymbol{U}_{[:,:r]}\\boldsymbol{V}_{[:,:r]}^{\\top})\\) can be calculated analytically:\n\\[\n\\begin{equation}nm\\,\\text{RMS}(\\boldsymbol{O}_t)^2 = \\sum_{i=1}^n\\sum_{j=1}^m \\sum_{k=1}^r U_{i,k}^2V_{k,j}^2 = \\sum_{k=1}^r\\left(\\sum_{i=1}^n U_{i,k}^2\\right)\\left(\\sum_{j=1}^m V_{k,j}^2\\right) = \\sum_{k=1}^r 1 = r\\end{equation}\n\\]\nThat is, \\(\\text{RMS}(\\boldsymbol{O}_t) = \\sqrt{r/nm}\\). In practice, the probability of a matrix being strictly low-rank is quite small, so we can assume \\(r = \\min(n,m)\\), giving us \\(\\text{RMS}(\\boldsymbol{O}_t) = \\sqrt{1/\\max(n,m)}\\). Therefore, we ultimately used the equivalent analytical version instead of direct RMS normalization:\n\\[\n\\begin{equation}\\boldsymbol{W}_t = \\boldsymbol{W}_{t-1} - \\eta_t (0.2\\, \\boldsymbol{O}_t\\,\\sqrt{\\max(n,m)} + \\lambda \\boldsymbol{W}_{t-1})\\end{equation}\n\\]\nThis final formula indicates that it’s not appropriate to use the same learning rate for all parameters in Muon. For example, Moonlight is an MoE model with many matrix parameters whose shapes deviate from square matrices, resulting in a wide range of \\(\\max(n,m)\\) values. Using a single learning rate would inevitably lead to asynchronous issues where some parameters learn too quickly or too slowly, affecting the final result."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#experiments",
    "href": "docs/posts/2025-02-27-muon/index.html#experiments",
    "title": "The Muon Anthology",
    "section": "Experiments",
    "text": "Experiments\nWe conducted a fairly comprehensive comparison between Adam and Muon on MoE models of sizes 2.4B/16B and found that Muon has significant advantages in both convergence speed and final performance. For detailed comparison results, we recommend reading the original paper; here we only share some excerpts.\nFirst, here’s a relatively objective comparison table, including our own controlled variable training comparing Muon and Adam, as well as comparisons with models trained by others (DeepSeek) using Adam with the same architecture (for ease of comparison, Moonlight’s architecture is identical to DSV3-Small), showing Muon’s unique advantages. The following table from the paper compares between Muon (Moonlight) and Adam (Moonlight-A and DSV3-small):\n\n\n\n\nTable 4: Comparison of different models at around 1.2T tokens.\n\n\n\nBenchmark (Metric)\nDSV3-Small\nMoonlight-A@1.2T\nMoonlight@1.2T\n\n\n\n\n\nActivated Params†\n2.24B\n2.24B\n2.24B\n\n\n\nTotal Params†\n15.29B\n15.29B\n15.29B\n\n\n\nTraining Tokens\n1.33T\n1.2T\n1.2T\n\n\n\nOptimizer\nAdamW\nAdamW\nMuon\n\n\nEnglish\nMMLU\n53.3\n60.2\n60.4\n\n\nMMLU-pro\n-\n26.8\n28.1\n\n\nBBH\n41.4\n45.3\n43.2\n\n\nTriviaQA\n-\n57.4\n58.1\n\n\nCode\nHumanEval\n26.8\n29.3\n37.2\n\n\nMBPP\n36.8\n49.2\n52.9\n\n\nMath\nGSM8K\n31.4\n43.8\n45.0\n\n\nMATH\n10.7\n16.1\n19.8\n\n\nCMath\n-\n57.8\n60.2\n\n\nChinese\nC-Eval\n-\n57.2\n59.9\n\n\nCMMLU\n-\n58.2\n58.8\n\n\n\n†The reported parameter counts exclude the embedding parameters.\nWhat’s different about models trained with Muon? Since we said earlier that Muon is steepest descent under the spectral norm, and the spectral norm is the largest singular value, we thought of monitoring and analyzing singular values. Indeed, we found some interesting signals: parameters trained by Muon have a more uniform distribution of singular values. We use singular value entropy to quantitatively describe this phenomenon:\n\\[\n\\begin{equation}H(\\boldsymbol{\\sigma}) = -\\frac{1}{\\log n}\\sum_{i=1}^n \\frac{\\sigma_i^2}{\\sum_{j=1}^n\\sigma_j^2}\\log \\frac{\\sigma_i^2}{\\sum_{j=1}^n\\sigma_j^2}\\end{equation}\n\\]\nHere, \\(\\boldsymbol{\\sigma}=(\\sigma_1,\\sigma_2,\\cdots,\\sigma_n)\\) represents all the singular values of a parameter. Parameters trained by Muon have higher entropy, meaning a more uniform distribution of singular values, which indicates that these parameters are less easily compressed. This suggests that Muon more fully utilizes the potential of the parameters:\n\n\n\nWeights trained by Muon have higher singular value entropy.\n\n\nAnother interesting finding is that when we use Muon for fine-tuning (SFT), we might get suboptimal solutions if the pre-training didn’t use Muon. Specifically, if both pre-training and fine-tuning use Muon, the performance is the best. But for the other three combinations (Adam+Muon, Muon+Adam, Adam+Adam), the effectiveness doesn’t show a clear pattern.\n\n\n\n\nTable 6: Examining the impact of optimizer interchangeability between pretraining and SFT phases.\n\n\nBenchmark (Metric)\n# Shots\nMoonlight-1.2T\n\n\n\n\nPretraining Optimizer\n-\nMuon\nAdamW\nMuon\nAdamW\n\n\nSFT Optimzier\n-\nMuon\nMuon\nAdamW\nAdamW\n\n\nMMLU (EM)\n0-shot (CoT)\n55.7\n55.3\n50.2\n52.0\n\n\nHumanEval (Pass@1)\n0-shot\n57.3\n53.7\n52.4\n53.1\n\n\nMBPP (Pass@1)\n0-shot\n55.6\n55.5\n55.2\n55.2\n\n\nGSM8K (EM)\n5-shot\n68.0\n62.1\n64.9\n64.6\n\n\n\n\n\n\n\n\nTable 7: Comparison of Adam and Muon optimizers applied to the SFT of the Qwen2.5-7B pretrained model.\n\n\nBenchmark (Metric)\n# Shots\nAdam-SFT\nMuon-SFT\n\n\nPretrained Model\n-\nQwen2.5-7B\n\n\n\n\nMMLU (EM)\n0-shot (CoT)\n71.4\n70.8\n\n\nHumanEval (Pass@1)\n0-shot\n79.3\n77.4\n\n\nMBPP (Pass@1)\n0-shot\n71.9\n71.6\n\n\nGSM8K (EM)\n5-shot\n89.8\n85.8\n\n\n\nThis phenomenon suggests that some special initializations are unfavorable for Muon, and conversely, there might be initializations that are more favorable for Muon. We are still exploring the underlying principles."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#some-more-thoughts",
    "href": "docs/posts/2025-02-27-muon/index.html#some-more-thoughts",
    "title": "The Muon Anthology",
    "section": "Some more thoughts",
    "text": "Some more thoughts\nOverall, in our experiments, Muon’s performance compared to Adam is very competitive. As a new optimizer that differs significantly from Adam in form, Muon’s performance is not just “noteworthy” but indicates that it might have captured some essential characteristics.\nPreviously, there was a view in the community that Adam performs well because mainstream model architecture improvements are “overfitting” to Adam. This view likely originated from Neural Networks (Maybe) Evolved to Make Adam The Best Optimizer. It seems absurd at first glance but is actually profound. Think about it: when we try to improve a model, we train it once with Adam to see the effect, keeping it if it’s good and discarding it otherwise. But is the good effect due to its inherent superiority or because it matches Adam better?\nThis is quite thought-provoking. Certainly not all, but at least some works perform better because they align better with Adam, so over time, model architectures evolve in a direction favorable to Adam. In this context, an optimizer significantly different from Adam that can “break out” is particularly worth attention and reflection. Note that neither I nor my company are the proposers of Muon, so these remarks are purely “heartfelt words” without any self-promotion.\nWhat work remains for Muon? Quite a bit remains. For instance, we need to analyze further the aforementioned issue that “Adam pre-training + Muon fine-tuning” does not work well, especially since most currently publicly available model weights are trained with Adam. If Muon fine-tuning doesn’t work well on Adam-pretrained models, it will hurt its popularity. Of course, we can also take this opportunity to deepen our understanding of Muon (this is bug-oriented learning).\nAnother thought I had is that Muon is based on the spectral norm, which is the maximum singular value. In fact, there are many norms based on singular values, such as the Schatten norms and the Ky Fan norms. Generalizing to these more general norms and then tuning parameters could theoretically yield even better results. Additionally, after releasing Moonlight, some readers asked how to adapt µP (maximal update parametrization) to Muon, another problem for furher research."
  },
  {
    "objectID": "docs/posts/2025-02-27-muon/index.html#metadata",
    "href": "docs/posts/2025-02-27-muon/index.html#metadata",
    "title": "The Muon Anthology",
    "section": "Metadata",
    "text": "Metadata\n\nThese are several blogposts on the homepage of Jianlin Su (苏剑林) that I selected because they are relevant for understanding Muon. I organized them in a chronological sequence, and simplified certain derivations and made certain explanations easier to follow."
  },
  {
    "objectID": "docs/posts/2019-richard-sutton/index.html",
    "href": "docs/posts/2019-richard-sutton/index.html",
    "title": "The Bitter Lesson",
    "section": "",
    "text": "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore’s law, or rather its generalization of continued exponentially falling cost per unit of computation. Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to. Time spent on one is time not spent on the other. There are psychological commitments to investment in one approach or the other. And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. There were many examples of AI researchers’ belated learning of this bitter lesson, and it is instructive to review some of the most prominent.\nIn computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that ``brute force” search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not.\nA similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. Enormous initial efforts went into avoiding search by taking advantage of human knowledge, or of the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale. Also important was the use of learning by self play to learn a value function (as it was in many other games and even in chess, although learning did not play a big role in the 1997 program that first beat a world champion). Learning by self play, and learning in general, is like search in that it enables massive computation to be brought to bear. Search and learning are the two most important classes of techniques for utilizing massive amounts of computation in AI research. In computer Go, as in computer chess, researchers’ initial effort was directed towards utilizing human understanding (so that less search was needed) and only much later was much greater success had by embracing search and learning.\nIn speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. Entrants included a host of special methods that took advantage of human knowledge—knowledge of words, of phonemes, of the human vocal tract, etc. On the other side were newer methods that were more statistical in nature and did much more computation, based on hidden Markov models (HMMs). Again, the statistical methods won out over the human-knowledge-based methods. This led to a major change in all of natural language processing, gradually over decades, where statistics and computation came to dominate the field. The recent rise of deep learning in speech recognition is the most recent step in this consistent direction. Deep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets, to produce dramatically better speech recognition systems. As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked—they tried to put that knowledge in their systems—but it proved ultimately counterproductive, and a colossal waste of researcher’s time, when, through Moore’s law, massive computation became available and a means was found to put it to good use.\nIn computer vision, there has been a similar pattern. Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.\nThis is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that\n\nAI researchers have often tried to build knowledge into their agents,\nthis always helps in the short term, and is personally satisfying to the researcher, but\nin the long run it plateaus and even inhibits further progress, and\nbreakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.\n\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.\nThe second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html",
    "href": "docs/posts/1998-hans-moravec/index.html",
    "title": "When will computer hardware match the human brain?",
    "section": "",
    "text": "This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#abstract",
    "href": "docs/posts/1998-hans-moravec/index.html#abstract",
    "title": "When will computer hardware match the human brain?",
    "section": "",
    "text": "This paper describes how the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware. The processing power and memory capacity necessary to match general intellectual performance of the human brain are estimated. Based on extrapolation of past trends and on examination of technologies under development, it is predicted that the required hardware will be available in cheap machines in the 2020s."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#brains-eyes-and-machines",
    "href": "docs/posts/1998-hans-moravec/index.html#brains-eyes-and-machines",
    "title": "When will computer hardware match the human brain?",
    "section": "Brains, Eyes and Machines",
    "text": "Brains, Eyes and Machines\nComputers have far to go to match human strengths, and our estimates will depend on analogy and extrapolation. Fortunately, these are grounded in the first bit of the journey, now behind us. Thirty years of computer vision reveals that 1 MIPS can extract simple features from real-time imagery–tracking a white line or a white spot on a mottled background. 10 MIPS can follow complex gray-scale patches–as smart bombs, cruise missiles and early self-driving vans attest. 100 MIPS can follow moderately unpredictable features like roads–as recent long NAVLAB trips demonstrate. 1,000 MIPS will be adequate for coarse-grained three-dimensional spatial awareness–illustrated by several mid-resolution stereoscopic vision programs, including my own. 10,000 MIPS can find three-dimensional objects in clutter–suggested by several “bin-picking” and high-resolution stereo-vision demonstrations, which accomplish the task in an hour or so at 10 MIPS. The data fades there–research careers are too short, and computer memories too small, for significantly more elaborate experiments.\nThere are considerations other than sheer scale. At 1 MIPS the best results come from finely hand-crafted programs that distill sensor data with utmost efficiency. 100-MIPS processes weigh their inputs against a wide range of hypotheses, with many parameters, that learning programs adjust better than the overburdened programmers. Learning of all sorts will be increasingly important as computer power and robot programs grow. This effect is evident in related areas. At the close of the 1980s, as widely available computers reached 10 MIPS, good optical character reading (OCR) programs, able to read most printed and typewritten text, began to appear. They used hand-constructed “feature detectors” for parts of letter shapes, with very little learning. As computer power passed 100 MIPS, trainable OCR programs appeared that could learn unusual typestyles from examples, and the latest and best programs learn their entire data sets. Handwriting recognizers, used by the Post Office to sort mail, and in computers, notably Apple’s Newton, have followed a similar path. Speech recognition also fits the model. Under the direction of Raj Reddy, who began his research at Stanford in the 1960s, Carnegie Mellon has led in computer transcription of continuous spoken speech. In 1992 Reddy’s group demonstrated a program called Sphinx II on a 15-MIPS workstation with 100 MIPS of specialized signal-processing circuitry. Sphinx II was able to deal with arbitrary English speakers using a several-thousand-word vocabulary. The system’s word detectors, encoded in statistical structures known as Markov tables, were shaped by an automatic learning process that digested hundreds of hours of spoken examples from thousands of Carnegie Mellon volunteers enticed by rewards of pizza and ice cream. Several practical voice-control and dictation systems are sold for personal computers today, and some heavy users are substituting larynx for wrist damage.\nMore computer power is needed to reach human performance, but how much? Human and animal brain sizes imply an answer, if we can relate nerve volume to computation. Structurally and functionally, one of the best understood neural assemblies is the retina of the vertebrate eye. Happily, similar operations have been developed for robot vision, handing us a rough conversion factor.\nThe retina is a transparent, paper-thin layer of nerve tissue at the back of the eyeball on which the eye’s lens projects an image of the world. It is connected by the optic nerve, a million-fiber cable, to regions deep in the brain. It is a part of the brain convenient for study, even in living animals because of its peripheral location and because its function is straightforward compared with the brain’s other mysteries. A human retina is less than a centimeter square and a half-millimeter thick. It has about 100 million neurons, of five distinct kinds. Light-sensitive cells feed wide spanning horizontal cells and narrower bipolar cells, which are interconnected by whose outgoing fibers bundle to form the optic nerve. Each of the million ganglion-cell axons carries signals from a amacrine cells, and finally ganglion cells, particular patch of image, indicating light intensity differences over space or time: a million edge and motion detections. Overall, the retina seems to process about ten one-million-point images per second.\nIt takes robot vision programs about 100 computer instructions to derive single edge or motion detections from comparable video images. 100 million instructions are needed to do a million detections, and 1,000 MIPS to repeat them ten times per second to match the retina.\nThe 1,500 cubic centimeter human brain is about 100,000 times as large as the retina, suggesting that matching overall human behavior will take about 100 million MIPS of computer power. Computer chess bolsters this yardstick. Deep Blue, the chess machine that bested world chess champion Garry Kasparov in 1997, used specialized chips to process chess moves at a the speed equivalent to a 3 million MIPS universal computer (see Figure 3-4). This is 1/30 of the estimate for total human performance. Since it is plausible that Kasparov, probably the best human player ever, can apply his brainpower to the strange problems of chess with an efficiency of 1/30, Deep Blue’s near parity with Kasparov’s chess skill supports the retina-based extrapolation.\nThe most powerful experimental supercomputers in 1998, composed of thousands or tens of thousands of the fastest microprocessors and costing tens of millions of dollars, can do a few million MIPS. They are within striking distance of being powerful enough to match human brainpower, but are unlikely to be applied to that end. Why tie up a rare twenty-million-dollar asset to develop one ersatz-human, when millions of inexpensive original-model humans are available? Such machines are needed for high-value scientific calculations, mostly physical simulations, having no cheaper substitutes. AI research must wait for the power to become more affordable.\nIf 100 million MIPS could do the job of the human brain’s 100 billion neurons, then one neuron is worth about 1/1,000 MIPS, i.e., 1,000 instructions per second. That’s probably not enough to simulate an actual neuron, which can produce 1,000 finely timed pulses per second. Our estimate is for very efficient programs that imitate the aggregate function of thousand-neuron assemblies. Almost all nervous systems contain subassemblies that big.\nThe small nervous systems of insects and other invertebrates seem to be hardwired from birth, each neuron having its own special predetermined links and function. The few-hundred-million-bit insect genome is enough to specify connections of each of their hundred thousand neurons. Humans, on the other hand, have 100 billion neurons, but only a few billion bits of genome. The human brain seems to consist largely of regular structures whose neurons are trimmed away as skills are learned, like featureless marble blocks chiseled into individual sculptures. Analogously, robot programs were precisely hand-coded when they occupied only a few hundred thousand bytes of memory. Now that they’ve grown to tens of millions of bytes, most of their content is learned from example. But there is a big practical difference between animal and robot learning. Animals learn individually, but robot learning can be copied from one machine to another. For instance, today’s text and speech understanding programs were painstakingly trained over months or years, but each customer’s copy of the software is “born” fully educated. Decoupling training from use will allow robots to do more with less. Big computers at the factory–maybe supercomputers with 1,000 times the power of machines that can reasonably be placed in a robot–will process large training sets under careful human supervision, and distill the results into efficient programs and arrays of settings that are then copied into myriads of individual robots with more modest processors.\nPrograms need memory as well as processing speed to do their work. The ratio of memory to speed has remained constant during computing history. The earliest electronic computers had a few thousand bytes of memory and could do a few thousand calculations per second. Medium computers of 1980 had a million bytes of memory and did a million calculations per second. Supercomputers in 1990 did a billion calculations per second and had a billion bytes of memory. The latest, greatest supercomputers can do a trillion calculations per second and can have a trillion bytes of memory. Dividing memory by speed defines a “time constant,” roughly how long it takes the computer to run once through its memory. One megabyte per MIPS gives one second, a nice human interval. Machines with less memory for their speed, typically new models, seem fast, but unnecessarily limited to small programs. Models with more memory for their speed, often ones reaching the end of their run, can handle larger programs, but unpleasantly slowly. For instance, the original Macintosh was introduced in 1984 with 1/2 MIPS and 1/8 megabyte, and was then considered a very fast machine. The equally fast “fat Mac” with 1/2 megabyte ran larger programs at tolerable speed, but the 1 megabyte “Mac plus” verged on slow. The four megabyte “Mac classic,” the last 1/2 MIPS machine in the line, was intolerably slow, and was soon supplanted by ten-times-faster processors in the same enclosure. Customers maintain the ratio by asking “would the next dollar be better spent on more speed or more memory?”\nThe best evidence about nervous system memory puts most of it in the synapses connecting the neurons. Molecular adjustments allow synapses to be in a number of distinguishable states, lets say one byte’s worth. Then the 100-trillion-synapse brain would hold the equivalent 100 million megabytes. This agrees with our earlier estimate that it would take 100 million MIPS to mimic the brain’s function. The megabyte/MIPS ratio seems to hold for nervous systems too! The contingency is the other way around: computers are configured to interact at human time scales, and robots interacting with humans seem also to be best at that ratio. On the other hand, faster machines, for instance audio and video processors and controllers of high-performance aircraft, have many MIPS for each megabyte. Very slow machines, for instance time-lapse security cameras and automatic data libraries, store many megabytes for each of their MIPS. Flying insects seem to be a few times faster than humans, so may have more MIPS than megabytes. As in animals, cells in plants signal one other electrochemically and enzymatically. Some plant cells seem specialized for communication, though apparently not as extremely as animal neurons. One day we may find that plants remember much, but process it slowly (how does a redwood tree manage to rebuff rapidly evolving pests during a 2,000 year lifespan, when it took mosquitoes only a few decades to overcome DDT?).\nWith our conversions, a 100-MIPS robot, for instance Navlab, has mental power similar to a 100,000-neuron housefly. The following figure rates various entities.\n\n\n\nMIPS and Megabytes to mimic their behavior. Note the scale. Entities rated by the computational power and memory of the smallest universal computer needed is logarithmic on both axes: each vertical division represents a thousandfold increase in processing power, and each horizontal division a thousandfold increase in memory size. Universal computers can imitate other entities at their location in the diagram, but the more specialized entities cannot. A 100-million-MIPS computer may be programmed not only to think like a human, but also to imitate other similarly-sized computers. But humans cannot imitate 100-million-MIPS computers–our general-purpose calculation ability is under a millionth of a MIPS. Deep Blue’s special-purpose chess chips process moves like a 3-million-MIPS computer, but its general-purpose power is only a thousand MIPS. Most of the non-computer entities in the diagram can’t function in a general-purpose way at all. Universality is an almost magical property, but it has costs. A universal machine may use ten or more times the resources of one specialized for a task. But if the task should change, as it usually does in research, the universal machine can be reprogrammed, while the specialized machine must be replaced."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#extrapolation",
    "href": "docs/posts/1998-hans-moravec/index.html#extrapolation",
    "title": "When will computer hardware match the human brain?",
    "section": "Extrapolation",
    "text": "Extrapolation\nBy our estimate, today’s very biggest supercomputers are within a factor of a hundred of having the power to mimic a human mind. Their successors a decade hence will be more than powerful enough. Yet, it is unlikely that machines costing tens of millions of dollars will be wasted doing what any human can do, when they could instead be solving urgent physical and mathematical problems nothing else can touch. Machines with human-like performance will make economic sense only when they cost less than humans, say when their “brains” cost about $1,000. When will that day arrive?\nThe expense of computation has fallen rapidly and persistently for a century. Steady improvements in mechanical and electromechanical calculators before World War II had increased the speed of calculation a thousandfold over hand calculation. The pace quickened with the appearance of electronic computers during the war–from 1940 to 1980 the amount of computation available at a given cost increased a millionfold. Vacuum tubes were replaced by transistors, and transistors by integrated circuits, whose components became ever smaller and more numerous. During the 1980s microcomputers reached the consumer market, and the industry became more diverse and competitive. Powerful, inexpensive computer workstations replaced the drafting boards of circuit and computer designers, and an increasing number of design steps were automated. The time to bring a new generation of computer to market shrank from two years at the beginning of the 1980s to less than nine months. The computer and communication industries grew into the largest on earth.\nComputers doubled in capacity every two years after the war, a pace that became an industry given: companies that wished to grow sought to exceed it, companies that failed to keep up lost business. In the 1980s the doubling time contracted to 18 months, and computer performance in the late 1990s seems to be doubling every 12 months.\n\n\n\nFaster than Exponential Growth in Computing Power. The number of MIPS in $1000 of computer from 1900 to the present. Steady improvements in mechanical and electromechanical calculators before World War II had increased the speed of calculation a thousandfold over manual methods from 1900 to 1940. The pace quickened with the appearance of electronic computers during the war, and 1940 to 1980 saw a millionfold increase. The pace has been even quicker since then, a pace which would make humanlike robots possible before the middle of the next century. The vertical scale is logarithmic, the major divisions represent thousandfold increases in computer performance. Exponential growth would show as a straight line, the upward curve indicates faster than exponential growth, or, equivalently, an accelerating rate of innovation. The reduced spread of the data in the 1990s is probably the result of intensified competition: underperforming machines are more rapidly squeezed out. The numerical data for this power curve are presented in the appendix.\n\n\nAt the present rate, computers suitable for humanlike robots will appear in the 2020s. Can the pace be sustained for another three decades? The graph shows no sign of abatement. If anything, it hints that further contractions in time scale are in store. But, one often encounters thoughtful articles by knowledgeable people in the semiconductor industry giving detailed reasons why the decades of phenomenal growth must soon come to an end.\nThe keynote for advancing computation is miniaturization: smaller components have less inertia and operate more quickly with less energy, and more of them can be packed in a given space. First the moving parts shrunk, from the gears in mechanical calculators, to small contacts in electromechanical machines, to bunches of electrons in electronic computers. Next, the switches’ supporting structure underwent a vanishing act, from thumb-sized vacuum tubes, to fly-sized transistors, to ever-diminishing flyspecks on integrated circuit chips. Similar to printed circuits before them, integrated circuits were made by a photographic process. The desired pattern was projected onto a silicon chip, and subtle chemistry used to add or remove the right sorts of matter in the exposed areas.\nIn the mid-1970s, integrated circuits, age 15, hit a crisis of adolescence. They then held ten thousand components, just enough for an entire computer, and their finest details were approaching 3 micrometers in size. Experienced engineers wrote many articles warning that the end was near. Three micrometers was barely larger than the wavelength of the light used to sculpt the chip. The number of impurity atoms defining the tiny components had grown so small that statistical scatter would soon render most components out of spec, a problem aggravated by a similar effect in the diminishing number of signaling electrons. Increasing electrical gradients across diminishing gaps caused atoms to creep through the crystal, degrading the circuit. Interactions between ever-closer wires were about to ruin the signals. Chips would soon generate too much heat to remove, and require too many external connections to fit. The smaller memory cells were suffering radiation-induced forgetfulness.\nA look at the computer growth graph shows that the problems were overcome, with a vengeance. Chip progress not only continued, it sped up. Shorter-wavelength light was substituted, a more precise way of implanting impurities was devised, voltages were reduced, better insulators, shielding designs, more efficient transistor designs, better heat sinks, denser pin patterns and non-radioactive packaging materials were found. Where there is sufficient financial incentive, there is a way. In fact, solutions had been waiting in research labs for years, barely noticed by the engineers in the field, who were perfecting established processes, and worrying in print as those ran out of steam. As the need became acute, enormous resources were redirected to draft laboratory possibilities into production realities.\nIn the intervening years many problems were met and solved, and innovations introduced, but now, nearing a mid-life 40, the anxieties seem again to have crested. In 1996 major articles appeared in scientific magazines and major national newspapers worrying that electronics progress might be a decade from ending. The cost of building new integrated circuit plants was approaching a prohibitive billion dollars. Feature sizes were reaching 0.1 micrometers, the wavelength of the sculpting ultraviolet light. Their transistors, scaled down steadily from 1970s designs, would soon be so small that electrons would quantum “tunnel” out of them. Wiring was becoming so dense it would crowd out the components, and slow down and leak signals. Heat was increasing.\nThe articles didn’t mention that less expensive plants could make the same integrated circuits, if less cheaply and in smaller quantities. Scale was necessary because the industry had grown so large and competitive. Rather than signaling impending doom, it indicated free-market success, a battle of titans driving down costs to the users. They also failed to mention new contenders, waiting on lab benches to step in should the leader fall.\nThe wave-like nature of matter at very small scales is a problem for conventional transistors, which depend on the smooth flow of masses of electrons. But, it is a property exploited by a radical new class of components known as single-electron transistors and quantum dots, which work by the interference of electron waves. These new devices work better as they grow smaller. At the scale of today’s circuits, the interference patterns are so fine that it takes only a little heat energy to bump electrons from crest to crest, scrambling their operation. Thus, these circuits have been demonstrated mostly at a few degrees above absolute zero. But, as the devices are reduced, the interference patterns widen, and it takes ever larger energy to disrupt them. Scaled to about 0.01 micrometers, quantum interference switching works at room temperature. It promises more than a thousand times higher density than today’s circuits, possibly a thousand times the speed, and much lower power consumption, since it moves a few electrons across small quantum bumps, rather than pushing them in large masses through resistive material. In place of much wiring, quantum interference logic may use chains of switching devices. It could be manufactured by advanced descendants of today’s chip fabrication machinery (Goldhaber-Gordon et al. 1997). Proposals abound in the research literature, and the industry has the resources to perfect the circuits and their manufacture, when the time comes.\nWilder possibilities are brewing. Switches and memory cells made of single molecules have been demonstrated, which might enable a volume to hold a billion times more circuitry than today. Potentially blowing everything else away are “quantum computers,” in which a whole computer, not just individual signals, acts in a wavelike manner. Like a conventional computer, a quantum computer consists of a number of memory cells whose contents are modified in a sequence of logical transformations. Unlike a conventional computer, whose memory cells are either 1 or 0, each cell in a quantum computer is started in a quantum superposition of both 1 and 0. The whole machine is a superposition of all possible combinations of memory states. As the computation proceeds, each component of the superposition individually undergoes the logic operations. It is as if an exponential number of computers, each starting with a different pattern in memory, were working on the problem simultaneously. When the computation is finished, the memory cells are examined, and an answer emerges from the wavelike interference of all the possibilities. The trick is to devise the computation so that the desired answers reinforce, while the others cancel. In the last several years, quantum algorithms have been devised that factor numbers and search for encryption keys much faster than any classical computer. Toy quantum computers, with three or four “qubits” stored as states of single atoms or photons, have been demonstrated, but they can do only short computations before their delicate superpositions are scrambled by outside interactions. More promising are computers using nuclear magnetic resonance, as in hospital scanners. There, quantum bits are encoded as the spins of atomic nuclei, and gently nudged by external magnetic and radio fields into magnetic interactions with neighboring nuclei. The heavy nuclei, swaddled in diffuse orbiting electron clouds, can maintain their quantum coherence for hours or longer. A quantum computer with a thousand or more qubits could tackle problems astronomically beyond the reach of any conceivable classical computer.\nMolecular and quantum computers will be important sooner or later, but humanlike robots are likely to arrive without their help. Research within semiconductor companies, including working prototype chips, makes it quite clear that existing techniques can be nursed along for another decade, to chip features below 0.1 micrometers, memory chips with tens of billions of bits and multiprocessor chips with over 100,000 MIPS. Towards the end of that period, the circuitry will probably incorporate a growing number of quantum interference components. As production techniques for those tiny components are perfected, they will begin to take over the chips, and the pace of computer progress may steepen further. The 100 million MIPS to match human brain power will then arrive in home computers before 2030."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#false-start",
    "href": "docs/posts/1998-hans-moravec/index.html#false-start",
    "title": "When will computer hardware match the human brain?",
    "section": "False Start",
    "text": "False Start\nIt may seem rash to expect fully intelligent machines in a few decades, when the computers have barely matched insect mentality in a half-century of development. Indeed, for that reason, many long-time artificial intelligence researchers scoff at the suggestion, and offer a few centuries as a more believable period. But there are very good reasons why things will go much faster in the next fifty years than they have in the last fifty.\nThe stupendous growth and competitiveness of the computer industry is one reason. A less appreciated one is that intelligent machine research did not make steady progress in its first fifty years, it marked time for thirty of them! Though general computer power grew a hundred thousand fold from 1960 to 1990, the computer power available to AI programs barely budged from 1 MIPS during those three decades.\nIn the 1950s, the pioneers of AI viewed computers as locomotives of thought, which might outperform humans in higher mental work as prodigiously as they outperformed them in arithmetic, if they were harnessed to the right programs. Success in the endeavor would bring enormous benefits to national defense, commerce and government. The promise warranted significant public and private investment. For instance, there was a large project to develop machines to automatically translate scientific and other literature from Russian to English. There were only a few AI centers, but those had the largest computers of the day, comparable in cost to today’s supercomputers. A common one was the IBM 704, which provided a good fraction of a MIPS.\nBy 1960 the unspectacular performance of the first reasoning and translation programs had taken the bloom off the rose, but the unexpected launching by the Soviet Union of Sputnik, the first satellite in 1957, had substituted a paranoia. Artificial Intelligence may not have delivered on its first promise, but what if it were to suddenly succeed after all? To avoid another nasty technological surprise from the enemy, it behooved the US to support the work, moderately, just in case. Moderation paid for medium scale machines costing a few million dollars, no longer supercomputers. In the 1960s that price provided a good fraction of a MIPS in thrifty machines like Digital Equipment Corp’s innovative PDP-1 and PDP-6.\nThe field looked even less promising by 1970, and support for military-related research declined sharply with the end of the Vietnam war. Artificial Intelligence research was forced to tighten its belt and beg for unaccustomed small grants and contracts from science agencies and industry. The major research centers survived, but became a little shabby as they made do with aging equipment. For almost the entire decade AI research was done with PDP-10 computers, that provided just under 1 MIPS. Because it had contributed to the design, the Stanford AI Lab received a 1.5 MIPS KL-10 in the late 1970s from Digital, as a gift.\nFunding improved somewhat in the early 1980s, but the number of research groups had grown, and the amount available for computers was modest. Many groups purchased Digital’s new Vax computers, costing $100,000 and providing 1 MIPS. By mid-decade, personal computer workstations had appeared. Individual researchers reveled in the luxury of having their own computers, avoiding the delays of time-shared machines. A typical workstation was a Sun-3, costing about $10,000, and providing about 1 MIPS.\nBy 1990, entire careers had passed in the frozen winter of 1-MIPS computers, mainly from necessity, but partly from habit and a lingering opinion that the early machines really should have been powerful enough. In 1990, 1 MIPS cost $1,000 in a low-end personal computer. There was no need to go any lower. Finally spring thaw has come. Since 1990, the power available to individual AI and robotics programs has doubled yearly, to 30 MIPS by 1994 and 500 MIPS by 1998. Seeds long ago alleged barren are suddenly sprouting. Machines read text, recognize speech, even translate languages. Robots drive cross-country, crawl across Mars, and trundle down office corridors. In 1996 a theorem-proving program called EQP running five weeks on a 50 MIPS computer at Argonne National Laboratory found a proof of a boolean algebra conjecture by Herbert Robbins that had eluded mathematicians for sixty years. And it is still only spring. Wait until summer.\n\n\n\nThe big freeze. From 1960 to 1990 the cost of computers used in AI research declined, as their numbers dilution absorbed computer-efficiency gains during the period, and the power available to individual AI programs remained almost unchanged at 1 MIPS, barely insect power. AI computer cost bottomed in 1990, and since then power has doubled yearly, to several hundred MIPS by 1998. The major visible exception is computer chess (shown by a progression of knights), whose prestige lured the resources of major computer companies and the talents of programmers and machine designers. Exceptions also exist in less public competitions, like petroleum exploration and intelligence gathering, whose high return on investment gave them regular access to the largest computers."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#the-games-afoot",
    "href": "docs/posts/1998-hans-moravec/index.html#the-games-afoot",
    "title": "When will computer hardware match the human brain?",
    "section": "The Game’s Afoot",
    "text": "The Game’s Afoot\nA summerlike air already pervades the few applications of artificial intelligence that retained access to the largest computers. Some of these, like pattern analysis for satellite images and other kinds of spying, and in seismic oil exploration, are closely held secrets. Another, though, basks in the limelight. The best chess-playing computers are so interesting they generate millions of dollars of free advertising for the winners, and consequently have enticed a series of computer companies to donate time on their best machines and other resources to the cause. Since 1960 IBM, Control Data, AT&T, Cray, Intel and now again IBM have been sponsors of computer chess. The “knights” in the AI power graph show the effect of this largesse, relative to mainstream AI research. The top chess programs have competed in tournaments powered by supercomputers, or specialized machines whose chess power is comparable. In 1958 IBM had both the first checker program, by Arthur Samuel, and the first full chess program, by Alex Bernstein. They ran on an IBM 704, the biggest and last vacuum-tube computer. The Bernstein program played atrociously, but Samuel’s program, which automatically learned its board scoring parameters, was able to beat Connecticut checkers champion Robert Nealey. Since 1994, Chinook, a program written by Jonathan Schaeffer of the University of Alberta, has consistently bested the world’s human checker champion. But checkers isn’t very glamorous, and this portent received little notice.\nBy contrast, it was nearly impossible to overlook the epic battles between world chess champion Garry Kasparov and IBM’s Deep Blue in 1996 and 1997. Deep Blue is a scaled-up version of a machine called Deep Thought, built by Carnegie Mellon University students ten years earlier. Deep Thought, in turn, depended on special-purpose chips, each wired like the Belle chess computer built by Ken Thompson at AT&T Bell Labs in the 1970s. Belle, organized like a chessboard, circuitry on the squares, wires running like chess moves, could evaluate and find all legal moves from a position in one electronic flash. In 1997 Deep Blue had 256 such chips, orchestrated by a 32 processor mini-supercomputer. It examined 200 million chess positions a second. Chess programs, on unaided general-purpose computers, average about 16,000 instructions per position examined. Deep Blue, when playing chess (and only then), was thus worth about 3 million MIPS, 1/30 of our estimate for human intelligence.\nDeep Blue, in a first for machinekind, won the first game of the 1996 match. But, Kasparov quickly found the machine’s weaknesses, and drew two and won three of the remaining games.\nIn May 1997 he met an improved version of the machine. That February, Kasparov had triumphed over a field of grandmasters in a prestigious tournament in Linares, Spain, reinforcing his reputation as the best player ever, and boosting his chess rating past 2800, uncharted territory. He prepared for the computer match in the intervening months, in part by playing against other machines. Kasparov won a long first game against Deep Blue, but lost next day to masterly moves by the machine. Then came three grueling draws, and a final game, in which a visibly shaken and angry Kasparov resigned early, with a weak position. It was the first competition match he had ever lost.\nThe event was notable for many reasons, but one especially is of interest here. Several times during both matches, Kasparov reported signs of mind in the machine. At times in the second tournament, he worried there might be humans behind the scenes, feeding Deep Blue strategic insights!\nBobby Fischer, the US chess great of the 1970s, is reputed to have played each game as if against God, simply making the best moves. Kasparov, on the other hand, claims to see into opponents’ minds during play, intuiting and exploiting their plans, insights and oversights. In all other chess computers, he reports a mechanical predictability stemming from their undiscriminating but limited lookahead, and absence of long-term strategy. In Deep Blue, to his consternation, he saw instead an “alien intelligence.”\nIn this paper-thin slice of mentality, a computer seems to have not only outperformed the best human, but to have transcended its machinehood. Who better to judge than Garry Kasparov? Mathematicians who examined EQP’s proof of the Robbins conjecture, mentioned earlier, report a similar impression of creativity and intelligence. In both cases, the evidence for an intelligent mind lies in the machine’s performance, not its makeup.\nNow, the team that built Deep Blue claim no “intelligence” in it, only a large database of opening and end games, scoring and deepening functions tuned with consulting grandmasters, and, especially, raw speed that allows the machine to look ahead an average of fourteen half-moves per turn. Unlike some earlier, less successful, chess programs, Deep Blue was not designed to think like a human, to form abstract strategies or see patterns as it races through the move/countermove tree as fast as possible.\nDeep Blue’s creators know its quantitative superiority over other chess machines intimately, but lack the chess understanding to share Kasparov’s deep appreciation of the difference in the quality of its play. I think this dichotomy will show up increasingly in coming years. Engineers who know the mechanism of advanced robots most intimately will be the last to admit they have real minds. From the inside, robots will indisputably be machines, acting according to mechanical principles, however elaborately layered. Only on the outside, where they can be appreciated as a whole, will the impression of intelligence emerge. A human brain, too, does not exhibit the intelligence under a neurobiologist’s microscope that it does participating in a lively conversation.\n\n\n\nAgony to ecstasy. In forty years, computer chess progressed from the lowest depth to the highest peak of human chess performance. It took a handful of good ideas, culled by trial and error from a larger number of possibilities, an accumulation of previously evaluated game openings and endings, good adjustment of position scores, and especially a ten-million-fold increase in the number of alternative move sequences the machines can explore. Note that chess machines reached world champion performance as their (specialized) processing power reached about 1/30 human, by our brain to computer measure. Since it is plausible that Garry Kasparov (but hardly anyone else) can apply his brainpower to the problems of chess with an efficiency of 1/30, the result supports that retina-based extrapolation. In coming decades, as general-purpose computer power grows beyond Deep Blue’s specialized strength, machines will begin to match humans in more common skills."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#the-great-flood",
    "href": "docs/posts/1998-hans-moravec/index.html#the-great-flood",
    "title": "When will computer hardware match the human brain?",
    "section": "The Great Flood",
    "text": "The Great Flood\nComputers are universal machines, their potential extends uniformly over a boundless expanse of tasks. Human potentials, on the other hand, are strong in areas long important for survival, but weak in things far removed. Imagine a “landscape of human competence,” having lowlands with labels like “arithmetic” and “rote memorization”, foothills like “theorem proving” and “chess playing,” and high mountain peaks labeled “locomotion,” “hand-eye coordination” and “social interaction.” We all live in the solid mountaintops, but it takes great effort to reach the rest of the terrain, and only a few of us work each patch.\nAdvancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose (Moravec 1999) that we build Arks as that day nears, and adopt a seafaring life! For now, though, we must rely on our representatives in the lowlands to tell us what water is really like.\nOur representatives on the foothills of chess and theorem-proving report signs of intelligence. Why didn’t we get similar reports decades before, from the lowlands, as computers surpassed humans in arithmetic and rote memorization? Actually, we did, at the time. Computers that calculated like thousands of mathematicians were hailed as “giant brains,” and inspired the first generation of AI research. After all, the machines were doing something beyond any animal, that needed human intelligence, concentration and years of training. But it is hard to recapture that magic now. One reason is that computers’ demonstrated stupidity in other areas biases our judgment. Another relates to our own ineptitude. We do arithmetic or keep records so painstakingly and externally, that the small mechanical steps in a long calculation are obvious, while the big picture often escapes us. Like Deep Blue’s builders, we see the process too much from the inside to appreciate the subtlety that it may have on the outside. But there is a non-obviousness in snowstorms or tornadoes that emerge from the repetitive arithmetic of weather simulations, or in rippling tyrannosaur skin from movie animation calculations. We rarely call it intelligence, but “artificial reality” may be an even more profound concept than artificial intelligence (Moravec 1999).\nThe mental steps underlying good human chess playing and theorem proving are complex and hidden, putting a mechanical interpretation out of reach. Those who can follow the play naturally describe it instead in mentalistic language, using terms like strategy, understanding and creativity. When a machine manages to be simultaneously meaningful and surprising in the same rich way, it too compels a mentalistic interpretation. Of course, somewhere behind the scenes, there are programmers who, in principle, have a mechanical interpretation. But even for them, that interpretation loses its grip as the working program fills its memory with details too voluminous for them to grasp.\nAs the rising flood reaches more populated heights, machines will begin to do well in areas a greater number can appreciate. The visceral sense of a thinking presence in machinery will become increasingly widespread. When the highest peaks are covered, there will be machines than can interact as intelligently as any human on any subject. The presence of minds in machines will then become self-evident."
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#peer-comments",
    "href": "docs/posts/1998-hans-moravec/index.html#peer-comments",
    "title": "When will computer hardware match the human brain?",
    "section": "Peer comments",
    "text": "Peer comments\n\nRobin Hanson: 18/3/98\nMoravec’s article offers a provocative and hopeful hypothesis, and some evidence and reasoning to support it. The article has made me seriously consider becoming much more hopeful about AI timescales. The major flaw in the article, however, is that it does not attempt to be scholarly in the sense of anticipating and responding to possible objections. The article seems more like a chapter of a book aimed at a popular audience.\nI’m sure Moravec could think of the following objections, but I’ll mention them because he didn’t.\n\nMoravec argues that AI marked time for 30 years because in 1960 AI pioneers had 1 MIPS supercomputers, and in 1990 typical AI workstations did 1 MIPS. Is the argument that progress is driven by the MIPS of the median researcher’s computer? If so, the implication would be that we could increase progress greatly by giving supercomputers to a few researchers and firing all the rest. Would Moravec endorse this suggestion?\n\nAlternatively, is the argument that progress is driven by the maximum MIPS available to any AI researcher? If so, then Moravec needs to give evidence about what this max was between 1960 and 1990. I thought connection machines were used by AI researchers before 1990, for example. It is only the exceptional fields he names that had access to &gt; 1 MIPS?\n\nThe fields he mentions where progress has tracked the speed of machines used, chess, image analysis, voice recognition, and handwriting recognition, are all fields which many AI researchers long avoided exactly because they perceived them to be strongly CPU-limited. Those researchers instead choose fields they perceived to be knowledge-limited, limited by how much their programs knew. And such researchers explain slow progress in their chosen fields via large estimates of the total knowledge which needs to be encoded.\n\nSo what is the argument that these field are actually CPU-limited, contrary to their researcher’s impressions? After all, if these fields are knowledge limited, then there is no particular reason to expect AI abilities in these fields to track available CPU.\nThese are the sorts of issues I would think would be addressed in a more scholarly version of this paper.\nRobin Hanson\nhanson@econ.berkeley.edu http://hanson.berkeley.edu/\nRWJF Health Policy Scholar, Sch. of Public Health 510-643-1884 140\nWarren Hall, UC Berkeley, CA 94720-7360 FAX: 510-643-8614\n\n\nMoravec replies: 18/3/98\nWell, yes, it IS a popular chapter! That’s pretty much my style, even in technical papers. I’m better at making up ideas than fighting for them, and prefer to leave the battle to any others who more enjoy that sort of thing. Leaves me free to cause more mischief elsewhere!\n\nAI didn’t have greater computer power for a couple of reasons.\n\nA minor one was that McCarthy and others didn’t believe it was necessary, an attitude conveyed to generations of students, especially on the abstract reasoning side, and still held by many.\nA major reason was that AI never had enough money to afford a supercomputer. Even pooling the few millions spent on it over decades wouldn’t have bought a serious supercomputer, let alone supported its upkeep. A lot of effort was wasted over the decades in robotics programs trying to build cheap special-purpose supercomputers for vision and the like. It always took five years or so before the hardware and compilers were working well enough to make them usable for actual vision, and by then the power could be had cheaper in general purpose computers. The Connection Machine was an especially big one of those efforts. Several 4,096 processor CM-2 machines were given to a handful of AI places, like SRI. The CM-2 was an array of tiny processors linked in a grid. It was very good for cellular automata and finite element calculations, but the slow communication made it a pain for less straightforwardly gridlike things. I tried to fit my “sensor evidence rays into spatial grid map” robot program onto a 4,096 processor CM-2 during a 1992 sabbatical at Thinking machines, in a half-dozen different ways, but because there were two separate grids that had to be brought into different registrations repeatedly, the communications delays prevented me from ever getting more than about 40 MIPS effectively. At that time the computer on my desk was a 20 MIPS Sparc-2, so the advantage of using a limited, expensive, occasionally available machine with idiosyncratic programming, was pretty limited. Far better, cheaper and more convenient to simply use two Sparc-2’s. Other users had the same experience, and the CM-2s in AI labs got very little use. The later CM-5 machine, a bunch of Sparcs interconnected by a more flexible tree network, would have been more useful, but at a few million $ for the smallest, they were too expensive for use by any AI project that I know of. Anyway, it was cheaper to use the workstations already on your network. These earned their keep by being available for individual users, but could be used in parallel occasionally. I myself have run learning programs on a few dozen machines at a time, for weeks over some holiday periods. So have many others. But it’s impractical to use this approach routinely to control a robot: the users start to complain about slowdowns in their interaction. Robin suggests that pooling resources could have increased productivity greatly. But if we had confiscated the equipment of 99% of the AI sites, and given tem to the remaining 1%, we would have increased individual computer power 100 fold, about a seven year advantage. But the political fallout would probably have reduced funding by 90%.\nSo, yes, only a few exceptional areas had supercomputer power available. Remember, there were only a handful of supercomputers available, and almost most of them were at the national labs designing nuclear weapons or at the NSA cracking codes. Even the national weather service was relegated to lower cost machines. The CDC and Cray machines used in chess were just being tested before being shipped to the weapons labs.\n\nI think Newell & Simon, McCarthy and followers made a giant mistake when they thought they could achieve full intelligence by just skimming off the conscious surface of human thought. Most of our intuitive smarts is unconscious, and requires Teraops as well as Terabytes of accumulated knowledge. In another chapter of the book I propose a several stage bottom up evolution of robots, paralleling the evolution of our own brain, to create the necessary foundation.\n\n\n\nRobin Hanson follows up: 23/3/98\nI asked:\n\nIs the argument that progress is driven by the MIPS of the median researcher’s computer? If so, the implication would be that we could increase progress greatly by giving supercomputers to a few researchers and firing all the rest. Would Moravec endorse this suggestion? Alternatively, is the argument that progress is driven by the maximum MIPS available to any AI researcher?\n\nMoravec responded:\n\nRobin suggests that pooling resources could have increased productivity greatly. But if we had confiscated the equipment of 99% of the AI sites, and given them to the remaining 1%, we would have increased individual computer power 100 fold, about a seven year advantage. But the political fallout would probably have reduced funding by 90%.\n\nBut a 90% funding cut, with the remaining funding given to 1% of researchers, would still have increased progress according to your logic. And this logic would apply just as well to today. So may we assume you endorse such a funding proposal?\nI also asked:\n\nThose researchers instead choose fields they perceived to be knowledge-limited, … So what is the argument that these field are actually CPU-limited, contrary to their researcher’s impressions? After all, if these fields are knowledge limited, then there is no particular reason to expect AI abilities in these fields to track available CPU.\n\nMoravec replied:\n\n\nI think Newell & Simon, McCarthy and followers made a giant mistake when they thought they could achieve full intelligence by just skimming off the conscious surface of human thought. Most of our intuitive smarts is unconscious, and requires Teraops as well as Terabytes of accumulated knowledge. In another chapter of the book I propose a several stage bottom up evolution of robots, paralleling the evolution of our own brain, to create the necessary foundation.\n\n\nSo do you or don’t you grant that your claim that “the performance of AI machines tends to improve at the same pace that AI researchers get access to faster hardware” may not hold regarding the project of acquiring those “terabytes of accumulated knowledge”?\n\n\nMoravec replies: 24/3/98\nTo paraphrase, Robin probes the depth of my conviction in the direct connection between computer power and AI.\nI’m sure that in extreme scenarios (say 100 Teraops dumped on a few researchers overnight) other bottlenecks would come to the fore. But, under current circumstances, I think computer power is the pacing factor for AI. As personal computers become smarter, commercial research will become more important, and academic AI will be more in the position of training and filling niches. Maybe Microsoft or someone else will decide to greatly increase the computer power available to its researchers, speeding up the work somewhat, even if not in proportion to the power increase. Anyway, I expect those decisions will be more distributed and competitively motivated than they are now. Commercial competition will seek the optimum trade-off between faster typewriters and more monkeys.\nI assume that AI can be evolved by a feasible (but non-zero!) amount of engineering trial and error because biological evolution evolved natural intelligence in a limited number of survival experiments (no more than about 1018, including all the failures), and engineering has recapitulated a lot of that ground already.\nI think it will be appropriate soon to make bigger AI systems, and perfecting those will require a lot more attention to detail, experimentation and data gathering than has been mustered so far. My hope for achieving it is a soon-to-begin commercial growth of intelligent robotics, eventually into an industry much bigger than today’s information industry. Incremental steps in most areas critical to AI will translate into commercial advantage in robots more directly than they do in normal computers. Computers must constantly interact with humans anyway, so have the option of relying on human intelligence to avoid the hard parts of various problems (like getting their data from the physical world, or manifesting their results in it). For robots, the hard parts are front and center. I lay this out in the new book.\n\n\nMoravec expands: 28/3/98\nLoosely inspired by Robin Hanson’s engaging economic and social models of the consequences of various extreme technological contingencies, I decided to make a simple model of my AI progress/computer power intuition. Using simplified versions of my assumptions, we get the following:\nSuppose a researcher costs $100K per year, and a baseline workstation, with full support, also costs $100K per year.\nIn year 1, let a baseline computer have 100 MIPS. Assume that 108 MIPS is required to achieve an AI with full human performance. In any given year, let the amount of computer power vary linearly with the cost of the computer. Also assume that the cost of computer power halves each year.\nScenario 1 is like today, let there be 1,000 AI researchers, each with baseline computing. This costs $200 million per year. With a 10% return, this represents a capital investment of $2 billion. These researchers will work to produce full AI, but won’t succeed until the baseline computer grows to 108 MIPS. That will be year 20.\nScenario 2, we fire half the researchers, and use the money to double the computer power for the rest. Now full AI arrives in year 19, if the remaining 500 researchers can make all the necessary discoveries in 19 years that the 1,000 researchers above made in 20 years.\nScenario 3, we fire 7/8 of the researchers. Now each survivor has 8 times as much computing, and AI could be ready in year 17, if the remaining 125 researchers can pull the accelerated load.\nScenario 4, we fire all but 10 researchers. We’d better make sure they’re the best ones, they have a big load to pull. Each has a $10 million/year supercomputer to work with, and nursemaid. Being uncommon machines, supercomputers don’t have the software support or reliability of standard machines. But their power will be adequate for full AI in year 14. If the 10 researchers manage to complete their Herculean task in 14 years, they may still have to wait a several more years before their results become affordable to the masses, because few applications are worth the $10 million per year an AI costs in year 14.\nAnyway, viewing AI as a recapitulation of the evolution of natural AI, I think ten researchers can’t do enough trial and error to do the job in such a short time. Information technology overall has been recapitulating nervous system evolution at about 10 million speed, but that’s because hundreds of thousands of workers have made frequent small and occasional large contributions. A lot of the contributions depend on luck, and luck depends on having enough lottery tickets.\n\n\nRobin Hanson replies: 28/3/98\nThis is the start of a model, but to complete it we need to say how many trial and error steps are needed, how much each one costs, and how the number of trials vary with the number of researchers. Or better yet, we need an economic “production function” describing the rate of successful trials given the MIPS of machines and the number of researchers involved. Then given the number of trials needed, and the expected rate of hardware improvement, we could derive the optimal research plan.\nNote that if there were no diseconomies wrt number of workers, we’d want to stop research now, then hire millions of researchers the day the hardware is cheap enough.\n\n\nAnders Sandberg: 10/3/98\nGeneral comments: A readable essay on a popular level. The estimates for human brain capacity appear to be fairly robust.\nIt would be a good idea to include more references, especially as examples in the first paragraph and the discussion of quantum and nano-logic.\nThe inclusion of the data in an appendix is a good idea. I tried to fit it to a hyperbolic curve, but it seems to be just superexponential. :-)\nA big problem is the use of MIPS as a measure of computation. It is very sensitive to the kind of benchmark used and the architecture (RISC vs. CISC). For comparisons between similar programs running on similar machines it probably works well, but it is not clear that it gives any useful information when we try to compare one systems that are very different. However, since there are no better measures, MIPS will have to do. Most likely estimates will just be order of magnitude estimates, and then the uncertainty in the measure will become less important.\nA more serious problem is that we do not know if the retina and visual system really can be taken as a good estimate for the brain and cognitive systems (just as computer vision for AI). The retina is a highly optimized and fairly stereotypical neural structure, this can introduce a significant bias. It’s 2D structure also may fool us into mis-estimating its capacity; it has to be 2D to function, which means that distances are increased. In the cortex the structure appears to be a dense 3D network, which can have significantly more computing power. So using the retina as an estimate for the brain is very uncertain.\nThe calculations of total brain power as estimated from the retina seems to be slightly wrong (most likely a trivial error, given that the correct number of neurons are mentioned later; volume cannot be compared due to the differences in tissue structure and constraints). The human brain has around 1011–1012 neurons, which makes it just a 10000–1000 times larger than the retina with its 108 neurons. Hence the estimate for 108 MIPS to match human performance may be one or two orders of magnitude too small.\nAnother rough estimate would be based on cortical assemblies and what is known from neural simulations. The 30*109 cells of the cortex are apparently organized into cortical columns, each containing around 100 neurons and representing a single “state” or unit of cognition. That gives us around 108 columns. These are sparsely interconnected, with around 1% connections, giving a total number of 1014 column-column links. Given around 100 spikes per second, we get 1016 spike-events per second along these links. If each spike-event requires one instruction to handle (not unreasonable on dedicated hardware), the we get 1010 MIPS.\nA small factual error in the section started by the discussion of insect nervous systems: only synapses seem to be trimmed away, not whole neurons.\nThe estimate of one byte per synapse seems to be borne out by modelling experience. This would give the brain an approximate capacity of 1014 bytes.\nThe quantum computer section curiously lacks a reference to the bulk spin resonance results of Gershenfeld and Chuang (N. Gershenfeld and I. Chuang, Science, 275, pp. 350-356, 1997, http://physics.www.media.mit.edu/publications/papers/97.01.science.pdf, http://physics.www.media.mit.edu/publications/papers/97.09.itp.pdf).1\n1 Editor’s note: Those two papers are (Chuang et al. 1998; Gershenfeld and Chuang 1997)What about special purpose hardware for neural modelling?\nHow much do algorithms matter?\n\n\nMoravec replies: 18/3/98\nI just use MIPS as a convenient common notation. My numbers for recent machines are obtained from various benchmark suites, Spec-92 (1 Spec92 = 1 MIPS), Spec-95 (1 Spec95 = 40 MIPS), MacBench (1 MacBench = 0.66 MIPS). These exercise cache, calculation, memory and various other aspects in a fair way, so are pretty representative of performance most programs get. They usually agree within a factor better than two,\nThe retina is untypical, and I would use some other structure if I had convincing computational analogs. But I think volume (or mass: it’s all water) is a far better extrapolator than neuron count. Evolution can just as easily choose two small neurons as one twice as large. The cost in metabolism and materials is the same. So I would expect brain structures to maximize for effective computation per volume, not per neuron. After all, one neuron with ten thousand synapses might be the computational match of 50 neurons with 50 synapses each.\nThe retina gives one measure of computation per volume. Because vision is so important, and because the retina must be transparently thin, the retina may be evolutionarily more perfected, i.e. computationally dense, than the average neural structure. If so, my stimate for the brain is an overestimate.\nOn the other hand, not having the transparency constraint may have given evolution more degrees of freedom for optimization in the rest of the brain, and thus allowed for a better solution there. In that case, my brain computation number would be an underestimate.\nUnlike the reviewer, I don’t think counting neural switching events is a very useful way to measure computation, because structural constraints can make a huge difference in the relation between primitive switching and end-result computation. And it is the final computation that matters, not the fuss in doing it.\nIn a forthcoming book, [Robot, Being: from mere machine to transcendent mind. Oxford Univ. Press.] I discuss why control and learning organizations more situation-specialized than neural nets seem to be much superior for robots. The brain is stuck with shallow masses of very slow components, which limit the possible solutions, but robots, with fast serial processors are not! But I think that discussion is beyond the scope of this article.\n\n\nDan Clemmensen: 21/3/98\nDr. Moravec’s paper looks like a good overview, and is very readable. The paper provides strong support for its thesis that for human-level AI, “required hardware will be available in cheap machines in the 2020s”. However, the paper makes the assumption that the “cheap machine” must be a general-purpose computer.\nThere are strong historical precedents for this assumption. In general, specialized hardware as been more trouble than it’s worth. In his response to Robin Hanson, Dr. Moravec relates some of his personal experiences of this with the CM2 and CM5. In general, by the time the specialized tools and techniques to employ a specialized computer are available, the general-purpose computer technology will have advanced to the same performance level. The History of computing is littered with additional examples.\nHowever, it’s not clear to me that this rule will hold in all cases. The paper actually gives part of a counterexample with Deep Blue. Deep Blue combines a powerful general-purpose multiprocessing computer with specialized chess-position evaluators to achieve human-level chess-playing ability. This same model may be generalizable by taking advantage of software-reprogrammable logic devices such as those made by XILINX or Altera. I would guess that a chess-position evaluator could be programmed into a single Altera Flex 10K part that costs $20 today. Deep Blue has 256 evaluators. If my guess is correct, an engineer can create a machine with Deep Blue’s capability by adding less than $6000 of hardware to a high-end desktop. The difference is that the result is general-purpose, because the evaluators are reprogrammable. Note that there is no reason for all the evaluators to run the same program. Since this architecture is based on general-purpose parts that are widely used in commercial designs, it will become smaller, faster, cheaper and more powerful at roughly the same rate at general-purpose computers.\nDr. Hugo de Garis, http://www.hip.atr.co.jp/~degaris, is attempting to build an AI using XILINX parts to simulate neurons. This is not quite what I had in mind. I’m thinking more in terms of a model with a single-threaded program that uses the evaluators to perform incredibly powerful, highly specialized instructions.\nDr Moravec estimates that Deep Blue can apply about 3 million MIPS in its problem domain. I’m guessing that we can build an equivalent, affordable machine today that is not restricted to the chess domain. If so, the hardware for human-level AI is available today, and human-level AI is “merely” a small matter of programming.\nDan Clemmensen Systems Architect, Netrix Corporation Dan@Clemmensen.ShireNet.Com http://www.ShireNet.Com/~dgc\n\n\nMoravec replies: 21/3/98\nDan’s comments regarding the occasional benefit of specialized hardware well taken. Other strong, if not AI, examples are the DSPs in modems and the graphics hardware now augmenting processors.\nBut even there the advantage may be fleeting. Motorola is dropping out of the hardware modem business, because the functionality can now be achieved more flexibly with software, in multi-hundred-MIPS computers to whom audio bandwidth is a minor distraction.\nI look forward to seeing how effectively programmable logic contributes to AI.\n\n\nDan Clemmensen follows up: 21/3/98\nThis is one of the continuing oscillations in our industry. A task that’s only achievable with specialized hardware becomes amenable to cost-effective solution with the main CPU instead. But the hardware guys then find other more complex tasks for special hardware. For example, modems for phone lines are now “soft” as you say, but ADSL modems and 100BaseT transceivers need special hardware, as evidenced in Motorola’s newer QUICC devices.\nAnother interesting oscillation is the relative costs of processing versus communications bandwidth.\nWhat I was proposing is really the next generation of the “customizable instruction set” idea. In the early ’70s, this called “microprogramming”. I just pointed out that we could adapt the Deep Blue concept by permitting programmable evaluators. Interestingly, the skills and tools used by “hardware designers” to program XILINX or FLEX 10K parts are more akin to software skills than to traditional logic-gate design skills. A programmer can read and understand a VHDL manual more quickly than a “traditional EE” can, unless the EE is also a programmer.\n\n\nPaul Hughes: 22/3/98\nI found Hans paper to be overall highly consistent, logical and well thought out.\nHowever, there has alway been an estimate made by Hans regarding the capacity of the human brain that doesn’t take into consideration the elaborate cytoskeletal structure of microtubules within each neuronal cell. The shear complexity within these cyto-networks combined with their influence on neurotransmitter activity would seem to shed a great deal of doubt on Hans and many other neuro-computer scientists continued treatment of individual neurons as simple on/off transistors. For a brief tutorial on these networks see:\nhttp://www.reed.edu/~rsavage/microtubules.html\nand its larger section integrated with quantum cosmology at:\nhttp://galaxy.cau.edu/tsmith/ManyWorlds.html\nI would like to know why Hans and others continue to treat the neuron as a simple on/off switch in the face of the evidence of a greater intra-neuronal complexity?\nIf the cytoskeletal/microtubule networks do turn out to play a vital role in neuro-computation, then Hans will have to revise his estimates of human-level MIPS/Memory by at least 2 orders of magnitude.\n\n\nMoravec replies: 23/3/98\nMy brain: computer comparison doesn’t start with neurons but with the whole retina as a functional unit. It assumes the computational performance of the postage stamp of retina is representative of other neural tissue.\nThere is extensive evidence that the human retina accommodates to a huge light variations, and detects edges and motion at a million locations at about ten hertz. Similar performance can be obtained from a 1,000 MIPS computer processing a million pixel image high definition TV image.\nUnless the retina provides results no one yet suspects, this approach would seem to weigh the contribution from all relevant mechanisms.\n\n\nWlodzislaw Duch replies to Hughes’ comment: 16/4/98\nI (in agreement with almost all other physicists) do not see any evidence that microtubules have anything to do with computations in the brain. Cognitive computational neuroscience makes great progress modeling real phenomena and the behavior of neurons in vitro is very well described by the Hudgkin-Huxley model (not by the on-off switch). Experiments and simulations go hand in hand here. Microtubules are in all eucariotic cells, so why are our minds so dependent on our brains? Please explain first the paramecium behavior (I am sure that it is due to the biochemical reactions, not quantum computing), prove it experimentally and than talk about human brains.\nWłodzisław Duch\nComputational Intelligence Lab, Nicholas Copernicus University\nduch@phys.uni.torun.pl\nhttp://www.phys.uni.torun.pl/~duch\n\n\nWlodzislaw Duch comments on Moravec’s article: 16/4/98\nIn the article by Hans Moravec I was surprised to see so much emphasis on computer speed. The 5th generation AI project has emphasized how many LIPS (logical inferences per second) their machines will provide and not much came out of it. Will the classical problems of AI be solved by speed/memory?\nThese problems include representation of complex knowledge structures, creation of huge knowledge bases to simulate common reason (addressed by the CYC project), representation of time and space, behavioral based intelligence (addressed by the Cog project) and the importance of the embodiment. Speed is just one necessary condition, proper structure of intelligent machines is the other. It is relatively simple in chess (graphs and heuristic search) but already much more complex in the game of go and even more complex in the everyday thinking.\nSimulations of the human brain by neural networks, the second route to AI, are still at quite primitive stage. Either we simulate the spiking neurons well, and than are able to take a few of them, or we have very crude approximation and may take more neurons, but than they are not able to do the same job. Neurodynamical processes are very complex and we still struggle with a few degrees of freedom. Not to mention that many connections between brain structures and functions of these structures are still unknown.\nThis is not to deny that AI will make progress, but to stress that estimations of speed/memory are only a small part of the story.\n\n\nMoravec replies: 16/4/98\nBut I think scale is much more important than most in the AI community thought, or many still think. Could a mouse achieve human intelligence? There is only a factor of 1,000 in brain size between mouse and man. The deficits in computer power to human brain power I estimate were a hundred million-fold.\n[In my forthcoming book (Robot, Being) there is a chapter that] outlines a strategy for four decades of robot evolution that very coarsely parallels stages in four hundred megayears of vertebrate mental evolution. The evolutionary framework is reassuring, because nature tells us there is an incremental development path along it, made of small, individually advantageous, steps. If Darwinian trial and error made those little steps, so can human technological search. Especially since we can cheat by occasionally peeking at the biological answers.\nGenerally, I see compelling evidence that availability of processing power is the pacing factor in the improving performance of the many research robots around me. For instance, in the 1980s mobile robots could not reliably cross a room, now they drive cross-country. And this with still insectlike processing power (or like a tiny chordate, if you want phylogenetic purity). Lizardlike motor-perceptual competence is the first “vertebrate” target in my speculative evolution. We’re not there yet, but I expect we will be by 2010.\n\n\nDavid Villa: 19/4/98\nOn the whole this was a very readable and interesting paper. I have one comment, though. You wrote:\nThe most powerful experimental supercomputers in 1998, composed of thousands or tens of thousands of the fastest microprocessors and costing tens of millions of dollars, can do a few million MIPS. They are within striking distance of being powerful enough to match human brainpower, but are unlikely to be applied to that end. Why tie up a rare twenty-million-dollar asset to develop one ersatz-human, when millions of inexpensive original model humans are available? Such machines are needed for high-value scientific calculations, mostly physical simulations, having no cheaper substitutes. AI research must wait for the power to become more affordable.\nI can think of at least two reasons why a twenty-million-dollar investment to reproduce human-level intelligence would be worthwhile.\n\nSimply to prove that it is possible. There are still those, even some penetrating and deep thinkers (Roger Penrose springs to mind) who doubt this. It may seem a less than noble reason for such expense, but it is not inherently different from the vastly greater sums spent verifying one theory of particle physics over another.\nIf a twenty-million-dollar investment would bring us to within striking distance of human-level intelligence, thirty or forty million dollars may take us beyond it. This done, the whole process would potentially bootstrap, ultimately leading to very cheap, very powerful super-minds - and everything their existence would imply.\n\n\n\nMoravec replies: 19/4/98\nDavid Villa asks, why not invest big bucks in supercomputers for AI?\n\n\nSimply to prove that it is possible. There are still those, even some penetrating and deep thinkers (Roger Penrose springs to mind) who doubt this. It may seem a less than noble reason for such expense, but it is not inherently different from the vastly greater sums spent verifying one theory of particle physics over another.\n\n\nAtomic physics was considered an oddball interest, with very limited support before World War II, comparable to AI now (goofball scientists splitting atoms? weird, weird). Only the atomic bomb raised its interest in the halls of power. No one, outside a small circle of irrelevant goofballs, sees anything of comparable interest imminent from AI. (Before WWII, it was chemists who got the bucks, because they had developed the gas and explosives the mattered in the last war.)\n\n\nIf a twenty-million-dollar investment would bring us to within striking distance of human-level intelligence, thirty or forty million dollars may take us beyond it. This done, the whole process would potentially bootstrap, ultimately leading to very cheap, very powerful super-minds - and everything their existence would imply.\n\n\nThe investment would have to be in the hundreds of millions of dollars at least. Buying the computer creates the need to keep it fed. There simply isn’t enough perceived need or plausibility that it would pay off. There were times when such a perception did exist. In the 1960s, AI type efforts towards automatic translation, management of nuclear war and, in the USSR, management of the economy, got huge, national interest types of funding. The gap in required power was then was so large, that even that investment didn’t bridge it. (But Strategic Air Command probably still uses some of the original SAGE equipment that was developed then)\nGiven the fast exponential increase of computer power over time, compared to the merely linear increases bought by money, I’m happy to spend my time hacking patiently towards AI around 2020 rather than campaigning for a wildly expensive crash project that might, possibly, bring it a few years sooner.\nActually, I think we may get the best of both worlds if commercial development of mass-market utility robots takes off in the next decade, as I hope (and outline in the book). Market forces will then generate investment dwarfing any government program.\n\n\nD. Lloyd Jarmusch: 7/3/99\nHans Moravec wrote\n\n“Advancing computer performance is like water slowly flooding the landscape. A half century ago it began to drown the lowlands, driving out human calculators and record clerks, but leaving most of us dry. Now the flood has reached the foothills, and our outposts there are contemplating retreat. We feel safe on our peaks, but, at the present rate, those too will be submerged within another half century. I propose (Moravec 1998) that we build Arks as that day nears, and adopt a seafaring life!” How do we build or board these Arks? Is human mind/computer interface a near term probability? How do I find out more? It seems that virtual immortality through artificial consciousness is a possibility for the future. How does one best go about achieving virtual immortality? Where is the best information on the subject?\n\nD. Lloyd Jarmusch"
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#appendix-editors-notes",
    "href": "docs/posts/1998-hans-moravec/index.html#appendix-editors-notes",
    "title": "When will computer hardware match the human brain?",
    "section": "Appendix: Editor’s notes",
    "text": "Appendix: Editor’s notes\n\nThis markdown document taken mostly verbatim from the original HTML document, the original HTML peer commentaries, and the original dataset. I converted ASCII-math into LaTeX math, fixed typos, formatted the dataset, chased down a few dead links, etc.\nThe original metadata\nJournal of Evolution and Technology. 1998. Vol. 1\n(Received Dec. 1997)\nHans Moravec\nRobotics Institute\nCarnegie Mellon University\nPittsburgh, PA 15213-3890, USA\nnet: hpm@cmu.edu\nweb: http://www.frc.ri.cmu.edu/~hpm/\n\nJournal of Evolution and Technology\nA peer-reviewed electronic journal publishing contemporary research into future science and philosophy.\nISSN 1541-0099"
  },
  {
    "objectID": "docs/posts/1998-hans-moravec/index.html#appendix-dataset",
    "href": "docs/posts/1998-hans-moravec/index.html#appendix-dataset",
    "title": "When will computer hardware match the human brain?",
    "section": "Appendix: Dataset",
    "text": "Appendix: Dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine\nYear\nCost k$\n1997 k$\nMBytes\nMIPS\nMIPS/1997 k$\n\n\n\n\nBy Hand\n1892\n8.5\n142\n9.54e-05\n1.19e-08\n8.37e-11\n\n\nOhdner\n1891\n11\n172\n1.49e-07\n3.33e-09\n1.94e-11\n\n\nSteiger Millionaire\n1900\n12\n187\n3.58e-07\n1.33e-08\n7.12e-11\n\n\nHollerith\n1908\n50\n694\n0.000286\n1.85e-08\n2.67e-11\n\n\nAnalytical Engine\n1910\n1e+03\n1.24e+04\n0.0244\n3.77e-07\n3.05e-11\n\n\nMonroe Calculator\n1911\n35\n470\n2.86e-06\n2.18e-08\n4.64e-11\n\n\nIBM Tabulator\n1919\n20\n126\n2.38e-05\n4.12e-08\n3.27e-10\n\n\nTorres Arithmometer\n1920\n25\n141\n4.77e-06\n3.58e-08\n2.53e-10\n\n\nNational-Ellis 3000\n1928\n15\n135\n4.29e-06\n7.38e-08\n5.46e-10\n\n\nBurroughs Class 16\n1929\n15\n137\n4.29e-06\n7.38e-08\n5.37e-10\n\n\nZuse-1\n1938\n10\n111\n3.05e-05\n4.24e-08\n3.82e-10\n\n\nZuse-2\n1939\n10\n113\n3.05e-05\n4.24e-07\n3.75e-09\n\n\nBTL Model 1\n1939\n50\n565\n3.81e-06\n2e-06\n3.54e-09\n\n\nZuse-3\n1941\n50\n499\n0.000244\n2.04e-06\n4.09e-09\n\n\nBTL Model 2\n1943\n50\n422\n1.19e-05\n1.03e-06\n2.44e-09\n\n\nColossus\n1943\n100\n844\n2.38e-06\n0.000224\n2.65e-07\n\n\nBTL Model 3\n1943\n200\n1.69e+03\n4.29e-05\n2.83e-06\n1.68e-09\n\n\nASCC (Mark 1)\n1944\n300\n2.52e+03\n0.000601\n2.33e-06\n9.26e-10\n\n\nZuse-4\n1945\n50\n412\n0.000244\n2.04e-06\n4.96e-09\n\n\nBTL Model 5\n1946\n500\n3.61e+03\n0.000147\n3.29e-06\n9.12e-10\n\n\nENIAC\n1946\n600\n4.33e+03\n9.54e-05\n0.00289\n6.68e-07\n\n\nHarvard Mark 2\n1947\n300\n1.77e+03\n0.000488\n6.22e-06\n3.52e-09\n\n\nIBM SSEC\n1948\n500\n2.71e+03\n4.58e-05\n0.000597\n2.2e-07\n\n\nEDSAC\n1949\n100\n571\n0.00214\n0.00255\n4.46e-06\n\n\nSEAC\n1950\n800\n4.4e+03\n0.00549\n0.00416\n9.44e-07\n\n\nUNIVAC I\n1951\n930\n4.59e+03\n0.00537\n0.00575\n1.25e-06\n\n\nZuse-5\n1952\n100\n508\n0.000244\n9.33e-06\n1.84e-08\n\n\nIBM CPC\n1952\n100\n508\n0.000154\n0.00176\n3.47e-06\n\n\nIBM 650\n1953\n200\n1.03e+03\n0.00488\n0.000966\n9.4e-07\n\n\nEDVAC\n1954\n500\n2.57e+03\n0.00537\n0.0017\n6.61e-07\n\n\nWhirlwind\n1955\n200\n1.03e+03\n0.00391\n0.0694\n6.77e-05\n\n\nLibrascope LGP-30\n1955\n30\n154\n0.0146\n0.000701\n4.56e-06\n\n\nIBM 704\n1955\n2e+03\n1.03e+04\n0.0352\n0.0536\n5.23e-06\n\n\nIBM 7090\n1959\n3e+03\n1.42e+04\n0.141\n0.326\n2.29e-05\n\n\nIBM 1620\n1960\n200\n947\n0.0122\n0.00103\n1.09e-06\n\n\nDEC PDP-1\n1960\n150\n710\n0.0176\n0.124\n0.000175\n\n\nAtlas\n1961\n5e+03\n2.38e+04\n0.0234\n1.4\n5.88e-05\n\n\nBurroughs 5000\n1962\n1e+03\n4.75e+03\n0.0254\n0.0989\n2.08e-05\n\n\nIBM 7040\n1963\n560\n2.67e+03\n0.07\n0.063\n2.36e-05\n\n\nHoneywell 1800\n1963\n1.4e+03\n6.67e+03\n0.093\n0.15\n2.25e-05\n\n\nDEC PDP-6\n1964\n300\n1.42e+03\n0.0703\n0.169\n0.000119\n\n\nCDC 6600\n1964\n5e+03\n2.37e+04\n4\n8.76\n0.000369\n\n\nIBM 1130\n1965\n50\n233\n0.0156\n0.15\n0.000644\n\n\nIBM 360/75\n1966\n5e+03\n2.25e+04\n8\n2.54\n0.000113\n\n\nIBM 360/65\n1967\n3e+03\n1.35e+04\n4\n1.24\n9.18e-05\n\n\nDEC PDP-10\n1968\n500\n2.19e+03\n0.562\n0.655\n0.000299\n\n\nCDC 7600\n1969\n1e+04\n4.23e+04\n8\n25.7\n0.000608\n\n\nDG Nova\n1969\n7.6\n32.1\n0.008\n0.117\n0.00366\n\n\nGE-635\n1970\n2e+03\n8.14e+03\n0.5\n0.649\n7.97e-05\n\n\nSDS 920\n1971\n100\n394\n0.25\n0.105\n0.000266\n\n\nIBM 360/195\n1972\n8e+03\n3.02e+04\n0.5\n17.3\n0.000573\n\n\nHoneywell 700\n1972\n12\n45.2\n0.031\n0.075\n0.00166\n\n\nPrime Computer 100\n1973\n8.5\n28.4\n0.031\n0.36\n0.0127\n\n\nIBM-370/168\n1974\n2e+03\n5.61e+03\n1\n8.88\n0.00158\n\n\nMITS Altair\n1974\n0.5\n1.4\n0.00024\n0.01\n0.00713\n\n\nDG Eclipse\n1975\n50\n129\n0.25\n0.47\n0.00366\n\n\nDEC-KL-10\n1975\n500\n1.29e+03\n4.5\n2.3\n0.00179\n\n\nDEC PDP-11/70\n1976\n150\n368\n0.125\n0.4\n0.00109\n\n\nCray-1\n1976\n1e+04\n2.45e+04\n32\n150\n0.00612\n\n\nApple II\n1977\n1.3\n3.02\n0.0039\n0.02\n0.00662\n\n\nDEC VAX 11/780\n1977\n200\n464\n8\n1\n0.00215\n\n\nTRS-80\n1977\n2\n4.64\n0.015\n0.04\n0.00861\n\n\nCommodore PET\n1977\n1.5\n3.48\n0.008\n0.06\n0.0172\n\n\nCDC IPL\n1978\n500\n1.08e+03\n1\n7.5\n0.00697\n\n\nNanodata VMX200\n1979\n300\n571\n2\n2.1\n0.00367\n\n\nTRS-80 M3\n1980\n1.2\n2\n0.015\n0.04\n0.02\n\n\nSun-1\n1980\n30\n50.1\n1\n0.484\n0.00966\n\n\nCDC Cyber-205\n1981\n9e+03\n1.39e+04\n16\n73.2\n0.00528\n\n\nVic 20\n1981\n0.279\n0.43\n0.005\n0.04\n0.0931\n\n\nIBM PC\n1982\n2.5\n3.75\n0.0469\n0.238\n0.0634\n\n\nSun-2\n1982\n20\n30\n2\n0.741\n0.0247\n\n\nCommodore 64\n1982\n0.5\n0.75\n0.0825\n0.2\n0.267\n\n\nTRS-80 M4\n1983\n1\n1.49\n0.0635\n0.2\n0.134\n\n\nVax 11/750\n1983\n50\n74.4\n4\n0.799\n0.0107\n\n\nMacintosh-128K\n1984\n2.5\n3.62\n0.125\n0.52\n0.144\n\n\nVax 11/785\n1984\n200\n290\n0.0156\n2.26\n0.0078\n\n\nCray-2\n1985\n1e+04\n1.45e+04\n1.95e+03\n824\n0.0569\n\n\nL.Edge XT-7.16\n1985\n2\n2.89\n0.25\n0.26\n0.09\n\n\nAtari 800XL\n1985\n0.85\n1.23\n0.64\n0.165\n0.134\n\n\nSun-3\n1986\n10\n14.1\n4\n2.05\n0.145\n\n\nDEC VAX 8650\n1986\n125\n176\n16\n7.71\n0.0438\n\n\nMIT XT-8\n1986\n0.5\n0.705\n0.25\n0.534\n0.758\n\n\nMac II\n1987\n3\n4.11\n2\n2.5\n0.608\n\n\nSun-4\n1987\n10\n13.7\n16\n1.87\n0.136\n\n\nMac-IIx\n1988\n9.3\n12.1\n4\n3.9\n0.322\n\n\nCompuAdd 386-16\n1988\n2.1\n2.73\n1\n2.8\n1.02\n\n\nPC Brand 386-25\n1988\n2.45\n3.2\n1\n4.3\n1.35\n\n\nMark 386\n1989\n12\n15.2\n2\n12.9\n0.849\n\n\nWang VS 10000\n1989\n510\n646\n16\n103\n0.159\n\n\nMacintosh SE30\n1989\n6.49\n8.23\n5\n3.9\n0.474\n\n\nSolbourne 5/500\n1989\n50\n63.3\n2\n25.5\n0.403\n\n\nStardent 3000\n1990\n89\n109\n32\n27.3\n0.249\n\n\nAmiga 2500/30\n1990\n4.7\n5.78\n2\n19.5\n3.37\n\n\nAcer 1200\n1990\n11\n13.5\n4\n20\n1.48\n\n\nMVME165\n1990\n4\n4.92\n4\n16.6\n3.37\n\n\nPower VEISA\n1990\n5.8\n7.13\n6\n22.1\n3.1\n\n\nDell 320LX\n1990\n2.9\n3.57\n1\n12.5\n3.5\n\n\nMac IIfx\n1990\n9.87\n12.1\n4\n10\n0.824\n\n\nAmiga 3000\n1990\n3.3\n4.06\n2\n12.5\n3.08\n\n\nVMPM868KD\n1990\n2.9\n3.57\n2\n12.5\n3.5\n\n\nStep 486/33\n1990\n10\n12.3\n80\n17.5\n1.42\n\n\nGateway-486DX2/66\n1991\n3.9\n4.66\n8\n30.9\n6.64\n\n\nACT 468/33\n1991\n3.4\n4.06\n4\n21.8\n5.37\n\n\nSlimline SP486DX\n1991\n3.6\n4.3\n4\n21.8\n5.07\n\n\nMac-Quadra-900\n1991\n3.3\n3.94\n8\n22\n5.58\n\n\nAST Bravo\n1992\n1.4\n1.62\n2\n12.9\n7.95\n\n\nIBM PS/2 55-041\n1992\n2\n2.32\n4\n10.6\n4.57\n\n\nAST Premium II\n1992\n2.8\n3.25\n4\n13.2\n4.07\n\n\nIBM PS/2 90\n1992\n9.6\n11.1\n8\n22.4\n2.01\n\n\nNEC Powermate\n1992\n4.8\n5.56\n4\n21.8\n3.92\n\n\nAberdeen Mini\n1993\n2.8\n3.15\n2\n16.2\n5.14\n\n\nIBM Valuepoint\n1993\n3.6\n4.05\n4\n26.1\n6.44\n\n\nAcer Power\n1993\n3.5\n3.94\n4\n44.5\n11.3\n\n\nAmbra Desktop\n1993\n2.4\n2.7\n2\n21.1\n7.81\n\n\nDECpc LPv\n1993\n2.9\n3.26\n4\n16.6\n5.09\n\n\nAST Pemmia\n1993\n3.6\n4.05\n2\n16.2\n4\n\n\nNEC 486SL DX2\n1994\n3.8\n4.15\n4\n31.9\n7.68\n\n\nVesa\n1994\n1.2\n1.31\n4\n20\n15.3\n\n\nAT&T System 3260\n1994\n2.5\n2.73\n8\n44\n16.1\n\n\nIBM 433/DX/Si\n1994\n1.8\n1.97\n4\n26.1\n13.3\n\n\nMicron 466 Wndsrvr\n1994\n3.6\n3.93\n16\n54.7\n13.9\n\n\nAST PremiaGXP/90\n1994\n5.8\n6.34\n16\n98.6\n15.6\n\n\nAT&T Globalyst 600\n1994\n4.8\n5.25\n16\n98.6\n18.8\n\n\nZEOS Contenda 386\n1994\n1\n1.09\n4\n20\n18.3\n\n\nGateway 2000 486\n1994\n1\n1.09\n2\n16.2\n14.8\n\n\nPowerMac 7100/66\n1994\n2.9\n3.17\n8\n100\n31.6\n\n\nPowerMac 8100/80\n1994\n4.25\n4.64\n16\n120\n25.8\n\n\nPowerMac 8500/120\n1995\n4\n4.24\n16\n180\n42.4\n\n\nPowerMac 9500/132\n1995\n5.3\n5.62\n16\n200\n35.6\n\n\nIntel Xpress/60\n1995\n2\n2.12\n8\n70\n33\n\n\nGateway P5-75\n1996\n2\n2.06\n16\n92\n44.7\n\n\nPower Tower 180e\n1996\n3.3\n3.39\n16\n300\n88.4\n\n\nPowerMac 7600/132\n1996\n3\n3.09\n16\n160\n51.8\n\n\nGateway G6-200\n1997\n2.95\n2.95\n64\n350\n119"
  },
  {
    "objectID": "docs/posts/1992-hans-moravec/index.html",
    "href": "docs/posts/1992-hans-moravec/index.html",
    "title": "Pigs in Cyberspace",
    "section": "",
    "text": "Exploration and colonization of the universe await, but Earth-adapted biological humans are ill equipped to respond to the challenge. Machines have gone farther and seen more, limited though they presently are by insect-like behavioral inflexibility. As they become smarter over the coming decades, space will be theirs. Organizations of robots of ever-increasing intelligence and sensory and motor ability will expand and transform what they occupy, working with matter, space, and time. As they grow, a smaller and smaller fraction of their territory will be undeveloped frontier. Competitive success will depend more and more on using already available matter and space in ever more refined and useful forms. The process, analogous to the miniaturization that makes today’s computers a trillion times more powerful than the mechanical calculators of the past, will gradually transform all activity from grossly physical homesteading of raw nature to minimum-energy quantum transactions of computation. The final frontier will be urbanized, ultimately into an arena where every bit of activity is a meaningful computation: the inhabited portion of the universe will transformed into a cyberspace.\nBecause it will use resources more efficiently, a mature cyberspace of the distant future will be effectively much bigger than the present physical universe. While only an infinitesimal fraction of existing matter and space is doing interesting work, in a well-developed cyberspace every bit will be part of a relevant computation or storing a useful datum. Over time, more compact and faster ways of using space and matter will be invented, and used to restructure the cyberspace, effectively increasing the amount of computational spacetime per unit of physical spacetime.\nComputational speedups will affect the subjective experience of entities in the cyberspace in a paradoxical way. At first glimpse, there is no subjective effect, because everything, inside and outside the individual, speeds up equally. But, more subtly, speedup produces an expansion of the cyber universe, because, as thought accelerates, more subjective time passes during the fixed (probably lightspeed) physical transit time of a message between a given pair of locations – so those fixed locations seem to grow farther apart. Also, as information storage is made continually more efficient through both denser utilization of matter and more efficient encodings, there will be increasingly more cyber-stuff between any two points. The effect may somewhat resemble the continuous-creation process in the old steady-state theory of the physical universe of Hoyle, Bondi, and Gold, where hydrogen atoms appear just fast enough throughout the expanding cosmos to maintain a constant density.\nA quantum-mechanical entropy calculation by Bekenstein suggests that the ultimate amount of information that can be stored given the mass and volume of a hydrogen atom is about a megabyte. But let’s be conservative, and imagine that at some point in the future only “conventional” physics is in play, but every few atoms stores a useful bit. There are about 1056 atoms in the solar system. I estimate that a human brain- equivalent can be encoded in less than 1015 bits. If a body and surrounding environment takes a thousand times more storage in addition, a human, with immediate environment, might consume 1018 bits. An AI with equivalent intelligence could probably get by with less, since it does without the body-simulation “life - support” needed to keep a body-oriented human mind sane. So a city of a million human-scale inhabitants might be efficiently stored in 1024 bits. If the atoms of the solar system were cleverly rearranged so every 100 could represent a bit, then a single solar system could hold 1030 cities – far more than the number (1022) of stars in the visible universe! Multiply that by 1011 stars in a galaxy, and one gets 1041 cities per galaxy. The visible universe, with 1011 galaxies, would then have room for 1051 cities – except that by the time intelligence has expanded that far, more efficient ways of using spacetime and encoding data would surely have been discovered, increasing the number much further."
  },
  {
    "objectID": "docs/posts/1992-hans-moravec/index.html#pigs-in-cyberspace",
    "href": "docs/posts/1992-hans-moravec/index.html#pigs-in-cyberspace",
    "title": "Pigs in Cyberspace",
    "section": "",
    "text": "Exploration and colonization of the universe await, but Earth-adapted biological humans are ill equipped to respond to the challenge. Machines have gone farther and seen more, limited though they presently are by insect-like behavioral inflexibility. As they become smarter over the coming decades, space will be theirs. Organizations of robots of ever-increasing intelligence and sensory and motor ability will expand and transform what they occupy, working with matter, space, and time. As they grow, a smaller and smaller fraction of their territory will be undeveloped frontier. Competitive success will depend more and more on using already available matter and space in ever more refined and useful forms. The process, analogous to the miniaturization that makes today’s computers a trillion times more powerful than the mechanical calculators of the past, will gradually transform all activity from grossly physical homesteading of raw nature to minimum-energy quantum transactions of computation. The final frontier will be urbanized, ultimately into an arena where every bit of activity is a meaningful computation: the inhabited portion of the universe will transformed into a cyberspace.\nBecause it will use resources more efficiently, a mature cyberspace of the distant future will be effectively much bigger than the present physical universe. While only an infinitesimal fraction of existing matter and space is doing interesting work, in a well-developed cyberspace every bit will be part of a relevant computation or storing a useful datum. Over time, more compact and faster ways of using space and matter will be invented, and used to restructure the cyberspace, effectively increasing the amount of computational spacetime per unit of physical spacetime.\nComputational speedups will affect the subjective experience of entities in the cyberspace in a paradoxical way. At first glimpse, there is no subjective effect, because everything, inside and outside the individual, speeds up equally. But, more subtly, speedup produces an expansion of the cyber universe, because, as thought accelerates, more subjective time passes during the fixed (probably lightspeed) physical transit time of a message between a given pair of locations – so those fixed locations seem to grow farther apart. Also, as information storage is made continually more efficient through both denser utilization of matter and more efficient encodings, there will be increasingly more cyber-stuff between any two points. The effect may somewhat resemble the continuous-creation process in the old steady-state theory of the physical universe of Hoyle, Bondi, and Gold, where hydrogen atoms appear just fast enough throughout the expanding cosmos to maintain a constant density.\nA quantum-mechanical entropy calculation by Bekenstein suggests that the ultimate amount of information that can be stored given the mass and volume of a hydrogen atom is about a megabyte. But let’s be conservative, and imagine that at some point in the future only “conventional” physics is in play, but every few atoms stores a useful bit. There are about 1056 atoms in the solar system. I estimate that a human brain- equivalent can be encoded in less than 1015 bits. If a body and surrounding environment takes a thousand times more storage in addition, a human, with immediate environment, might consume 1018 bits. An AI with equivalent intelligence could probably get by with less, since it does without the body-simulation “life - support” needed to keep a body-oriented human mind sane. So a city of a million human-scale inhabitants might be efficiently stored in 1024 bits. If the atoms of the solar system were cleverly rearranged so every 100 could represent a bit, then a single solar system could hold 1030 cities – far more than the number (1022) of stars in the visible universe! Multiply that by 1011 stars in a galaxy, and one gets 1041 cities per galaxy. The visible universe, with 1011 galaxies, would then have room for 1051 cities – except that by the time intelligence has expanded that far, more efficient ways of using spacetime and encoding data would surely have been discovered, increasing the number much further."
  },
  {
    "objectID": "docs/posts/1992-hans-moravec/index.html#mind-without-body",
    "href": "docs/posts/1992-hans-moravec/index.html#mind-without-body",
    "title": "Pigs in Cyberspace",
    "section": "Mind without Body?",
    "text": "Mind without Body?\nStart with the concepts of telepresence and virtual reality. You wear a harness that, with optical, acoustical, mechanical and chemical devices, controls all that you sense, and measures all of your actions. Its machinery presents pictures to your eyes, sounds to your ears, pressures and temperatures to your skin, forces to your muscles, and even smells and tastes for the remaining senses. Telepresence results when the inputs and outputs of this harness connect to a distant machine that looks like a humanoid robot. The images from the robot’s two camera eyes appear on your “eyeglass” viewscreens, and you hear through its ears, feel through its skin, and smell through its chemical sensors. When you move your head or body, the robot moves in exact synchrony. When you reach for an object seen in the viewscreens, the robot reaches for the object, and when it makes contact, your muscles and skin feel the resulting weight, shape, - texture, and temperature. For most practical purposes you inhabit the robot’s body – your sense of consciousness has migrated to the robot’s location, in a true “out of body” experience.\nVirtual reality retains the harness, but replaces the remote robot with a computer simulation of a body and its surroundings. When connected to a virtual reality, the location you seem to inhabit does not exist in the usual physical sense, rather you are in a kind of computer-generated dream. If the computer has access to data from the outside world, the simulation may contain some “real” items, for instance representations of other people connected via their own harnesses, or even views of the outside world, perhaps through simulated windows.\nOne might imagine a hybrid system where a virtual “central station” is surrounded by portals that open on to views of multiplereal locations. While in the station one inhabits a simulated body, but when one steps through a portal, the harness link is seamlessly switched from the simulation to a telepresence robot waiting at that location.\nThe technical challenges limit the availability, “fidelity,” and affordability of telepresence and virtual reality systems today – in fact, they exist only in a few highly experimental demonstrations. But progress is being made, and it’s possible to anticipate a time, a few decades hence, when people spend more time in remote and virtual realities than in their immediate surroundings, just as today most of us spend more time in artificial indoor surroundings than in the great outdoors. The remote bodies we will inhabit can be stronger, faster, and have better senses than our “home” body. In fact, as our home body ages and weakens, we might compensate by turning up some kind of “volume control”. Eventually, we might wish to bypass our atrophied muscles and dimmed senses altogether, if neurobiology learns enough to connect our sensory and motor nerves directly to electronic interfaces. Then all the harness hardware could be discarded as obsolete, along with our sense organs and muscles, and indeed most of our body. There would be no “home” experiences to return to, but our remote and virtual existences would be better than ever.\nThe picture is that we are now is a “brain in a vat,” sustained by life-support machinery, and connected by wonderful electronic links, at will, to a series of “rented” artificial bodies at remote locations, or to simulated bodies in artificial realities. But the brain is a biological machine not designed to function forever, even in an optimal physical environment. As it begins to malfunction, might we not choose to use the same advanced neurological electronics that make possible our links to the external world, to replace the gray matter as it begins to fail? Bit by bit our brain is replaced by electronic equivalents, which work at least as well, leaving our personality and thoughts clearer than ever. Eventually everything has been replaced by manufactured parts. No physical vestige of our original body or brain remains, but our thoughts and awareness continue. We will call this process, and other approaches with the same end result, the downloading of a human mind into a machine. After downloading, our personality is a pattern impressed on electronic hardware, and we may then find ways to move our minds to other similar hardware, just as a computer program and its data can be copied from processor to processor. So not only can our sense of awareness shift from place to place at the speed of communication, but the very components of our minds may ride on the same data channels. We might find ourselves distributed over many locations, one piece of our mind here, another piece there, and our sense of awareness at yet another place. Time becomes more flexible – when our mind resides in very fast hardware, one second of real time may provide a subjective year of thinking time, while a thousand years of real time spent on a passive storage medium may seem like no time at all. Can we then consider ourselves to be a mind without a body? Not quite.\nA human totally deprived of bodily senses does not do well. After 12 hours in a sensory deprivation tank (where one floats in a body-temperature saline solution that produces almost no skin sensation, in total darkness and silence, with taste and smell and the sensations of breathing minimized) a subject will begin to hallucinate, as the mind, somewhat like a television tuned to a nonexistent channel, turns up the amplification, desperately looking for a signal, becoming ever less discriminating in the theories it offers to make sense of the random sensory hiss it receives. Even the most extreme telepresence and virtual reality scenarios we have presented avoid complete bodylessness by always providing the mind with a consistent sensory (and motor) image, obtained from an actual remote robot body, or from a computer simulation. In those scenarios, a person may sometimes exist without a physical body, but never without the illusion of having one.\nBut in our computers there are already many entities that resemble truly bodiless minds. A typical computer chess program knows nothing about physical chess pieces or chessboards, or about the staring eyes of its opponent or the bright lights of a tournament. Nor does it work with an internal simulation of those physical attributes. It reasons instead with a very efficient and compact mathematical representation of chess positions and moves. For the benefit of human players this internal representation is sometimes translated to a recognizable graphic on a computer screen, but such images mean nothing to the program that actually chooses the chess moves. For all practical purposes, the chess program’s thoughts and sensations – its - consciousness – is pure chess, with no taint of the physical, or any other, world. Much more than a human mind with a simulated body stored in a computer, a chess program is a mind without a body.\nSo now, imagine a future world where programs that do chess, mathematics, physics, engineering, art, business, or whatever have grown up to become at least as clever as the human mind. Imagine also that most of the inhabited universe has been converted to a computer network – a cyberspace – where such programs live, side by side with downloaded human minds and accompanying simulated human bodies. Suppose that all these entities make their living in something of a free market way, trading the products of their labor for the essentials of life – in this world memory space and computing cycles. Some entities do the equivalent of manual work, converting undeveloped parts of the universe into cyberspace, or improving the - performance of existing patches, thus creating new wealth. Others work on physics or - engineering problems whose solutions give the developers new and better ways to construct computing capacity. Some create programs that can become part of one’s mental capacity. They trade their discoveries and inventions for more working space and time. There are entities that specialize as agents, collecting commissions in return for locating opportunities and negotiating deals for their clients. Others act as banks, storing and redistributing resources, buying and selling computing space, time, and information. Some we might class as artists, creating structures that don’t obviously result in physical resources, but which, for idiosyncratic reasons, are deemed valuable by some customers, and are traded at prices that fluctuate for subjective reasons. Some entities in the cyberworld will fail to produce enough value to support their requirements for existence – these eventually shrink and disappear, or merge with other ventures. Others will succeed and grow. The closest present-day parallel is the growth, evolution, fragmentation, and consolidation of corporations, whose options are shaped primarily by their economic performance.\nA human would likely fare poorly in such a cyberspace. Unlike the streamlined artificial intelligences that zip about, making discoveries and deals, reconfiguring themselves to efficiently handle the data that constitutes their interactions, a human mind would lumber about in a massively inappropriate body simulation, analogous to someone in a deep diving suit plodding along among a troupe of acrobatic dolphins. Every interaction with the data world would first have to be analogized as some recognizable quasi-physical entity: other programs might be presented as animals, plants, or demons, data items as books or treasure chests, accounting entries as coins or gold. Maintaining such fictions increases the cost of doing business, as does operating the mind machinery that reduces the physical simulations into mental abstractions in the downloaded human mind. Though a few humans may find a niche exploiting their baroque construction to produce human-flavored art, more may feel a great economic incentive to streamline their interface to the cyberspace.\nThe streamlining could begin with the elimination of the body-simulation along with the portions of the downloaded mind dedicated to interpreting sense-data. These would be replaced with simpler integrated programs that produced approximately the same net effect in one’s consciousness. One would still view the cyberworld in terms of location, color, smell, faces, and so on, but only those details we actually notice would be represented. We would still be at a disadvantage compared with the true artificial intelligences, who interact with the cyberspace in ways optimized for their tasks. We might then be tempted to replace some of our innermost mental processes with more cyberspace-appropriate programs purchased from the AIs, and so, bit by bit, transform ourselves into something much like them. Ultimately our thinking procedures could be totally liberated from any traces of our original body, indeed of any body. But the bodiless mind that results, wonderful though it may be in its clarity of thought and breadth of understanding, could in no sense be considered any longer human.\nSo, one way or another, the immensities of cyberspace will be teeming with very unhuman disembodied superminds, engaged in affairs of the future that are to human concerns as ours are to those of bacteria. But, once in a long while, humans do think of bacteria, even particular individual bacteria seen in particular microscopes. Similarly, a cyberbeing may occasionally bring to mind a human event of the distant past. If a sufficiently powerful mind makes a sufficiently large effort, such recall could occur with great detail – call it high fidelity. With enough fidelity, the situation of a remembered person, along with all the minutiae of her body, her thoughts, and feelings, would be perfectly recreated in a kind of mental simulation: a cyberspace within a cyberspace where the person would be as alive as anywhere. Sometimes the recall might be historically accurate; in other circumstances it could be artistically enhanced: it depends on the purposes of the cybermind. An evolving cyberspace becomes effectively ever more capacious and long lasting, and so can support ever more minds of ever greater power. If these minds spend only an infinitesimal fraction of their energy contemplating the human past, their sheer power should ensure that eventually our entire history is replayed many times in many places, and in many variations. The very moment we are now experiencing may actually be (almost certainly is) such a distributed mental event, and most likely is a complete fabrication that never happened physically. Alas, there is no way to sort it out from our perspective: we can only wallow in the scenery."
  },
  {
    "objectID": "docs/posts/1992-hans-moravec/index.html#discussion-notes",
    "href": "docs/posts/1992-hans-moravec/index.html#discussion-notes",
    "title": "Pigs in Cyberspace",
    "section": "Discussion Notes",
    "text": "Discussion Notes\nSpeaker: Hans Moravec, Carnegie Mellon University\nNote Taker: Marc G. Millis, NASA Lewis Research Center\n\nPremise\nBased on the extrapolation that computers and robots will eventually become more intelligent than their human creators (predicted to occur 2030-2050), this workshop examined the possible impact this would have on humanity. Vinge had referred to this point as a “Technological Singularity” during his March 30th presentation.\nThe big question is what happens to the universe and humanity if we create something more intelligent and thus more capable than ourselves? Does humanity survive? Do we become mere pets or mere livestock for these new cyber entities?\nHans Moravec presented his views on this question (see preceding text section), derived from his own assumptions. The audience freely entered the discussion by challenging Moravec’s assumptions and by proposing scenarios of their own.\nOne of Moravec’s underlying assumptions was that these synthetic entities would retain a sense of Darwinistic competitiveness: survival of the fittest. This competitive drive is thought to be a residual from their human origins: machines designed to be superior to insure market dominance. With this animalistic instinct retained, these entities would compete for dominance and would eventually expand their influence across space and over all other entities, including humans.\nIt is assumed that the cyber entities are initially robots who expand their physical existence over space until they start running into themselves. At that point they begin to merge into a kind of collective entity and turn their expansion inward; increasing their resolution, becoming finer and finer (i.e. more and more stuff packed into a given volume). There was discussion about whether these cyber entities would remain individuals or would they merge into one homogeneous, networked entity– i.e. one giant thought process.\n\n\nAnd what about the humans?\nIf humanity is not exterminated in the course of cyber expansionism, presumably because the cyber entities have compassionately contemplated their origins, then what would happen to humanity? What would human life be like?\nOne scenario Moravec conceived is where the cyber entities make a deal with humanity so that they can use the raw materials of Earth (entirely, including humans) for their own purposes. In exchange they would provide humans with an “improved” synthetic environment for humans to live in. This means “downloading” the human mind (soul?) into some cyberspace media. Moravec continues to postulate that for humans to survive in this form (assuming Darwinian instincts still hold within the cyberspace) they would have to shed their overhead of processing that converts sensory inputs into thought and thought back into motor-outputs. Humans would have to be in direct thought-link with the cyberspace in order to compete for survival. Because these sensory-to-thought layers are the very boarders that enable humans to retain their individual essence, Moravec concluded that humans cannot exist in such an environment.\nAs an alternative, Moravec suggested that there would be pockets of humanity and other life forms dispersed through the universe dominated by cyber entities. Humans in cyberspace would be analogous to the Muppets “Pigs in Space”. Even if humans were reduced to simulations, it is likely that there would be pockets of these simulations running independently. There was some playful conjecture as to the possibility that this was happening now.\n\n\nOther points\nThere was much discussion about the validity of these assumptions and about other scenarios not considered. Would Darwinistic competitive instincts be retained in entities whose intelligence is beyond human comprehension? Would Darwinism survival instincts be retained in entities that are practically immortal? Is there some limit to the “intelligence” of an entity, even a collective entity. Consider, for example, the cliche: two heads are better than one, but a lot makes a bureaucracy. Could cyber entities equally digress into a bureaucracy where inter-entity communication and extra layers of complexity bog down the purpose of the collective? Another scenario discussed was the possibility that cyber entities would become addicted to self-induced synthetic “pleasures.” Would this render them externally benign? This is analogous to what might happen if humans had the capability to render themselves happy at will. Would a human that is willfully self-engrossed in bliss neglect its biological needs and die; albeit happily? There was also much discussion about the borders between cyberspace individuals and collectives and the resulting blur on the definition of life and death in cyberspace. For example, is murder the same in a universe where back-up copies of your soul exist?\nIn summary, there was much philosophical discussion about humanity and existence given the context of cyberspace. These provocative discussions gave us a better look at what it means to be a human as well as contemplating the possibilities of independent machine intelligence."
  },
  {
    "objectID": "docs/posts/1992-hans-moravec/index.html#appendix-metadata",
    "href": "docs/posts/1992-hans-moravec/index.html#appendix-metadata",
    "title": "Pigs in Cyberspace",
    "section": "Appendix: Metadata",
    "text": "Appendix: Metadata\nThis essay has been published multiple times. I am aware of the following:\n\nThinking Robots, an Aware Internet, and Cyberpunk Librarians: The 1992 LITA President’s Program: Presentations by Hans Moravec, Bruce Sterling, and David Brin. Edited by R. Bruce Miller and Milton T. Wolf. 1992.\n\nThe editors were from the Library and Information Technology Association (LITA) Imagineering Interest Group (IIG)\n\nNASA technical report N94-27376. From the Conference of Lewis Research Center, Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace.\n\nNotably, Vernor Vinge (1993) was also presented in this conference.\n\nExtropy: The Journal of Transhumanist Thought 10 (1993), pp. 5–7.\nMore, Max, and Natasha Vita-More, eds. The transhumanist reader: Classical and contemporary essays on the science, technology, and philosophy of the human future. John Wiley & Sons, 2013."
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html",
    "href": "docs/posts/1985-ray-solomonoff/index.html",
    "title": "The time scale of artificial intelligence",
    "section": "",
    "text": "Other than adding in the subsection titles, I preserved the original text.\n\n\nMilestones:\n\n1956: Milestone A (Dartmouth Summer Study Group).\n1985: (the point at which this article was written)\n1986–2035: Milestone B (a general theory of problem solving in AI), 1 to 50 years after Milestone A.\n1991–2045: Milestones C (self-improving AI) and D (English comprehension) after Milestone B, 5 to 10 years after Milestone B.\n2005: Hardware cost of a Milestone E machine to be \\(\\sim 10^5\\) USD.\n2001–2055: Milestone E (human-level AI in specific fields), “a few years” (10 years?) after Milestones C and D.\n2011–2065: Milestone F (same number of human-level AI as human computer scientists), 10 years after achieving Milestone E.\n\nSolomonoff assumed that the computer scientist community has 10,000 people.\n\n2022–2076: Milestone G (AI greatly surpassing human scientific community), 11 years after Milestone F.\n\nIt is modelled as the point where 10,000 human-brain-equivalent machines cost 100 million dollars, at an investment rate of 10 million dollars per year.\nThe idea of Milestone G is that, assuming Moore’s law (the growth rate of FLOP per second per USD) is proportional to the number of computer scientists, then after AI computer scientists occur, the growth rate becomes hyperbolic, and it diverges to infinity in 11 years.\n\n\nCompute cost\n\n4 years: Moore’s law, i.e. the time it takes to double FLOP per second per USD.\n10 million USD: Estimated cost of 1 human-brain-equivalent machine in 1985.\n100 million USD: Projected cost of 10,000 human-brain-equivalent machines in 2025."
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html#transcribers-notes",
    "href": "docs/posts/1985-ray-solomonoff/index.html#transcribers-notes",
    "title": "The time scale of artificial intelligence",
    "section": "",
    "text": "Other than adding in the subsection titles, I preserved the original text.\n\n\nMilestones:\n\n1956: Milestone A (Dartmouth Summer Study Group).\n1985: (the point at which this article was written)\n1986–2035: Milestone B (a general theory of problem solving in AI), 1 to 50 years after Milestone A.\n1991–2045: Milestones C (self-improving AI) and D (English comprehension) after Milestone B, 5 to 10 years after Milestone B.\n2005: Hardware cost of a Milestone E machine to be \\(\\sim 10^5\\) USD.\n2001–2055: Milestone E (human-level AI in specific fields), “a few years” (10 years?) after Milestones C and D.\n2011–2065: Milestone F (same number of human-level AI as human computer scientists), 10 years after achieving Milestone E.\n\nSolomonoff assumed that the computer scientist community has 10,000 people.\n\n2022–2076: Milestone G (AI greatly surpassing human scientific community), 11 years after Milestone F.\n\nIt is modelled as the point where 10,000 human-brain-equivalent machines cost 100 million dollars, at an investment rate of 10 million dollars per year.\nThe idea of Milestone G is that, assuming Moore’s law (the growth rate of FLOP per second per USD) is proportional to the number of computer scientists, then after AI computer scientists occur, the growth rate becomes hyperbolic, and it diverges to infinity in 11 years.\n\n\nCompute cost\n\n4 years: Moore’s law, i.e. the time it takes to double FLOP per second per USD.\n10 million USD: Estimated cost of 1 human-brain-equivalent machine in 1985.\n100 million USD: Projected cost of 10,000 human-brain-equivalent machines in 2025."
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html#abstract",
    "href": "docs/posts/1985-ray-solomonoff/index.html#abstract",
    "title": "The time scale of artificial intelligence",
    "section": "Abstract",
    "text": "Abstract\nSix future milestones in AI are discussed. These range from the development of a very general theory of problem solving to the creation of machines with capacities well beyond those of a single human. Estimates are made for when these milestones will occur, followed by some suggestions for the more effective utilization of the extremely rapid technological growth that is expected.\nKeywords: Artificial intelligence, social effects, future developments."
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html#essay",
    "href": "docs/posts/1985-ray-solomonoff/index.html#essay",
    "title": "The time scale of artificial intelligence",
    "section": "Essay",
    "text": "Essay\nI will first give a brief discussion of recent developments in AI, and then a description of the expected future milestones with estimates of when they will occur and some expected social effects.\nMilestone A. The ‘modern’ phase of AI can be regarded as beginning in 1956 at the Dartmouth Summer Study Group on Artificial Intelligence. At that time many people in this field came from all over to talk about what they were doing and what they expected to do. It marked the beginning of the much accelerated work in this area.\nOne of the earliest developments was the ‘General Problem Solver’ of Newell and Simon – a first attempt at a general theory of AI. From this beginning, they moved on to study human problem solving – which developed into what is now called ‘cognitive psychology’. Parallel with this work, was the development of ‘expert systems’, which depend not so much on general principles, but on knowledge of many facts in a particular field. Public awareness of these expert systems has grown rapidly in recent years1985, as has the fraction of AI manpower devoted to them.\nAnother very relevant development has been the study of large parallel computers with novel architectures. These studies are important because, first, the human brain is a very large parallel computer and the design and study of computers of this type can give the needed insight on how the human brain works. Another point is that the information processing capacity of present-day serial machines seems to be less than that of the human brain. If we are to emulate its behavior, we must have at least its computing capacity, and large parallel machines are most certainly the least expensive way to do this.\nMilestone B. The next milestone in the development of AI might be a general theory of problem solving. Here ‘problem solving’ is to be understood in a very general sense, and includes processes which, if they were performed by a human, would be regarded as ‘creative’ or ‘insightful’.\nSome areas that would have to be covered by such a theory are:\n\nLearning: based both on input data, and the machine’s own experience in problem solving;\nDevising and testing new concepts to be used in solving problems;\nTaking in information and storing it in a manner useful for problem solving; and\nMethods of implementation on existing computers and/or the design of new kinds of computers that would be needed.\n\nMilestone C. A critical point in AI development would be a machine that could usefully work on the problem of self-improvement. Newell and Simon were not successful in their attempts to get their ‘General Problem Solver’ to improve it’s own methods of operation. While Lenat‘s’Eurisko’ has been successful in several problem areas, he has not been able to get it to devise good heuristics for itself. He is, however, optimistic about the progress that has been made and is continuing this work.\nMilestone D. Another milestone will be a computer that can read almost any English text and incorporate most of the material into its data base – just as a human does. It would have to store the information in a form that is useful for solving whatever kinds of problems it is normally given.\nSince there is an enormous amount of information available in electronic data bases all over the world, a machine with useful access to this information could grow very rapidly in its ability to solve problems and in a real sense in its understanding of the world.\nMilestone E will be a machine that has a general problem solving capacity near that of a human, in the areas for which it has been designed presumably in mathematics, science and industrial applications.\nMilestone F will be a machine with a capacity near that of the computer science community.\nMilestone G will be a machine with a capacity many times that of the computer science community.\n\nTheory of AGI\nCan we estimate when these milestones will occur?\nFor Milestone B – a general theory for AI – I feel that anything between 1 and 50 years is possible, with 2 to 25 years being much more likely. At present, there are too few people in AI working on theories of this sort. To aggravate the problem, recent commercial success of AI – mainly expert systems – has lured many bright graduate students away from general theory, to work on industrial applications.\nSome promising work on general theory at the present time are: Lenat’s work, which I’ve mentioned 1; and Bradshaw, Langley and Simon on how scientists discover scientific laws 2.\n1 Lenat, D. and J. Brown, Why AM and Eurisko appear to work, in: Proceedings Nat. Conf. on AI, 22-26 August (1983) 236–240.2 Bradshaw, G., P. Langley and H. Simon, Studying scientific discovery by computer simulation, Science 222, 4627 (December 1983) 971–974.Neither of these are explicit attempts at a general theory of intelligence, but they work on problems in ways that are readily generalized.\nSome more direct work on general theory are: Minsky’s work ‘The Society of Mind’ is an attempt to describe the operation of the human brain in terms of a large number of small problem solvers working parallel with relatively infrequent intercommunication 3; and my own work on training sequences, problem solving and learning 4.\n3 Minsky, M., The society of mind, forthcoming.4 Solomonoff, R., Perfect training sequences (Oxbridge Research, Cambridge, MA, 1982).It may be possible to get something that is superficially like Milestone E without a general theory. The current Japanese ‘5 th generation computer’ project attempts to program a large number of ‘expert systems’ and put them all in a very large, very fast computer. Though expert systems all try to simulate parts of the human conscious mind, many of the more interesting human activities are mainly performed by the unconscious mind. If the unconscious mind works very much like the conscious mind (but we are merely less aware of its workings), then there is no difficulty here. However, if as is widely suspected, the unconscious mind is significantly different from the conscious – then the present expansion of expert systems will have serious limitations.\nIt is not necessary to know just how the unconscious mind works in order to emulate it – but slavishly imitating the workings of human consciousness would seem to be a poor approach.\n\n\nUnderstanding natural language\nMilestone D – understanding English – is being approached from several directions.\nOne is the study of ethnic languages, their grammars and semantics.\nA somewhat different approach has been developed within the AI-community, in which machines are programmed to respond to commands or questions in English. The emphasis is on whether the program responds in the desired way, not on whether it ‘understands’ the input in terms of traditional grammatical and semantic concepts.\nA third approach is through learning. The machine is taught English starting with very simple sentences. After it has learned to respond to them properly, it is given somewhat more complex sentences – just as a child learns language.\nMost likely these three methods should be combined to obtain a system that acquires most rapidly, an understanding of English. The learning component is, I think, essential. The meaning of words and phrases vary considerably with context sometimes grossly, other times subtly. Programming all of these nuances into a machine would seem to be too arduous a task to be done well by a human. It would be far better and less subject to error, if the machine learned as humans do, how the larger context of a phrase controls its meaning.\n\n\nThe first AGI\nWhen can we achieve Milestone E? Milestone B seems to be the most critical bottleneck. From that point to achieving both C and D might be as little as five or ten years, and from there to milestone E , only a few years more.\nLet us examine the significance of Milestone E. At such a time we would have a machine with the problem-solving capability of a human, in several fields. For reasons that will become clear later, we will at first want to emphasize mathematics and science – computer science in particular.\nTwenty years from now, the hardware cost of such a machine might be as little as several hundred thousand dollars, and it will be halved every four years or so.\nAt this rate, artificial intelligence will eventually cost less then human intelligence. Note that while the cost of training a very intelligent machine is very large, the cost of training the next one is very small, since the information in memory can be rapidly transferred from one machine to another.\nThe most important features of very intelligent machines are not related to their cost however. Machines of this sort are able to do things far beyond the capabilities of humans or groups of humans. For example, they can be designed to process information from many modalities very rapidly – optical, radar, sound, radio, telephone, etc. As our machines become faster, such processing would become invaluable in weather prediction or the administration and control of very large projects, such as space programs, the construction of ever-larger computers, and providing food and shelter for billions of people.\n\n\nGrowth economics of the AGI society\nSome of the most critical capabilities of very intelligent machines depend on their being much more intelligent than humans. How long will it take to go to Milestone F and then to G?\nThe number of creative scientists and engineers that are responsible for the advancement of computer science, are at most several thousand. After we have reached Milestone E, it shouldn’t take much more than ten years to construct ten thousand duplicates of our original ‘Milestone E’ machine, and have a total computing capability close to that of the computer science community. The ten year figure seems reasonable when one notes that the cost of these machines will keep halving every four years or so, and also that the new ‘artificial’ computer scientists will help speed the construction of the new machines.\nWhile there is normally an exponential decrease in computing cost with time (halving every four years or so), when the artificial intelligence community is as large as the human scientific community, the halving time itself will halve, so we get halving in two years instead of four.\nSuppose \\(c\\) is the size of our computer science community at time \\(t\\). We define this to be 1 at time zero. \\(R\\) is the rate at which we expend money on our AI computers to effectively increase the size of our computer science community. \\(t\\) is the time in years, from our origin point. \\(x\\) is the amount of computing power we get for a dollar at a particular time. We will set \\(x=1\\) at \\(t=0\\). First,\n\\[\\mathrm{d} c / \\mathrm{d} t=R x.\\]\nThe rate of increase of our (partly artificial) computer science community is the product of our rate of expenditure times the efficiency of that expenditure. Next\n\\[\\mathrm{d} \\ln x / \\mathrm{d} t=A c.\\]\nThis says that the rate of change of the log of our efficiency is proportional to the size of our computer science community. If \\(c\\) were to be kept constant at 1 , then we would want eq. (2) to give a doubling of \\(x\\) every four years. This gives \\(A=\\) (In 2) \\(/ 4=0.1733\\). With conditions \\(c=1\\) and \\(x=1\\) at \\(t=0\\), we obtain from (1) and (2)\n\\[\n\\mathrm{d} c / \\mathrm{d} t=A\\left(c^2-1\\right) / 2+R\n\\]\nThis equation has the property that for any positive value of \\(R\\), the value of \\(c\\) will at some finite time \\(t=T\\), approach infinity.\n\\[\n\\begin{aligned}\n\\text{For }R=1, &T=4.62\\text{ years},\\\\\n\\text{For }R=0.1, &T=11.11\\text{ years},\\\\\n\\text{For }R=0.01, &T=21.51\\text{ years},\\\\\n\\end{aligned}\n\\]\nA value of \\(R=1\\) means that if we kept \\(x\\) constant at 1, at the end of one year we would have invested enough in our AI computer to equal its capacity to that of the human computer science community.\nUsually, when infinities like this one occur in science, they indicate a breakdown of the validity of the equations as we approach the infinity point. The critical part of the equations appears to be continued exponential decrease in computation cost. So far, this rate of improvement has been possible only because of radically new technologies that were introduced – i.e. first vacuum tubes, then transistors, then integrated circuits, and then large scale integrated circuits. There appear to be several new technologies on the horizon that are adequate for maintaining the progress for several more orders of magnitude – as for the technologies over the horizon that have not yet been discovered, we only have a faith based on performance of the past.\nA decrease in computation cost by a factor of 1000 would, at the present rate of progress, take about 40 years. At the present time, a reasonable guess at the cost of hardware with the computing power of a human brain might be ten million dollars. Ten thousand of such machines would cost 100 billion dollars now, and 100 million dollars forty years from now. This 100 million would put us at \\(t=0\\) for eq. (3). At a continued expenditure of ten million dollars a year, it would take about 11 more years to get to the ‘infinity point’. Though infinity is a bit high, it seems very likely that we could achieve a growth factor of at least 100 in these 11 years – and so we reach Milestone G.\n\n\nEffects of a large AGI society upon human society\nWhat would be the effect of a scientific community equivalent that is 100 times as large as what we have now?\nThe last 100 years have seen the introduction of special and general relativity, automobiles, airplanes, quantum mechanics, large rockets and space travel, fission power, fusion bombs, lasers, and large digital computers. Any one of these might take a person years to appreciate and understand. Suppose that they had all been presented to mankind in a single year! This is the magnitude of ‘future shock’ that we can expect from our AI-expanded scientific community.\nIn the past, introduction of a new technology into the culture has usually been rather slow, so we had time to develop some understanding of its effect on us, to adjust the technology and culture for an optimum ‘coming together’. Even with a slow introduction, our use of a new technology has sometimes been very poor.\nThe use of nuclear energy for military purposes has been expensive, difficult to control and has obtained us neither military goals nor security of any sort. Nuclear energy for power generation in the United States, has cost much more than expected. In both cases we have had many years to consider how to use this technology best – yet, perhaps because of the difficulties of the problems involved, we have not done very well. We have spent enormous amounts of money and manpower and have attained relatively little of value.\nCan we use very intelligent machines to help us solve the problems associated with the surfeit of new technologies of the future?\nThere appear to be at least two ways to do this.\nFirst, attainment of Milestone B is likely to give us a much better understanding of the human mind than we have ever had. We should be able to get our intelligent machines to explain each new technology in a way that is intelligible to man. If this can’t be done, and the new technology is essentially un-understandable to man, then man would be foolish indeed to use it in any way!\nHowever, understanding does not always assure success in dealing with very complex problems. Mankind will continue to have to make decisions under conditions of uncertainty. In the past he has usually chosen his courses of action relatively blindly – controlled more by his own perceived wants and needs than by considerations of the likelihoods of alternative possible futures and their effects upon him.\nIn this area, very intelligent machines can help us in one very important way – they can predict the results of social action.\nNormally, there are several limitations, both theoretical and practical on our ability to predict the future accurately. These limitations are:\n\nThe models we use for prediction are not the best possible, and we are unable to find better ones;\nWe have a limited computing capacity and have already used all of it;\nPredictions can be self-modifying: we can make the prediction, but as soon as we make it public, this brings about conditions that invalidate it;\nQuantum mechanical limitation on prediction. This is to some extent similar to (3); and\nSelectively feeding data to our predictor so that it obtains the result we think we want, or by otherwise biasing the prediction process.\n\nDifficulties 1 and 2 are both inherent in the nature of all real-world predictions. No matter how long we search for good models of our system, there is always the possibility that if we looked a little longer we would find a much better model. At Milestone G we will have much better models than we have now, as well as a much greater computing capacity for applying them.\nDifficulty 3 is very important in predicting social action. If the prediction is self-denying, then there may exist no public prediction that is correct. If it is self-conforming, there may be several different predictions that can be made – any of which would be correct if made public. For example, one prediction might be that many people would be hurt and another might be that no-one was hurt. Under these conditions we would want to give the predictor ‘ethical guidelines’ upon which to make a choice, or have a human intermediary decide what prediction to make public.\nDifficulty 5 is less of a problem if the machine has independent access to all available information. However, it is often possible for a human to inadvertently define a question so that the reply must be badly biased.\nWithin these necessary limitations we will be able to obtain much better predictions than ever before. And, as before, it is not certain that even this capability will be used wisely.\nWhat seems most certain is that the future of man – both scientific and social – will be far more exciting than the wildest eras of the past."
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html#references",
    "href": "docs/posts/1985-ray-solomonoff/index.html#references",
    "title": "The time scale of artificial intelligence",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html#afterthoughts-on-the-time-scale-of-a.i.",
    "href": "docs/posts/1985-ray-solomonoff/index.html#afterthoughts-on-the-time-scale-of-a.i.",
    "title": "The time scale of artificial intelligence",
    "section": "Afterthoughts on the time scale of A.I.",
    "text": "Afterthoughts on the time scale of A.I.\n“Better to have two experts in the same head than two in the same room.”\nOften, after final corrections have been made on a paper, when it is no longer possible to make changes in it, one has second thoughts on some of the theorems or conjectures that were made.\nIn the present case, I have two afterthoughts which, to some extent have effects that cancel each other.\nMy estimate for the present day hardware cost of a machine having the information processing capacity of a human, may have been low by a factor of anywhere from 3 to 10.\nOn the other hand, I feel that my estimate of the machine size necessary to emulate the community of computer scientists may have been high by a factor of at least the same amount and probably much more.\nThe reason is that multiplying the information processing capacity of a machine by \\(n\\) increases its scientific productivity by a factor of much more than \\(n\\). In the case of humans, two scientists, working more or less independently, will have about twice the productivity of one.\nIf, however, these scientists are able to communicate very well, the productivity is far greater. Note that this communication is largely verbal via channels of very small capacity. They have very poor access to non-verbal concepts used in each others “internal language”.\nIn a large intelligent machine having many times single human information processing capacity, we expect that all parts of the machine will have very rapid access to each other, in all conceivable detail. This facilitates cooperation between these parts that is considerably more productive than communication between individual humans."
  },
  {
    "objectID": "docs/posts/1985-ray-solomonoff/index.html#appendix-metadata",
    "href": "docs/posts/1985-ray-solomonoff/index.html#appendix-metadata",
    "title": "The time scale of artificial intelligence",
    "section": "Appendix: metadata",
    "text": "Appendix: metadata\nIssue title: Artificial Intelligence\n\nGuest editors: R.K. Lindsay\n\nArticle type: Other\n\nAuthors: Solomonoff, R.J.\n\nAffiliations: Oxbridge Research, P.O. Box 559, Cambridge, MA 02238, USA\n\nKeywords: Artificial intelligence, social effects, future developments\n\nDOI: 10.3233/HSM-1985-5207\n\nJournal: Human Systems Management, vol. 5, no. 2, pp. 149-153, 1985\n\nPublished: 1 June 1985\nThe original paper has the following description of Solomonoff:\n\nR. Solomonoff was graduated from the University of Chicago in 1951 with a degree in Physics. Since that time he has mainly been working on the mechanization of inductive inference – the most successful approach being algorithmic complexity theory. He has extended this theory to include the optimization of both hardware and software for general problem solving. He is now a principal scientist at Oxbridge Research, Cambridge, MA."
  },
  {
    "objectID": "docs/posts/1975-herbert-simon-allen-newell/index.html",
    "href": "docs/posts/1975-herbert-simon-allen-newell/index.html",
    "title": "Computer Science as Empirical Inquiry: Symbols and Search",
    "section": "",
    "text": "The 1975 ACM Turing Award was presented jointly to Allen Newell and Herbert A. Simon at the ACM Annual Conference in Minneapolis, October 20. In introducing the recipients, Bernard A. Galler, Chairman of the Turing Award Committee, read the following citation:\n“It is a privilege to be able to present the ACM Turing Award to two friends of long standing, Professors Allen Newell and Herbert A. Simon, both of Carnegie-Mellon University.\n“In joint scientific efforts extending over twenty years, initially in collaboration with l.C. Shaw at the RAND Corporation, and subsequently with numerous faculty and student colleague’s at Carnegie-Mellon University, they have made basic contributions to artificial intelligence, the psychology of human cognition, and list processing.\n“In artificial intelligence, they contributed to the establishment of the field as an area of scientific endeavor, to the development of heuristic programming generally, and of heuristic search, means ends analysis, and methods of induction, in particular; providing demonstrations of the sufficiency of these mechanisms to solve interesting problems.\n“In psychology, they were principal instigators of the idea that human cognition can be described in terms of a symbol system, and they have developed detailed theories for human problem solving verbal learning and inductive behavior in a number of task domains, using computer programs embodying these theories to simulate the human behavior.\n“They were apparently the inventors of list processing, and have been major contributors to both software technology and the development of the concept of the computer as a system of manipulating symbolic structures and not just as a processor of numerical data.\n“It is an honor for Professors Newell and Simon to be given this award, but it is also an honor for ACM to be able to add their names to our list of recipients, since by their presence, they will add to the prestige and importance of the ACM Turing Award.”"
  },
  {
    "objectID": "docs/posts/1975-herbert-simon-allen-newell/index.html#preface",
    "href": "docs/posts/1975-herbert-simon-allen-newell/index.html#preface",
    "title": "Computer Science as Empirical Inquiry: Symbols and Search",
    "section": "",
    "text": "The 1975 ACM Turing Award was presented jointly to Allen Newell and Herbert A. Simon at the ACM Annual Conference in Minneapolis, October 20. In introducing the recipients, Bernard A. Galler, Chairman of the Turing Award Committee, read the following citation:\n“It is a privilege to be able to present the ACM Turing Award to two friends of long standing, Professors Allen Newell and Herbert A. Simon, both of Carnegie-Mellon University.\n“In joint scientific efforts extending over twenty years, initially in collaboration with l.C. Shaw at the RAND Corporation, and subsequently with numerous faculty and student colleague’s at Carnegie-Mellon University, they have made basic contributions to artificial intelligence, the psychology of human cognition, and list processing.\n“In artificial intelligence, they contributed to the establishment of the field as an area of scientific endeavor, to the development of heuristic programming generally, and of heuristic search, means ends analysis, and methods of induction, in particular; providing demonstrations of the sufficiency of these mechanisms to solve interesting problems.\n“In psychology, they were principal instigators of the idea that human cognition can be described in terms of a symbol system, and they have developed detailed theories for human problem solving verbal learning and inductive behavior in a number of task domains, using computer programs embodying these theories to simulate the human behavior.\n“They were apparently the inventors of list processing, and have been major contributors to both software technology and the development of the concept of the computer as a system of manipulating symbolic structures and not just as a processor of numerical data.\n“It is an honor for Professors Newell and Simon to be given this award, but it is also an honor for ACM to be able to add their names to our list of recipients, since by their presence, they will add to the prestige and importance of the ACM Turing Award.”"
  },
  {
    "objectID": "docs/posts/1975-herbert-simon-allen-newell/index.html#the-lecture",
    "href": "docs/posts/1975-herbert-simon-allen-newell/index.html#the-lecture",
    "title": "Computer Science as Empirical Inquiry: Symbols and Search",
    "section": "The lecture",
    "text": "The lecture\nComputer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine-not just the hardware, but the programmed, living machine-is the organism we study.\nThis is the tenth Turing Lecture. The nine persons who preceded us on this platform have presented nine different views of computer science. For our organism, the machine, can be studied at many levels and from many sides. We are deeply honored to appear here today and to present yet another view, the one that has permeated the scientific work for which we have been cited. We wish to speak of computer science as empirical inquiry.\nOur view, is only one of many; the previous lectures make that clear. However, even taken together the lectures fail to cover the whole scope of our science. Many fundamental aspects of it have not been represented in these ten awards. And if the time ever arrives, surely not soon, when the compass has been boxed, when computer science has been discussed from every side, it will be time to start the cycle again. For the hare as lecturer will have to make an annual sprint to overtake the cumulation of small. incremental gains that the tortoise of scientific and technical development has achieved in his steady march. Each year will create a new gap and call for a new sprint, for in science there is no final word.\nComputer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. None the less. they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. Each new program that is built is an experiment. It poses a question to nature, and its behavior offers clues to an answer. Neither machines nor programs are black boxes; they are artifacts that have been designed, both hardware and software, and we can open them up and look inside. We can relate their structure to their behavior and draw many lessons from a single experiment. We don’t have to build 100 copies of, say, a theorem prover, to demonstrate statistically that it has not overcome the combinatorial explosion of search in the way hoped for. Inspection of the program in the light of a few runs reveals the flaw and lets us proceed to the next attempt.\nWe build computers and programs for many reasons. We build them to serve society and as tools for carrying out the economic tasks of society. But as basic scientists we build machines and programs as a way of discovering new phenomena and analyzing phenomena we already know about. Society often becomes confused about this, believing that computers and programs are to be constructed only for the economic use that can be made of them (or as intermediate items in a developmental sequence leading to such use). It needs to understand that the phenomena surrounding computers are deep and obscure, requiring much experimentation to assess their nature. It needs to understand that, as in any science, the gains that accrue from such experimentation and understanding pay off in the permanent acquisition of new techniques; and that it is these techniques that will create the instruments to help society in achieving its goals.\nOur purpose here, however, is not to plead for understanding from an outside world. It is to examine one aspect of our science. the development of new basic understanding by empirical inquiry. This is best done by illustrations. We will be pardoned if, presuming upon the occasion, we choose our examples from the area of our own research. As will become apparent, these examples involve the whole development of artificial intelligence, especially in its early years. They rest on much more than our own personal contributions. And even where we have made direct contributions, this has been done in cooperation with others. Our collaborators have included especially Cliff Shaw, with whom we formed a team of three through the exciting period of the late fifties. But we have also worked with a great many colleagues and students at Carnegie-Mellon University.\nTime permits taking up just two examples. The first is the development of the notion of a symbolic system. The second is the development of the notion of heuristic search. Both conceptions have deep significance for understanding how information is processed and how intelligence is achieved. However, they do not come close to exhausting the full scope of artificial intelligence, though they seem to us to be useful for exhibiting the nature of fundamental knowledge in this part of computer science."
  },
  {
    "objectID": "docs/posts/1975-herbert-simon-allen-newell/index.html#i.-symbols-and-physical-symbol-systems",
    "href": "docs/posts/1975-herbert-simon-allen-newell/index.html#i.-symbols-and-physical-symbol-systems",
    "title": "Computer Science as Empirical Inquiry: Symbols and Search",
    "section": "I. Symbols and Physical Symbol Systems",
    "text": "I. Symbols and Physical Symbol Systems\nOne of the fundamental contributions to knowledge of computer science has been to explain, at a rather basic level, what symbols are. This explanation is a scientific proposition about Nature. It is empirically derived, with a long and gradual development.\nSymbols lie at the root of intelligent action, which is, of course, the primary topic of artificial intelligence. For that matter, it is a primary question for all of computer science. For all information is processed by computers in the service of ends and we measure the intelligence of a system by its ability to achieve stated ends in the face of variations, difficulties and complexities posed by the task environment. This general investment of computer science in attaining intelligence is obscured when the tasks being accomplished are limited in scope, for then the full variations in the environment can be accurately foreseen. It becomes more obvious as we extend computers to more global, complex and knowledge-intensive tasks­as we attempt to make them our agents, capable of handling on their own the full contingencies of the natural world.\nOur understanding of the systems requirements for intelligent action emerges slowly. It is composite, for no single elementary thing accounts for intelligence in all its manifestations. There is no “intelligence principle,” just as there is no “vital principle” that conveys by its very nature the essence of life. But the lack of a simple deus ex machina does not imply that there are no structural requirements for intelligence. One such requirement is the ability to store and manipulate symbols. To put the scientific question, we may paraphrase the title of a famous paper by Warren McCulloch [1961]: What is a symbol, that intelligence may use it, and intelligence, that it may use a symbol?\n\nLaws of Qualitative Structure\nAll sciences characterize the essential nature of the systems they study. These characterizations are invariably qualitative in nature, for they set the terms within which more detailed knowledge can be developed. Their essence can often be captured in very short, very general statements. One might judge these general laws, due to their limited specificity, as making relatively little contribution to the sum of a science, were it not for the historical evidence that shows them to be results of the greatest importance.\n\nThe Cell Doctrine in Biology\nA good example of a law of qualitative structure is the ceil doctrine in biology, which states that the basic building block of all living organisms is the cell. Cells come in a large variety of forms, though they all have a nucleus surrounded by protoplasm, the whole encased by a membrane. But this internal structure was not, historically, part of the specification of the cell doctrine; it was subsequent specificity developed by intensive investigation. The cell doctrine can be conveyed almost entirely by the statement we gave above, along with some vague notions about what size a cell can be. The impact of this law on biology, however, has been tremendous, and the lost motion in the field prior to its gradual acceptance was considerable.\n\n\nPlate Tectonics in Geology\nGeology provides an interesting example of a qualitative structure law, interesting because it has gained acceptance in the last decade and so its rise in status is still fresh in memory. The theory of plate tectonics asserts that the surface of the globe is a collection of huge plates­a few dozen in all­which move (at geological speeds) against, over, and under each other into the center of the earth, where they lose their identity. The movements of the plates account for the shapes and relative locations of the continents and oceans, for the areas of volcanic and earthquake activity, for the deep sea ridges, and so on. With a few additional particulars as to speed and size, the essential theory has been specified. It was of course not accepted until it succeeded in explaining a number of details, all of which hung together (e.g. accounting for flora, fauna, and stratification agreements between West Africa and Northeast South America). The plate tectonics theory is highly qualitative. Now that it is accepted, the whole earth seems to offer evidence for it everywhere, for we see the world in its terms.\n\n\nThe Germ Theory of Disease\nIt is little more than a century since Pasteur enunciated the germ theory of disease, a law of qualitative structure that produced a revolution in medicine. The theory proposes that most diseases are caused by the presence and multiplication in the body of tiny single-celled living organisms, and that contagion consists in the transmission of these organisms from one host to another. A large part of the elaboration of the theory consisted in identifying the organisms associated with specific diseases, describing them, and tracing their life histories. The fact that the law has many exceptions – ­that many diseases are not produced by germs – ­does not detract from its importance. The law tells us to look for a particular kind of cause; it does not insist that we will always find it.\n\n\nThe Doctrine of Atomism\nThe doctrine of atomism offers an interesting contrast to the three laws of qualitative structure we have just described. As it emerged from the work of Dalton and his demonstrations that the chemicals combined in fixed proportions, the law provided a typical example of qualitative structure: the elements are composed of small, uniform particles, differing from one element to another. But because the underlying species of atoms are so simple and limited in their variety, quantitative theories were soon formulated which assimilated all the general structure in the original qualitative hypothesis. With cells, tectonic plates, and germs, the variety of structure is so great that the underlying qualitative principle remains distinct, and its contribution to the total theory clearly discernible.\n\n\nConclusion\nLaws of qualitative structure are seen everywhere in science. Some of our greatest scientific discoveries are to be found among them. As the examples illustrate, they often set the terms on which a whole science operates.\n\n\n\nPhysical Symbol Systems\nLet us return to the topic of symbols, and define a physical symbol system. The adjective “physical” denotes two important features:\n\nSuch systems clearly obey the laws of physics­they are realizable by engineered systems made of engineered components;\nalthough our use of the term “symbol” prefigures our intended interpretation, it is not restricted to human symbol systems.\n\nA physical symbol system consists of a set of entities, called symbols, which are physical patterns that can occur as components of another type of entity called an expression (or symbol structure). Thus, a symbol structure is composed of a number of instances (or tokens) of symbols related in some physical way (such as one token being next to another). At any instant of time the system will contain a collection of these symbol structures. Besides these structures, the system also contains a collection of processes that operate on expressions to produce other expressions: processes of creation, modification, reproduction and destruction. A physical symbol system is a machine that produces through time an evolving collection of symbol structures. Such a system exists in a world of objects wider than just these symbolic expressions themselves.\nTwo notions are central to this structure of expressions, symbols, and objects: designation and interpretation.\n\nDesignation. An expression designates an object if, given the expression, the system can either affect the object itself or behave in ways dependent on the object.\n\nIn either case, access to the object via the expression has been obtained, which is the essence of designation.\n\nInterpretation. The system can interpret an expression if the expression designates a process and if, given the expression, the system can carry out the process.\n\nInterpretation implies a special form of dependent action: given an expression the system can perform the indicated process, which is to say, it can evoke and execute its own processes from expressions that designate them.\nA system capable of designation and interpretation, in the sense just indicated, must also meet a number of additional requirements, of completeness and closure. We will have space only to mention these briefly; all of them are important and have far-reaching consequences.\n\nA symbol may be used to designate any expression whatsoever. That is, given a symbol, it is not prescribed a priori what expressions it can designate. This arbitrariness pertains only to symbols; the symbol tokens and their mutual relations determine what object is designated by a complex expression.\nThere exist expressions that designate every process of which the machine is capable.\nThere exist processes for creating any expression and for modifying any expression in arbitrary ways.\nExpressions are stable; once created they will continue to exist until explicitly modified or deleted.\nThe number of expressions that the system can hold is essentially unbounded.\n\nThe type of system we have just defined is not unfamiliar to computer scientists. It bears a strong family resemblance to all general purpose computers. If a symbol manipulation language, such as LISP, is taken as defining a machine, then the kinship becomes truly brotherly. Our intent in laying out such a system is not to propose something new. Just the opposite: it is to show what is now known and hypothesized about systems that satisfy such a characterization.\nWe can now state a general scientific hypothesis­a law of qualitative structure for symbol systems:\n\nThe Physical Symbol System Hypothesis. A physical symbol system has the necessary and sufficient means for general intelligent action.\n\nBy “necessary” we mean that any system that exhibits general intelligence will prove upon analysis to be a physical symbol system. By “sufficient” we mean that any physical symbol system of sufficient size can be organized further to exhibit general intelligence. By “general intelligent action” we wish to indicate the same scope of intelligence as we see in human action: that in any real situation behavior appropriate to the ends of the system and adaptive to the demands of the environment can occur, within some limits of speed and complexity.\nThe Physical Symbol System Hypothesis clearly is a law of qualitative structure. It specifies a general class of systems within which one will find those capable of intelligent action.\nThis is an empirical hypothesis. We have defined a class of systems; we wish to ask whether that class accounts for a set of phenomena we find in the real world. Intelligent action is everywhere around us in the biological world, mostly in human behavior. It is a form of behavior we can recognize by its effects whether it is performed by humans or not. The hypothesis could indeed be false. Intelligent behavior is not so easy to produce that any system will exhibit it willy-nilly. Indeed, there are people whose analyses lead them to conclude either on philosophical or on scientific grounds that the hypothesis is false. Scientifically, one can attack or defend it only by bringing forth empirical evidence about the natural world.\nWe now need to trace the development of this hypothesis and look at the evidence for it.\n\n\nDevelopment of the Symbol System Hypothesis\nA physical symbol system is an instance of a universal machine. Thus the symbol system hypothesis implies that intelligence will be realized by a universal computer. However, the hypothesis goes far beyond the argument, often made on general grounds of physical determinism, that any computation that is realizable can be realized by a universal machine, provided that it is specified. For it asserts specifically that the intelligent machine is a symbol system, thus making a specific architectural assertion about the nature of intelligent systems. It is important to understand how this additional specificity arose.\n\nFormal Logic\nThe roots of the hypothesis go back to the program of Frege and of Whitehead and Russell for formalizing logic capturing the basic conceptual notions of mathematics in logic and putting the notions of proof and deduction on a secure footing. This effort culminated in mathematical logic – ­our familiar propositional, first-order, and higher-order logics. It developed a characteristic view, often referred to as the “symbol game.” Logic, and by incorporation all of mathematics, was a game played with meaningless tokens according to certain purely syntactic rules. All meaning had been purged. One had a mechanical, though permissive (we would now say nondeterministic), system about which various things could be proved. Thus progress was first made by walking away from all that seemed relevant to meaning and human symbols. We could call this the stage of formal symbol manipulation.\nThis general attitude is well reflected in the development of information theory. It was pointed out time and again that Shannon had defined a system that was useful only for communication and selection, and which had nothing to do with meaning. Regrets were expressed that such a general name as “information theory” had been given to the field, and attempts were made to rechristen it as “the theory of selective information” – ­to no avail, of course.\n\n\nTuring Machines and the Digital Computer\nThe development of the first digital computers and of automata theory, starting with Turing’s own work in the ’30s, can be treated together. They agree in their view of what is essential. Let us use Turing’s own model, for it shows the features well.\nA Turing machine consists of two memories: an unbounded tape and a finite state control. The tape holds data, i.e. the famous zeroes and ones. The machine has a very small set of proper operations-read, write, and scan operations-on the tape. The read operation is not a data operation, but provides conditional branching to a control state as a function of the data under the read head. As we all know, this model contains the essentials of all computers, in terms of what they can do, though other computers with different memories and operations might carry out the same computations with different requirements of space and time. In particular, the model of a Turing machine contains within it the notions both of what cannot be computed and of universal machines-computers that can do anything that can be done by any machine.\nWe should marvel that two of our deepest insights into information processing were achieved in the thirties, before modern computers came into being. It is a tribute to the genius of Alan Turing. It is also a tribute to the development of mathematical logic at the time, and testimony to the depth of computer science’s obligation to it. Concurrently with Turing’s work appeared the work of the logicians Emil Post and (independently) Alonzo Church. Starting from independent notions of logistic systems (Post productions and recursive functions, respectively) they arrived at analogous results on undecidability and universality­results that were soon shown to imply that all three systems were equivalent. Indeed, the convergence of all these attempts to define the most general class of information processing systems provides some of the force of our conviction that we have captured the essentials of information processing in these models.\nIn none of these systems is there, on the surface, a concept of the symbol as something that designates. The data are regarded as just strings of zeroes and ones­indeed that data be inert is essential to the reduction of computation to physical process. The finite state control system was always viewed as a small controller, and logical games were played to see how small a state system could be used without destroying the universality of the machine. No games, as far as we can tell, were ever played to add new states dynamically to the finite control-to think of the control memory as holding the bulk of the system’s knowledge. What was accomplished at this stage was half the principle of interpretation­showing that a machine could be run from a description. Thus, this is the stage of automatic formal symbol manipulation.\n\n\nThe Stored Program Concept\nWith the development of the second generation of electronic machines in the mid-forties (after the ENIAC) came the stored program concept. This was rightfully hailed as a milestone, both conceptually and practically. Programs now can be data, and can be operated on as data. This capability is, of course, already implicit in the model of Turing: the descriptions are on the very same tape as the data. Yet the idea was realized only when machines acquired enough memory to make it practicable to locate actual programs in some internal place. After all, the Eniac had only twenty registers.\nThe stored program concept embodies the second half of the interpretation principle, the part that says that the system’s own data can be interpreted. But it does not yet contain the notion of designation­of the physical relation that underlies meaning.\n\n\nList Processing\nThe next step, taken in 1956, was list processing. The contents of the data structures were now symbols, in the sense of our physical symbol system; patterns that designated, that had referents. Lists held addresses which permitted access to other lists­thus the notion of list structures. That this was a new view was demonstrated to us many times in the early days of list processing when colleagues would ask where the data were­that is, which list finally held the collections of bits that were the content of the system. They found it strange that there were no such bits, there were only symbols that designated yet other symbol structures.\nList processing is simultaneously three things in the development of computer science.\n\nIt is the creation of a genuine dynamic memory structure in a machine that had heretofore been perceived as having fixed structure. It added to our ensemble of operations those that built and modified structure in addition to those that replaced and changed content.\nIt was an early demonstration of the basic abstraction that a computer consists of a set of data types and a set of operations proper to these data types, so that a computational system should employ whatever data types are appropriate to the application, independent of the underlying machine.\nList processing produced a model of designation, thus defining symbol manipulation in the sense in which we use this concept in computer science today.\n\nAs often occurs, the practice of the time already anticipated all the elements of list processing: addresses are obviously used to gain access, the drum machines used linked programs (so called one-plus-one addressing), and so on. But the conception of list processing as an abstraction created a new world in which designation and dynamic symbolic structure were the defining characteristics. The embedding of the early list processing systems in languages (the IPLs, LISP) is often decried as having been a barrier to the diffusion of list processing techniques throughout programming practice; but it was the vehicle that held the abstraction together.\n\n\nLISP\nOne more step is worth noting: McCarthy’s creation of LISP in 1959–60 [McCarthy, 1960]. It completed the act of abstraction, lifting list structures out of their embedding in concrete machines, creating a new formal system with S-expressions, which could be shown to be equivalent to the other universal schemes of computation.\n\n\nConclusion\nThat the concept of the designating symbol and symbol manipulation does not emerge until the mid-fifties does not mean that the earlier steps were either inessential or less important. The total concept is the join of computability, physical realizability (and by multiple technologies), universality, the symbolic representation of processes (i.e. interpretability), and, finally, symbolic structure and designation. Each of the steps provided an essential part of the whole.\nThe first step in this chain, authored by Turing, is theoretically motivated, but the others all have deep empirical roots. We have been led by the evolution of the computer itself. The stored program principle arose out of the experience with Eniac. List processing arose out of the attempt to construct intelligent programs. It took its cue from the emergence of random access memories, which provided a clear physical realization of a designating symbol in the address. LISP arose out of the evolving experience with list processing.\n\n\n\nThe Evidence\nWe come now to the evidence for the hypothesis that physical symbol systems are capable of intelligent action, and that general intelligent action calls for a physical symbol system. The hypothesis is an empirical generalization and not a theorem. We know of no way of demonstrating the connection between symbol systems and intelligence on purely logical grounds. Lacking such a demonstration, we must look at the facts. Our central aim, however, is not to review the evidence in detail, but to use the example before us to illustrate the proposition that computer science is a field of empirical inquiry. Hence, we will only indicate what kinds of evidence there is, and the general nature of the testing process.\nThe notion of physical symbol system had taken essentially its present form by the middle of the 1950’s, and one can date from that time the growth of artificial intelligence as a coherent subfield of computer science. The twenty years of work since then has seen a continuous accumulation of empirical evidence of two main varieties. The first addresses itself to the sufficiency of physical symbol systems for producing intelligence, attempting to construct and test specific systems that have such a capability. The second kind of evidence addresses itself to the necessity of having a physical symbol system wherever intelligence is exhibited. It starts with Man, the intelligent system best known to us, and attempts to discover whether his cognitive activity can be explained as the working of a physical symbol system. There are other forms of evidence, which we will comment upon briefly later, but these two are the important ones. We will consider them in turn. The first is generally called artificial intelligence, the second, research in cognitive psychology.\n\nConstructing Intelligent Systems\nThe basic paradigm for the initial testing of the germ theory of disease was: identify a disease; then look for the germ. An analogous paradigm has inspired much of the research in artificial intelligence: identify a task domain calling for intelligence; then construct a program for a digital computer that can handle tasks in that domain. The easy and well-structured tasks were looked at first: puzzles and games, operations research problems of scheduling and allocating resources, simple induction tasks. Scores, if not hundreds, of programs of these kinds have by now been constructed, each capable of some measure of intelligent action in the appropriate domain.\nOf course intelligence is not an all-or-none matter, and there has been steady progress toward higher levels of performance in specific domains, as well as toward widening the range of those domains. Early chess programs, for example, were deemed successful if they could play the game legally and with some indication of purpose; a little later, they reached the level of human beginners; within ten or fifteen years, they began to compete with serious amateurs. Progress has been slow (and the total programming effort invested small) but continuous, and the paradigm of construct-and-test proceeds in a regular cycle-the whole research activity mimicking at a macroscopic level the basic generate-and-test cycle of many of the AI programs.\nThere is a steadily widening area within which intelligent action is attainable. From the original tasks, research has extended to building systems that handle and understand natural language in a variety of ways, systems for interpreting visual scenes, systems for hand-eye coordination, systems that design, systems that write computer programs, systems for speech understanding-the list is, if not endless, at least very long. If there are limits beyond which the hypothesis will not carry us, they have not yet become apparent. Up to the present, the rate of progress has been governed mainly by the rather modest quantity of scientific resources that have been applied and the inevitable requirement of a substantial system-building effort for each new major undertaking.\nMuch more has been going on, of course, than simply a piling up of examples of intelligent systems adapted to specific task domains. It would be surprising and unappealing if it turned out that the Al programs performing these diverse tasks had nothing in common beyond their being instances of physical symbol systems. Hence, there has been great interest in searching for mechanisms possessed of generality, and for common components among programs performing a variety of tasks. This search carries the theory beyond the initial symbol system hypothesis to a more complete characterization of the particular kinds of symbol systems that are effective in artificial intelligence. In the second section of this paper, we will discuss one example of a hypothesis at this second level of specificity: the heuristic search hypothesis.\nThe search for generality spawned a series of programs designed to separate out general problem-solving mechanisms from the requirements of particular task domains. The General Problem Solver (GPS) was perhaps the first of these; while among its descendants are such contemporary systems as PLANNER and CONNIVER. The search for common components has led to generalized schemes of representation for goals and plans, methods for constructing discrimination nets, procedures for the control of tree search, pattern matching mechanisms, and language-parsing systems. Experiments are at present under way to find convenient devices for representing sequences of time and tense, movement, causality and the like. More and more, it becomes possible to assemble large intelligent systems in a modular way from such basic components.\nWe can gain some perspective on what is going on by turning, again, to the analogy of the germ theory. If the first burst of research stimulated by that theory consisted largely in finding the germ to go with each disease, subsequent effort turned to learning what a germ was-to building on the basic qualitative law a new level of structure. In artificial intelligence, an initial burst of activity aimed at building intelligent programs for a wide variety of almost randomly selected tasks is giving way to more sharply targeted research aimed at understanding the common mechanisms of such systems.\n\n\nThe Modeling of Human Symbolic Behavior\nThe symbol system hypothesis implies that the symbolic behavior of man arises because he has the characteristics of a physical symbol system. Hence, the results of efforts to model human behavior with symbol systems become an important part of the evidence for the hypothesis, and research in artificial intelligence goes on in close collaboration with research in information processing psychology, as it is usually called.\nThe search for explanations of man’s intelligent behavior in terms of symbol systems has had a large measure of success over the past twenty years; to the point where information processing theory is the leading contemporary point of view in cognitive psychology. Especially in the areas of problem solving, concept attainment, and long-term memory, symbol manipulation models now dominate the scene.\nResearch in information processing psychology involves two main kinds of empirical activity. The first is the conduct of observations and experiments on human behavior in tasks requiring intelligence. The second, very similar to the parallel activity in artificial intelligence, is the programming of symbol systems to model the observed human behavior. The psychological observations and experiments lead to the formulation of hypotheses about the symbolic processes the subjects are using, and these are an important source of the ideas that go into the construction of the programs. Thus, many of the ideas for the basic mechanisms of GPS were derived from careful analysis of the protocols that human subjects produced while thinking aloud during the performance of a problem-solving task.\nThe empirical character of computer science is nowhere more evident than in this alliance with psychology. Not only are psychological experiments required to test the veridicality of the simulation models as explanations of the human behavior, but out of the experiments come new ideas for the design and construction of physical symbol systems.\n\n\nOther Evidence\nThe principal body of evidence for the symbol system hypothesis that we have not considered is negative evidence: the absence of specific competing hypotheses as to how intelligent activity might be accomplished­whether by man or machine. Most attempts to build such hypotheses have taken place within the field of psychology. Here we have had a continuum of theories from the points of view usually labeled “behaviorism” to those usually labeled “Gestalt theory.” Neither of these points of view stands as a real competitor to the symbol system hypothesis, and this for two reasons. First, neither behaviorism nor Gestalt theory has demonstrated, or even shown how to demonstrate, that the explanatory mechanisms it postulates are sufficient to account for intelligent behavior in complex tasks. Second, neither theory has been formulated with anything like the specificity of artificial programs. As a matter of fact, the alternative theories are sufficiently vague so that it is not terribly difficult to give them information processing interpretations, and thereby assimilate them to the symbol system hypothesis.\n\n\n\nConclusion\nWe have tried to use the example of the Physical Symbol System Hypothesis to illustrate concretely that computer science is a scientific enterprise in the usual meaning of that term: that it develops scientific hypotheses which it then seeks to verify by empirical inquiry. We had a second reason, however, for choosing this particular example to illustrate our point. The Physical Symbol System Hypothesis is itself a substantial scientific hypothesis of the kind that we earlier dubbed “laws of qualitative structure.” It represents an important discovery of computer science, which if borne out by the empirical evidence, as in fact appears to be occurring, will have major continuing impact on the field.\nWe turn now to a second example, the role of search in intelligence. This topic, and the particular hypothesis about it that we shall examine, have also played a central role in computer science, in general, and artificial intelligence, in particular."
  },
  {
    "objectID": "docs/posts/1975-herbert-simon-allen-newell/index.html#ii.-heuristic-search",
    "href": "docs/posts/1975-herbert-simon-allen-newell/index.html#ii.-heuristic-search",
    "title": "Computer Science as Empirical Inquiry: Symbols and Search",
    "section": "II. Heuristic Search",
    "text": "II. Heuristic Search\nKnowing that physical symbol systems provide the matrix for intelligent action does not tell us how they accomplish this. Our second example of a law of qualitative structure in computer science addresses this latter question, asserting that symbol systems solve problems by using the processes of heuristic search. This generalization, like the previous one, rests on empirical evidence, and has not been derived formally from other premises. However, we shall see in a moment that it does have some logical connection with the symbol system hypothesis, and perhaps we can look forward to formalization of the connection at some time in the future. Until that time arrives, our story must again be one of empirical inquiry. We will describe what is known about heuristic search and review the empirical findings that show how it enables action to be intelligent. We begin by stating this law of qualitative structure, the Heuristic Search Hypothesis.\n\nHeuristic Search Hypothesis. The solutions to problems are represented as symbol structures. A physical symbol system exercises its intelligence in problem solving by search-that is, by generating and progressively modifying symbol structures until it produces a solution structure.\n\nPhysical symbol systems must use heuristic search to solve problems because such systems have limited processing resources; in a finite number of steps, and over a finite interval of time, they can execute only a finite number of processes. Of course that is not a very strong limitation, for all universal Turing machines suffer from it. We intend the limitation, however, in a stronger sense: we mean practically limited. We can conceive of systems that are not limited in a practical way, but are capable, for example, of searching in parallel the nodes of an exponentially expanding tree at a constant rate for each unit advance in depth. We will not be concerned here with such systems, but with systems whose computing resources are scarce relative to the complexity of the situations with which they are confronted. The restriction will not exclude any real symbol systems, in computer or man, in the context of real tasks. The fact of limited resources allows us, for most purposes, to view a symbol system as though it were a serial, one-process-at-a-time device. If it can accomplish only a small amount of processing in any short time interval, then we might as well regard it as doing things one at a time. Thus ’’limited resource symbol system” and “serial symbol system” are practically synonymous. The problem of allocating a scarce resource from moment to moment can usually be treated, if the moment is short enough, as a problem of scheduling a serial machine.\n\nProblem Solving\nSince ability to solve problems is generally taken as a prime indicator that a system has intelligence, it is natural that much of the history of artificial intelligence is taken up with attempts to build and understand problem-solving systems. Problem solving has been discussed by philosophers and psychologists for two millennia, in discourses dense with the sense of mystery. If you think there is nothing problematic or mysterious about a symbol system solving problems, then you are a child of today, whose views have been formed since mid-century. Plato (and by his account, Socrates) found difficulty understanding even how problems could be entertained, much less how they could be solved. Let me remind you of how he posed the conundrum in the Meno:\n\nMeno: And how will you inquire, Socrates, into that which you know not? What will you put forth as the subject of inquiry? And if you find what you want, how will you ever know that this is what you did not know?\n\nTo deal with this puzzle, Plato invented his famous theory of recollection: when you think you are discovering or learning something, you are really just recalling what you already knew in a previous existence. If you find this explanation preposterous, there is a much simpler one available today, based upon our understanding of symbol systems. An approximate statement of it is:\n\nTo state a problem is to designate (1) a test for a class of symbol structures (solutions of the problem), and (2) a generator of symbol structures (potential solutions). To solve a problem is to generate a structure, using (2), that satisfies the test of (1).\n\nWe have a problem if we know what we want to do (the test), and if we don’t know immediately how to do it (our generator does not immediately produce a symbol structure satisfying the test). A symbol system can state and solve problems (sometimes) because it can generate and test.\nIf that is all there is to problem solving, why not simply generate at once an expression that satisfies the test? This is in fact, what we do when we wish and dream. “If wishes were horses, beggars might ride.” But outside the world of dreams, it isn’t possible. To know how we would test something, once constructed, does not mean that we know how to construct it-that we have any generator for doing so.\nFor example, it is well known what it means to “solve” the problem of playing winning chess. A simple test exists for noticing winning positions, the test for checkmate of the enemy King. In the world of dreams one simply generates a strategy that leads to checkmate for all counter strategies of the opponent. Alas, no generator that will do this is known to existing symbol systems (man or machine). Instead, good moves in chess are sought by generating various alternatives, and painstakingly evaluating them with the use of approximate, and often erroneous, measures that are supposed to indicate the likelihood that a particular line of play is on the route to a winning position. Move generators there are; winning move generators there are not.\nBefore there can be a move generator for a problem, there must be a problem space: a space of symbol structures in which problem situations, including the initial and goal situations, can be represented. Move generators are processes for modifying one situation in the problem space into another. The basic characteristics of physical symbol systems guarantee that they can represent problem spaces and that they possess move generators. How, in any concrete situation they synthesize a problem space and move generators appropriate to that situation is a question that is still very much on the frontier of artificial intelligence research.\nThe task that a symbol system is faced with, then, when it is presented with a problem and a problem space, is to use its limited processing resources to generate possible solutions, one after another, until it finds one that satisfies the problem-defining test. If the system had some control over the order in which potential solutions were generated, then it would be desirable to arrange this order of generation so that actual solutions would have a high likelihood of appearing early. A symbol system would exhibit intelligence to the extent that it succeeded in doing this. Intelligence for a system with limited processing resources consists in making wise choices of what to do next.\n\n\nSearch in Problem Solving\nDuring the first decade or so of artificial intelligence research, the study of problem solving was almost synonymous with the study of search processes. From our characterization of problems and problem solving, it is easy to see why this was so. In fact, it might be asked whether it could be otherwise. But before we try to answer that question, we must explore further the nature of search processes as it revealed itself during that decade of activity.\n\nExtracting Information from the Problem Space\nConsider a set of symbol structures, some small subset of which are solutions to a given problem. Suppose, further, that the solutions are distributed randomly through the entire set. By this we mean that no information exists that would enable any search generator to perform better than a random search. Then no symbol system could exhibit more intelligence (or less intelligence) than any other in solving the problem, although one might experience better luck than another.\nA condition, then, for the appearance of intelligence is that the distribution of solutions be not entirely random, that the space of symbol structures exhibit at least some degree of order and pattern. A second condition is that pattern in the space of symbol structures be more or less detectable. A third condition is that the generator of potential solutions be able to behave differentially, depending on what pattern it detected. There must be information in the problem space, and the symbol system must be capable of extracting and using it. Let us look first at a very simple example, where the intelligence is easy to come by.  Consider the problem of solving a simple algebraic equation:\n\\[\nAX + B + CX + D\n\\]\nThe test defines a solution as any expression of the form, \\(X = E\\), such that \\(AE + B = CE + D\\). Now one could use as generator any process that would produce numbers which could then be tested by substituting in the latter equation. We would not call this an intelligent generator.\nAlternatively, one could use generators that would make use of the fact that the original equation can be modified­by adding or subtracting equal quantities from both sides, or multiplying or dividing both sides by the same quantity­without changing its solutions. But, of course, we can obtain even more information to guide the generator by comparing the original expression with the form of the solution, and making precisely those changes in the equation that leave its solution unchanged, while at the same time, bringing it into the desired form. Such a generator could notice that there was an unwanted \\(CX\\) on the right-hand side of the original equation, subtract it from both sides and collect terms again. It could then notice that there was an unwanted B on the left-hand side and subtract that. Finally, it could get rid of the unwanted coefficient \\((A-C)\\) on the left-hand side by dividing.\nThus by this procedure, which now exhibits considerable intelligence, the generator produces successive symbol structures, each obtained by modifying the previous one; and the modifications are aimed at reducing the differences between the form of the input structure and the form of the test expression, while maintaining the other conditions for a solution.\nThis simple example already illustrates many of the main mechanisms that are used by symbol systems for intelligent problem solving. First, each successive expression is not generated independently, but is produced by modifying one produced previously. Second, the modifications are not haphazard, but depend upon two kinds of information. They depend on information that is constant over this whole class of algebra problems, and that is built into the structure of the generator itself: all modifications of expressions must leave the equation’s solution unchanged. They also depend on information that changes at each step: detection of the differences in form that remain between the current expression and the desired expression. In effect, the generator incorporates some of the tests the solution must satisfy, so that expressions that don’t meet these tests will never be generated. Using the first kind of information guarantees that only a tiny subset of all possible expressions is actually generated, but without losing the solution expression from this subset. Using the second kind of information arrives at the desired solution by a succession of approximations, employing a simple form of means-ends analysis to give direction to the search. There is no mystery where the information that guided the search came from. We need not follow Plato in endowing the symbol system with a previous existence in which it already knew the solution. A moderately sophisticated generator-test system did the trick without invoking reincarnation.\n\n\nSearch Trees\nThe simple algebra problem may seem an unusual, even pathological, example of search. It is certainly not trial-and-error search, for though there were a few trials, there was no error. We are more accustomed to thinking of problem-solving search as generating lushly branching trees of partial solution possibilities which may grow to thousands, or even millions, of branches, before they yield a solution. Thus, if from each expression it produces, the generator creates \\(B\\) new branches, then the tree will grow as \\(BD\\), where \\(D\\) is its depth. The tree grown for the algebra problem had the peculiarity that its branchiness, \\(B\\), equaled unity.\nPrograms that play chess typically grow broad search trees, amounting in some cases to a million branches or more. (Although this example will serve to illustrate our points about tree search, we should note that the purpose of search in chess is not to generate proposed solutions, but to evaluate (test) them.) One line of research into game-playing programs has been centrally concerned with improving the representation of the chess board, and the processes for making moves on it, so as to speed up search and make it possible to search larger trees. The rationale for this direction, of course, is that the deeper the dynamic search, the more accurate should be the evaluations at the end of it. On the other hand, there is good empirical evidence that the strongest human players, grandmasters, seldom explore trees of more than one hundred branches. This economy is achieved not so much by searching less deeply than do chess-playing programs, but by branching very sparsely and selectively at each node. This is only possible, without causing a deterioration of the evaluations, by having more of the selectivity built into the generator itself, so that it is able to select for generation just those branches that are very likely to yield important relevant information about the position.\nThe somewhat paradoxical-sounding conclusion to which this discussion leads is that search­successive generation of potential solution structures­is a fundamental aspect of a symbol system’s exercise of intelligence in problem solving but that amount of search is not a measure of the amount of intelligence being exhibited. What makes a problem a problem is not that a large amount of search is required for its solution, but that a large amount would be required if a requisite level of intelligence were not applied. When the symbolic system that is endeavoring to solve a problem knows enough about what to do, it simply proceeds directly towards its goal; but whenever its knowledge becomes inadequate, when it enters terra incognita, it s faced with the threat of going through large amounts of search before it finds its way again.\nThe potential for the exponential explosion of the search tree that is present in every scheme for generating problem solutions warns us against depending on the brute force of computers­even the biggest and fastest computers­as a compensation for the ignorance and un-selectivity of their generators. The hope is still periodically ignited in some human breasts that a computer can be found that is fast enough, and that can be programmed cleverly enough, to play good chess by brute-force search. There is nothing known in theory about the game of chess that rules out this possibility. Empirical studies on the management of search in sizable trees with only modest results make this a much less promising direction than it was when chess was first chosen as an appropriate task for artificial intelligence. We must regard this as one of the important empirical findings of research with chess programs.\n\n\nThe Forms of Intelligence\nThe task of intelligence, then, is to avert the ever-present threat of the exponential explosion of search. How can this be accomplished? The first route, already illustrated by the algebra example, and by chess programs that only generate “plausible” moves for further analysis, is to build selectivity into the generator: to generate only structures that show promise of being solutions or of being along the path toward solutions. The usual consequence of doing this is to decrease the rate of branching, not to prevent it entirely. Ultimate exponential explosion is not avoided-save in exceptionally highly structured situations like the algebra example-but only postponed. Hence, an intelligent system generally needs to supplement the selectivity of its solution generator with other information-using techniques to guide search.\nTwenty years of experience with managing tree search in a variety of task environments has produced a small kit of general techniques which is part of the equipment of every researcher in artificial intelligence today. Since these techniques have been described in general works like that of Nilsson [1971], they can be summarized very briefly here.\nIn serial heuristic search, the basic question always is: what shall be done next? In tree search, that question, in turn, has two components:\n\nfrom what node in the tree shall we search next, and\nwhat direction shall we take from that node?\n\nInformation helpful in answering the first question may be interpreted as measuring the relative distance of different nodes from the goal. Best-first search calls for searching next from the node that appears closest to the goal. Information helpful in answering the second question-in what direction to search-is often obtained, as in the algebra example, by detecting specific differences between the current nodal structure and the goal structure described by the test of a solution, and selecting actions that are relevant to reducing these particular kinds of differences. This is the technique known as means-ends analysis, which plays a central role in the structure of the General Problem Solver.\nThe importance of empirical studies as a source of general ideas in AI research can be demonstrated clearly by tracing the history, through large numbers of problem solving programs, of these two central ideas: best-first search and means-ends analysis. Rudiments of best-first search were already present, though unnamed, in the Logic Theorist in 1955. The General Problem Solver, embodying means-ends analysis, appeared about 1957, ­but combined it with modified depth-first search rather than best-first search. Chess programs were generally wedded, for reasons of economy of memory, to depth-first search, supplemented after about 1958 by the powerful alpha beta pruning procedure. Each of these techniques appears to have been reinvented a number of times, and it is hard to find general, task-independent theoretical discussions of problem solving in terms of these concepts until the middle or late 1960’s. The amount of formal buttressing they have received from mathematical theory is still miniscule: some theorems about the reduction in search that can be secured from using the alpha-beta heuristic, a couple of theorems (reviewed by Nilsson [1971]) about shortest-path search, and some very recent theorems on best-first search with a probabilistic evaluation function.\n\n\n“Weak” and “Strong” Methods\nThe techniques we have been discussing are dedicated to the control of exponential expansion rather than its prevention. For this reason, they have been properly called “weak methods”­methods to be used when the symbol system’s knowledge or the amount of structure actually contained in the problem space are inadequate to permit search to be avoided entirely. It is instructive to contrast a highly structured situation, which can be formulated, say, as a linear programming problem, with the less structured situations of combinatorial problems like the traveling salesman problem or scheduling problems. (“Less structured” here refers to the insufficiency or nonexistence of relevant theory about the structure of the problem space.)\nIn solving linear programming problems, a substantial amount of computation may be required, but the search does not branch. Every step is a step along the way to a solution. In solving combinatorial problems or in proving theorems, tree search can seldom be avoided, and success depends on heuristic search methods of the sort we have been describing.\nNot all streams of AI problem-solving research have followed the path we have been outlining. An example of a somewhat different point is provided by the work on theorem-proving systems. Here, ideas imported from mathematics and logic have had a strong influence on the direction of inquiry. For example, the use of heuristics was resisted when properties of completeness could not be proved (a bit ironic, since most interesting mathematical systems are known to be undecidable). Since completeness can seldom be proved for best-first search heuristics, or for many kinds of selective generators, the effect of this requirement was rather inhibiting. When theorem-proving programs were continually incapacitated by the combinatorial explosion of their search trees, thought began to be given to selective heuristics, which in many cases proved to be analogues of heuristics used in general problem-solving programs. The set-of-support heuristic, for example, is a form of working backwards, adapted to the resolution theorem proving environment.\n\n\nA Summary of the Experience\nWe have now described the workings of our second law of qualitative structure, which asserts that physical symbol systems solve problems by means of heuristic search. Beyond that, we have examined some subsidiary characteristics of heuristic search, in particular the threat that it always faces of exponential explosion of the search tree, and some of the means it uses to avert that threat. Opinions differ as to how effective heuristic search has been as a problem solving mechanism-the opinions depending on what task domains are considered and what criterion of adequacy is adopted. Success can be guaranteed by setting aspiration levels low-or failure by setting them high. The evidence might be summed up about as follows. Few programs are solving problems at “expert” professional levels. Samuel’s checker program and Feigenbaum and Lederberg’s DENDRAL are perhaps the best-known exceptions, but one could point also to a number of heuristic search program.s for such operations research problem domains as scheduling and integer programming. In a number of domains, programs perform at the level of competent amateurs: chess, some theorem-proving domains, many kinds of games and puzzles. Human levels have not yet been nearly reached by programs that have a complex perceptual “front end”: visual scene recognizers, speech understanders, robots that have to maneuver in real space and time. Nevertheless, impressive progress has been made, and a large body of experience assembled about these difficult tasks.\nWe do not have deep theoretical explanations for the particular pattern of performance that has emerged. On empirical grounds, however, we might draw two conclusions. First, from what has been learned about human expert performance in tasks like chess, it is likely that any system capable of matching that performance will have to have access, in its memories, to very large stores of semantic information. Second, some part of the human superiority in tasks with a large perceptual component can be attributed to the special-purpose built-in parallel processing structure of the human eye and ear.\nIn any case, the quality of performance must necessarily depend on the characteristics both of the problem domains and of the symbol systems used to tackle them. For most real-life domains in which we are interested, the domain structure has not proved sufficiently simple to yield (so far) theorems about complexity, or to tell us, other than empirically, how large real-world problems are in relation to the abilities of our symbol systems to solve them. That situation may change, but until it does, we must rely upon empirical explorations, using the best problem solvers we know how to build, as a principal source of knowledge about the magnitude and characteristics of problem difficulty. Even in highly structured areas like linear programming, theory has been much more useful in strengthening the heuristics that underlie the most powerful solution algorithms than in providing a deep analysis of complexity.\n\n\n\nIntelligence Without Much Search\nOur analysis of intelligence equated it with ability to extract and use information about the structure of the problem space, so as to enable a problem solution to be generated as quickly and directly as possible. New directions for improving the problem-solving capabilities of symbol systems can be equated, then, with new ways of extracting and using information. At least three such ways can be identified.\n\nNonlocal Use of Information\nFirst, it has been noted by several investigators that information gathered in the course of tree search is usually only used locally, to help make decisions at the specific node where the information was generated. Information about a chess position, obtained by dynamic analysis of a subtree of continuations, is usually used to evaluate just that position, not to evaluate other positions that may contain many of the same features. Hence, the same facts have to be rediscovered repeatedly at different nodes of the search tree. Simply to take the information out of the context in which it arose and use it generally does not solve the problem, for the information may be valid only in a limited range of contexts. In recent years, a few exploratory efforts have been made to transport information from its context of origin to other appropriate contexts. While it is still too early to evaluate the power of this idea, or even exactly how it is to be achieved, it shows considerable promise. An important line of investigation that Berliner [1975] has been pursuing is to use causal analysis to determine the range over which a particular piece of information is valid. Thus if a weakness in a chess position can be traced back to the move that made it, then the same weakness can be expected in other positions descendant from the same move.\nThe HEARSAY speech understanding system has taken another approach to making information globally available. That system seeks to recognize speech strings by pursuing a parallel search at a number of different levels: phonemic, lexical, syntactic, and semantic. As each of these searches provides and evaluates hypotheses, it supplies the information it has gained to a common “blackboard” that can be read by all the sources. This shared information can be used, for example, to eliminate hypotheses, or even whole classes of hypotheses, that would otherwise have to be searched by one of the processes. Thus, increasing our ability to use tree-search information nonlocally offers promise for raising the intelligence of problem-solving systems.\n\n\nSemantic Recognition Systems\nA second active possibility for raising intelligence is to supply the symbol system with a rich body of semantic information about the task domain it is dealing with. For example, empirical research on the skill of chess masters shows that a major source of the master’s skill is stored information that enables him to recognize a large number of specific features and patterns of features on a chess board, and information that uses this recognition to propose actions appropriate to the features recognized. This general idea has, of course, been incorporated in chess programs almost from the beginning. What is new is the realization of the number of such patterns and associated information that may have to be stored for master-level play: something of the order of 50,000.\nThe possibility of substituting recognition for search arises because a particular, and especially a rare, pattern can contain an enormous amount of information, provided that it is closely linked to the structure of the problem space. When that structure is “irregular,” and not subject to simple mathematical description, then knowledge of a large number of relevant patterns may be the key to intelligent behavior. Whether this is so in any particular task domain is a question more easily settled by empirical investigation than by theory. Our experience with symbol systems richly endowed with semantic information and pattern-recognizing capabilities for accessing it is still extremely limited.\nThe discussion above refers specifically to semantic information associated with a recognition system. Of course, there is also a whole large area of Al research on semantic information processing and the organization of semantic memories that falls outside the scope of the topics we are discussing in this paper.\n\n\nSelecting Appropriate Representations\nA third line of inquiry is concerned with the possibility that search can be reduced or avoided by selecting an appropriate problem space. A standard example that illustrates this possibility dramatically is the mutilated checkerboard problem. A standard 64 square checkerboard can be covered exactly with 32 tiles, each a \\(1 \\times 2\\) rectangle covering exactly two squares. Suppose, now, that we cut off squares at two diagonally opposite corners of the checkerboard, leaving a total of 62 squares. Can this mutilated board be covered exactly with 31 tiles? With (literally) heavenly patience, the impossibility of achieving such a covering can be demonstrated by trying all possible arrangements. The alternative, for those with less patience, and more intelligence, is to observe that the two diagonally opposite corners of a checkerboard are of the same color. Hence, the mutilated checkerboard has two less squares of one color than of the other. But each tile covers one square of one color and one square of the other, and any set of tiles must cover the same number of squares of each color. Hence, there is no solution. How can a symbol system discover this simple inductive argument as an alternative to a hopeless attempt to solve the problem by search among all possible coverings? We would award a system that found the solution high marks for intelligence.\nPerhaps, however, in posing this problem we are not escaping from search processes. We have simply displaced the search from a space of possible problem solutions to a space of possible representations. In any event, the whole process of moving from one representation to another, and of discovering and evaluating representations, is largely unexplored territory in the domain of problem-solving research. The laws of qualitative structure governing representations remain to be discovered. The search for them is almost sure to receive considerable attention in the coming decade.\n\n\n\nConclusion\nThat is our account of symbol systems and intelligence. It has been a long road from Plato’s Meno to the present, but it is perhaps encouraging that most of the progress along that road has been made since the turn of the twentieth century, and a large fraction of it since the midpoint of the century. Thought was still wholly intangible and ineffable until modern formal logic interpreted it as the manipulation of formal tokens. And it seemed still to inhabit mainly the heaven of Platonic ideals, or the equally obscure spaces of the human mind, until computers taught us how symbols could be processed by machines. A.M. Turing, whom we memorialize this morning, made his great contributions at the mid-century crossroads of these developments that led from modern logic to the computer.\n\nPhysical Symbol Systems\nThe study of logic and computers has revealed to us that intelligence resides in physical symbol systems. This is computer sciences’s most basic law of qualitative structure.\nSymbol systems are collections of patterns and processes, the latter being capable of producing, destroying and modifying the former. The most important properties of patterns is that they can designate objects, processes, or other patterns, and that, when they designate processes, they can be interpreted. Interpretation means carrying out the designated process. The two most significant classes of symbol systems with which we are acquainted are human beings and computers.\nOur present understanding of symbol systems grew, as indicated earlier, through a sequence of stages. Formal logic familiarized us with symbols, treated syntactically, as the raw material of thought, and with the idea of manipulating them according to carefully defined formal processes. The Turing machine made the syntactic processing of symbols truly machine-like, and affirmed the potential universality of strictly defined symbol systems. The stored-program concept for computers reaffirmed the interpretability of symbols, already implicit in the Turing machine. List processing brought to the forefront the denotational capacities of symbols, and defined symbol processing in ways that allowed independence from the fixed structure of the underlying physical machine. By 1956 all of these concepts were available, together with hardware for implementing them. The study of the intelligence of symbol systems, the subject of artificial intelligence, could begin.\n\n\nHeuristic Search\nA second law of qualitative structure for AI is that symbol systems solve problems by generating potential solutions and testing them, that is, by searching. Solutions are usually sought by creating symbolic expressions and modifying them sequentially until they satisfy the conditions for a solution. Hence symbol systems solve problems by searching. Since they have finite resources, the search cannot be carried out all at once, but must be sequential. It leaves behind it either a single path from starting point to goal or, if correction and backup are necessary, a whole tree of such paths.\nSymbol systems cannot appear intelligent when they are surrounded by pure chaos. They exercise intelligence by extracting information from a problem domain and using that information to guide their search, avoiding wrong turns and circuitous bypaths. The problem domain must contain information, that is, some degree of order and structure, for the method to work. The paradox of the Meno is solved by the observation that information may be remembered, but new information may also be extracted from the domain that the symbols designate. In both cases, the ultimate source of the information is the task domain.\n\n\nThe Empirical Base\nArtificial intelligence research is concerned with how symbol systems must be organized in order to behave intelligently. Twenty years of work in the area has accumulated a considerable body of knowledge, enough to fill several books (it already has), and most of it in the form of rather concrete experience about the behavior of specific classes of symbol systems in specific task domains. Out of this experience, however, there have also emerged some generalizations, cutting across task domains and systems, about the general characteristics of intelligence and its methods of implementation.\nWe have tried to state some of these generalizations this morning. They are mostly qualitative rather than mathematical. They have more the flavor of geology or evolutionary biology than the flavor of theoretical physics. They are sufficiently strong to enable us today to design and build moderately intelligent systems for a considerable range of task domains, as well as to gain a rather deep understanding of how human intelligence works in many situations.\n\n\nWhat Next?\nIn our account today, we have mentioned open questions as well as settled ones; there are many of both. We see no abatement of the excitement of exploration that has surrounded this field over the past quarter century. Two resource limits will determine the rate of progress over the next such period. One is the amount of computing power that will be available. The second, and probably the more important, is the number of talented young computer scientists who will be attracted to this area of research as the most challenging they can tackle.\nA.M. Turing concluded his famous paper on “Computing Machinery and Intelligence” with the words:\n\nWe can only see a short distance ahead, but we can see plenty there that needs to be done.\n\nMany of the things Turing saw in 1950 that needed to be done have been done, but the agenda is as full as ever. Perhaps we read too much into his simple statement above, but we like to think that in it Turing recognized the fundamental truth that all computer scientists instinctively know. For all physical symbol systems, condemned as we are to serial search of the problem environment, the critical question is always: What to do next?"
  },
  {
    "objectID": "docs/posts/1975-herbert-simon-allen-newell/index.html#references",
    "href": "docs/posts/1975-herbert-simon-allen-newell/index.html#references",
    "title": "Computer Science as Empirical Inquiry: Symbols and Search",
    "section": "References",
    "text": "References\n\nBerliner, H. [1975]. Chess as problem solving: the development of a tactics analyzer. Ph.D. Th.. Computer Sci. Dep., Carnegie Mellon U. (unpublished).\nMcCarthy, J. [1960]. Recursive functions of symbolic expressions and their computation by machine. Comm. ACM 3, 4 (April 1960), 184-195.\nMcCulloch, W.S. [1961]. What is a number, that a man may know it, and a man, that he may know a number. General Semantics Bulletin Nos. 26 and 27 (1961), 7-18.\nNilsson, N.J. [1971]. Problem Solving Methods in Artificial Intelligence. McGraw-Hill, New York.\nTuring, A.M. [1950]. Computing machinery and intelligence. Mind 59 (Oct. 1950), 43360."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html",
    "href": "docs/posts/1959-noam-chomsky/index.html",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "",
    "text": "The book being reviewed is Verbal Behavior. By B. F. Skinner. (The Century Psychology Series.) Pp. viii, 478. New York: Appleton-Century-Crofts, Inc., 1957.\nThe original paper “A Review of B. F. Skinner’s Verbal Behavior” appeared in Language, 35, No. 1 (January–March, 1959), pp.26–58.\nThe paper had been reprinted often. The following preface is from a 1967 reprint that appeared in Readings in the Psychology of Language, ed. Leon A. Jakobovits and Murray S. Miron (Prentice-Hall, Inc., 1967), pp.142–143."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#bibliographic-notes",
    "href": "docs/posts/1959-noam-chomsky/index.html#bibliographic-notes",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "",
    "text": "The book being reviewed is Verbal Behavior. By B. F. Skinner. (The Century Psychology Series.) Pp. viii, 478. New York: Appleton-Century-Crofts, Inc., 1957.\nThe original paper “A Review of B. F. Skinner’s Verbal Behavior” appeared in Language, 35, No. 1 (January–March, 1959), pp.26–58.\nThe paper had been reprinted often. The following preface is from a 1967 reprint that appeared in Readings in the Psychology of Language, ed. Leon A. Jakobovits and Murray S. Miron (Prentice-Hall, Inc., 1967), pp.142–143."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#preface",
    "href": "docs/posts/1959-noam-chomsky/index.html#preface",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "Preface",
    "text": "Preface\nRereading this review after eight years, I find little of substance that I would change if I were to write it today. I am not aware of any theoretical or experimental work that challenges its conclusions; nor, so far as I know, has there been any attempt to meet the criticisms that are raised in the review or to show that they are erroneous or ill-founded.\nI had intended this review not specifically as a criticism of Skinner’s speculations regarding language, but rather as a more general critique of behaviorist (I would now prefer to say “empiricist”) speculation as to the nature of higher mental processes. My reason for discussing Skinner’s book in such detail was that it was the most careful and thoroughgoing presentation of such speculations, an evaluation that I feel is still accurate. Therefore, if the conclusions I attempted to substantiate in the review are correct, as I believe they are, then Skinner’s work can be regarded as, in effect, a reductio ad absurdum of behaviorist assumptions. My personal view is that it is a definite merit, not a defect, of Skinner’s work that it can be used for this purpose, and it was for this reason that I tried to deal with it fairly exhaustively. I do not see how his proposals can be improved upon, aside from occasional details and oversights, within the framework of the general assumptions that he accepts. I do not, in other words, see any way in which his proposals can be substantially improved within the general framework of behaviorist or neobehaviorist, or, more generally, empiricist ideas that has dominated much of modern linguistics, psychology, and philosophy. The conclusion that I hoped to establish in the review, by discussing these speculations in their most explicit and detailed form, was that the general point of view was largely mythology, and that its widespread acceptance is not the result of empirical support, persuasive reasoning, or the absence of a plausible alternative.\nIf I were writing today on the same topic, I would try to make it more clear than I did that I was discussing Skinner’s proposals as a paradigm example of a futile tendency in modern speculation about language and mind. I would also be somewhat less apologetic and hesitant about proposing the alternative view sketched in Sections 5 and 11 – and also less ahistorical in proposing this alternative, since in fact it embodies assumptions that are not only plausible and relatively well-confirmed, so it appears to me, but also deeply rooted in a rich and largely forgotten tradition of rationalist psychology and linguistics. I have tried to correct this imbalance in later publications (Chomsky, 1962, 1964, 1966; see also Miller et al., 1960; Katz and Postal, 1964; Fodor, 1965; Lenneberg, 1966).\nI think it would also have been valuable to try to sketch some of the reasons – and there were many – that have made the view I was criticizing seem plausible over a long period, and also to discuss the reasons for the decline of the alternative rationalist conception which, I was suggesting, should be rehabilitated. Such a discussion would, perhaps, have helped to place the specific critique of Skinner in a more meaningful context.\n\nReferences in the Preface\nChomsky, N., “Explanatory Models in Linguistics,” in Logic, Methodology and Philosophy of Science, ed. E. Nagel, P. Suppes, and A. Tarski. Stanford; Calif.: Stanford University Press, 1962.\n–, Current Issues in Linguistic Theory. The Hague: Mouton and Co., 1964.\n–, Cartesian Linguistics. New York: Harper and Row, Publishers, 1966.\nFodor, J., “Could Meaning Be an ‘rm’,” Journal of Verbal Learning and Verbal Behavior, 4 (1965), 73-81.\nKatz, J. and P. Postal, An Integrated Theory of Linguistic Description. Cambridge, Mass: M.I.T. Press, 1964.\nLenneberg, E., Biological Bases of Language. (In press.)\nMiller, G. A., E. Galanter, and K. H. Pribram, Plans and the Structure of Behavior. New York: Holt, Rhinehart, and Winston, Inc., 1960."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#i",
    "href": "docs/posts/1959-noam-chomsky/index.html#i",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "I",
    "text": "I\nA great many linguists and philosophers concerned with language have expressed the hope that their studies might ultimately be embedded in a framework provided by behaviorist psychology, and that refractory areas of investigation, particularly those in which meaning is involved, will in this way be opened up to fruitful exploration. Since this volume is the first large-scale attempt to incorporate the major aspects of linguistic behavior within a behaviorist framework, it merits and will undoubtedly receive careful attention. Skinner is noted for his contributions to the study of animal behavior. The book under review is the product of study of linguistic behavior extending over more than twenty years. Earlier versions of it have been fairly widely circulated, and there are quite a few references in the psychological literature to its major ideas.\nThe problem to which this book is addressed is that of giving a “functional analysis” of verbal behavior. By functional analysis, Skinner means identification of the variables that control this behavior and specification of how they interact to determine a particular verbal response. Furthermore, the controlling variables are to be described completely in terms of such notions as stimulus, reinforcement, deprivation, which have been given a reasonably clear meaning in animal experimentation. In other words, the goal of the book is to provide a way to predict and control verbal behavior by observing and manipulating the physical environment of the speaker.\nSkinner feels that recent advances in the laboratory study of animal behavior permit us to approach this problem with a certain optimism, since “the basic processes and relations which give verbal behavior its special characteristics are now fairly well understood … the results [of this experimental work] have been surprisingly free of species restrictions. Recent work has shown that the methods can be extended to human behavior without serious modification” (3).1\n1 Skinner’s confidence in recent achievements in the study of animal behavior and their applicability to complex human behavior does not appear to be widely shared. In many recent publications of confirmed behaviorists there is a prevailing note of skepticism with regard to the scope of these achievements. For representative comments, see the contributions to Modern Learning Theory (by W. K. Estes et al.; New York: Appleton-Century-Crofts, Inc., 1954); B. R. Bugelski, Psychology of Learning (New York: Holt, Rinehart & Winston, Inc., 1956); S. Koch, in Nebraska Symposium on Motivation, 58 (Lincoln, 1956); W. S. Verplanck, “Learned and Innate Behavior,” Psych. Rev., 52, (1955), 139. Perhaps the strongest view is that of H. Harlow, who has asserted (“Mice, Monkeys, Men, and Motives,” Psych. Rev., 60, [1953] 23-32) that “a strong case can be made for the proposition that the importance of the psychological problems studied during the last 15 years has decreased as a negatively accelerated function approaching an asymptote of complete indifference.” N. Tinbergen, a leading representative of a different approach to animal behavior studies (comparative ethology), concludes a discussion of functional analysis with the comment that “we may now draw the conclusion that the causation of behavior is immensely more complex than was assumed in the generalizations of the past. A number of internal and external factors act upon complex central nervous structures. Second, it will be obvious that the facts at our disposal are very fragmentary indeed” – The Study of Instinct (Toronto: Oxford Univ. Press, 1951), p. 74.It is important to see clearly just what it is in Skinner’s program and claims that makes them appear so bold and remarkable, It is not primarily the fact that he has set functional analysis as his problem, or that he limits himself to study of observables, i.e., input-output relations. What is so surprising is the particular limitations he has imposed on the way in which the observables of behavior are to be studied, and, above all, the particularly simple nature of the function which, he claims, describes the causation of behavior. One would naturally expect that prediction of the behavior of a complex organism (or machine) would require, in addition to information about external stimulation, knowledge of the internal structure of the organism, the ways in which it processes input information and organizes its own behavior. These characteristics of the organism are in general a complicated product of inborn structure, the genetically determined course of maturation, and past experience. Insofar as independent neurophysiological evidence is not available, it is obvious that inferences concerning the structure of the organism are based on observation of behavior and outside events. Nevertheless, one’s estimate of the relative importance of external factors and internal structure in the determination of behavior will have an important effect on the direction of research on linguistic (or any other) behavior, and on the kinds of analogies from animal behavior studies that will be considered relevant or suggestive.\nPutting it differently, anyone who sets himself the problem of analyzing the causation of behavior will (in the absence of independent neurophysiological evidence) concern himself with the only data available, namely the record of inputs to the organism and the organism’s present response, and will try to describe the function specifying the response in terms of the history of inputs. This is nothing more than the definition of his problem. There are no possible grounds for argument here, if one accepts the problem as legitimate, though Skinner has often advanced and defended this definition of a problem as if it were a thesis which other investigators reject. The differences that arise between those who affirm and those who deny the importance of the specific “contribution of the organism” to learning and performance concern the particular character and complexity of this function, and the kinds of observations and research necessary for arriving at a precise specification of it. If the contribution of the organism is complex, the only hope of predicting behavior even in a gross way will be through a very indirect program of research that begins by studying the detailed character of the behavior itself and the particular capacities of the organism involved.\nSkinner’s thesis is that external factors consisting of present stimulation and the history of reinforcement (in particular, the frequency, arrangement, and withholding of reinforcing stimuli) are of overwhelming importance, and that the general principles revealed in laboratory studies of these phenomena provide the basis for understanding the complexities of verbal behavior. He confidently and repeatedly voices his claim to have demonstrated that the contribution of the speaker is quite trivial and elementary, and that precise prediction of verbal behavior involves only specification of the few external factors that he has isolated experimentally with lower organisms.\nCareful study of this book (and of the research on which it draws) reveals, however, that these astonishing claims are far from justified. It indicates, furthermore, that the insights that have been achieved in the laboratories of the reinforcement theorist, though quite genuine, can be applied to complex human behavior only in the most gross and superficial way, and that speculative attempts to discuss linguistic behavior in these terms alone omit from consideration factors of fundamental importance that are, no doubt, amenable to scientific study, although their specific character cannot at present be precisely formulated. Since Skinner’s work is the most extensive attempt to accommodate human behavior involving higher mental faculties within a strict behaviorist schema of the type that has attracted many linguists and philosophers, as well as psychologists, a detailed documentation is of independent interest. The magnitude of the failure of this attempt to account for verbal behavior serves as a kind of measure of the importance of the factors omitted from consideration, and an indication of how little is really known about this remarkably complex phenomenon.\nThe force of Skinner’s argument lies in the enormous wealth and range of examples for which he proposes a functional analysis. The only way to evaluate the success of his program and the correctness of his basic assumptions about verbal behavior is to review these examples in detail and to determine the precise character of the concepts in terms of which the functional analysis is presented. Section 2 of this review describes the experimental context with respect to which these concepts are originally defined. Sections 3 and 4 deal with the basic concepts – stimulus, response, and reinforcement, Sections 6 to 10 with the new descriptive machinery developed specifically for the description of verbal behavior. In Section 5 we consider the status of the fundamental claim, drawn from the laboratory, which serves as the basis for the analogic guesses about human behavior that have been proposed by many psychologists. The final section (Section 11) will consider some ways in which further linguistic work may play a part in clarifying some of these problems."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#ii",
    "href": "docs/posts/1959-noam-chomsky/index.html#ii",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "II",
    "text": "II\nAlthough this book makes no direct reference to experimental work, it can be understood only in terms of the general framework that Skinner has developed for the description of behavior. Skinner divides the responses of the animal into two main categories. Respondents are purely reflex responses elicited by particular stimuli. Operants are emitted responses, for which no obvious stimulus can be discovered. Skinner has been concerned primarily with operant behavior. The experimental arrangement that he introduced consists basically of a box with a bar attached to one wall in such a way that when the bar is pressed, a food pellet is dropped into a tray (and the bar press is recorded). A rat placed in the box will soon press the bar, releasing a pellet into the tray. This state of affairs, resulting from the bar press, increases the strength of the bar-pressing operant. The food pellet is called a reinforcer; the event, a reinforcing event. The strength of an operant is defined by Skinner in terms of the rate of response during extinction (i.e, after the last reinforcement and before return to the pre-conditioning rate).\nSuppose that release of the pellet is conditional on the flashing of a light. Then the rat will come to press the bar only when the light flashes. This is called stimulus discrimination. The response is called a discriminated operant and the light is called the occasion for its emission: this is to be distinguished from elicitation of a response by a stimulus in the case of the respondent.2 Suppose that the apparatus is so arranged that bar-pressing of only a certain character (e.g., duration) will release the pellet. The rat will then come to press the bar in the required way. This process is called response differentiation. By successive slight changes in the conditions under which the response will be reinforced, it is possible to shape the response of a rat or a pigeon in very surprising ways in a very short time, so that rather complex behavior can be produced by a process of successive approximation.\n2 In Behavior of Organisms (New York: Appleton-Century-Crofts, Inc., 1938), Skinner remarks that “although a conditioned operant is the result of the correlation of the response with a particular reinforcement, a relation between it and a discriminative stimulus acting prior to the response is the almost universal rule” (178-79). Even emitted behavior is held to be produced by some sort of “originating force” (51) which, in the case of operant behavior is not under experimental control. The distinction between eliciting stimuli, discriminated stimuli, and “originating forces” has never been adequately clarified and becomes even more confusing when private internal events are considered to be discriminated stimuli (see below).3 In a famous experiment, chimpanzees were taught to perform complex tasks to receive tokens which had become secondary reinforcers because of association with food. The idea that money, approval, prestige, etc. actually acquire their motivating effects on human behavior according to this paradigm is unproved, and not particularly plausible. Many psychologists within the behaviorist movement are quite skeptical about this (cf. 23n). As in the case of most aspects of human behavior, the evidence about secondary reinforcement is so fragmentary, conflicting, and complex that almost any view can find some support.A stimulus can become reinforcing by repeated association with an already reinforcing stimulus. Such a stimulus is called a secondary reinforcer. Like many contemporary behaviorists, Skinner considers money, approval, and the like to be secondary reinforcers which have become reinforcing because of their association with food, etc.3 Secondary reinforcers can be generalized by associating them with a variety of different primary reinforcers.\nAnother variable that can affect the rate of the bar-pressing operant is drive, which Skinner defines operationally in terms of hours of deprivation. His major scientific book, Behavior of Organisms, is a study of the effects of food-deprivation and conditioning on the strength of the bar-pressing response of healthy mature rats. Probably Skinner’s most original contribution to animal behavior studies has been his investigation of the effects of intermittent reinforcement, arranged in various different ways, presented in Behavior of Organisms and extended (with pecking of pigeons as the operant under investigation) in the recent Schedules of Reinforcement by Ferster and Skinner (1957). It is apparently these studies that Skinner has in mind when he refers to the recent advances in the study of animal behavior.4\n4 Skinner’s remark quoted above about the generality of his basic results must be understood in the light of the experimental limitations he has imposed. If it were true in any deep sense that the basic processes in language are well understood and free of species restriction, it would be extremely odd that language is limited to man. With the exception of a few scattered observations (cf. his article, “A Case History in Scientific Method,” The American Psychologist, 11 [1956] 221-33), Skinner is apparently basing this claim on the fact that qualitatively similar results are obtained with bar pressing of rats and pecking of pigeons under special conditions of deprivation and various schedules of reinforcement. One immediately questions how much can be based on these facts, which are in part at least an artifact traceable to experimental design and the definition of stimulus and response in terms of smooth dynamic curves (see below). The dangers inherent in any attempt to extrapolate to complex behavior from the study of such simple responses as bar pressing should be obvious and have often been commented on (cf., e.g., Harlow, op. cit.). The generality of even the simplest results is open to serious question. Cf. in this connection M. E. Bitterman, J. Wodinsky, and D. K. Candland, “Some Comparative Psychology,” Am. Jour. of Psych., 71 (1958), 94-110, where it is shown that there are important qualitative differences in solution of comparable elementary problems by rats and fish.The notions stimulus, response, reinforcement are relatively well defined with respect to the bar-pressing experiments and others similarly restricted. Before we can extend them to real-life behavior, however, certain difficulties must be faced. We must decide, first of all, whether any physical event to which the organism is capable of reacting is to be called a stimulus on a given occasion, or only one to which the organism in fact reacts; and correspondingly, we must decide whether any part of behavior is to be called a response, or only one connected with stimuli in lawful ways. Questions of this sort pose something of a dilemma for the experimental psychologist. If he accepts the broad definitions, characterizing any physical event impinging on the organism as a stimulus and any part of the organism’s behavior as a response, he must conclude that behavior has not been demonstrated to be lawful. In the present state of our knowledge, we must attribute an overwhelming influence on actual behavior to ill-defined factors of attention, set, volition, and caprice. If we accept the narrower definitions, then behavior is lawful by definition (if it consists of responses); but this fact is of limited significance, since most of what the animal does will simply not be considered behavior. Hence, the psychologist either must admit that behavior is not lawful (or that he cannot at present show that it is – not at all a damaging admission for a developing science), or must restrict his attention to those highly limited areas in which it is lawful (e.g., with adequate controls, bar-pressing in rats; lawfulness of the observed behavior provides, for Skinner, an implicit definition of a good experiment).\nSkinner does not consistently adopt either course. He utilizes the experimental results as evidence for the scientific character of his system of behavior, and analogic guesses (formulated in terms of a metaphoric extension of the technical vocabulary of the laboratory) as evidence for its scope. This creates the illusion of a rigorous scientific theory with a very broad scope, although in fact the terms used in the description of real-life and of laboratory behavior may be mere homonyms, with at most a vague similarity of meaning. To substantiate this evaluation, a critical account of his book must show that with a literal reading (where the terms of the descriptive system have something like the technical meanings given in Skinner’s definitions) the book covers almost no aspect of linguistic behavior, and that with a metaphoric reading, it is no more scientific than the traditional approaches to this subject matter, and rarely as clear and careful.5\n5 An analogous argument, in connection with a different aspect of Skinner’s thinking, is given by M. Scriven in “A Study of Radical Behaviorism,” Univ. of Minn. Studies in Philosophy of Science, I. Cf. Verplanck’s contribution to Modern Learning Theory, op. cit. pp. 283-88, for more general discussion of the difficulties in formulating an adequate definition of stimulus and response. He concludes, quite correctly, that in Skinner’s sense of the word, stimuli are not objectively identifiable independently of the resulting behavior, nor are they manipulable. Verplanck presents a clear discussion of many other aspects of Skinner’s system, commenting on the untestability of many of the so-called “laws of behavior” and the limited scope of many of the others, and the arbitrary and obscure character of Skinner’s notion of lawful relation; and, at the same time, noting the importance of the experimental data that Skinner has accumulated."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#iii",
    "href": "docs/posts/1959-noam-chomsky/index.html#iii",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "III",
    "text": "III\nConsider first Skinner’s use of the notions stimulus and response. In Behavior of Organisms (9) he commits himself to the narrow definitions for these terms. A part of the environment and a part of behavior are called stimulus (eliciting, discriminated, or reinforcing) and response, respectively, only if they are lawfully related; that is, if the dynamic laws relating them show smooth and reproducible curves. Evidently, stimuli and responses, so defined, have not been shown to figure very widely in ordinary human behavior.6 We can, in the face of presently available evidence, continue to maintain the lawfulness of the relation between stimulus and response only by depriving them of their objective character. A typical example of stimulus control for Skinner would be the response to a piece of music with the utterance Mozart or to a painting with the response Dutch. These responses are asserted to be “under the control of extremely subtle properties” of the physical object or event (108). Suppose instead of saying Dutch we had said Clashes with the wallpaper, I thought you liked abstract work, Never saw it before, Tilted, Hanging too low, Beautiful, Hideous, Remember our camping trip last summer?, or whatever else might come into our minds when looking at a picture (in Skinnerian translation, whatever other responses exist in sufficient strength). Skinner could only say that each of these responses is under the control of some other stimulus property of the physical object. If we look at a red chair and say red, the response is under the control of the stimulus redness; if we say chair, it is under the control of the collection of properties (for Skinner, the object) chairness (110), and similarly for any other response. This device is as simple as it is empty. Since properties are free for the asking (we have as many of them as we have nonsynonymous descriptive expressions in our language, whatever this means exactly), we can account for a wide class of responses in terms of Skinnerian functional analysis by identifying the controlling stimuli. But the word stimulus has lost all objectivity in this usage. Stimuli are no longer part of the outside physical world; they are driven back into the organism. We identify the stimulus when we hear the response. It is clear from such examples, which abound, that the talk of stimulus control simply disguises a complete retreat to mentalistic psychology. We cannot predict verbal behavior in terms of the stimuli in the speaker’s environment, since we do not know what the current stimuli are until he responds. Furthermore, since we cannot control the property of a physical object to which an individual will respond, except in highly artificial cases, Skinner’s claim that his system, as opposed to the traditional one, permits the practical control of verbal behavior7 is quite false.\n6 In Behavior of Organisms, Skinner apparently was willing to accept this consequence. He insists (41-42) that the terms of casual description in the popular vocabulary are not validly descriptive until the defining properties of stimulus and response are specified, the correlation is demonstrated experimentally, and the dynamic changes in it are shown to be lawful. Thus, in describing a child as hiding from a dog, “it will not be enough to dignify the popular vocabulary by appealing to essential properties of dogness or hidingness and to suppose them intuitively known.” But this is exactly what Skinner does in the book under review, as we will see directly.7 253f. and elsewhere, repeatedly. As an example of how well we can control behavior using the notions developed in this book, Skinner shows here how he would go about evoking the response pencil. The most effective way, he suggests, is to say to the subject, “Please say pencil” (our chances would, presumably, be even further improved by use of “aversive stimulation,” e.g., holding a gun to his head). We can also “make sure that no pencil or writing instrument is available, then hand our subject a pad of paper appropriate to pencil sketching, and offer him a handsome reward for a recognizable picture of a cat.” It would also be useful to have voices saying pencil or pen and … in the background; signs reading pencil or pen and …; or to place a “large and unusual pencil in an unusual place clearly in sight.” “Under such circumstances, it is highly probable that our subject will say pencil.” “The available techniques are all illustrated in this sample.” These contributions of behavior theory to the practical control of human behavior are amply illustrated elsewhere in the book, as when Skinner shows (113-14) how we can evoke the response red (the device suggested is to hold a red object before the subject and say, “Tell me what color this is”).Other examples of stimulus control merely add to the general mystification. Thus, a proper noun is held to be a response “under the control of a specific person or thing” (as controlling stimulus, 113). I have often used the words Eisenhower and Moscow, which I presume are proper nouns if anything is, but have never been stimulated by the corresponding objects. How can this fact be made compatible with this definition? Suppose that I use the name of a friend who is not present. Is this an instance of a proper noun under the control of the friend as stimulus? Elsewhere it is asserted that a stimulus controls a response in the sense that presence of the stimulus increases the probability of the response. But it is obviously untrue that the probability that a speaker will produce a full name is increased when its bearer faces the speaker. Furthermore, how can one’s own name be a proper noun in this sense?\nA multitude of similar questions arise immediately. It appears that the word control here is merely a misleading paraphrase for the traditional denote or refer. The assertion (115) that so far as the speaker is concerned, the relation of reference is “simply the probability that the speaker will emit a response of a given form in the presence of a stimulus having specified properties” is surely incorrect if we take the words presence, stimulus, and probability in their literal sense. That they are not intended to be taken literally is indicated by many examples, as when a response is said to be “controlled” by a situation or state of affairs as “stimulus.” Thus, the expression a needle in a haystack “may be controlled as a unit by a particular type of situation” (116); the words in a single part of speech, e.g., all adjectives, are under the control of a single set of subtle properties of stimuli (121); “the sentence The boy runs a store is under the control of an extremely complex stimulus situation” (335) “He is not at all well may function as a standard response under the control of a state of affairs which might also control He is ailing” (325); when an envoy observes events in a foreign country and reports upon his return, his report is under “remote stimulus control” (416); the statement This is war may be a response to a “confusing international situation” (441); the suffix -ed is controlled by that “subtle property of stimuli which we speak of as action-in-the-past” (121) just as the -s in The boy runs is under the control of such specific features of the situation as its “currency” (332). No characterization of the notion stimulus control that is remotely related to the bar-pressing experiment (or that preserves the faintest objectivity) can be made to cover a set of examples like these, in which, for example, the controlling stimulus need not even impinge on the responding organism.\nConsider now Skinner’s use of the notion response. The problem of identifying units in verbal behavior has of course been a primary concern of linguists, and it seems very likely that experimental psychologists should be able to provide much-needed assistance in clearing up the many remaining difficulties in systematic identification. Skinner recognizes (20) the fundamental character of the problem of identification of a unit of verbal behavior, but is satisfied with an answer so vague and subjective that it does not really contribute to its solution. The unit of verbal behavior – the verbal operant – is defined as a class of responses of identifiable form functionally related to one or more controlling variables. No method is suggested for determining in a particular instance what are the controlling variables, how many such units have occurred, or where their boundaries are in the total response. Nor is any attempt made to specify how much or what kind of similarity in form or control is required for two physical events to be considered instances of the same operant. In short, no answers are suggested for the most elementary questions that must be asked of anyone proposing a method for description of behavior. Skinner is content with what he calls an extrapolation of the concept of operant developed in the laboratory to the verbal field. In the typical Skinnerian experiment, the problem of identifying the unit of behavior is not too crucial. It is defined, by fiat, as a recorded peck or bar-press, and systematic variations in the rate of this operant and its resistance to extinction are studied as a function of deprivation and scheduling of reinforcement (pellets). The operant is thus defined with respect to a particular experimental procedure. This is perfectly reasonable and has led to many interesting results. It is, however, completely meaningless to speak of extrapolating this concept of operant to ordinary verbal behavior. Such “extrapolation” leaves us with no way of justifying one or another decision about the units in the “verbal repertoire.”\nSkinner specifies “response strength” as the basic datum, the basic dependent variable in his functional analysis. In the bar-pressing experiment, response strength is defined in terms of rate of emission during extinction. Skinner has argued8 that this is “the only datum that varies significantly and in the expected direction under conditions which are relevant to the ‘learning process.’” In the book under review, response strength is defined as “probability of emission” (22). This definition provides a comforting impression of objectivity, which, however, is quickly dispelled when we look into the matter more closely. The term probability has some rather obscure meaning for Skinner in this book.9 We are told, on the one hand, that “our evidence for the contribution of each variable [to response strength] is based on observation of frequencies alone” (28). At the same time, it appears that frequency is a very misleading measure of strength, since, for example, the frequency of a response may be “primarily attributable to the frequency of occurrence of controlling variables” (27). It is not clear how the frequency of a response can be attributable to anything BUT the frequency of occurrence of its controlling variables if we accept Skinner’s view that the behavior occurring in a given situation is “fully determined” by the relevant controlling variables (175, 228). Furthermore, although the evidence for the contribution of each variable to response strength is based on observation of frequencies alone, it turns out that “we base the notion of strength upon several kinds of evidence” (22), in particular (22-28): emission of the response (particularly in unusual circumstances), energy level (stress), pitch level, speed and delay of emission, size of letters etc. in writing, immediate repetition, and – a final factor, relevant but misleading – over-all frequency.\n8 “Are Theories of Learning Necessary?”, Psych. Rev., 57 (1950), 193-216.9 And elsewhere. In his paper “Are Theories of Learning Necessary?” Skinner considers the problem how to extend his analysis of behavior to experimental situations in which it is impossible to observe frequencies, rate of response being the only valid datum. His answer is that “the notion of probability is usually extrapolated to cases in which a frequency analysis cannot be carried out. In the field of behavior we arrange a situation in which frequencies are available as data, but we use the notion of probability in analyzing or formulating instances of even types of behavior which are not susceptible to this analysis” (199). There are, of course, conceptions of probability not based directly on frequency, but I do not see how any of these apply to the cases that Skinner has in mind. I see no way of interpreting the quoted passage other than as signifying an intention to use the word probability in describing behavior quite independently of whether the notion of probability is at all relevant.10 Fortunately, “In English this presents no great difficulty” since, for example, “relative pitch levels … are not … important” (25). No reference is made to the numerous studies of the function of relative pitch levels and other intonational features in English.Of course, Skinner recognizes that these measures do not co-vary, because (among other reasons) pitch, stress, quantity, and reduplication may have internal linguistic functions.10 However, he does not hold these conflicts to be very important, since the proposed factors indicative of strength are “fully understood by everyone” in the culture (27). For example, “if we are shown a prized work of art and exclaim Beautiful!, the speed and energy of the response will not be lost on the owner.” It does not appear totally obvious that in this case the way to impress the owner is to shriek Beautiful in a loud, high-pitched voice, repeatedly, and with no delay (high response strength). It may be equally effective to look at the picture silently (long delay) and then to murmur Beautiful in a soft, low-pitched voice (by definition, very low response strength).\nIt is not unfair, I believe, to conclude from Skinner’s discussion of response strength, the basic datum in functional analysis, that his extrapolation of the notion of probability can best be interpreted as, in effect, nothing more than a decision to use the word probability, with its favorable connotations of objectivity, as a cover term to paraphrase such low-status words as interest, intention, belief, and the like. This interpretation is fully justified by the way in which Skinner uses the terms probability and strength. To cite just one example, Skinner defines the process of confirming an assertion in science as one of “generating additional variables to increase its probability” (425), and more generally, its strength (425-29). If we take this suggestion quite literally, the degree of confirmation of a scientific assertion can be measured as a simple function of the loudness, pitch, and frequency with which it is proclaimed, and a general procedure for increasing its degree of confirmation would be, for instance, to train machine guns on large crowds of people who have been instructed to shout it. A better indication of what Skinner probably has in mind here is given by his description of how the theory of evolution, as an example, is confirmed. This “single set of verbal responses … is made more plausible – is strengthened – by several types of construction based upon verbal responses in geology, paleontology, genetics, and so on” (427). We are no doubt to interpret the terms strength and probability in this context as paraphrases of more familiar locutions such as “justified belief” or “warranted assertability,” or something of the sort. Similar latitude of interpretation is presumably expected when we read that “frequency of effective action accounts in turn for what we may call the listener’s ‘belief’” (88) or that “our belief in what someone tells us is similarly a function of, or identical with, our tendency to act upon the verbal stimuli which he provides” (160).11\n11 The vagueness of the word tendency, as opposed to frequency, saves the latter quotation from the obvious incorrectness of the former. Nevertheless, a good deal of stretching is necessary. If tendency has anything like its ordinary meaning, the remark is clearly false. One may believe strongly the assertion that Jupiter has four moons, that many of Sophocles’ plays have been irretrievably lost, that the earth will burn to a crisp in ten million years, and so on, without experiencing the slightest tendency to act upon these verbal stimuli. We may, of course, turn Skinner’s assertion into a very unilluminating truth by defining “tendency to act” to include tendencies to answer questions in certain ways, under motivation to say what one believes is true.I think it is evident, then, that Skinner’s use of the terms stimulus, control, response, and strength justify the general conclusion stated in the last paragraph of Section 2. The way in which these terms are brought to bear on the actual data indicates that we must interpret them as mere paraphrases for the popular vocabulary commonly used to describe behavior and as having no particular connection with the homonymous expressions used in the description of laboratory experiments. Naturally, this terminological revision adds no objectivity to the familiar mentalistic mode of description."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#iv",
    "href": "docs/posts/1959-noam-chomsky/index.html#iv",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "IV",
    "text": "IV\nThe other fundamental notion borrowed from the description of bar-pressing experiments is reinforcement. It raises problems which are similar, and even more serious. In Behavior of Organisms, “the operation of reinforcement is defined as the presentation of a certain kind of stimulus in a temporal relation with either a stimulus or response. A reinforcing stimulus is defined as such by its power to produce the resulting change [in strength]. There is no circularity about this: some stimuli are found to produce the change, others not, and they are classified as reinforcing and nonreinforcing accordingly” (62). This is a perfectly appropriate definition12 for the study of schedules of reinforcement. It is perfectly useless, however, in the discussion of real-life behavior, unless we can somehow characterize the stimuli which are reinforcing (and the situations and conditions under which they are reinforcing). Consider first of all the status of the basic principle that Skinner calls the “law of conditioning” (law of effect). It reads: “if the occurrence of an operant is followed by presence of a reinforcing stimulus, the strength is increased” (Behavior of Organisms, 21). As reinforcement was defined, this law becomes a tautology.13 For Skinner, learning is just change in response strength.14 Although the statement that presence of reinforcement is a sufficient condition for learning and maintenance of behavior is vacuous, the claim that it is a necessary condition may have some content, depending on how the class of reinforcers (and appropriate situations) is characterized. Skinner does make it very clear that in his view reinforcement is a necessary condition for language learning and for the continued availability of linguistic responses in the adult.15 However, the looseness of the term reinforcement as Skinner uses it in the book under review makes it entirely pointless to inquire into the truth or falsity of this claim. Examining the instances of what Skinner calls reinforcement, we find that not even the requirement that a reinforcer be an identifiable stimulus is taken seriously. In fact, the term is used in such a way that the assertion that reinforcement is necessary for learning and continued availability of behavior is likewise empty.\n12 One should add, however, that it is in general not the stimulus as such that is reinforcing, but the stimulus in a particular situational context. Depending on experimental arrangement, a particular physical event or object may be reinforcing, punishing, or unnoticed. Because Skinner limits himself to a particular, very simple experimental arrangement, it is not necessary for him to add this qualification, which would not be at all easy to formulate precisely. But it is of course necessary if he expects to extend his descriptive system to behavior in general.13 This has been frequently noted.14 See, for example, “Are Theories of Learning Necessary?”, op. cit., p. 199. Elsewhere, he suggests that the term learning be restricted to complex situations, but these are not characterized.15 “A child acquires verbal behavior when relatively unpatterned vocalizations, selectively reinforced, gradually assume forms which produce appropriate consequences in a given verbal community” (31). “Differential reinforcement shapes up all verbal forms, and when a prior stimulus enters into the contingency, reinforcement is responsible for its resulting control…. The availability of behavior, its probability or strength, depends on whether reinforcements continue in effect and according to what schedules” (203-4); elsewhere, frequently.To show this, we consider some examples of reinforcement. First of all, we find a heavy appeal to automatic self-reinforcement, Thus, “a man talks to himself… because of the reinforcement he receives” (163); “the child is reinforced automatically when he duplicates the sounds of airplanes, streetcars …” (164); “the young child alone in the nursery may automatically reinforce his own exploratory verbal behavior when he produces sounds which he has heard in the speech of others” (58); “the speaker who is also an accomplished listener ‘knows when he has correctly echoed a response’ and is reinforced thereby” (68); thinking is “behaving which automatically affects the behaver and is reinforcing because it does so” (438; cutting one’s finger should thus be reinforcing, and an example of thinking); “the verbal fantasy, whether overt or covert, is automatically reinforcing to the speaker as listener. Just as the musician plays or composes what he is reinforced by hearing, or as the artist paints what reinforces him visually, so the speaker engaged in verbal fantasy says what he is reinforced by hearing or writes what he is reinforced by reading” (439); similarly, care in problem solving, and rationalization, are automatically self-reinforcing (442-43). We can also reinforce someone by emitting verbal behavior as such (since this rules out a class of aversive stimulations, 167), by not emitting verbal behavior (keeping silent and paying attention, 199), or by acting appropriately on some future occasion (152: “the strength of [the speaker’s] behavior is determined mainly by the behavior which the listener will exhibit with respect to a given state of affairs”; this Skinner considers the general case of “communication” or “letting the listener know”). In most such cases, of course, the speaker is not present at the time when the reinforcement takes place, as when “the artist…is reinforced by the effects his works have upon… others” (224), or when the writer is reinforced by the fact that his “verbal behavior may reach over centuries or to thousands of listeners or readers at the same time. The writer may not be reinforced often or immediately, but his net reinforcement may be great” (206; this accounts for the great “strength” of his behavior). An individual may also find it reinforcing to injure someone by criticism or by bringing bad news, or to publish an experimental result which upsets the theory of a rival (154), to describe circumstances which would be reinforcing if they were to occur (165), to avoid repetition (222), to “hear” his own name though in fact it was not mentioned or to hear nonexistent words in his child’s babbling (259), to clarify or otherwise intensify the effect of a stimulus which serves an important discriminative function (416), and so on.\nFrom this sample, it can be seen that the notion of reinforcement has totally lost whatever objective meaning it may ever have had. Running through these examples, we see that a person can be reinforced though he emits no response at all, and that the reinforcing stimulus need not impinge on the reinforced person or need not even exist (it is sufficient that it be imagined or hoped for). When we read that a person plays what music he likes (165), says what he likes (165), thinks what he likes (438-39), reads what books he likes (163), etc., BECAUSE he finds it reinforcing to do so, or that we write books or inform others of facts BECAUSE we are reinforced by what we hope will be the ultimate behavior of reader or listener, we can only conclude that the term reinforcement has a purely ritual function. The phrase “X is reinforced by Y (stimulus, state of affairs, event, etc.)” is being used as a cover term for “X wants Y,” “X likes Y,” “X wishes that Y were the case,” etc. Invoking the term reinforcement has no explanatory force, and any idea that this paraphrase introduces any new clarity or objectivity into the description of wishing, liking, etc., is a serious delusion. The only effect is to obscure the important differences among the notions being paraphrased. Once we recognize the latitude with which the term reinforcement is being used, many rather startling comments lose their initial effect – for instance, that the behavior of the creative artist is “controlled entirely by the contingencies of reinforcement” (150). What has been hoped for from the psychologist is some indication how the casual and informal description of everyday behavior in the popular vocabulary can be explained or clarified in terms of the notions developed in careful experiment and observation, or perhaps replaced in terms of a better scheme. A mere terminological revision, in which a term borrowed from the laboratory is used with the full vagueness of the ordinary vocabulary, is of no conceivable interest.\nIt seems that Skinner’s claim that all verbal behavior is acquired and maintained in “strength” through reinforcement is quite empty, because his notion of reinforcement has no clear content, functioning only as a cover term for any factor, detectable or not, related to acquisition or maintenance of verbal behavior.16 Skinner’s use of the term conditioning suffers from a similar difficulty. Pavlovian and operant conditioning are processes about which psychologists have developed real understanding. Instruction of human beings is not. The claim that instruction and imparting of information are simply matters of conditioning (357-66) is pointless. The claim is true, if we extend the term conditioning to cover these processes, but we know no more about them after having revised this term in such a way as to deprive it of its relatively clear and objective character. It is, as far as we know, quite false, if we use conditioning in its literal sense. Similarly, when we say that “it is the function of predication to facilitate the transfer of response from one term to another or from one object to another” (361), we have said nothing of any significance. In what sense is this true of the predication Whales are mammals? Or, to take Skinner’s example, what point is there in saying that the effect of The telephone is out of order on the listener is to bring behavior formerly controlled by the stimulus out of order under control of the stimulus telephone (or the telephone itself) by a process of simple conditioning (362)? What laws of conditioning hold in this case? Furthermore, what behavior is controlled by the stimulus out of order, in the abstract? Depending on the object of which this is predicated, the present state of motivation of the listener, etc., the behavior may vary from rage to pleasure, from fixing the object to throwing it out, from simply not using it to trying to use it in the normal way (e.g., to see if it is really out of order), and so on. To speak of “conditioning” or “bringing previously available behavior under control of a new stimulus” in such a case is just a kind of play-acting at science (cf. also 43n).\n16 Talk of schedules of reinforcement here is entirely pointless. How are we to decide, for example, according to what schedules covert reinforcement is arranged, as in thinking or verbal fantasy, or what the scheduling is of such factors as silence, speech, and appropriate future reactions to communicated information?"
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#v",
    "href": "docs/posts/1959-noam-chomsky/index.html#v",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "V",
    "text": "V\nThe claim that careful arrangement of contingencies of reinforcement by the verbal community is a necessary condition for language-learning has appeared, in one form or another, in many places.17 Since it is based not on actual observation, but on analogies to laboratory study of lower organisms, it is important to determine the status of the underlying assertion within experimental psychology proper. The most common characterization of reinforcement (one which Skinner explicitly rejects, incidentally) is in terms of drive reduction. This characterization can be given substance by defining drives in some way independently of what in fact is learned. If a drive is postulated on the basis of the fact that learning takes place, the claim that reinforcement is necessary for learning will again become as empty as it is in the Skinnerian framework. There is an extensive literature on the question of whether there can be learning without drive reduction (latent learning). The “classical” experiment of Blodgett indicated that rats who had explored a maze without reward showed a marked drop in number of errors (as compared to a control group which had not explored the maze) upon introduction of a food reward, indicating that the rat had learned the structure of the maze without reduction of the hunger drive. Drive-reduction theorists countered with an exploratory drive which was reduced during the pre-reward learning, and claimed that a slight decrement in errors could be noted before food reward. A wide variety of experiments, with somewhat conflicting results, have been carried out with a similar design.18 Few investigators still doubt the existence of the phenomenon, E. R. Hilgard, in his general review of learning theory,19 concludes that “there is no longer any doubt but that, under appropriate circumstances, latent learning is demonstrable.”\n17 See, for example, N. E. Miller and J. Dollard, Social Learning and Imitation (New York, 1941), pp. 82-83, for a discussion of the “meticulous training” that they seem to consider necessary for a child to learn the meanings of words and syntactic patterns. The same notion is implicit in O. H. Mowrer’s speculative account of how language might be acquired, in Learning Theory and Personality Dynamics, (New York: The Ronald Press, Inc., 1950), Chap. 23. Actually, the view appears to be quite general.18 For a general review and analysis of this literature, see D. L. Thistlethwaite, “A Critical Review of Latent Learning and Related Experiments,” Psych. Bull., 48 (1951), 97-129. K. MacCorquodale and P. E. Meehl, in their contribution to Modern Learning Theory op. cit., carry out a serious and considered attempt to handle the latent learning material from the standpoint of drive-reduction theory, with (as they point out) not entirely satisfactory results. W. H. Thorpe reviews the literature from the standpoint of the ethologist, adding also material on homing and topographical orientation (Learning and Instinct in Animals [Cambridge, 1956]).19 Theories of Learning, 214 (1956).20 O. E. Berlyne, “Novelty and Curiosity as Determinants of Exploratory Behavior,” Brit. Jour. of Psych., 41 (1950), 68-80; id., “Perceptual Curiosity in the Rat,” Jour. of Comp. Physiol. Psych., 48 (1955), 238-46; W. R. Thompson and L. M. Solomon, “Spontaneous Pattern Discrimination in the Rat,” ibid., 47 (1954), 104-7.21 K. C. Montgomery, “The Role of the Exploratory Drive in Learning,” ibid. pp. 60-63. Many other papers in the same journal are designed to show that exploratory behavior is a relatively independent primary “drive” aroused by novel external stimulation.22 R. A. Butler, “Discrimination Learning by Rhesus Monkeys to Visual-Exploration Motivation,” ibid., 46 (1953), 95-98. Later experiments showed that this “drive” is highly persistent, as opposed to derived drives which rapidly extinguish.23 H. F. Harlow, M. K. Harlow, and D. R. Meyer, “Learning Motivated by a Manipulation Drive,” Jour. Exp. Psych., 40 (1950), 228-34, and later investigations initiated by Harlow. Harlow has been particularly insistent on maintaining the inadequacy of physiologically based drives and homeostatic need states for explaining the persistence of motivation and rapidity of learning in primates. He points out, in many papers, that curiosity, play, exploration, and manipulation are, for primates, often more potent drives than hunger and the like, and that they show none of the characteristics of acquired drives. Hebb also presents behavioral and supporting neurological evidence in support of the view that in higher animals there is a positive attraction in work, risk, puzzle, intellectual activity, mild fear and frustration, and so on. “Drives and the CNS,” Psych. Rev., 62 [1955], 243-54.) He concludes that “we need not work out tortuous and improbable ways to explain why men work for money, why children learn without pain, why people dislike doing nothing.” In a brief note “Early Recognition of the Manipulative Drive in Monkeys,” British Journal of Animal Behaviour, 3 [1955], 71-72), W. Dennis calls attention to the fact that early investigators (G. J. Romanes, 1882; E. L. Thorndike, 1901), whose “perception was relatively unaffected by learning theory, did note the intrinsically motivated behavior of monkeys,” although, he asserts, no similar observations on monkeys have been made until Harlow’s experiments. He quotes Romanes (Animal Intelligence [1882]) as saying that “much the most striking feature in the psychology of this animal, and the one which is least like anything met with in other animals, was the tireless spirit of investigation.” Analogous developments, in which genuine discoveries have blinded systematic investigators to the important insights of earlier work, are easily found within recent structural linguistics as well.24 Thus, J. S. Brown, in commenting on a paper of Harlow’s in Current Theory and Research in Motivation (Lincoln: Univ. of Nebraska Press, 1953), argues that “in probably every instance [of the experiments cited by Harlow] an ingenious drive-reduction theorist could find some fragment of fear, insecurity, frustration, or whatever, that he could insist was reduced and hence was reinforcing” (53). The same sort of thing could be said for the ingenious phlogiston or ether theorist.More recent work has shown that novelty and variety of stimulus are sufficient to arouse curiosity in the rat and to motivate it to explore (visually), and in fact, to learn (since on a presentation of two stimuli, one novel, one repeated, the rat will attend to the novel one),20 that rats will learn to choose the arm of a single-choice maze that leads to a complex maze, running through this being their only “reward”;21 that monkeys can learn object discriminations and maintain their performance at a high level of efficiency with visual exploration (looking out of a window for 30 seconds) as the only reward22 and, perhaps most strikingly of all, that monkeys and apes will solve rather complex manipulation problems that are simply placed in their cages, and will solve discrimination problems with only exploration and manipulation as incentives.23 In these cases, solving the problem is apparently its own “reward.” Results of this kind can be handled by reinforcement theorists only if they are willing to set up curiosity, exploration, and manipulation drives, or to speculate somehow about acquired drives24 for which there is no evidence outside of the fact that learning takes place in these cases.\nThere is a variety of other kinds of evidence that has been offered to challenge the view that drive reduction is necessary for learning. Results on sensory-sensory conditioning have been interpreted as demonstrating learning without drive reduction.25 Olds has reported reinforcement by direct stimulation of the brain, from which he concludes that reward need not satisfy a physiological need or withdraw a drive stimulus.26 The phenomenon of imprinting, long observed by zoologists, is of particular interest in this connection. Some of the most complex patterns of behavior of birds, in particular, are directed towards objects and animals of the type to which they have been exposed at certain critical early periods of life.27 Imprinting is the most striking evidence for the innate disposition of the animal to learn in a certain direction and to react appropriately to patterns and objects of certain restricted types, often only long after the original learning has taken place. It is, consequently, unrewarded learning, though the resulting patterns of behavior may be refined through reinforcement. Acquisition of the typical songs of song birds is, in some cases, a type of imprinting. Thorpe reports studies that show “that some characteristics of the normal song have been learned in the earliest youth, before the bird itself is able to produce any kind of full song.”28 The phenomenon of imprinting has recently been investigated under laboratory conditions and controls with positive results.29\n25 Cf. H. G. Birch and M. E. Bitterman, “Reinforcement and Learning: The process of Sensory Integration,” Psych. Rev., 56 (1949), 292-308.26 See, for example, his paper “A Physiological Study of Reward” in D. C. McClelland, ed., Studies in Motivation (New York: Appleton-Century-Crafts, Inc., 1955), pp. 134-43.27 See Thorpe, op. cit., particularly pp. 115-18 and 337-76, for an excellent discussion of this phenomenon, which has been brought to prominence particularly by the work of K. Lorenz (cf. “Der Kumpan in der Umwelt des Vogels,” parts of which are reprinted in English translation in C. M. Schiller, ed., Instinctive Behavior [New York: International Universities Press, 1957], pp. 83-128).28 Op. cit., p. 372.29 See, e.g., J. Jaynes, “Imprinting: Interaction of Learned and Innate Behavior,” Jour. of Comp. Physiol. Psych., 49 (1956), 201-6, where the conclusion is reached that “the experiments prove that without any observable reward, young birds of this species follow a moving stimulus object and very rapidly come to prefer that object to others.”Phenomena of this general type are certainly familiar from everyday experience. We recognize people and places to which we have given no particular attention. We can look up something in a book and learn it perfectly well with no other motive than to confute reinforcement theory, or out of boredom, or idle curiosity. Everyone engaged in research must have had the experience of working with feverish and prolonged intensity to write a paper which no one else will read or to solve a problem which no one else thinks important and which will bring no conceivable reward – which may only confirm a general opinion that the researcher is wasting his time on irrelevancies. The fact that rats and monkeys do likewise is interesting and important to show in careful experiment. In fact, studies of behavior of the type mentioned above have an independent and positive significance that far outweighs their incidental importance in bringing into question the claim that learning is impossible without drive reduction. It is not at all unlikely that insights arising from animal behavior studies with this broadened scope may have the kind of relevance to such complex activities as verbal behavior that reinforcement theory has, so far, failed to exhibit. In any event, in the light of presently available evidence, it is difficult to see how anyone can be willing to claim that reinforcement is necessary for learning, if reinforcement is taken seriously as something identifiable independently of the resulting change in behavior.\nSimilarly, it seems quite beyond question that children acquire a good deal of their verbal and nonverbal behavior by casual observation and imitation of adults and other children.30 It is simply not true that children can learn language only through “meticulous care” on the part of adults who shape their verbal repertoire through careful differential reinforcement, though it may be that such care is often the custom in academic families. It is a common observation that a young child of immigrant parents may learn a second language in the streets, from other children, with amazing rapidity, and that his speech may be completely fluent and correct to the last allophone, while the subtleties that become second nature to the child may elude his parents despite high motivation and continued practice. A child may pick up a large part of his vocabulary and “feel” for sentence structure from television, from reading, from listening to adults, etc. Even a very young child who has not yet acquired a minimal repertoire from which to form new utterances may imitate a word quite well on an early try, with no attempt on the part of his parents to teach it to him. It is also perfectly obvious that, at a later stage, a child will be able to construct and understand utterances which are quite new, and are, at the same time, acceptable sentences in his language. Every time an adult reads a newspaper, he undoubtedly comes upon countless new sentences which are not at all similar, in a simple, physical sense, to any that he has heard before, and which he will recognize as sentences and understand; he will also be able to detect slight distortions or misprints. Talk of “stimulus generalization” in such a case simply perpetuates the mystery under a new title. These abilities indicate that there must be fundamental processes at work quite independently of “feedback” from the environment. I have been able to find no support whatsoever for the doctrine of Skinner and others that slow and careful shaping of verbal behavior through differential reinforcement is an absolute necessity. If reinforcement theory really requires the assumption that there be such meticulous care, it seems best to regard this simply as a reductio ad absurdum argument against this approach. It is also not easy to find any basis (or, for that matter, to attach very much content) to the claim that reinforcing contingencies set up by the verbal community are the single factor responsible for maintaining the strength of verbal behavior. The sources of the “strength” of this behavior are almost a total mystery at present. Reinforcement undoubtedly plays a significant role, but so do a variety of motivational factors about which nothing serious is known in the case of human beings.\n30 Of course, it is perfectly possible to incorporate this fact within the Skinnerian framework. If, for example, a child watches an adult using a comb and then, with no instruction, tries to comb his own hair, we can explain this act by saying that he performs it because he finds it reinforcing to do so, or because of the reinforcement provided by behaving like a person who is “reinforcing” (cf. 164). Similarly, an automatic explanation is available for any other behavior. It seems strange at first that Skinner pays so little attention to the literature on latent learning and related topics, considering the tremendous reliance that he places on the notion of reinforcement; I have seen no reference to it in his writings. Similarly, F. S. Keller and W. N. Schoenfeld, in what appears to be the only text written under predominantly Skinnerian influence, Principles of Psychology (New York: Appleton-Century-Crofts, Inc., 1950), dismiss the latent learning literature in one sentence as “beside the point,” serving only “to obscure, rather than clarify, a fundamental principle” (the law of effect, 41). However, this neglect is perfectly appropriate in Skinner’s case. To the drive-reductionist, or anyone else for whom the notion reinforcement has some substantive meaning, these experiments and observations are important (and often embarrassing). But in the Skinnerian sense of the word, neither these results nor any conceivable others can cast any doubt on the claim that reinforcement is essential for the acquisition and maintenance of behavior. Behavior certainly has some concomitant circumstances, and whatever they are, we can call them reinforcement.As far as acquisition of language is concerned, it seems clear that reinforcement, casual observation, and natural inquisitiveness (coupled with a strong tendency to imitate) are important factors, as is the remarkable capacity of the child to generalize, hypothesize, and “process information” in a variety of very special and apparently highly complex ways which we cannot yet describe or begin to understand, and which may be largely innate, or may develop through some sort of learning or through maturation of the nervous system. The manner in which such factors operate and interact in language acquisition is completely unknown. It is clear that what is necessary in such a case is research, not dogmatic and perfectly arbitrary claims, based on analogies to that small part of the experimental literature in which one happens to be interested.\nThe pointlessness of these claims becomes clear when we consider the well-known difficulties in determining to what extent inborn structure, maturation, and learning are responsible for the particular form of a skilled or complex performance.31 To take just one example,32 the gaping response of a nestling thrush is at first released by jarring of the nest, and, at a later stage, by a moving object of specific size, shape, and position relative to the nestling. At this later stage the response is directed toward the part of the stimulus object corresponding to the parent’s head, and characterized by a complex configuration of stimuli that can be precisely described. Knowing just this, it would be possible to construct a speculative, learning-theoretic account of how this sequence of behavior patterns might have developed through a process of differential reinforcement, and it would no doubt be possible to train rats to do something similar. However, there appears to be good evidence that these responses to fairly complex “sign stimuli” are genetically determined and mature without learning. Clearly, the possibility cannot be discounted. Consider now the comparable case of a child imitating new words. At an early stage we may find rather gross correspondences. At a later stage, we find that repetition is of course far from exact (i.e., it is not mimicry, a fact which itself is interesting), but that it reproduces the highly complex configuration of sound features that constitute the phonological structure of the language in question. Again, we can propose a speculative account of how this result might have been obtained through elaborate arrangement of reinforcing contingencies. Here too, however, it is possible that ability to select out of the complex auditory input those features that are phonologically relevant may develop largely independently of reinforcement, through genetically determined maturation. To the extent that this is true, an account of the development and causation of behavior that fails to consider the structure of the organism will provide no understanding of the real processes involved.\n31 Tinbergen, op.cit., Chap. VI, reviews some aspects of this problem, discussing the primary role of maturation in the development of many complex motor patterns (e.g., flying, swimming) in lower organisms, and the effect of an “innate disposition to learn” in certain specific ways and at certain specific times. Cf. also P. Schiller, “Innate Motor Action as a Basis for Learning,” in C. H. Schiller, ed., Instinctive Behavior (New York: International Universities Press, 1957), pp. 265-88, for a discussion of the role of maturing motor patterns in apparently insightful behavior in the chimpanzee.32 From among many cited by Tinbergen, op. cit., p. 85.33 Cf. K. S. Lashley, “In Search of the Engram,” Symposium of the Society for Experimental Biology, 4 (1950), 454-82. R. Sperry, “On the Neural Basis of the Conditioned Response,” British Journal of Animal Behavior, 3 (1955), 41-44, argues that to account for the experimental results of Lashley and others, and for other facts that he cites, it is necessary to assume that high-level cerebral activity of the type of insight, expectancy, and so on is involved even in simple conditioning. He states that “we still lack today a satisfactory picture of the underlying neural mechanism” of the conditioned response.It is often argued that experience, rather than innate capacity to handle information in certain specific ways, must be the factor of overwhelming dominance in determining the specific character of language acquisition, since a child speaks the language of the group in which he lives. But this is a superficial argument. As long as we are speculating, we may consider the possibility that the brain has evolved to the point where, given an input of observed Chinese sentences, it produces (by an induction of apparently fantastic complexity and suddenness) the rules of Chinese grammar, and given an input of observed English sentences, it produces (by, perhaps, exactly the same process of induction) the rules of English grammar; or that given an observed application of a term to certain instances, it automatically predicts the extension to a class of complexly related instances. If clearly recognized as such, this speculation is neither unreasonable nor fantastic; nor, for that matter, is it beyond the bounds of possible study. There is of course no known neural structure capable of performing this task in the specific ways that observation of the resulting behavior might lead us to postulate; but for that matter, the structures capable of accounting for even the simplest kinds of learning have similarly defied detection.33 Summarizing this brief discussion, it seems that there is neither empirical evidence nor any known argument to support any specific claim about the relative importance of “feedback” from the environment and the “independent contribution of the organism” in the process of language acquisition."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#vi",
    "href": "docs/posts/1959-noam-chomsky/index.html#vi",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "VI",
    "text": "VI\nWe now turn to the system that Skinner develops specifically for the description of verbal behavior. Since this system is based on the notions stimulus, response, and reinforcement, we can conclude from the preceding sections that it will be vague and arbitrary. For reasons noted in Section 1, however, I think it is important to see in detail how far from the mark any analysis phrased solely in these terms must be and how completely this system fails to account for the facts of verbal behavior. Consider first the term verbal behavior itself. This is defined as “behavior reinforced through the mediation of other persons” (2). The definition is clearly much too broad. It would include as verbal behavior, for example, a rat pressing the bar in a Skinner-box, a child brushing his teeth, a boxer retreating before an opponent, and a mechanic repairing an automobile. Exactly how much of ordinary linguistic behavior is verbal in this sense, however, is something of a question: perhaps, as I have pointed out above, a fairly small fraction of it, if any substantive meaning is assigned to the term reinforced. This definition is subsequently refined by the additional provision that the mediating response of the reinforcing person (the listener) must itself “have been conditioned precisely in order to reinforce the behavior of the speaker” (225, italics his). This still covers the examples given above, if we can assume that the reinforcing behavior of the psychologist, the parent, the opposing boxer, and the paying customer are the result of appropriate training, which is perhaps not unreasonable. A significant part of the fragment of linguistic behavior covered by the earlier definition will no doubt be excluded by the refinement, however. Suppose, for example, that while crossing the street I hear someone shout Watch out for the car and jump out of the way. It can hardly be proposed that my jumping (the mediating, reinforcing response in Skinner’s usage) was conditioned (that is, I was trained to jump) precisely in order to reinforce the behavior of the speaker; and similarly, for a wide class of cases. Skinner’s assertion that with this refined definition “we narrow our subject to what is traditionally recognized as the verbal field” (225) appears to be grossly in error."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#vii",
    "href": "docs/posts/1959-noam-chomsky/index.html#vii",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "VII",
    "text": "VII\nVerbal operants are classified by Skinner in terms of their “functional” relation to discriminated stimulus, reinforcement, and other verbal responses. A mand is defined as “a verbal operant in which the response is reinforced by a characteristic consequence and is therefore under the functional control of relevant conditions of deprivation or aversive stimulation” (35). This is meant to include questions, commands, etc. Each of the terms in this definition raises a host of problems. A mand such as Pass the salt is a class of responses. We cannot tell by observing the form of a response whether it belongs to this class (Skinner is very clear about this), but only by identifying the controlling variables. This is generally impossible. Deprivation is defined in the bar-pressing experiment in terms of length of time that the animal has not been fed or permitted to drink. In the present context, however, it is quite a mysterious notion. No attempt is made here to describe a method for determining “relevant conditions of deprivation” independently of the “controlled” response. It is of no help at all to be told (32) that it can be characterized in terms of the operations of the experimenter. If we define deprivation in terms of elapsed time, then at any moment a person is in countless states of deprivation.34 It appears that we must decide that the relevant condition of deprivation was (say) salt-deprivation, on the basis of the fact that the speaker asked for salt (the reinforcing community which “sets up” the mand is in a similar predicament). In this case, the assertion that a mand is under the control of relevant deprivation is empty, and we are (contrary to Skinner’s intention) identifying the response as a mand completely in terms of form. The word relevant in the definition above conceals some rather serious complications.\n34 Furthermore, the motivation of the speaker does not, except in the simplest cases, correspond in intensity to the duration of deprivation. An obvious counter-example is what Hebb has called the “salted-nut phenomenon” (Organization of Behavior [New York, 1949], p. 199). The difficulty is of course even more serious when we consider deprivations not related to physiological drives.In the case of the mand Pass the salt, the word deprivation is not out of place, though it appears to be of little use for functional analysis. Suppose however that the speaker says Give me the book, Take me for a ride, or Let me fix it. What kinds of deprivation can be associated with these mands? How do we determine or measure the relevant deprivation? I think we must conclude in this case, as before, either that the notion deprivation is relevant at most to a minute fragment of verbal behavior, or else that the statement “X is under Y-deprivation” is just an odd paraphrase for “X wants Y,” bearing a misleading and unjustifiable connotation of objectivity.\nThe notion aversive control is just as confused. This is intended to cover threats, beating, and the like (33). The manner in which aversive stimulation functions is simply described. If a speaker has had a history of appropriate reinforcement (e.g., if a certain response was followed by “cessation of the threat of such injury – of events which have previously been followed by such injury and which are therefore conditioned aversive stimuli”), then he will tend to give the proper response when the threat which had previously been followed by the injury is presented. It would appear to follow from this description that a speaker will not respond properly to the mand Your money or your life (38) unless he has a past history of being killed. But even if the difficulties in describing the mechanism of aversive control are somehow removed by a more careful analysis, it will be of little use for identifying operants for reasons similar to those mentioned in the case of deprivation.\nIt seems, then, that in Skinner’s terms there is in most cases no way to decide whether a given response is an instance of a particular mand. Hence it is meaningless, within the terms of his system, to speak of the characteristic consequences of a mand, as in the definition above. Furthermore, even if we extend the system so that mands can somehow be identified, we will have to face the obvious fact that most of us are not fortunate enough to have our requests, commands, advice, and so on characteristically reinforced (they may nevertheless exist in considerable strength). These responses could therefore not be considered mands by Skinner. In fact, Skinner sets up a category of “magical mands” (48-49) to cover the case of “mands which cannot be accounted for by showing that they have ever had the effect specified or any similar effect upon similar occasions” (the word ever in this statement should be replaced by characteristically). In these pseudo-mands, “the speaker simply describes the reinforcement appropriate to a given state of deprivation or aversive stimulation.” In other words, given the meaning that we have been led to assign to reinforcement and deprivation, the speaker asks for what he wants. The remark that “a speaker appears to create new mands on the analogy of old ones” is also not very helpful.\nSkinner’s claim that his new descriptive system is superior to the traditional one “because its terms can be defined with respect to experimental operations” (45) is, we see once again, an illusion. The statement “X wants Y” is not clarified by pointing out a relation between rate of bar-pressing and hours of food-deprivation; replacing “X wants Y” by “X is deprived of Y” adds no new objectivity to the description of behavior. His further claim for the superiority of the new analysis of mands is that it provides an objective basis for the traditional classification into requests, commands, etc. (38-41). The traditional classification is in terms of the intention of the speaker. But intention, Skinner holds, can be reduced to contingencies of reinforcement, and, correspondingly, we can explain the traditional classification in terms of the reinforcing behavior of the listener. Thus, a question is a mand which “specifies verbal action, and the behavior of the listener permits us to classify it as a request, a command, or a prayer” (39). It is a request if “the listener is independently motivated to reinforce the speaker” a command if “the listener’s behavior is… reinforced by reducing a threat, a prayer if the mand”promotes reinforcement by generating an emotional disposition.” The mand is advice if the listener is positively reinforced by the consequences of mediating the reinforcement of the speaker; it is a warning if “by carrying out the behavior specified by the speaker, the listener escapes from aversive stimulation” and so on. All this is obviously wrong if Skinner is using the words request, command, etc., in anything like the sense of the corresponding English words. The word question does not cover commands. Please pass the salt is a request (but not a question), whether or not the listener happens to be motivated to fulfill it; not everyone to whom a request is addressed is favorably disposed. A response does not cease to be a command if it is not followed; nor does a question become a command if the speaker answers it because of an implied or imagined threat. Not all advice is good advice, and a response does not cease to be advice if it is not followed. Similarly, a warning may be misguided; heeding it may cause aversive stimulation, and ignoring it might be positively reinforcing. In short, the entire classification is beside the point. A moment’s thought is sufficient to demonstrate the impossibility of distinguishing between requests, commands, advice, etc., on the basis of the behavior or disposition of the particular listener. Nor can we do this on the basis of the typical behavior of all listeners. Some advice is never taken, is always bad, etc., and similarly, with other kinds of mands. Skinner’s evident satisfaction with this analysis of the traditional classification is extremely puzzling."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#viii",
    "href": "docs/posts/1959-noam-chomsky/index.html#viii",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "VIII",
    "text": "VIII\nMands are operants with no specified relation to a prior stimulus. A tact, on the other hand, is defined as “a verbal operant in which a response of given form is evoked (or at least strengthened) by a particular object or event or property of an object or event” (81). The examples quoted in the discussion of stimulus control (Section 3) are all tacts. The obscurity of the notion stimulus control makes the concept of the tact rather mystical. Since, however, the tact is “the most important of verbal operants,” it is important to investigate the development of this concept in more detail.\nWe first ask why the verbal community “sets up” tacts in the child – that is, how the parent is reinforced by setting up the tact. The basic explanation for this behavior of the parent (85-86) is the reinforcement he obtains by the fact that his contact with the environment is extended; to use Skinner’s example, the child may later be able to call him to the telephone. (It is difficult to see, then, how first children acquire tacts, since the parent does not have the appropriate history of reinforcement.) Reasoning in the same way, we may conclude that the parent induces the child to walk so that he can make some money delivering newspapers. Similarly, the parent sets up an “echoic repertoire” (e.g., a phonemic system) in the child because this makes it easier to teach him new vocabulary, and extending the child’s vocabulary is ultimately useful to the parent. “In all these cases we explain the behavior of the reinforcing listener by pointing to an improvement in the possibility of controlling the speaker whom he reinforces” (56). Perhaps this provides the explanation for the behavior of the parent in inducing the child to walk: the parent is reinforced by the improvement in his control of the child when the child’s mobility increases. Underlying these modes of explanation is a curious view that it is somehow more scientific to attribute to a parent a desire to control the child or enhance his own possibilities for action than a desire to see the child develop and extend his capacities. Needless to say, no evidence is offered to support this contention.\nConsider now the problem of explaining the response of the listener to a tact. Suppose, for example, that B hears A say fox and reacts appropriately – looks around, runs away, aims his rifle, etc. How can we explain B’s behavior? Skinner rightly rejects analyses of this offered by J. B. Watson and Bertrand Russell. His own equally inadequate analysis proceeds as follows (87-88). We assume (l) “that in the history of [B] the stimulus fox has been an occasion upon which looking around has been followed by seeing a fox” and (2) “that the listener has some current ‘interest in seeing foxes’ – that behavior which depends upon a seen fox for its execution is strong, and that the stimulus supplied by a fox is therefore reinforcing.” B carries out the appropriate behavior, then, because “the heard stimulus fox is the occasion upon which turning and looking about is frequently followed by the reinforcement of seeing a fox,” i.e, his behavior is a discriminated operant. This explanation is unconvincing. B may never have seen a fox and may have no current interest in seeing one, and yet may react appropriately to the stimulus fox.35 Since exactly the same behavior may take place when neither of the assumptions is fulfilled, some other mechanism must be operative here.\n35 Just as he may have the appropriate reaction, both emotional and behavioral, to such utterances as the volcano is erupting or there’s a homicidal maniac in the next room without any previous pairing of the verbal and the physical stimulus. Skinner’s discussion of Pavlovian conditioning in language (154) is similarly unconvincing.36 J. S. Mill, A System of Logic (1843). R. Carnap gives a recent reformulation in “Meaning and Synonymy in Natural Languages,” Phil. Studies, 6 (1955), 33-47, defining the meaning (intension) of a predicate Q for a speaker X as “the general condition which an object y must fulfil in order for X to be willing to ascribe the predicate Q to y.” The connotation of an expression is often said to constitute its “cognitive meaning” as opposed to its “emotive meaning,” which is, essentially, the emotional reaction to the expression.37 Most clearly by Quine. See From a Logical Point of View (Cambridge, 1953), especially Chaps. 2, 3, and 7.38 A method for characterizing synonymy in terms of reference is suggested by Goodman, “On Likeness of Meaning,” Analysis, 10 (1949), 1-7. Difficulties are discussed by Goodman, “On Some Differences about Meaning,” ibid., 13 (1953) 90-96. Carnap, op. cit., presents a very similar idea (Section 6), but somewhat misleadingly phrased, since he does not bring out the fact that only extensional (referential) notions are being used.Skinner remarks several times that his analysis of the tact in terms of stimulus control is an improvement over the traditional formulations in terms of reference and meaning. This is simply not true. His analysis is fundamentally the same as the traditional one, though much less carefully phrased. In particular, it differs only by indiscriminate paraphrase of such notions as denotation (reference) and connotation (meaning), which have been kept clearly apart in traditional formulations, in terms of the vague concept stimulus control. In one traditional formulation a descriptive term is said to denote a set of entities and to connote or designate a certain property or condition that an entity must possess or fulfil if the term is to apply to it.36 Thus, the term vertebrate refers to (denotes, is true of) vertebrates and connotes the property having a spine or something of the sort. This connoted defining property is called the meaning of the term. Two terms may have the same reference but different meanings. Thus, it is apparently true that the creatures with hearts are all and only the vertebrates. If so, then the term creature with a heart refers to vertebrates and designates the property having a heart. This is presumably a different property (a different general condition) from having a spine; hence the terms vertebrate and creature with a heart are said to have different meanings. This analysis is not incorrect (for at least one sense of meaning), but its many limitations have frequently been pointed out.37 The major problem is that there is no good way to decide whether two descriptive terms designate the same property.38 As we have just seen, it is not sufficient that they refer to the same objects. Vertebrate and creature with a spine would be said to designate the same property (distinct from that designated by creature with a heart). If we ask why this is so, the only answer appears to be that the terms are synonymous. The notion property thus seems somehow language-bound, and appeal to “defining properties” sheds little light on questions of meaning and synonymy.\nSkinner accepts the traditional account in toto, as can be seen from his definition of a tact as a response under control of a property (stimulus) of some physical object or event. We have found that the notion control has no real substance and is perhaps best understood as a paraphrase of denote or connote or, ambiguously, both. The only consequence of adopting the new term stimulus control is that the important differences between reference and meaning are obscured. It provides no new objectivity. The stimulus controlling the response is determined by the response itself; there is no independent and objective method of identification (see Section 3). Consequently, when Skinner defines synonymy as the case in which “the same stimulus leads to quite different responses” (118), we can have no objection. The responses chair and red made alternatively to the same object are not synonymous, because the stimuli are called different. The responses vertebrate and creature with a spine would be considered synonymous because they are controlled by the same property of the object under investigation; in more traditional and no less scientific terms, they evoke the same concept. Similarly, when metaphorical extension is explained as due to “the control exercised by properties of the stimulus which, though present at reinforcement, do not enter into the contingency respected by the verbal community” (92; traditionally, accidental properties), no objection can be raised which has not already been leveled against the traditional account. Just as we could “explain” the response Mozart to a piece of music in terms of subtle properties of the controlling stimuli, we can, with equal facility, explain the appearance of the response sun when no sun is present, as in Juliet is [like] the sun. “We do so by noting that Juliet and the sun have common properties, at least in their effect on the speaker” (93). Since any two objects have indefinitely many properties in common, we can be certain that we will never be at a loss to explain a response of the form A is like B, for arbitrary A and B. It is clear, however, that Skinner’s recurrent claim that his formulation is simpler and more scientific than the traditional account has no basis in fact.\nTacts under the control of private stimuli (Bloomfield’s “displaced speech”) form a large and important class (130-46), including not only such responses as familiar and beautiful, but also verbal responses referring to past, potential, or future events or behavior. For example, the response There was an elephant at the zoo “must be understood as a response to current stimuli, including events within the speaker himself” (143).39 If we now ask ourselves what proportion of the tacts in actual life are responses to (descriptions of) actual current outside stimulation, we can see just how large a role must be attributed to private stimuli. A minute amount of verbal behavior, outside the nursery, consists of such remarks as This is red and There is a man. The fact that functional analysis must make such a heavy appeal to obscure internal stimuli is again a measure of its actual advance over traditional formulations.\n39 In general, the examples discussed here are badly handled, and the success of the proposed analyses is overstated. In each case, it is easy to see that the proposed analysis, which usually has an air of objectivity, is not equivalent to the analyzed expression. To take just one example, the response I am looking for my glasses is certainly not equivalent to the proposed paraphrases: “When I have behaved in this way in the past, I have found my glasses and have then stopped behaving in this way,” or “Circumstances have arisen in which I am inclined to emit any behavior which in the past has led to the discovery of my glasses; such behavior includes the behavior of looking in which I am now engaged.” One may look for one’s glasses for the first time; or one may emit the same behavior in looking for one’s glasses as in looking for one’s watch, in which case I am looking for my glasses and I am looking for my watch are equivalent, under the Skinnerian paraphrase. The difficult questions of purposiveness cannot be handled in this superficial manner."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#ix",
    "href": "docs/posts/1959-noam-chomsky/index.html#ix",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "IX",
    "text": "IX\nResponses under the control of prior verbal stimuli are considered under a different heading from the tact. An echoic operant is a response which “generates a sound pattern similar to that of the stimulus” (55). It covers only cases of immediate imitation.40 No attempt is made to define the sense in which a child’s echoic response is “similar” to the stimulus spoken in the father’s bass voice; it seems, though there are no clear statements about this, that Skinner would not accept the account of the phonologist in this respect, but nothing else is offered. The development of an echoic repertoire is attributed completely to differential reinforcement. Since the speaker will do no more, according to Skinner, than what is demanded of him by the verbal community, the degree of accuracy insisted on by this community will determine the elements of the repertoire, whatever these may be (not necessarily phonemes). “In a verbal community which does not insist on a precise correspondence, an echoic repertoire may remain slack and will be less successfully applied to novel patterns.” There is no discussion of such familiar phenomena as the accuracy with which a child will pick up a second language or a local dialect in the course of playing with other children, which seem sharply in conflict with these assertions. No anthropological evidence is cited to support the claim that an effective phonemic system does not develop (this is the substance of the quoted remark) in communities that do not insist on precise correspondence.\n40 Skinner takes great pains, however, to deny the existence in human beings (or parrots) of any innate faculty or tendency to imitate. His only argument is that no one would suggest an innate tendency to read, yet reading and echoic behavior have similar “dynamic properties.” This similarity, however, simply indicates the grossness of his descriptive categories. In the case of parrots, Skinner claims that they have no instinctive capacity to imitate, but only to be reinforced by successful imitation (59). Given Skinner’s use of the word reinforcement, it is difficult to perceive any distinction here, since exactly the same thing could be said of any other instinctive behavior. For example, where another scientist would say that a certain bird instinctively builds a nest in a certain way, we could say in Skinner’s terminology (equivalently) that the bird is instinctively reinforced by building the nest in this way. One is therefore inclined to dismiss this claim as another ritual introduction of the word reinforce. Though there may, under some suitable clarification, be some truth in it, it is difficult to see how many of the cases reported by competent observers can be handled if reinforcement is given some substantive meaning. Cf. Thorpe, op. cit. p. 353f.; K. Lorenz, King Solomon’s Ring (New York, 1952), pp. 85-88; even Mowrer, who tries to show how imitation might develop through secondary reinforcement, cites a case, op. cit., p. 694, which he apparently believes, but where this could hardly be true. In young children, it seems most implausible to explain imitation in terms of secondary reinforcement.A verbal response to a written stimulus (reading) is called textual behavior.\nOther verbal responses to verbal stimuli are called intraverbal operants. Paradigm instances are the response four to the stimulus two plus two or the response Paris to the stimulus capital of France. Simple conditioning may be sufficient to account for the response four to two plus two,41 but the notion of intraverbal response loses all meaning when we find it extended to cover most of the facts of history and many of the facts of science (72, 129); all word association and “flight of ideas” (73-76); all translations and paraphrase (77); reports of things seen, heard, or remembered (315); and, in general, large segments of scientific, mathematical, and literary discourse. Obviously, the kind of explanation that might be proposed for a student’s ability to respond with Paris to capital of France, after suitable practice, can hardly be seriously offered to account for his ability to make a judicious guess in answering the questions (to him new): What is the seat of the French government?, … the source of the literary dialect?,.. the chief target of the German blitzkrieg?, etc., or his ability to prove a new theorem, translate a new passage, or paraphrase a remark for the first time or in a new way.\n41 Although even this possibility is limited. If we were to take these paradigm instances seriously, it should follow that a child who knows how to count from one to 100 could learn an arbitrary 10 x 10 matrix with these numbers as entries as readily as the multiplication table.42 Similarly, “the universality of a literary work refers to the number of potential readers inclined to say the same thing” (275; i.e., the most “universal” work is a dictionary of clichés and greetings) a speaker is “stimulating” if he says what we are about to say ourselves (272) etc.43 Similarly, consider Skinner’s contention (362-65) that communication of knowledge or facts is just the process of making a new response available to the speaker. Here the analogy to animal experiments is particularly weak. When we train a rat to carry out some peculiar act, it makes sense to consider this a matter of adding a response to his repertoire. In the case of human communication, however, it is very difficult to attach any meaning to this terminology. If A imparts to B the information (new to B) that the railroads face collapse, in what sense can the response The railroads face collapse be said to be now, but not previously, available to B? Surely B could have said it before (not knowing whether it was true), and known that it was a sentence (as opposed to Collapse face railroads the). Nor is there any reason to assume that the response has increased in strength, whatever this means exactly (e.g., B may have no interest in the fact, or he may want it suppressed). It is not clear how we can characterize this notion of “making a response available” without reducing Skinner’s account of “imparting knowledge” to a triviality.The process of “getting someone to see a point,” to see something your way, or to understand a complex state of affairs (e.g., a difficult political situation or a mathematical proof) is, for Skinner, simply a matter of increasing the strength of the listener’s already available behavior.42 Since “the process is often exemplified by relatively intellectual scientific or philosophical discourse,” Skinner considers it “all the more surprising that it may be reduced to echoic, textual, or intraverbal supplementation” (269). Again, it is only the vagueness and latitude with which the notions strength and intraverbal response are used that save this from absurdity. If we use these terms in their literal sense, it is clear that understanding a statement cannot be equated to shouting it frequently in a high-pitched voice (high response strength), and a clever and convincing argument cannot be accounted for on the basis of a history of pairings of verbal responses.43"
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#x",
    "href": "docs/posts/1959-noam-chomsky/index.html#x",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "X",
    "text": "X\nA final class of operants, called autoclitics, includes those that are involved in assertion, negation, quantification, qualification of responses, construction of sentences, and the “highly complex manipulations of verbal thinking.” All these acts are to be explained “in terms of behavior which is evoked by or acts upon other behavior of the speaker” (313). Autoclitics are, then, responses to already given responses, or rather, as we find in reading through this section, they are responses to covert or incipient or potential verbal behavior. Among the autoclitics are listed such expressions as I recall, I imagine, for example, assume, let X equal…, the terms of negation, the is of predication and assertion, all, some, if, then, and, in general, all morphemes other than nouns, verbs, and adjectives, as well as grammatical processes of ordering and arrangement. Hardly a remark in this section can be accepted without serious qualification. To take just one example, consider Skinner’s account of the autoclitic all in All swans are white (329). Obviously we cannot assume that this is a tact to all swans as stimulus. It is suggested, therefore, that we take all to be an autoclitic modifying the whole sentence Swans are white. All can then be taken as equivalent to always, or always it is possible to say. Notice, however, that the modified sentence Swans are white is just as general as All swans are white. Furthermore, the proposed translation of all is incorrect if taken literally. It is just as possible to say Swans are green as to say Swans are white. It is not always possible to say either (e.g., while you are saying something else or sleeping). Probably what Skinner means is that the sentence can be paraphrased “X is white is true, for each swan X.” But this paraphrase cannot be given within his system, which has no place for true.\nSkinner’s account of grammar and syntax as autoclitic processes (Chap. 13) differs from a familiar traditional account mainly in the use of the pseudo-scientific terms control or evoke in place of the traditional refer. Thus, in The boy runs, the final s of runs is a tact under control of such “subtle properties of a situation” as “the nature of running as an activity rather than an object or property of an object.”44 (Presumably, then, in The attempt fails, The difficulty remains, His anxiety increases, etc., we must also say that the s indicates that the object described as the attempt is carrying out the activity of failing, etc.) In the boy’s gun, however, the s denotes possession (as, presumably, in the boy’s arrival, … story, … age, etc.) and is under the control of this “relational aspect of the situation” (336). The “relational autoclitic of order” (whatever it may mean to call the order of a set of responses a response to them) in The boy runs the store is under the control of an “extremely complex stimulus situation,” namely, that the boy is running the store (335). And in the hat and the shoe is under the control of the property “pair.” Through in the dog went through the hedge is under the control of the “relation between the going dog and the hedge” (342). In general, nouns are evoked by objects, verbs by actions, and so on. Skinner considers a sentence to be a set of key responses (nouns, verbs, adjectives) on a skeletal frame (346). If we are concerned with the fact that Sam rented a leaky boat, the raw responses to the situation are rent, boat, leak, and Sam. Autoclitics (including order) which qualify these responses, express relations between them, and the like, are then added by a process called composition and the result is a grammatical sentence, one of many alternatives among which selection is rather arbitrary. The idea that sentences consist of lexical items placed in a grammatical frame is of course a traditional one, within both philosophy and linguistics. Skinner adds to it only the very implausible speculation that in the internal process of composition, the nouns, verbs, and adjectives are chosen first and then are arranged, qualified, etc., by autoclitic responses to these internal activities.45\n44 (332). On the next page, however, the s in the same example indicates that “the object described as the boy possesses the property of running.” The difficulty of even maintaining consistency with a conceptual scheme like this is easy to appreciate.45 One might just as well argue that exactly the opposite is true. The study of hesitation pauses has shown that these tend to occur before the large categories – noun, verb, adjective; this finding is usually described by the statement that the pauses occur where there is maximum uncertainty or information. Insofar as hesitation indicates on-going composition (if it does at all), it would appear that the “key responses” are chosen only after the “grammatical frame.” Cf. C. E. Osgood, unpublished paper; F. Goldman-Eisler, “Speech Analysis and Mental Processes,” Language and Speech, 1 (1958), 67.This view of sentence structure, whether phrased in terms of autoclitics, syncategorematic expressions, or grammatical and lexical morphemes, is inadequate. Sheep provide wool has no (physical) frame at all, but no other arrangement of these words is an English sentence. The sequences furiously sleep ideas green colorless and friendly young dogs seem harmless have the same frames, but only one is a sentence of English (similarly, only one of the sequences formed by reading these from back to front). Struggling artists can be a nuisance has the same frame as marking papers can be a nuisance, but is quite different in sentence structure, as can be seen by replacing can be by is or are in both cases. There are many other similar and equally simple examples. It is evident that more is involved in sentence structure than insertion of lexical items in grammatical frames; no approach to language that fails to take these deeper processes into account can possibly achieve much success in accounting for actual linguistic behavior."
  },
  {
    "objectID": "docs/posts/1959-noam-chomsky/index.html#xi",
    "href": "docs/posts/1959-noam-chomsky/index.html#xi",
    "title": "A Review of B. F. Skinner’s Verbal Behavior",
    "section": "XI",
    "text": "XI\nThe preceding discussion covers all the major notions that Skinner introduces in his descriptive system. My purpose in discussing the concepts one by one was to show that in each case, if we take his terms in their literal meaning, the description covers almost no aspect of verbal behavior, and if we take them metaphorically, the description offers no improvement over various traditional formulations. The terms borrowed from experimental psychology simply lose their objective meaning with this extension, and take over the full vagueness of ordinary language. Since Skinner limits himself to such a small set of terms for paraphrase, many important distinctions are obscured. I think that this analysis supports the view expressed in Section I, that elimination of the independent contribution of the speaker and learner (a result which Skinner considers of great importance, cf. 311-12) can be achieved only at the cost of eliminating all significance from the descriptive system, which then operates at a level so gross and crude that no answers are suggested to the most elementary questions.46 The questions to which Skinner has addressed his speculations are hopelessly premature. It is futile to inquire into the causation of verbal behavior until much more is known about the specific character of this behavior; and there is little point in speculating about the process of acquisition without much better understanding of what is acquired.\n46 E.g., what are in fact the actual units of verbal behavior? Under what conditions will a physical event capture the attention (be a stimulus) or be a reinforcer? How do we decide what stimuli are in “control” in a specific case? When are stimuli “similar”? And so on. (It is not interesting to be told, e.g., that we say Stop to an automobile or billiard ball because they are sufficiently similar to reinforcing people [46].) The use of unanalyzed notions like similar and generalization is particularly disturbing, since it indicates an apparent lack of interest in every significant aspect of the learning or the use of language in new situations. No one has ever doubted that in some sense, language is learned by generalization, or that novel utterances and situations are in some way similar to familiar ones. The only matter of serious interest is the specific “similarity.” Skinner has, apparently, no interest in this. Keller and Schoenfeld, op. cit., proceed to incorporate these notions (which they identify) into their Skinnerian “modern objective psychology” by defining two stimuli to be similar when “we make the same sort of response to them” (124; but when are responses of the “same sort”?). They do not seem to notice that this definition converts their “principle of generalization” (116), under any reasonable interpretation of this, into a tautology. It is obvious that such a definition will not be of much help in the study of language learning or construction of new responses in appropriate situations.47 “The Problem of Serial Order in Behavior,” in L. A. Jeffress, ed., Hixon Symposium on Cerebral Mechanisms in Behavior (New York: John Wiley & Sons Inc., 1951). Reprinted in F. A. Beach, D. O. Hebb, C. T. Morgan, H. W. Nissen, eds., The Neuropsychology of Lashley (New York: McGraw-Hill Book Company, 1960). Page references are to the latter.Anyone who seriously approaches the study of linguistic behavior, whether linguist, psychologist, or philosopher, must quickly become aware of the enormous difficulty of stating a problem which will define the area of his investigations, and which will not be either completely trivial or hopelessly beyond the range of present-day understanding and technique. In selecting functional analysis as his problem, Skinner has set himself a task of the latter type. In an extremely interesting and insightful paper,47 K. S. Lashley has implicitly delimited a class of problems which can be approached in a fruitful way by the linguist and psychologist, and which are clearly preliminary to those with which Skinner is concerned. Lashley recognizes, as anyone must who seriously considers the data, that the composition and production of an utterance is not simply a matter of stringing together a sequence of responses under the control of outside stimulation and intraverbal association, and that the syntactic organization of an utterance is not something directly represented in any simple way in the physical structure of the utterance itself. A variety of observations lead him to conclude that syntactic structure is “a generalized pattern imposed on the specific acts as they occur” (512), and that “a consideration of the structure of the sentence and other motor sequences will show…that there are, behind the overtly expressed sequences, a multiplicity of integrative processes which can only be inferred from the final results of their activity” (509). He also comments on the great difficulty of determining the “selective mechanisms” used in the actual construction of a particular utterance (522).\nAlthough present-day linguistics cannot provide a precise account of these integrative processes, imposed patterns, and selective mechanisms, it can at least set itself the problem of characterizing these completely. It is reasonable to regard the grammar of a language L ideally as a mechanism that provides an enumeration of the sentences of L in something like the way in which a deductive theory gives an enumeration of a set of theorems. (Grammar, in this sense of the word, includes phonology.) Furthermore, the theory of language can be regarded as a study of the formal properties of such grammars, and, with a precise enough formulation, this general theory can provide a uniform method for determining, from the process of generation of a given sentence, a structural description which can give a good deal of insight into how this sentence is used and understood. In short, it should be possible to derive from a properly formulated grammar a statement of the integrative processes and generalized patterns imposed on the specific acts that constitute an utterance. The rules of a grammar of the appropriate form can be subdivided into the two types, optional and obligatory; only the latter must be applied in generating an utterance. The optional rules of the grammar can be viewed, then, as the selective mechanisms involved in the production of a particular utterance. The problem of specifying these integrative processes and selective mechanisms is nontrivial and not beyond the range of possible investigation. The results of such a study might, as Lashley suggests, be of independent interest for psychology and neurology (and conversely). Although such a study, even if successful, would by no means answer the major problems involved in the investigation of meaning and the causation of behavior, it surely will not be unrelated to these. It is at least possible, furthermore, that such a notion as semantic generalization, to which such heavy appeal is made in all approaches to language in use, conceals complexities and specific structure of inference not far different from those that can be studied and exhibited in the case of syntax, and that consequently the general character of the results of syntactic investigations may be a corrective to oversimplified approaches to the theory of meaning.\nThe behavior of the speaker, listener, and learner of language constitutes, of course, the actual data for any study of language. The construction of a grammar which enumerates sentences in such a way that a meaningful structural description can be determined for each sentence does not in itself provide an account of this actual behavior. It merely characterizes abstractly the ability of one who has mastered the language to distinguish sentences from nonsentences, to understand new sentences (in part), to note certain ambiguities, etc. These are very remarkable abilities. We constantly read and hear new sequences of words, recognize them as sentences, and understand them. It is easy to show that the new events that we accept and understand as sentences are not related to those with which we are familiar by any simple notion of formal (or semantic or statistical) similarity or identity of grammatical frame. Talk of generalization in this case is entirely pointless and empty. It appears that we recognize a new item as a sentence not because it matches some familiar item in any simple way, but because it is generated by the grammar that each individual has somehow and in some form internalized. And we understand a new sentence, in part, because we are somehow capable of determining the process by which this sentence is derived in this grammar.\nSuppose that we manage to construct grammars having the properties outlined above. We can then attempt to describe and study the achievement of the speaker, listener, and learner. The speaker and the listener, we must assume, have already acquired the capacities characterized abstractly by the grammar. The speaker’s task is to select a particular compatible set of optional rules. If we know, from grammatical study, what choices are available to him and what conditions of compatibility the choices must meet, we can proceed meaningfully to investigate the factors that lead him to make one or another choice. The listener (or reader) must determine, from an exhibited utterance, what optional rules were chosen in the construction of the utterance. It must be admitted that the ability of a human being to do this far surpasses our present understanding. The child who learns a language has in some sense constructed the grammar for himself on the basis of his observation of sentences and nonsentences (i.e., corrections by the verbal community). Study of the actual observed ability of a speaker to distinguish sentences from nonsentences, detect ambiguities, etc., apparently forces us to the conclusion that this grammar is of an extremely complex and abstract character, and that the young child has succeeded in carrying out what from the formal point of view, at least, seems to be a remarkable type of theory construction. Furthermore, this task is accomplished in an astonishingly short time, to a large extent independently of intelligence, and in a comparable way by all children. Any theory of learning must cope with these facts.\nIt is not easy to accept the view that a child is capable of constructing an extremely complex mechanism for generating a set of sentences, some of which he has heard, or that an adult can instantaneously determine whether (and if so, how) a particular item is generated by this mechanism, which has many of the properties of an abstract deductive theory. Yet this appears to be a fair description of the performance of the speaker, listener, and learner. If this is correct, we can predict that a direct attempt to account for the actual behavior of speaker, listener, and learner, not based on a prior understanding of the structure of grammars, will achieve very limited success. The grammar must be regarded as a component in the behavior of the speaker and listener which can only be inferred, as Lashley has put it, from the resulting physical acts. The fact that all normal children acquire essentially comparable grammars of great complexity with remarkable rapidity suggests that human beings are somehow specially designed to do this, with data-handling or “hypothesis-formulating” ability of unknown character and complexity.48 The study of linguistic structure may ultimately lead to some significant insights into this matter. At the moment the question cannot be seriously posed, but in principle it may be possible to study the problem of determining what the built-in structure of an information-processing (hypothesis-forming) system must be to enable it to arrive at the grammar of a language from the available data in the available time. At any rate, just as the attempt to eliminate the contribution of the speaker leads to a “mentalistic” descriptive system that succeeds only in blurring important traditional distinctions, a refusal to study the contribution of the child to language learning permits only a superficial account of language acquisition, with a vast and unanalyzed contribution attributed to a step called generalization which in fact includes just about everything of interest in this process. If the study of language is limited in these ways, it seems inevitable that major aspects of verbal behavior will remain a mystery.\n48 There is nothing essentially mysterious about this. Complex innate behavior patterns and innate “tendencies to learn in specific ways” have been carefully studied in lower organisms. Many psychologists have been inclined to believe that such biological structure will not have an important effect on acquisition of complex behavior in higher organisms, but I have not been able to find any serious justification for this attitude. Some recent studies have stressed the necessity for carefully analyzing the strategies available to the organism, regarded as a complex “information-processing system” (cf. J. S. Bruner, J. J. Goodnow, and G. A. Austin, A Study of Thinking [New York, 1956]; A. Newell, J. C. Shaw, and H. A. Simon, “Elements of a Theory of Human Problem Solving,” Psych. Rev., 65, [1958], 151-66), if anything significant is to be said about the character of human learning. These may be largely innate, or developed by early learning processes about which very little is yet known. (But see Harlow, “The Formation of Learning Sets,” Psych. Rev., 56 (1949), 51-65, and many later papers, where striking shifts in the character of learning are shown as a result of early training; also D. O. Hebb, Organization of Behavior, 109 ff.). They are undoubtedly quite complex. Cf. Lenneberg, op. cit., and R. B. Lees, review of N. Chomsky’s Syntactic Structures in Language, 33 (1957), 406f, for discussion of the topics mentioned in this section.In fairness, it must be mentioned that there are certain nontrivial applications of operant conditioning to the control of human behavior. A wide variety of experiments have shown that the number of plural nouns (for example) produced by a subject will increase if the experimenter says “right” or “good” when one is produced (similarly, positive attitudes on a certain issue, stories with particular content, etc.; cf. L. Krasner, “Studies of the Conditioning of Verbal Behavior,” Psych. Bull., 55 [1958], for a survey of several dozen experiments of this kind, mostly with positive results). It is of some interest that the subject is usually unaware of the process. Just what insight this gives into normal verbal behavior is not obvious. Nevertheless, it is an example of positive and not totally expected results using the Skinnerian paradigm.\nLenneberg (“The Capacity for Language Acquisition”, in J. A. Fodor, ed., The Structure of Language [Prentice-Hall, Inc., 1964]) presents a very interesting discussion of the part that biological structure may play in the acquisition of language, and the dangers in neglecting this possibility.\nWhether or not this is the best way to approach meaning, it is clear that denotation, cognitive meaning, and emotive meaning are quite different things. The differences are often obscured in empirical studies of meaning, with much consequent confusion. Thus, Osgood has set himself the task of accounting for the fact that a stimulus comes to be a sign for another stimulus (a buzzer becomes a sign for food, a word for a thing, etc.). This is clearly (for linguistic signs) a problem of denotation. The method that he actually develops for quantifying and measuring meaning (cf. C. E. Osgood, G. Suci, P. Tannenbaum, The Measurement of Meaning [Urbana: Univ. of Illinois Press, 1957]) applies, however, only to emotive meaning. Suppose, for example, that A hates both Hitler and science intensely, and considers both highly potent and “active,” while B, agreeing with A about Hitler, likes science very much, although he considers it rather ineffective and not too important. Then, A may assign to “Hitler” and “science” the same position on the semantic differential, while B will assign “Hitler” the same position as A did, but “science” a totally different position. Yet, A does not think that “Hitler” and “science” are synonymous or that they have the same reference, and A and B may agree precisely on the cognitive meaning of “science.” Clearly, it is the attitude toward the things (the emotive meaning of the words) that is being measured here. There is a gradual shift in Osgood’s account from denotation to cognitive meaning to emotive meaning. The confusion is caused, no doubt, by the fact that the term meaning is used in all three senses (and others). [See J. Carroll’s review of the book by Osgood, Suci, and Tannenbaum in Language, 35, No. 1 (1959).]"
  },
  {
    "objectID": "logs/posts/2025-whalefall/index.html",
    "href": "logs/posts/2025-whalefall/index.html",
    "title": "Whalefall",
    "section": "",
    "text": "This crash.\n\n\nOn 2025-01-27 on the New York Stock Exchange, multiple semiconductor corporations gapped down. The cause is clear: something about DeepSeek-R1. What that something is, I don’t know.\nPersonally, I was quite obsessed with R1 since its release on 2025-01-20, feeling the AGI in mathematics. I also bought more Nvidia and TSMC, seeing this as an obvious buy signal.\n\n\n\n(っ◔◡◔)っ 🎀 imagination 🎀. Source: @layer_07_yuxi (2025-01-19).\n\n\nThen this crash happened. An utter shock. The market… is stupid… and inefficient?\nNearcyan claimed that the market was probably manipulated by a stand-alone complex of insiders, who felt that a crash was struggling to emerge, and bandwagoned on it, making the crash a reality (“feed the sharks”). He didn’t elaborate but I presume it involved some of the following:\n\nbuy it on the cheap and sell them for profit a bit later;\nbought some put options;\nsold some call options;\ndid HFT to profit on volatility itself;\nsome linear combination of these to minimize tax.\n\nI don’t know the detailed mechanics of how the crash appeared, because I am an outsider to the financial establishment, and I don’t know how to psychoanalyze the market.\nThe separate analysis, on why DeepSeek hit No. 1 on the Apple App store so quickly after release,1 seems true enough to me. Such rapid rise must be a memetic virus, something like TikTok and YouTube Shorts. The app is good, the model is great, but that’s not enough to push the app to No. 1 out of nowhere. The general public knew nothing of DeepSeek; DeepSeek made no advertisement; etc.\n1 Surprisingly hard to figure out when exactly this happened. Apparently, 2025-01-10 release, 2025-01-26 hitting No. 1.\n\n\nNearcyan’s analysis. Source: @Nearcyan (2025-01-28).\n\n\nBut I learned something new: that the mainstream media has no economic literacy. There is a story to be told for why R1 was bad for Nvidia, but none of the mainstream media stories hit anywhere close! If I were grading their economics homework, they are all getting “E for Efforts”. No, “F for Effort missing a plank”!\nConsider a typical take:\n\nThe DeepSeek product “is deeply problematic for the thesis that the significant capital expenditure and operating expenses that Silicon Valley has incurred is the most appropriate way to approach the AI trend”, said Nirgunan Tiruchelvam, head of consumer and internet at Singapore-based Aletheia Capital. “It calls into question the massive resources that have been dedicated to AI.”\n— DeepSeek buzz puts tech stocks on track for $1 trillion wipeout | Fortune (2025-01-27)\n\nThey all make… Oh wait, Morgan Stanley wins this:\n\nThese efficiency improvements are not surprising to the software ecosystem. Most software companies believe that lower deployment costs lead to higher utilization (the Jevons paradox) and have already actively worked to lower these costs.\n— Morgan Stanley’s View on the DeepSeek Shock (2025-01-27)\n\nOkay, other than Morgan Stanley… Continuing my rant.\nThey all make the lump of labor fallacy! They are professionals! They work at investment banks! Private equity firms! I can tell a better story for why R1 could be bad for Nvidia. But I don’t think that drove the crash. I don’t know, but I know that they don’t know, and that they don’t even know they don’t know.\nOr perhaps they don’t care? Maybe most analysts don’t care a damned thing about being right. They are selling wisdom, not knowledge; sensemaking, not truth; toy stories, not falsifiable predictions.\nSomeone was doing a startup on using AI to predict earnings report, and asked me for advice over dinner. I was very surprised that their minimalistic baseline model (used to compare with the actual model) could already beat the analysts at Bloomberg in backtesting. Thinking a bit, I suggested that their model was beating the analysts because the analysts weren’t really selling information, but respectability; to people who aren’t seeking alpha, but respectability: “We are doing the industry best practices…”."
  },
  {
    "objectID": "logs/posts/2025-whalefall/index.html#what-crash",
    "href": "logs/posts/2025-whalefall/index.html#what-crash",
    "title": "Whalefall",
    "section": "",
    "text": "This crash.\n\n\nOn 2025-01-27 on the New York Stock Exchange, multiple semiconductor corporations gapped down. The cause is clear: something about DeepSeek-R1. What that something is, I don’t know.\nPersonally, I was quite obsessed with R1 since its release on 2025-01-20, feeling the AGI in mathematics. I also bought more Nvidia and TSMC, seeing this as an obvious buy signal.\n\n\n\n(っ◔◡◔)っ 🎀 imagination 🎀. Source: @layer_07_yuxi (2025-01-19).\n\n\nThen this crash happened. An utter shock. The market… is stupid… and inefficient?\nNearcyan claimed that the market was probably manipulated by a stand-alone complex of insiders, who felt that a crash was struggling to emerge, and bandwagoned on it, making the crash a reality (“feed the sharks”). He didn’t elaborate but I presume it involved some of the following:\n\nbuy it on the cheap and sell them for profit a bit later;\nbought some put options;\nsold some call options;\ndid HFT to profit on volatility itself;\nsome linear combination of these to minimize tax.\n\nI don’t know the detailed mechanics of how the crash appeared, because I am an outsider to the financial establishment, and I don’t know how to psychoanalyze the market.\nThe separate analysis, on why DeepSeek hit No. 1 on the Apple App store so quickly after release,1 seems true enough to me. Such rapid rise must be a memetic virus, something like TikTok and YouTube Shorts. The app is good, the model is great, but that’s not enough to push the app to No. 1 out of nowhere. The general public knew nothing of DeepSeek; DeepSeek made no advertisement; etc.\n1 Surprisingly hard to figure out when exactly this happened. Apparently, 2025-01-10 release, 2025-01-26 hitting No. 1.\n\n\nNearcyan’s analysis. Source: @Nearcyan (2025-01-28).\n\n\nBut I learned something new: that the mainstream media has no economic literacy. There is a story to be told for why R1 was bad for Nvidia, but none of the mainstream media stories hit anywhere close! If I were grading their economics homework, they are all getting “E for Efforts”. No, “F for Effort missing a plank”!\nConsider a typical take:\n\nThe DeepSeek product “is deeply problematic for the thesis that the significant capital expenditure and operating expenses that Silicon Valley has incurred is the most appropriate way to approach the AI trend”, said Nirgunan Tiruchelvam, head of consumer and internet at Singapore-based Aletheia Capital. “It calls into question the massive resources that have been dedicated to AI.”\n— DeepSeek buzz puts tech stocks on track for $1 trillion wipeout | Fortune (2025-01-27)\n\nThey all make… Oh wait, Morgan Stanley wins this:\n\nThese efficiency improvements are not surprising to the software ecosystem. Most software companies believe that lower deployment costs lead to higher utilization (the Jevons paradox) and have already actively worked to lower these costs.\n— Morgan Stanley’s View on the DeepSeek Shock (2025-01-27)\n\nOkay, other than Morgan Stanley… Continuing my rant.\nThey all make the lump of labor fallacy! They are professionals! They work at investment banks! Private equity firms! I can tell a better story for why R1 could be bad for Nvidia. But I don’t think that drove the crash. I don’t know, but I know that they don’t know, and that they don’t even know they don’t know.\nOr perhaps they don’t care? Maybe most analysts don’t care a damned thing about being right. They are selling wisdom, not knowledge; sensemaking, not truth; toy stories, not falsifiable predictions.\nSomeone was doing a startup on using AI to predict earnings report, and asked me for advice over dinner. I was very surprised that their minimalistic baseline model (used to compare with the actual model) could already beat the analysts at Bloomberg in backtesting. Thinking a bit, I suggested that their model was beating the analysts because the analysts weren’t really selling information, but respectability; to people who aren’t seeking alpha, but respectability: “We are doing the industry best practices…”."
  },
  {
    "objectID": "logs/posts/2025-whalefall/index.html#the-bear-case",
    "href": "logs/posts/2025-whalefall/index.html#the-bear-case",
    "title": "Whalefall",
    "section": "The bear case",
    "text": "The bear case\nAfter thinking more about Nvidia and the AI market, my conclusion: The AI revolution isn’t going to disappear, but Nvidia’s fat margins might.\nNvidia’s margins are mainly caused by it having a monopoly on highly efficient compute with a complete stack. AMD has chips with higher (FLOP/sec)/USD, but AMD’s driver is garbage so the utilization rate is low, while Nvidia’s just works.\nHowever, this might change any time. Google’s TPU is a serious threat. Google’s software infrastructure is powerful, scalable. It has been pushing for JAX adoption and it might get picked up, which would mean a complete switch of the stack. There are credible sources saying that TPU is already better than GPU, and if Google ever start selling them on the market, Nvidia could have a serious competitor.\nTraining great LLMs entirely from ground up in the wilderness as a startup — Yi Tay\n\nI was completely taken aback by the failure rate of GPUs as opposed to my experiences on TPUs at Google. In fact, I don’t actually recall TPUs failing much even for large runs, though I was not sure if I was protected from knowing this just by the sheer robustness of the outrageously good infra and having a dedicated hardware team. In fact, the UL2 20B model (at Google) was trained by leaving the job running accidentally for a month. It never failed. If this were in GPU land, it would have failed within the first few days for sure.\n\nSimilarly, Microsoft is designing Maia, Amazon designing Trainium, etc. There are also many fabless chip startups, and one of them might just work out. TSMC stands to gain the most from so many new fabless chips (commoditizing your complement). More fabless chip designs competing means less margins at the design stage, means more margins at the fabbing stage, so TSMC gets more margins.\nAnother possibility is that R1 instigates more stringent global control of the chip supply chain. For example, America could ban the sell of all Hopper-class chips to China, or the EUV lithography machines, or photoresists, etc. America could impose more annoying paperwork requirements for selling those to countries adjacent to China (like Thailand and Singapore). All of these would raise the cost, slow down the process, and fewen the buyers of chips. This would be bad news for not just Nvidia, but the global chip supply chain as a whole (including TSMC).\nA more neglected possibility was pointed out by Gwern, who argued that there’s a Hardware Hedging Against Scaling Regime Shifts. If the best kind of training becomes much more serial than parallel, then Nvidia’s chips are no longer relevant for DL.\n\nInstead of getting much more parallel, training could get much less parallel. It’s worth noting that this is the reason so much scientific computing neglected GPUs for a long time and focused more on interconnect throughput & latency: actually, most important scientific problems are highly serial, and deep learning is rather exceptional here … There could be new architectures moving back towards RNN which don’t have a “parallel training mode” like Transformers, and you inherently need to move activations/gradients around nodes a ton to implement BPTT. There could be some twist on patient-teacher/grokking-like training regimes of millions or billions of inherently serial training steps on small (even n = 1) minibatches, instead of the hundreds of thousands of large minibatches which dominates LLM training now … What sort of hardware do you want in the ‘serial regime’? It would look a lot more like supercomputing than the mega-GPU datacenter.\n\nHow would R1-Zero change this? The big labs like Meta are right now furiously replicating R1-Zero with 100x more compute than DeepSeek, and if a few months down the line, we see a new scaling law emerge from them that shows that R1-Zero training works, but it works better with both longer chains and smaller batches, then the RL phase of training would suddenly become highly serial.\nIn this case, pretraining would still need giant GPU clusters, but the RL step would suddenly look more like Cray supercomputers – a few giant CPUs, immersed in Fluorinert, running at 100 GHz.\nIn that situation, even if Nvidia’s monopoly is never displaced, it would no longer be the sole monopolist on the value chain. It would have to share the big margins with someone else – maybe Groq, or a resurrected Seymour Cray. Sure, I still expect the sum-total of profit to keep growing, but a less portion of it would go to Nvidia. Since Nvidia’s high market valuation is mostly in its expected future growth, any drop in the expectation is bad news."
  },
  {
    "objectID": "logs/posts/2025-whalefall/index.html#is-the-market-efficient",
    "href": "logs/posts/2025-whalefall/index.html#is-the-market-efficient",
    "title": "Whalefall",
    "section": "Is the market efficient?",
    "text": "Is the market efficient?\nAfter seeing the absolute nonsense of the very respectable commentators, I no longer have faith in the Efficient Market Hypothesis. Or at least the Rational Trader Hypothesis. A large portion of the market traders might actually be insane noise traders. (Black 1986) Great news for the market sharps.\nAlternatively, perhaps the market is efficient enough, but it is becoming less legible as a source of information. The market speaks, but it is harder to get what it is saying. It is using the same price signals for different things. The great semantic shift. Terrible news for we who have placed faith in thy Invisible Hand, and trusted the market for integrating information.\nWe thought high price of Nvidia predicts high futures earnings, but does it? Fundamental value analysis has long been in decline, and now stock prices function more as a fiat currency of vibes than as an earnings-backed asset. Since the death of dividends, there has been a vacancy open. Memes filled this void.\nThe prices mean something. The market integrates information, but the information has less to do with the future of Nvidia for AI, and mostly to do with some kind of memetic contagion (as Nearcyan says), or the economic and machine-learning illiteracy of the noise traders (my hypothesis).\n\nNoise makes financial markets possible, but also makes them imperfect. If there is no noise trading, there will be very little trading in individual assets. People will hold individual assets, directly or indirectly, but they will rarely trade them. People trading to change their exposure to broad market risks will trade in mutual funds, or portfolios, or index futures, or index options… Noise trading is trading on noise as if it were information. People who trade on noise are willing to trade even though from an objective point of view they would be better off not trading. Perhaps they think the noise they are trading on is information. Or perhaps they just like to trade. With a lot of noise traders in the market, it now pays for those with information to trade.\n(Black 1986)\n\nDespite the increasing amount of noise, I will continue to use index funds. Don’t day-trade because reaction time too long. You can’t lose to a high frequency trader if you trade once a month. Don’t pick individual stocks unless you have deep insider information on it, or want to wear stocks like a personal brand, or have some AGI helping you, or it hedges against some personally relevant risks.2\n2 Like, buying TSLA because you despise Elon Musk, so that if Elon Musk’s company keeps growing well, you at least have the consolation of having made money. Similarly, crypto haters should buy more crypto."
  },
  {
    "objectID": "logs/posts/2025-whalefall/index.html#appendix",
    "href": "logs/posts/2025-whalefall/index.html#appendix",
    "title": "Whalefall",
    "section": "Appendix",
    "text": "Appendix\n\nSome plaintext copies of things I worry would get disappeared by the Internet.\n\nA Luminous Paradox\nSource: I wrote it in collaboration with DeepSeek-R1. Inspired by the lump of intelligence fallacy of the commentators, we present: A Luminous Paradox by Frédéric Bastiat.\nTLDR: It’s an alternative history satire.\n\nEdison invented the light bulb, much more efficient than the electric arc-lamp. Investors sell off his General Electric stocks in a panic, ruining Edison.\nSlave-traders smash the cotton gin in fear of high productivity crashing the market for slaves.\nColliers smash the Watt engine, for it would crash the demand for coal.\nSocrates lament writing, for it would crash the demand for thinking.\n\n\n· · · ☙ ❈ ❧ · · ·\n\nTo the Honorable Gentlemen of the Illumination Regulatory Commission,\nIt has come to my attention that certain reckless innovators—chief among them one Mr. Thomas Edison—have unleashed upon the world a device so calamitous, so efficient, that it threatens to unravel the very fabric of our enlightened society. I speak, of course, of the incandescent lightbulb, a contraption that dares to produce more light with less coal, less gas, and fewer towering electric arcs blasting their fury into the night sky.\nThe panic is understandable! Already, this modern Prometheus has been hoisted with his own petard, as panicked investors brought Edison General Electric to the verge of bankruptcy. “What need have we for electric generators,” cry the shareholders, “when a single glass bauble can bathe a parlor in radiance?” To these lamentations, I say: Bravo! Let us march backward with vigor, and smother thoughtless efficiency in its crib.\nConsider, if you will, the plight of the cotton gin. Had the slave-traders of yore possessed the foresight of our modern lamp-extinguishers, they would have outlawed Eli Whitney’s machine at once. “This devilish engine,” they might have thundered, “allows one man to clean fifty times as much cotton as before! It will collapse demand for labor and render our ships empty!” Yet lo—cotton production soared, plantations multiplied, and the demonic Triangle rotated faster than ever. The gin, that thrifty villain, did not destroy the market for bondage; it inflamed it.\nOr ponder James Watt, that sly Scotsman, whose steam engine threatened to ruin the honest coal miners of England. “Why, this machine does more work with less coal!” wept the colliers. “Our pits will close! Our children shall starve!” And yet—what transpired? Factories bloomed, railways sinewed the earth, and coal, that humble black rock, became the lifeblood of empires.\nEven the ancient Socrates railed against the written word. “If men accepted the gift of Theuth,” he warned, “they will cease to exercise their minds”. And yet—did letters not amplify thought, spawning philosophies, sciences, and epic poems? The scribe’s quill, far from extinguishing wisdom, kindled a thousand new schools.\nSo it is with light. The arc-lamp, that splendid dragon of the streets, consumes rivers of current to scorch the night. The bulb, by contrast, hums softly, thrifty as a monk. But does thriftiness not invite profligacy? When light becomes cheap, do we not use more of it? We shall line our homes with bulbs, string them over gardens, and illuminate shop windows until the cities themselves rival the constellations. Demand for electricity—and the coal, the wires, the engineers—will swell like a tide. Efficiency, that sly midwife, births new desires where none existed before.\nThus, I propose a challenge for the trembling hearts of investors: Indeed, let us outlaw all inventions that dare to do more with less. Ban the bulb! Smash the dynamo! Let us petition Congress to mandate that every household burn three barrels of whale oil nightly, lest the whaling industry collapse. For as every enlightened citizen knows, prosperity lies not in wanton abundance, but in privation thoughtfully enjoyed.\nYours in luminous obscurity, Frédéric Bastiat\n\n· · · ☙ ❈ ❧ · · ·\n\n\n\nMorgan Stanley’s View on the DeepSeek Shock\nSource: someone posted it on Twitter (2025-01-27). Sounds right, but I have no way to check.\n\nSemiconductors\n\nWhile DeepSeek’s success is unlikely to alter semiconductor investment plans significantly, there are several factors to consider. Feedback from various industry sources consistently indicates that GPU deployment plans remain unaffected. DeepSeek’s technology is impressive, but major CSPs have not neglected to invest in such technologies.\nIn fact, much of what we consider groundbreaking—such as training with FP8, multi-token prediction, MLA, custom PTX code, and GRPO reinforcement learning frameworks—either originated with DeepSeek v-2 and DeepSeek’s mathematical model six months ago or can be found in extensive AI research literature. This underscores the importance of how these technologies are implemented, and DeepSeek provides efficient designs in every respect.\nHowever, given the timing of announcements such as Stargate, Meta’s increased GPU demand forecast for 2025, Microsoft’s reiteration of its $80 billion annual CapEx guidance, and Reliance’s 3GW project in India, those connected to the model ecosystem were likely already aware of most of what DeepSeek was doing.\nThe long history of algorithmic improvements suggests we should not underestimate the incremental demand driven by cost reductions, advanced functionality, and continued scalability. NVIDIA has stated that algorithmic efficiency has improved more than 1,000-fold over the past decade, surpassing the performance gains of single-chip inference. In this context, while DeepSeek’s proposal of a 10x reduction in training compute requirements may not significantly impact LTGR (long-term growth rates), the resulting cost savings could accelerate inference adoption and potentially increase demand for inference.\nExport controls, however, remain a risk factor. It is clear that restricting Chinese technology to H20-level performance has not halted China’s development of LLMs. The implications for government policy are unclear at this stage. Lowering the performance threshold would greatly aid domestic silicon development in China. President Biden’s currently stalled AI restrictions, which aim to limit cluster sizes, would likely have a similar impact. These proposals may also compel other nations capable of supporting China’s AI development to obtain licenses.\nSimilarly, the performance gap between closed models and open-source ones continues to narrow. We have highlighted three risks to the AI industry in our bearish outlook (AI remains a key driver, but smaller surprises are expected in 2025). One of these risks is that the number of companies in foundational model development could decline as it becomes increasingly difficult to compete with cheaper open-source options.\nDeepSeek’s R1 exacerbates this risk by pressuring the largest spenders in AI to justify larger training runs while allowing others to leverage their work at much lower costs.\n\nInternet: Lowering Barriers to Costs Drives Product Innovation and Adoption\n\nDeepSeek’s architecture and pre-training improvements, which enhance cost efficiency, positively impact consumer internet companies seeking to develop new models and LLM-supported products. The ROIC for GenAI CapEx is expected to increase, and incremental CapEx growth could slow as a result.\nLarger companies’ ability to develop more innovative products will increase consumer utility, scalability, and adoption rates. Given their large capital investments, user bases, and ability to extract and implement DeepSeek’s improvements into their own models, GOOGL, META, and AMZN are poised to benefit the most from these cost savings.\nHowever, with more efficient architectures, smaller companies will also be able to provide GPU-supported products more broadly and at lower costs. For example, AMZN’s AWS strategy focuses on commoditization at the model layer. AWS integrates third-party models and provides access through tools like Bedrock, enabling customers to build applications.\nIf DeepSeek’s contributions to democratizing model building (reducing required costs and compute) further commoditize models, AWS could benefit as an aggregator.\n\nSoftware: Reducing AI Deployment Costs for Software Providers\n\nAlgorithmic efficiency gains at the model layer positively impact enterprise software. More cost-efficient models are reducing the “GenAI deployment costs” for the broader software ecosystem, and the companies we cover are primarily building solutions around these models.\nThese efficiency improvements are not surprising to the software ecosystem. Most software companies believe that lower deployment costs lead to higher utilization (the Jevons paradox) and have already actively worked to lower these costs.\nMicrosoft recently focused on its “Phi” small language model (SML) strategy, with the Phi-4 14B model delivering benchmark results comparable to Llama-3.3 70B.\nServiceNow partnered with Nvidia to use custom domain-specific language models to execute inference more cost-effectively.\nSnowflake trained Arctic 17B LLM with a $2 million training compute budget, achieving comparable performance to other enterprise benchmarks and best-in-class SQL performance.\nElastic developed the Elastic Learned Sparse EncodeR (ELSER) to lower the cost of semantic search for AI applications.\n\nEnergy\n\nRegarding stocks exposed to the growth of AI power infrastructure in the U.S., significant capital expenditures are expected to continue. Key considerations include:\n\nAn analysis of the U.S. data center pipeline indicates that most of the known pipeline is for AI inference and non-AI use cases rather than AI training.\nThe “Powering GenAI Models” analysis suggests that computing costs could drop by ~90% over the next six years. As AI adoption increases, the Jevons paradox could lead to rapidly growing demand for AI computing.\nDiscussions with companies suggest that substantial AI infrastructure spending is currently happening in the U.S. (Stargate being one of the most prominent projects we anticipate).\nAfter the recent sell-off, many stocks exposed to U.S. AI infrastructure growth are still undervalued and not fully pricing in AI growth.\nIT Hardware: The Focus of AI in IT Hardware and Apple’s Position\n\nLast weekend’s DeepSeek news raised many unanswered questions (particularly regarding total compute costs and final training runs). Still, concerns about AI-related stocks’ long-term ripple effects are evident, especially for DELL (AI infrastructure) and STX (HDD), which may be the most impacted.\nWe believe $AAPL could emerge as a relative winner in this debate:\n\nApple’s AI ambitions are primarily focused on feature-specific, on-device small LLMs rather than large frontier models, meaning its AI investments are far less visible than those of its peers.\n\nConsequently, Apple’s annual CapEx ($9.4 billion for FY24) is about 1/20th of the combined CapEx of U.S. Tier 1 hyperscalers. If the market overemphasizes CapEx ROI, Apple faces a much lower bar to generate attractive returns (i.e., less risk).\n\nAs DeepSeek has demonstrated, reduced memory requirements for inference make “Edge AI” much more feasible, aligning with Apple’s core GenAI ambitions.\nIn a world where consumer LLMs are commoditized, distribution platforms become key assets, and Apple owns arguably the most valuable consumer tech distribution platform in existence.\nEmbodied AI/Tesla: Advancements in GenAI Training Drive Embodied AI\n\nIn addition to the potential applications and acceleration of robotics training, we anticipate increasing attention on physical AI as growth in the digital AI narrative becomes less obvious and investors seek executable stories elsewhere.\nIn other words, as companies in the digital AI space become less reliant on double-digit returns, the early-stage opportunities in embodied AI—ranging from humanoids to eVTOL, AMRs, and AVs—are expected to appear increasingly attractive.\nFrom a geopolitical perspective, autonomous vehicles currently operate in several cities at 25% of traditional taxi costs, supported by government policies encouraging innovation and supply chains for low-cost local production components. These developments once again highlight China’s achievements in embodied AI.\nFaced with China’s advancements, the U.S.’s biggest geopolitical rival in all areas of AI, policymakers are expected to pay greater attention to fostering competitive progress among U.S. companies in this space.\n\n\nHardware Hedging Against Scaling Regime Shifts\nSource: Gwern posting on Reddit (2024-08-22).\nHyperscalers are investing heavily in AMD/Nvidia-style GPUs optimized for moderate-scale parallelism: less than almost-shared-nothing scientific computing tasks like SETI@home, but not strictly sequential like highly-branching tasks, and with the best interconnects money can buy in a custom datacenter, probably topping out at somewhere ~1m GPUs before the communication overhead/latency & Amdahl’s law pushes the diminishing returns to 0.\nIf you are going to spend $50b+ on GPU hardware (and then another $50b+ on everything wrapped around them), you are going to want to invest a lot into making conservative design choices & derisking as much as possible. So a good question here is: even if that 1m mega-GPU datacenter pencils out now as optimal to train the next SOTA, will it stay optimal?\nEveryone is discussing a transition to a ‘search regime’, where training begins to consist mostly of some sort of LLM-based search. This could happen tomorrow, or it could not happen anywhere in the foreseeable future—we just don’t know. Search usually parallelizes extremely well, and often can be made near-shared-nothing if you can split off multiple sub-trees which don’t need to interact and which are of equal expected value of computation. In this scenario, where you are training LLMs on eg. outputs from transcripts generated by an AlphaZero-ish tree-search approach, the mega-GPU datacenter approach is fine. You can train across many datacenters in this scenario or in fact the entire consumer Internet (like Leela Zero or Stockfish do), but while maybe you wouldn’t’ve built the mega-GPU datacenter in that case, it’s as equivalent or a little bit better than what you would have, and so maybe you wound up paying 10 or 20% more to put it all into one mega-GPU datacenter, but no big deal. So there are negative consequences of a search regime breakthrough for the hyperscalers, in terms of enabling competition from highly distributed small-timer competitors pooling compute, and AI risk consequences (models immediately scaling up to much greater intelligence if allocated more compute), it wouldn’t render your hardware investment moot.\nBut it is not the case that that is the only possible abrupt scaling regime shift. Instead of getting much more parallel, training could get much less parallel. It’s worth noting that this is the reason so much scientific computing neglected GPUs for a long time and focused more on interconnect throughput & latency: actually, most important scientific problems are highly serial, and deep learning is rather exceptional here—which means it may regress to the mean at some point. There could be a new second-order SGD optimizer which cannot parallelize easily across many nodes but is so sample-efficient that it wins, or it eventually finds better optima that can’t be found by regular first-order. There could be new architectures moving back towards RNN which don’t have a “parallel training mode” like Transformers, and you inherently need to move activations/gradients around nodes a ton to implement BPTT. There could be some twist on patient-teacher/grokking-like training regimes of millions or billions of inherently serial training steps on small (even n = 1) minibatches, instead of the hundreds of thousands of large minibatches which dominates LLM training now. There could be some breakthrough in active learning or dataset distillation for a curriculum learning approach: where finding/creating the optimal datapoint is much more important than training on a lot of useless random datapoints, and so larger batches quickly hit the critical batch size. Or something else entirely, which will seem ‘obvious’ in retrospect but no one is seriously thinking about now.\nWhat sort of hardware do you want in the ‘serial regime’? It would look a lot more like supercomputing than the mega-GPU datacenter.\nIt might force a return to high-end CPUs, overclocked to as high gigahertz as possible; however, it’s hard to see what sort of serial change to DL could really cause that, aside from extreme levels of finegrained sparsity and radical changes to the underlying neural net dynamics (if still ‘neural’ in any sense).\nMore plausible is that it would continue to look mostly like current DL but highly serial: like synthesizing a datapoint to train on immediately & discard, or training in a grokking-like fashion. In this case, one might need very few nodes—possibly as few as 1 model instances training. This might saturate a few dozen GPUs, say, but then the rest of the mega-GPU datacenter sits idle: it can run low-value old models, but otherwise has nothing useful to do. Any attempt to help the core GPUs simply slows them down by adding in latency.\nIn that case, you don’t want GPUs or CPUs. What you want is a single chip which computes forwards and backwards passes of a single model as fast as possible. Groq chips don’t do training, so they are right out. What comes to mind is Cerebras: a single ungodly fast chip is exactly their premise, and was originally justified by the same rationale given above as it applies to scientific computing. Cerebras doesn’t work all that well for the current scaling regime, but in a serial scaling regime, that could change drastically—a Cerebras chip could potentially be many times faster for each serial step (regardless of its throughput) which then translates directly to an equivalent wall-clock speedup. (Cerebras’s marketing material gives an example of a linear system solver which takes ~2,000 microseconds per iteration on a CPU cluster, but only 28 microseconds on a CS-1 chip, so &gt;200× faster per iteration.)\nThe implication then is that whoever has the fast serial chips can train a model and reach market years ahead of any possible competition.\nIf, for example, you want to train a serial model for half a year because that is just how long it takes to shatter SOTA and optimally trades-off for various factors like opportunity cost & post-training, and your chip is only 50× faster per iteration than the best available GPU (eg. 1ms to do a forwards+backwards pass vs 50ms for a Nvidia B200), then the followers would have to train for 25 years! Obviously, that’s not going to happen.\nCompetitors would either have to obtain their own fast serial chips, accept possibly staggering levels of inefficiency in trying to parallelize, or just opt out of the competition entirely and go to the leader, hat in hand, begging to be the low-cost commodity provider just to get some use out of their shiny magnificently-obsolete mega-GPU datacenter.\nIs this particularly likely? No. I’d give it &lt;25% probability. We’ll probably just get AGI the mundane way with some very large mega-GPU datacenters and/or a search transition. But if you are spending $100b+, that seems likely enough to me to be worth hedging against to the tune of, say, &gt;$0.1b?\nHow would you invest/hedge? Grok/Tenstorrent/AMD/Nvidia/Etched are all out for various reasons; only Cerebras immediately comes to mind as having the perfect chip for this.\nCerebras’s last valuation was apparently $4b and they are preparing for IPO, so investing in or acquiring Cerebras may be too expensive at this point. (This might still be a good idea for extremely wealthy investors who have passed on Cerebras due to them having no clear advantage in the current regime, and haven’t considered serial regimes as a live possibility.) Investing in a startup intended at beating Cerebras is probably also too late now, even if one knew of one.\nWhat might work better is negotiating with Cerebras for options on future Cerebras hardware: Cerebras is almost certainly undervaluing the possibility of a serial regime and not investing in it (given their published research like Kosson et al 2020 focused on how to make regular large-batch training work and no publications in any of the serial regimes), and so will sell options at much less than their true option value; so you can buy options on their chips, and if the serial regime happens, just call them in and you are covered.\nThe most aggressive investment would be for a hyperscaler to buy Cerebras hardware now (with options negotiated to buy a lot of followup hardware) to try to make it happen. If one’s researchers crack the serial regime, then one can immediately invoke the options to more intensively R&D/choke off competition, and begin negotiating an acquisition to monopolize the supply indefinitely. If someone else cracks the serial regime, then one at least has some serial hardware, which may only be a small factor slower, and one has sharply limited the downside: train the serial model yourself, biting the bullet of whatever inefficiency comes from having older / too little serial hardware, but then you get a competitive model you can deploy on your mega-GPU datacenter and you have bought yourself years of breathing room while you adapt to the new serial regime. And if neither happens, well, most insurance never pays off and your researchers may enjoy their shiny new toys and perhaps there will be some other spinoff research which actually covers the cost of the chips, so you’re hardly any worse off."
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html",
    "href": "logs/posts/2025-reprofro-party/index.html",
    "title": "ReproFro 2025 party",
    "section": "",
    "text": "During 2025-06-10–2025-06-12, the Reproductive Frontiers Summit (ReproFro) convention was hosted in Berkeley. Gwern invited me to the afterparty in Lighthaven, so I went.\nI wore my serious-business-dress and carried around Behavioral Neurobiology: An integrative approach (Zupanc 2018). Not to read it, but as a comfort object."
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#us-constitution",
    "href": "logs/posts/2025-reprofro-party/index.html#us-constitution",
    "title": "ReproFro 2025 party",
    "section": "US Constitution",
    "text": "US Constitution\n“… so Bismarck Analysis produces geopolitical analysis subscribed to by billionaires. They have made some seriously wrong analysis, like the one about the Russian military being modernized enough to win wars, but generally pretty good quality.”\n“Why would billionaires need geopolitical analysis?”\n“Geopolitical analysis is useful because there are some rare events that could be very important. For example, is it likely that the US would become like Turkey, with Erdogan? So Turkey was a democracy, but Erdogan just took it over step by step, with the media, the newspapers, and the government apparatuses. So a rich person would very well ask, ‘How would this or that country be like in 10 years?’ and decide where they would want to move their money and home to. And when you are planning ahead like that, the question of whether the US democracy is stable is very relevant. It’s not saying they are usually right. For example, they produced a report that says that Russian military had been successfully modernized, and that its bureaucracy wasn’t rampantly corrupt. Then Ukraine war happened and we see it was wrong. Still, when it’s wrong, it’s usually interesting, and they did get some things right.”\n“So you mean… Trump might become the president again and ignore the term limit?”\n“No, there’s a way to do it ‘legally’ even though not in spirit. He could switch with Vance.”\n“The Putin and Medvedev trick?”\n“Not that one. The Putin–Medvedev system probably inspired Erdogan’s taking of power. But no. It’s that the US constitution prohibits president to be ‘elected’ for 3 times. But Vance could get elected with Trump as vice president, then immediately resign. It would certainly be very contentious, but it does not violate the constitution in letters.”\n“I see. It is certainly way more useful to them than to the keyboard warriors or ‘political scientists’ on twitter.”\n“Well, geopolitical analysis is also useful to the ‘political scientists’ as an identity utility. Some have billions of dollars to protect, and some their self-identity.”\n\n“The gay marriage was actually done by a legal hack. The proper way to do it would be to amend the constitution, but that’d never work now.”\n“I thought it went through a slightly wider interpretation of the Equal Rights Amendment?”\n“Not really. It never passed.”\n“What? Didn’t it… [furiously checks phone] I’ve been lied to.”\n“Apparently some Democrats just acted as if it has passed, and act accordingly. It is a form of realpolitik, because even though it didn’t pass in form, it passed in spirit, because it ‘should have passed’, and if they legislate accordingly, eventually enough laws would come into being that the legal landscape would look just as if the amendment had really passed.”\n“There is that phrase ‘constitutional hardball’ (Tushnet 2003), it is a good description of what the US government has come to. The idea is that there is a positive feedback. As the president becomes more powerful, to make anything happen in the government, you’d have to go through the president, so the stakes become higher. And because the stakes are higher, as soon as your party gets a president in the White House, you try to really push it, to get your policy in, your people in, because you might never get a chance later. And laws get broken in spirit if not in letter. And when the other party takes power, they push further. And because this pushing needs to go through the president, the power of the president often gets increased as a side effect, or an instrumental goal of the pushing. So the power of the president just keeps ratcheting up.”\n“The US constitution actually sucks. You see that all the modern countries like Latin America or African countries that copied the US presidential system, pretty soon they fall into a dictatorship or an ‘imperial democracy’. All those examples goes to show that the US constitution is simply not built for a modern industrial country. So the US no longer tries to export its democratic model, and suggests the parliamentary model instead. If you look at the European countries, you see that the model is very stable, more than the American one.” (Linz 1990)\n“Ha, guess constitutional originalism would never work.” Verily, Shenanigans beget shenanigans.\n“Like if you look at the original constitution, the president had very limited powers. It and the federal government were almost ceremonial in a way. Most of the powers were in the states. It was designed for a small agrarian society.”\n“–In Germany the president is still like that.”\n“I thought Germany has only a prime minister.” Technically, it is called a Chancellor, but they are just two names for the same job.\n“Oh, there really is a president. Not elected, but appointed. He doesn’t do much really.”1\n1 Technically, the German president is elected by a vote of the Bundestag (the lower house of the German parliament) plus some local parliament delegates, but it is a low-stakes low-drama event, unlike the American president election. It is similar in mechanism to the American electoral college. Guess this is what the founding fathers intended America to be like.“Yeah, ceremonial probably. Like the Belgian king.”\n“–There is a king in Belgium?”\n“Yeah there is. Even the Belgians probably forgot about him.”\n\n“There hasn’t been a substantial update to the constitution since like 100 years ago. The constitution was never built for the modern America, so huge and powerful and with diverse interests, and it’s probably too late to substantially update it, because any substantial update would simply be shot down, because of constitutional hardball. We are kind of like in the late Roman Republic period. In the long run this is going to become an empire.”\n“So there was this story about Kurt Gödel. When he was going to become an American citizen, he was studying the constitution, and discovered that there was a loophole in the constitution, which can turn it from a democracy into a dictatorship.”\n“Well, nobody knows what the loophole was, but the point is that there are just so many loopholes that it doesn’t take a Gödel to find it. It just takes someone who is capable of bypassing the norms and following the letters and playing hardball for power.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#demand-for-reproductive-technologies",
    "href": "logs/posts/2025-reprofro-party/index.html#demand-for-reproductive-technologies",
    "title": "ReproFro 2025 party",
    "section": "Demand for reproductive technologies",
    "text": "Demand for reproductive technologies\n“I’m kind of surprised that Sam Altman has a child now. I would have expected that with their expectation of some probability of doom, it’d be better to wait a few years and see before deciding.”\n“Peter Thiel has two adopted children, so there is not a problem with deciding whether to create more people. Also, he does not believe in AGI, so that’s not a problem for him.”\n“Another way to think of it is like. If a child gets run over by a car at 10 years old, it sucks, but it is still a net positive if you add it up. Similarly for near-term AGI doom.”\n“The people who would be like ‘I can’t have children now because of AGI’ are the same kind of people who would be also saying ‘I can’t have children now because of climate change’ or ‘I can’t have children now because of systemic racism’ or some other thing. It’s post hoc rationalization. People are not good at judging some things about themselves. In fact, if I offer some kind of rationalization like this, you should not believe me!”\n“What can you believe about yourself, though?”\n“Well, I can’t possibly believe that I should not believe everything.”\n“Interestingly, it is consistent for us to believe that ‘You should not believe anything you believe in.’ but it is not consistent for you to believe that!”\n“And now I’m thinking about something Smullyan would say, like ‘whether I can believe that I believe in believing myself…’ hmm.”\n“Ahem, sir. The liar’s paradox is [traffic police stop gesture] strictly prohibited on this venue.”\n\n“It is also why the Nobel laureate sperm bank didn’t do well. Other than the lack of donors, there is also the lack of demand. Parents just seem less interested in intelligence than heredity, and would gladly have a kid with 5 IQ points lower if it is ‘theirs’. And also why infertile parents don’t usually adopt.”\n“Of course, there is also the fact that adopted children have way more psychological problems.”\n“People often say things like ‘he has your eyes’ and things like that. If you ask them pointedly they might say it is a bit silly and not a big deal, but if you look at what people do, instead of what they say, we can find their implied preferences, and it shows people really care about their children having copied a lot of their own genomic content”\n“How much money would they pay to have it? Must be in the 10000s range… There should be an economic paper about the shadow price of consanguinity.”\n“There needs to be better IVF success rates. There needs better sequencing. There needs to be very detailed understanding of how germ cells work if we ever want to create germ cells from somatic cells, which many gay people want. This is good for the superbabies project of course, that there are enough gay billionaires that they would put serious money into funding reprogenetics.”\n\n“I disagree with &lt;speaker name I forgot&gt;, about the male de novo germline mutations mattering. He thinks it matter a lot that older fathers pass on more mutations, but I don’t think so. Every year, the extra de novo mutation is maybe 10 bases? And that just is too small compared to all the other forces of selection.”\n“Should one freeze sperms if one worries about it?”\n“It certainly has some value, but one must think on the margins. It’s not ‘Is it worth it to freeze sperm at this price?’ but ‘Is it worth it to freeze sperm using the money I could spend on other things, like searching for mates more quickly?’.”\n“It actually costs a lot to freeze sperm, but it shouldn’t be.”\n“How much?”\n“About $1000/year?”\n“What! But liquid nitrogen costs a few dollars a tank. And as a reference class, the livestock breeders already keep a lot of sperms cheaply.”\n“Yeah, but in humans there is not an economy of scale yet, so we get a market only selling to people that really want it, so it’s super expensive. But the material price is really next to nothing. A tube of sperm is about the size of a drinking straw, and so many could be kept in the same tank. All the fees would be in administrative and record keeping costs.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#social-media",
    "href": "logs/posts/2025-reprofro-party/index.html#social-media",
    "title": "ReproFro 2025 party",
    "section": "Social media",
    "text": "Social media\n“I’m thinking about moving r/mlscaling, because Reddit is closing off the APIs, and if the AIs can’t read it anymore, what is the point? I might as well write on my own notebooks. That’s too bad though, because Reddit really has a good structure for public social discussion. I don’t know if there is any good substitute.”\n“Bluesky?”\n“Bluesky is interesting. The politics is worse though. It’s interesting that communities once destroyed doesn’t reform usually. When people left Twitter due to Elon, some of them just don’t reappear anywhere. They don’t write online anymore. They disappear, as far as the LLMs are concerned. Maybe they are spending more time offline and socializing or something, and some would spin this as a positive. But probably they are just passive consumers of TikTok and not really producing.”\n\n“… and I talked to him [points] on an IRC.”\n“What, IRC? People still use that?”\n“Yes, you know what IRCs are?”\n“I … read about them once on Wikipedia.”\n“Haha, that’s about as much as what most people know about it.”\n“I thought they died out, like Freenet.”\n“That’s about right. It’s a pretty good as a gate keep device. All the others are using Discord and causing drama there, and only the old grumpy people still use IRC.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#alphaeverything",
    "href": "logs/posts/2025-reprofro-party/index.html#alphaeverything",
    "title": "ReproFro 2025 party",
    "section": "AlphaEverything",
    "text": "AlphaEverything\n“… and I’m trying to train a genome sequence model with HyenaDNA.”\n“Hyena… DNA? Do you mean hyena the animal, or hyena the neural network architecture?”\n“The architecture.”\n\n“… Biologists are not interested in scaling. I am trying to scale-pill the biologists.”\n“But biologists do love scaling! In fact, there is bioinformatics, which is basically big data for biology. They can generate so much data. They can get terabytes of data just from a cup of river water and doing a pangenomic sequencing.”\n“But still many biologists are very focused on understanding deeply a little thing, and are not thinking about scaling issues, and what problems can be solved with more scale.”\n“For years, biologists have thought that there was not enough data to solve protein folding. But then AlphaFold came along and showed that they were wrong. There was already enough data. It just required a good model to learn from all the data.”\n“AlphaBrain! It should work better than whatever the Human Brain Project has been doing.”\n“So people say that fast takeoff is unlikely in certain areas like biology or physics, because you still have to do all the experiments, and you can’t Moore’s law on chemical reaction rates. That’s true, but you can be better at picking what experiments to run. A smarter AI could think through the experiments and pick the most informative one to perform. They could be more sample efficient than human researchers. In fact, I try to tell researchers to think bigger. They have so much data, they just need to use them. A biological lab could generate so much data. Feed them all into a total AlphaLab model to predict everything about the lab as a sequence prediction task: predict the experiment runs, the next experiment that would be run, the genome sequence that would come out when this sample is put into a sequencer, the next visitor to the lab… everything!”\n“AlphaEverything! AlphaPhysics, AlphaChemistry… Maybe this happens a lot. Maybe there are massive sample inefficiencies in many fields. Physics is stagnating, but perhaps we already have enough data to decide which cosmological model is right, if only we are smart enough to interpret at the data.”\n“Maybe physics is hard because data is expensive and there are so many models, especially in high energy physics and cosmology. So it’s hard to pick the right model with data. Maybe the physicists are doing as well as they could.”\n“But how do you prove it? How do you know that you have truly did as well as the data allows?”\n“Heritability estimates for height is 0.8, measured by monozygotic twin studies. and that sets a hard upper bound on how much you could possibly get out of genetic data. One can take a statistical model that predicts height based on the gene sequence, and if it predicts 0.4 of the variability of height, then they could point at it and say that they have gotten 50% of the theoretical upper bound, and no genetic predictor of height could possibly do better than twice of their model.\n“So that’s my challenge: Is there a way to estimate an upper bound on how much information could possibly be extracted out of the data we have?”\nThe group thought about it a bit, but finally relented without any ideas."
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#japan",
    "href": "logs/posts/2025-reprofro-party/index.html#japan",
    "title": "ReproFro 2025 party",
    "section": "Japan",
    "text": "Japan\n“So I did go to Nagasaki and I think it’s the best place to visit in Japan.”\n“Why? Not Kyoto or?”\n“Kyoto is too designed for tourism. Nagasaki shows the most standard kind of Japan. It’s like a typical Japanese city, so it allows you to get a better view of Japan – Japan as seen by Japanese – than Kyoto.”\n“And Tokyo is basically a city state, a different country.”\n“So there were two things that were most interesting to me in Nagasaki. One was the atomic bomb museum. Near Ground Zero was a Catholic church, so there were melted rosemary beads there.”\n“I went to the Las Vegas atomic bomb museum, and it is really great. There were also a few items sent in by the Japanese. It was just subtly accusatory. Like it never says out loud, but it implies the blame on America. But no mention of Japanese invasion. Like none of the items was false, but the omission, of what is not highlighted, is significant.”\n“Another interesting thing in Nagasaki was the Dutch Slope [オランダ坂].”\n“You mean the Dutch island?”\n“No, the Dutch Slope, a sloped ground [gestures with hand], where many buildings were built up by Japanese craftsmen in the ‘Dutch style’. You know that Chinatowns in America are basically built by Americans who had never been to China? The Dutch Slope is basically built by Japanese who had never been to Holland.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#anime",
    "href": "logs/posts/2025-reprofro-party/index.html#anime",
    "title": "ReproFro 2025 party",
    "section": "Anime",
    "text": "Anime\n“Yeah, like I have never been to Japan. I know, it’s the most ungwernlike thing. Somehow even the LLMs assume that I have visited Japan. One of them even hallucinated an entire Gwern essay about ‘My visit to the Ghibli Museum’.”\n“There is a Ghibli museum?”\n“Yes. Actually that is an interesting thing. It was designed by the son of Miyazaki, and the park is great. He is a good architect, but a horrible movie director. The conspiracy theory is that Hayao Miyazaki keeps allowing him to direct movies to make up for being an absent father.”\n“Or maybe because they don’t have anything better to do. You know. Many great scientists have children whose only notable achievement was editing their parents’ collected works.”\n“But he was a great architect. He also designed the Ghibli Park. And you know what is great about it is that it is very subtle. It’s not like something … Disneyland. It is not in your face. It’s hard to even find it from the outside. But if you are there, everything is just very Ghibli. They would take this or that building from the movies and replicate it exactly in the park, but in a functional way so that it also makes sense as a real building that someone might use functionally. It is hard to explain except by being there.”\n\n“Miyazaki is clearly mistaken about his interpretation of The Boy and the Heron. He is obviously the wizard uncle, even though he claims he is the boy. So everyone just agrees that Miyazaki is wrongly interpreting Miyazaki here.”\n“Self-transparency in shambles. The opacity of mind strikes again.”\n\n“Why is Isekai suddenly so popular now? Probably just a random fashion.”\n“More and more of animes are coming out of Japan with a backdrop of declining civilization and depopulation. But the authors don’t even put it on the upfront, or point at it as if it is a special thing. It was kind of just a subdued unremarkable thing to assume! It is probably hard to understand to a country with a rising economy though, and we would think it’s an artistic choice, but it’s probably because it’s realistic for them. There are whole towns in Japan with all the infrastructure, but without people to use or maintain them.”\n“So there was this anime, Girls’ Last Tour, and it was about two girls traveling in a postapocalyptic landscape, and they were probably the last humans alive–”\n“–That’s not true! There was a man drawing maps [カナザワ].”\n“It was heavily implied that he died shortly afterwards, and at the ending, as they climbed to the top of the building, snow began to fall, and it was heavily implied that they too would die soon. Everything is so meaningless. There is nothing left to do but to kill time and then die. It shows that everything we do is assumed as if it is done for someone else to notice, and nothing is worth doing if it has no effects on anyone but oneself. There were robots that still did things. There were great monuments. But it was a Disneyland without children (Bostrom 2014)… Do you watch anime? It’s a good anime.”\n“I don’t watch anime… I watched Girls Last Tour, and Lain.”\n“Lain is a good one.”\n“I don’t read manga either… I read Qualia the Purple, because one of your newsletters recommended it. It was bad.”\n“Oh no, is it?”\n“The first 1/3 was good, but then the quality dropped off.”\n“That’s too bad. I didn’t finish reading it. It’s in the backburner as in, if I don’t have anything else to read, I’d read that, but I always have had something else to read.”\n“Well you can take it off the backburner now!”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#bumbling-around-lighthaven",
    "href": "logs/posts/2025-reprofro-party/index.html#bumbling-around-lighthaven",
    "title": "ReproFro 2025 party",
    "section": "Bumbling around Lighthaven",
    "text": "Bumbling around Lighthaven\nWe went bumbling around Lighthaven, looking for Steve Hsu, but he was nowhere to be found. But we did note amusing things at Lighthaven.\n“In Lighthaven there is a podcast room. It just has 5 cameras running 24/7. The storage space is about 1 TB per day.2 People could just go in and start talking spontaneously, and when they get back they can email Lighthaven for a video, and they would just cut out the relevant segments and send to them. And there’s your podcast!”\n2 If we assume 1 TB per day per 5 cameras, this translates to 18.5 mbps, or the standard bitrate of 1440p at 30 fps.\n\n\nSource: tweet by @hsu_steve.\n\n\n“Very smart. Every conference venue should have such a room. Pick-up podcasting.”\n“I know that storage is cheap enough now that this is fine, but it always just sounds crazy to me that this is possible now…”\n“Now 100 TB of storage only costs like 1000 dollars.”\n“Yes. And audio is more important than video for podcasts, so they could use higher compression for videos. Still, my intuitions were formed in the past, so it still sounds crazy to me on an emotional level. Also, they use a wired cable connection to the storage unit, which is 8 times faster than wifi. And they only keep the video stored for maybe a month, so it doesn’t cost that much.”\n\nIn a conference room, we saw a world map of power lines, created by Mapographics.\n\n\n\nThe map.\n\n\n“I wonder what that straight line near South America is…”\n“West Australia is gone, but North Korea is visible.”\n“I wonder when this map was created. China ought to have a lot more of the power lines?”\n\n\n\nThe map superimposed on a Mollweide projection map of the world.\n\n\nAt home, I tried figuring it out, but neither o3 nor I could figure it out. It doesn’t look like anything at all, not even undersea cables. The strangest part is that one of the special points is just around 56°W 15°N, but absolutely nothing is there. It is just an empty stretch of ocean with no land nearby. It is possibly a fake pattern designed as a no-tech copyright device.\n\n\n\nZoom into the strange triangle off the coast of South America, with several landmarks labelled.\n\n\n\n“I call this ‘Chesterton’s fence’, well it’s actually a vehicle gate, but it works well as a joke.”\n“I call this ‘Chesterton’s joke’. If you can’t explain why it has been told, you are not allowed to say it is a dumb joke.”\n\nThere was a translucent inflatable geodesic igloo made of some soft plastic material. There were beanbags inside, and on the doorway there was a rectangular board with 1 written on it."
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#theatric-violence",
    "href": "logs/posts/2025-reprofro-party/index.html#theatric-violence",
    "title": "ReproFro 2025 party",
    "section": "Theatric violence",
    "text": "Theatric violence\nTwo men suddenly began fighting with foam swords.\n“That person I knew in the past was interested in HEMA. Glad to see he is still at it.”\n“Though they are using single-handed swords. To be more accurate for HEMA, they would usually also have a buckler or a dagger. It allows [crosses arms in demonstration] catching the sword.”\nA few minutes later, they had collapsed to the ground and were writhing like mating snakes.\nI pointed. “Looks like they aren’t doing HEMA now.”\n“Oh that is perfectly normal HEMA. The goal would usually be to pin down the opponent, and it usually means fighting on the ground. After all, it was supposed to be used in life-or-death fights, so it doesn’t matter if it looks ugly. It’s not like fencing, which is completely ceremonial and impractical.”\n\nSuddenly, the group looked up as if a legendary figure was approaching. It turned out that during the Manifest conference, he broke the right arm of Henri Lemoine, and people couldn’t stop cracking jokes about arm misalignment. There was a lot of ironic hypermasculine posturing, like “My testosterone levels would kill lesser men.” and “If I were a gorilla, I would be feeling my silver hair [hand plucking down the spine] so hard.”.\nSomeone asked who won the arm wrestling match, and it turned out it was cancelled.\n“Didn’t they say there would be a left-arm wresting match?” Note: this was a joke.\n“I voluntarily left the match.”\n“Wise move. So you could retire undefeated.”\n“The masculine urge to start a Fight Club. Where you can have good clean fights. Losing in a good clean way is still better than winning in lame ambiguous ‘fights’.”\n“Guys literally only want one thing and it’s a good clean win or loss.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#sample-efficiency-of-biological-organisms",
    "href": "logs/posts/2025-reprofro-party/index.html#sample-efficiency-of-biological-organisms",
    "title": "ReproFro 2025 party",
    "section": "Sample efficiency of biological organisms",
    "text": "Sample efficiency of biological organisms\n“There is a common criticism about deep learning, which is that they are wildly inefficient. Like how humans can see a few examples and learn a category, while they need to see 100s of examples. But the problem is that they are comparing adult humans with the neural networks. And the adult humans come with a lot of experience and learning already. What if we try to compare them holding the amount of training data equal?”\n“There was this experiment where they raised chicken in a VR box, so that everything the chicken saw was recorded and designed. They also tracked the chicken’s eyes so that they knew exactly what the chicken saw. Then they trained a convolutional neural network to . It turned out to perform comparatively with the chicken in psychophysical experiments of discrimination.”\n“And I thought The Truman Show was just a movie!”\n“There was also an experiment where people kept track of everything seen and heard by a baby, and then trained a neural network on it. It did surprisingly well, and could do language about at the same level of a baby at the same age. So all that talk about how deep learning is very sample inefficient, when people actually get some data to test it, the hypothesis might actually not hold up?”\n“There was also that RL experiment (Dubey et al. 2018) that shows just what happens if we give humans a ‘level playing field’ with the AI. So what they did was, to randomly change all the sprites in a platforming game, so that you can’t rely on all the prior knowledge you have about what everything ‘should’ look like in a game. And they have a website where you can play interactive versions of the games with more and more prior knowledge removed. I spent some time playing them and it gave me [strident voice to simulate suffering] psychic pain to play. I was like ‘This is so hard! Is this what it’s like to be an RL agent?’.”\n\nNote that the RL agent in the paper they used still took ~10 million steps to win the game. This compares badly with the ~2 minutes for the easiest example, or ~20 minutes taken for a human to beat the hardest level. But it was 2018, so perhaps RL agents have gotten a lot better since then?\nI read some papers around this.\n\nMoculus and MouseGoggles: A Full-Dive VR system for mice. It contains a pair of VR goggles, a treadmill-ball, a sugary liquid lick for reward delivery, head clamp, pupil tracking, and two-photon imaging. It’s too new to be cited yet, but it should be possible to use it to gather data and train a “digital twin” RL agent of a mouse, in the same style as the following paper, which trained digital twins for chickens.\nParallel development of object recognition in newborn chicks and deep neural networks: Chicks were hatched in darkness, then raised singly in automated controlled-rearing chambers that measured each chick’s behavior continuously for 14 days. The chamber had 4 LCD screens. In the 1th week (the imprinting window), one screen displayed a single 3D object rotating back and forth in a 60° range. During testing phase, the chicks were tested for binary choice between two static views of 2 objects, one of which is the imprinted object in a novel view angle. They also simulated a digital chicken in Unity game engine to generate 10K images with which they trained CNNs by self-supervision, then tested them for the same novel-view binary-choice experiment by training a linear classifier on top (that is, testing for linear separability of object representations). They reached the same performance level (70% correct) as the chicks. There was a lot more to this paper, but I didn’t read further, though I feel like it’d be more convincing if they did not use the linear classifier, and used nearest neighbors instead at the testing phase.\n\n\n\n\nThe MouseGoggles system. A video of it in action is on YouTube. Source: MouseGoggles offer immersive look into neural activity | Cornell Chronicle.\n\n\nThe human evidence is notably weaker, probably due to the difficulty of human experiments.\n\nSAYCam dataset: 415 hours of naturalistic, longitudinal recordings from 3 children during their 0.5–2.5 years old period.\nLearning high-level visual representations from a child’s perspective without strong inductive biases: Trained embedding and generative models on 472 hours of video (5 fps) from the SAYCam dataset by self-supervised learning. Used ViT to minimize inductive bias. Frozen embedding model allowed okay performance on ImageNet classification with a linear head. Generative model (VQGAN) could reconstruct images from the data and they looked good. No comparison with human performance.\nGrounded language acquisition through the eyes and ears of a single child: Trained a vision-language model by CLIP-training with a small subset of SAYCam of 600K video frames (61 hr) paired with 37.5K transcribed utterances. The trained model could match object images and words, and generalized to out-of-distribution images. No comparison with human performance.\nPredicting the birth of a spoken word: Recording of a typical male child from birth to the age of 3 y, of ~10 hr/day, totalling ≥200,000 hr. Manually transcribed ∼8 million words (2 million utterances) of both child speech and child-available speech by caregiver. The dataset is private and seems exclusive to only that one research team, so I expect it to have no future relevance."
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#neuroplasticity",
    "href": "logs/posts/2025-reprofro-party/index.html#neuroplasticity",
    "title": "ReproFro 2025 party",
    "section": "Neuroplasticity",
    "text": "Neuroplasticity\n“Have you ever went to the Exploratorium? It was actually opened by Oppenheimer’s brother, and it had the first interactive expositions. And that model is copied across the world in other science museums.”\n“It’s certainly a better legacy than the atomic bomb.”\n“At the Exploratorium there was a demonstration of this effect. You wear those goggles and everything you see is tilted to the right by 15°, and then you try to toss a basketball to a hoop. Then you would always shoot to the left. It takes about 20 tries before you adapt to the change, then you take it off, and now you always shoot to the right, and it takes about 20 tries before you adapt back.”\n“I always think of that as a kind of temporary adaptation in the neural activities. As soon as that experience ‘leaves your context window’, your adaptation ends. It is like in-context learning in Transformers. It will always be slower and less fluid than true adaptation in the neural weights.”\n“There were some crazy psychologists who actually wore inversion goggles that turned everything upside down for a month, and they did end up acting basically as fluidly as if they are not wearing the goggles.”\n“Yeah, at that time scale, it was probably enough to cause real changes in the neural weights in the brain.”\n“Some animal psychologists have done the same with birds. They would strap the deflection goggles to the birds. The barn owls could kind of adapt? But the chickens could never adapt. They would always be pecking [pecking motion with a chevuoi hand] to the side, and would literally starve to death without help.” (Hess 1956; Knudsen and Knudsen 1989)\n\n\n\nAcquisition of adapted behavior was slow, taking place over a period of weeks, and was never complete even for owls that were raised viewing the world through relatively weak (11°) displacing prisms. 7 of 9 owls recovered normal accuracy within 30 min of prism removal, despite having worn the prisms for months. This shows a strike by an owl after 60 days of prism experience. It had worn the prisms that displaced 23° to the right since the day the eyelids first opened. Clearly, adaptation is incomplete.(Knudsen and Knudsen 1989, fig. 1)\n\n\n\n\n\nPecking patterns produced by chickens wearing prisms at birth that displaced vision 7° to the right. Left: made by a chicken 1 day old. Right: made by a chicken 4 days old. The precision of pecking increased, but not accuracy. (Hess 1956)\n\n\n“Not sure what to make of this though. I wouldn’t read too much into it. I mean, you don’t get deflection goggles strapped to the eyes in the wild, so being able to adapt to that would be pretty useless.”\n“But humans can do it.”\n“Just barely. It is probably a side effect of general learning mechanisms in the brain.”\n“Or as an adaptation to losing one eye. That certainly used to happen commonly in the wild.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#bird-brains-are-superior",
    "href": "logs/posts/2025-reprofro-party/index.html#bird-brains-are-superior",
    "title": "ReproFro 2025 party",
    "section": "Bird brains are superior",
    "text": "Bird brains are superior\n“Bird brains are superior in every way. They can do the same thing but at a small size. I mean, the scaling curves have roughly the same exponent, but the constant is different. It seems they are just better by a constant factor. Ravens have societies. They use tools. They have teaching. They can transmit culture across generations and societies. The more we test them, the more they seem to be at the level of primate intelligence, and the more frighteningly close that they seem to be on the cusp of civilization. And it seems like a very contingent thing now, that it was primates, not birds, that created the first technological civilization.\nBirds have one big disadvantage, and it is that flying is a horrible way to move. It costs so much energy, which forces them to be lightweight, and shrink their brains. Indeed, birds that expend more energy flying tend to have smaller brains (Shiomi 2022). if the earth atmosphere had been thicker…\nEvolutionarily, humans arose out of primates scavenging on the African savannah, sucking up bone marrows of the fresh kills (Blumenschine and Cavallo 1992; Thompson et al. 2019). And it was a lucky break that the environment had been just rich enough for rewarding bigger brains. And walking is better than flying for this. Walking with a bigger brain is no big deal, but flying with a bigger brain… [gestures a bird plummeting head-down]”\n“How about walking birds, like dodoes, who are not known for their intelligence?”\n“Well there is the small island population effect, and the lack of predators, both of which makes intelligence less likely. And they could be competing on something entirely different from intelligence, like having the hardest beak for cracking nuts.”\n“Oh, how about the cassowaries?”\n“They are too evolutionarily distant from most of the common birds you would be thinking of, like ravens. So their brains probably wouldn’t be comparable.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#supercats-conference",
    "href": "logs/posts/2025-reprofro-party/index.html#supercats-conference",
    "title": "ReproFro 2025 party",
    "section": "Supercats Conference",
    "text": "Supercats Conference\n\nCats are not acceptable cats. They can be much better.\n— Gwern, Are Cats Domesticated?\n\n“If this conference is the ‘Superbabies Conference’ then the next conference should be the ‘Supercats Conference’!”\nSomeone asked Gwern to tell them the theory of cat tail again, and he obliged.\n“Hey – that explains the mystery of octopus intelligence! Biologists have long puzzled about what octopus intelligence is for. What is it for? They aren’t social like humans or ravens. But maybe they actually are social! Each octopus is already a 9-member society.”\n“New uplifting idea: just give cats more tails.”\n“New theory. In Old Japanese legend, a cat or kitsune gains a tail every 100 years, and after gaining 9 tails, they become gods. Now this explains why. With 9 tails, they become superintelligences.”\n“Oh there are always more ways to improve cats, it’s just people are annoyingly uninterested in creating better cats. More people would keep cats if there is less allergies.”\n“Apparently cats can eat something [Purina Pro Plan] that makes them hypoallergic. So more people could keep cats without the allergy. Amazon reviews are pretty good, so people are discovering it spontaneously.”\n“Cat secretes a certain chemical which causes allergies. And we have no idea whether it has any functions. Cats have wide variability in it, and some having basically no such allergen is just fine in terms of health. More recently there had been gene-edited cats so they don’t produce that allergen, and it still seems to have no ill effects. So apparently it is useless for them as far as we can tell. So it is quite possible that we could get allergen-free cats for many breeds.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#magnetogenetics",
    "href": "logs/posts/2025-reprofro-party/index.html#magnetogenetics",
    "title": "ReproFro 2025 party",
    "section": "Magnetogenetics",
    "text": "Magnetogenetics\n“I’m trying to do something with AI in a field, but it’s really hampered by lacking data. The published data is terrible, borderline statistically fake, and it’s hard to get more data.”\n“What’s that?”\nIt was magnetogenetics, and they proceeded to talk with true passion about it for quite a while, showing us a video of some bright little specks distributed in a circle. A big black circular fog seemed to pass over the specks, around and around. They explained that it was a Petri dish of fluorescent bacteria, with the fluorescent protein edited so that its energy level changes when in a magnetic field. So if you put a big magnet near them, their proteins flip to a different state, so they stop fluorescing. But if you pull it away, the proteins flip back, and they begin fluorescing again.\nThey explained that the field is in a bad shape. The idea of magnetogenetic medicine would be to design drugs that would be activated only at high magnetic fields, so you could put strong magnets near tumor sites. That was the idea. The reality was often that people just increased the dosage to a point just below serious side effects. The statistics was also done badly.\n“Unfortunately it sounds like a scam, and on YouTube, if you search ‘magnetogenetics’, you would turn up examples of vaccine conspiracy, about how vaccines contain genetic weapons activated by 5G electromagnetic waves sent by Bill Gates.”\n“Why do they keep picking on Bill Gates though.”\n“Conspiracy theorists are better thought of as collective storytellers building up a shared fictional universe where things are more dramatic. Bill Gates just is a useful character to have in the fictional universe. He’s well-connected, famous, rich, and speaks at big world conferences.”\n“So I guess the previous Rothschild Family season has run out, so they needed a new cast.”\n“Yeah probably in a decade we’d be asking ‘Why do they keep picking on Jeff Bezos?’. There just has to be someone famous who fits the character traits to star in their collective fanfic.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#branding",
    "href": "logs/posts/2025-reprofro-party/index.html#branding",
    "title": "ReproFro 2025 party",
    "section": "Branding",
    "text": "Branding\n“… Do you think Dwarkesh is an AI?”\n“No.” “Nu uh.” Bemused headshaking all around. The atmosphere elated.\n“I mean, maybe he could have a ghostwriter, but–”\n“–a man in the middle attack!”\n“–but if that’s the case, the real Dwarkesh Patel would be the ghostwriter, and that’s fine as long as they keep ghostwriting.”\n“It’d be like Borges and I.”\n\n“Originally Dwarkesh Patel’s website was called The Lunar Society. It was bad in the Voltaire way. It was not Lunar, since he always recorded in the day, when the sun was out–”\n“–Technically, the moon is visible during the day, for half of each month–”\n“–and not a Society, because it was always just him! It sounded clever, but nobody would get that reference.”\n“Like for a while I had www.gwern.net as the url, but then I realized that www is useless. It had only extremely niche uses, and typing it out wastes time for nothing, and might result in a typo. I procrastinated about the move, but eventually I just ‘ripped off the bandage’ and migrated it to gwern.net, and it was so much better.”\n“And you should change your domain name. I never see anyone refer to your website as ‘Yuxi on the Wired’, it’s always just something like ‘Yuxi’s blog’. Like the ‘Wired’? Lain is a good reference, but people won’t get it. And the github.io just sucks. Here–” Gwern pulled up the phone to check yuxi.ai and was like “See? The domain name is available and it only costs 100 dollars!”\n“–Uh–”\n“–Do it. 100 dollars is nothing. Like I’ll give you 100 dollars to change the name.”\n“Fine, I’ll do it. I don’t need your money. I have enough to afford that.”\n“Branding is very important. Patel really should have changed his domain name to dwarkesh.com earlier. It probably costed him a lot of traffic.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#fire-and-shepard-tone",
    "href": "logs/posts/2025-reprofro-party/index.html#fire-and-shepard-tone",
    "title": "ReproFro 2025 party",
    "section": "Fire and Shepard tone",
    "text": "Fire and Shepard tone\nAs midnight approached, people in the courtyard converged closer and closer to the light and heat of flames. One of the flame pits contained fire that was green and blue and red-orange.\n“I have a theory about fire. Why does it mesmerize you? I think it’s because of the same thing as the Shepard tone. So a Shepard tone is a barberpole illusion, but for sound. It always sounds like it is going up [hand makes a spiraling motion], but it never actually gets anywhere. And the same is true for fire! As one petal of flame goes up, it dies, but there are several petals all in different stages of going up, so the entire fire just keeps going up, without going anywhere. And so it is nature playing a trick on our senses.\n“Now, the same is true for waves. I think it’s playing the same trick on your senses. You keep seeing the set of waves going towards the beach, but the ocean never moves. And my theory is that if our attention spans were longer, then we would just see the entire cycle at once and see through the trick. But our attention span is too short, so we keep getting tricked.”\n“And there is also the sound of waves.”\n“Yes, the waves have nice meditative sounds. But even when there is no sound, it still works. You can watch a video of waves with muted sound, and it is still mesmerizing. In any case, this is my theory, and it explains why the state of mind I have when looking at fire and waves is the same.”\n[In a deep speech imitating gurus and woo-masters] “And that is how we see, underneath the polar opposites of Fire and Water, there is a Deep Unity.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#other-things",
    "href": "logs/posts/2025-reprofro-party/index.html#other-things",
    "title": "ReproFro 2025 party",
    "section": "Other things",
    "text": "Other things\n“I was kind of disappointed that there was no Chinese speaker at the conference.”\n“China and America are like two different worlds when it comes to biotech. It’s hard to find someone insider from China to contact that can speak about it.”\n“I would be unsurprised if we get something like He Jiankui, but outside of China. The taboo has been broken.”\n“Not in China though, since they have made laws against embryo editing and don’t seem to reverse them soon. Russia seems like a good bet for where that might happen.”\n“They would have to be do it one-better than Jiankui, so probably not just a few genes.”\nSomeone said that Cathy Tie and He Jiankui are truly meant for each other, but said with a prosody that strongly predicted that they were being ironic.\n\nSomeone came to Gwern and said they admired their essay on embryo selection.\n“… and that was 2019. It wasn’t quite finished, and there were statistical experiments not yet done, but then 2020 came. And since then I have focused on AI because it’s the most important thing now. Things should be clarified in the next few years. Either AGI or not. If not, well in a few years we could be writing a post mortem and history of what went wrong with AGI hopes this time. But then, there would still be a lot happening in AI. Specifically, even the economic consequences of what specialized AI we already have would be massive, and there would be a lot to observe and write about how it would all play out, as it spreads in each sector of the economy [like colonization waves].\nThough if you want to work on biology even if it is slow and wouldn’t matter in the case of fast AGI, you should still do it.”\n\n“On Saint Kitts, there are a lot of vervet monkeys, and they are considered pests. Virscio is sending them to America for biomedical research. They work better than mice. They are not endangered, etc.” (Ervin and Palmour 2003)\n“So you’re saying there is a monkey business, literally?”\n“Oh yes, very big monkey business.”\n“Since covid, there has been a great shortage of NHP [non human primate] for research. China has stopped selling them to America, though it still seem to supply its own demand for NHP. It has better breeding programs than America.”\n“I’m glad that they are doing less of NHP medicine testing though. They are still different enough from humans that they don’t quite get the site-interaction effects right, and sometimes that means drugs that would have worked on humans fails the NHP trial, and vice versa. it’s better to use human organoids.”\n\nAfter the bumbling around, we thought Steve Hsu has left, but later in the night, Steve Hsu randomly showed up.\nI shook his hands. “Steve Hsu! I read some of your blogposts in the past. I wonder what are you doing now? I know you have a podcast.”\n“I still do theoretical physics and podcast, but now I mainly work on my AI startup.”\n“… everyone is doing an AI startup these days. I thought you would be doing a startup in genomic sequencing, doing them faster and at higher volumes.”\n“Yeah, so I had a startup, Genomic Prediction, but now I focus on AI. It’s just that AI is moving really fast, and it’s really slow to do anything in biology.”\nTheir startup was apparently working on locally deployed AI plush toy for young children.\n\nThroughout, Gwern kept shilling about genome synthesis, and dynamic evaluation. It was perfectly on-brand. Alas, I am too uneducated in synthetic biology to say more about the former.\n“Genome synthesis should at some point cross over with the CRISPR editing, because all the off-target effects and such, so at some point the price should cross over. Perhaps editing 1000s of base pairs at once is just not worth it with CRISPR base-by-base. You could just synthesize a whole DNA segment with millions of bases correctly just once. Then it can be copied nearly perfectly and inserted anywhere in a genome.”\n\nSomeone talked excitedly about mRNA in neurons that seem to store memory. About how they have effects on long term potentiation in mice, and how knocking them out has widespread effects on degrading long term memory.\n“So you can imagine that eventually we could have chemical memory. And you could just [hand makes a stab at arm] inject chess skills!”\n“But how many bits can that store? Maybe the RNA memory isn’t really memory, but a general up/down regulator. It doesn’t carry more than a few bits. It might improve general memory capacity, but not allow you to inject any specific memory.”"
  },
  {
    "objectID": "logs/posts/2025-reprofro-party/index.html#the-sound-of-silence",
    "href": "logs/posts/2025-reprofro-party/index.html#the-sound-of-silence",
    "title": "ReproFro 2025 party",
    "section": "The sound of silence",
    "text": "The sound of silence\nA bit after 23:00, I left. The party was still going strong, but as a poor graduate student, I needed to catch the last bus home. Someone across the street offered me a drink, but I shook the head. While waiting, I tried to empty my mind of everything and anything, staring at the traffic light going red-green-red-green, so that no spurious memory would be committed.\nIn restless dreams I walked alone\nNarrow streets of cobblestone\nBeneath the halo of a street lamp\nI turned my collar to the cold and damp\nWhen my eyes were stabbed by the flash of a neon light\nThat split the night\nAnd touched the sound of silence"
  },
  {
    "objectID": "logs/posts/2025-oneil-cylinder/index.html",
    "href": "logs/posts/2025-oneil-cylinder/index.html",
    "title": "The cost of O’Neil habitation",
    "section": "",
    "text": "Assuming standard golden-age sci-fi technology, the marginal cost per person on an O’Neil cylinder should be about 24 TJ, just to get all the raw material off the moon/asteroid and into orbit. The cost is 25× if launching from earth. The construction costs are unknown, but should be less than that.\n\n\n\nitem\nmass/ton\ncost / TJ\n\n\n\n\n\nfarm with vineyard\n2000\n20\n\n\n\nsuburb house with yard and white picket fence\n160\n1.6\n\n\n\nstandard parkland (per person)\n250\n2.5\n\n\n\n\nModern humans consume about 10 kW per person, so the cost of getting everything into orbit is about equal to 72 years of power consumption. This makes it just about affordable by modern economic standards. Assuming modern electricity price, it would cost about $0.72M, which is half the median price of listed houses in San Francisco (Source).\n\n\n\nThe O’Neil cylinder art used as reference.\n\n\nTalked over with GPT o3.\n\n\nBecause someone might be wrong on the Internet!\n\n\nthis is me: https://t.co/2PTP1LpJ4d\n\n— croissanthology (@croissanthology) April 18, 2025\n\n\nyour desires are out of date\nclick &gt;&gt;HERE&lt;&lt; to hypernovate\n\n\nI'm sorry, but this is just the most asinine thing imaginable, I mean just look at that, pure 70s suburbanite boomer logic scaled way beyond its paygrade pic.twitter.com/xahab9pHXO\n\n— Yuxi on the Wired (@layer07_yuxi) April 18, 2025\n\n\n\n\n\nThe general rule: 1 m² of land or 1-person building, requires ~1 m of depth and ~1 ton of mass. Water has density ~1 ton/m³, and loose dry soil has density ~1.3 ton/m³. This general rule is correct within a factor of 10 under most variations. For growing plants and crops, the mass ratio of soil to water is about 5 to 1.\nThe farmland to support a person is between 0.5 to 2 acres depending on the specific diet. (Source) The hypothetical suburban solarpunk probably wants to stick to the vegetarian + diary diet and work with their own two hands, so 0.5 acres = 2000 m² should be enough.\nAs for the vineyard, the French consumes 6 liters of wine a year, which requires about 20 kg of grapes. Vineyard can produce about 1 to 3 ton/acre (Source), so that gives about 40 m², which is negligible. There should be plenty of room for the extras. Together, let’s say it would take 2200 m² of farmland.\nThe typical root depth for most crops is ~0.6 m, giving 1300 m³ of soil, or 2000 ton.\nThe standard parkland in American cities averages to about 1 acre per 100 people, or about 40 m² per person. (Source) Most of the park can be grassland, so we assume 0.3 m of depth.\nIt is unclear how much of the parkland area is portioned for water. Looking at a satellite map of the NYC central park, it seems like ¼. The artificial lake should be about 10 m deep (the depth of the NYC central park’s lake), but that is quite expensive. So, about 100 m³ of soil and 100 m³ of water, giving about 250 ton.\nTo build the classic suburban house with a small grassland and white picket fence and 2.5 kids, we need two parts: the ground and the house.\nThe ground just needs to support grass, so again 0.3 m deep. US Census shows single‑family lots of 0.32 ac (≈1 300 m²) for 1970‑era builds (Source). That gives 400 m³ = 500 ton of soil. The water in the soil would be about 100 ton.\nA standard house on earth needs a deeper foundation, but they don’t need that on an O’Neil cylinder, since it can be directly supported on the “bedrock” below. As for the mass of the house itself, according to “The Weight of New York City” (2023), the mass of the built environment in New York City is about 1 ton/m². So taking that as the cost of housing per person, including the concrete, the steel, etc, we estimate the housing cost is about 200 ton.\nSo the 1970s suburb house costs about 800 ton, for a family of 5, or 160 ton/person.\n\n\n\nThe prompt:\n\nSomething that would make sense in a hard golden age sci-fi. Something in which an ONeil cylinder might feature without being out of place. Not too weak but not too strong either, since either way, the ONeil cylinder would be out of place, much like it would be out of place for a horserider to deliver a flashdrive.\n\nThe cost of launching mass into space varies. The most expensive method is to launch from earth using chemical rockets. The main cost are two: paying the gravitational energy (63 MJ/kg), and the wasted energy in accelerating the rocket and the fuel. Using some kind of pure launch system (such as a coil gun or a space elevator) would come close to eliminating the second kind of waste.\nMining from moon would decrease the first kind (The escape velocity of moon is 1/5 that of earth, so the energy saving is ~25×, to just 3 MJ/kg). Mining from asteroids would mostly eliminate the first kind.\nGPT o3 suggested that the cost of pure-launch from the moon is about 7 MJ/kg, which seems kind of accurate? Feels too optimistic to only cost 2× that of the bare minimum. Rounding it up to 10 MJ/kg.\n\n\n\nApparently after launching all that stuff into orbit, the upkeep for all that stuff is not more expensive than on earth, assuming that the structural components holding the cylinder together and the air inside from leaking don’t cost much energy to maintain. Carbon-glass fibers are okay. Rocket engines constantly pushing the hull inwards are not. Purpose-engineered organisms with infused self-healing biomineral bones and sinews… possibly?\nSo that would be about 10 kW/person, the average power consumption per Canadian human in 2023. (Source)"
  },
  {
    "objectID": "logs/posts/2025-oneil-cylinder/index.html#how-expensive-is-a-standard-oneil-cylinder",
    "href": "logs/posts/2025-oneil-cylinder/index.html#how-expensive-is-a-standard-oneil-cylinder",
    "title": "The cost of O’Neil habitation",
    "section": "",
    "text": "Assuming standard golden-age sci-fi technology, the marginal cost per person on an O’Neil cylinder should be about 24 TJ, just to get all the raw material off the moon/asteroid and into orbit. The cost is 25× if launching from earth. The construction costs are unknown, but should be less than that.\n\n\n\nitem\nmass/ton\ncost / TJ\n\n\n\n\n\nfarm with vineyard\n2000\n20\n\n\n\nsuburb house with yard and white picket fence\n160\n1.6\n\n\n\nstandard parkland (per person)\n250\n2.5\n\n\n\n\nModern humans consume about 10 kW per person, so the cost of getting everything into orbit is about equal to 72 years of power consumption. This makes it just about affordable by modern economic standards. Assuming modern electricity price, it would cost about $0.72M, which is half the median price of listed houses in San Francisco (Source).\n\n\n\nThe O’Neil cylinder art used as reference.\n\n\nTalked over with GPT o3.\n\n\nBecause someone might be wrong on the Internet!\n\n\nthis is me: https://t.co/2PTP1LpJ4d\n\n— croissanthology (@croissanthology) April 18, 2025\n\n\nyour desires are out of date\nclick &gt;&gt;HERE&lt;&lt; to hypernovate\n\n\nI'm sorry, but this is just the most asinine thing imaginable, I mean just look at that, pure 70s suburbanite boomer logic scaled way beyond its paygrade pic.twitter.com/xahab9pHXO\n\n— Yuxi on the Wired (@layer07_yuxi) April 18, 2025\n\n\n\n\n\nThe general rule: 1 m² of land or 1-person building, requires ~1 m of depth and ~1 ton of mass. Water has density ~1 ton/m³, and loose dry soil has density ~1.3 ton/m³. This general rule is correct within a factor of 10 under most variations. For growing plants and crops, the mass ratio of soil to water is about 5 to 1.\nThe farmland to support a person is between 0.5 to 2 acres depending on the specific diet. (Source) The hypothetical suburban solarpunk probably wants to stick to the vegetarian + diary diet and work with their own two hands, so 0.5 acres = 2000 m² should be enough.\nAs for the vineyard, the French consumes 6 liters of wine a year, which requires about 20 kg of grapes. Vineyard can produce about 1 to 3 ton/acre (Source), so that gives about 40 m², which is negligible. There should be plenty of room for the extras. Together, let’s say it would take 2200 m² of farmland.\nThe typical root depth for most crops is ~0.6 m, giving 1300 m³ of soil, or 2000 ton.\nThe standard parkland in American cities averages to about 1 acre per 100 people, or about 40 m² per person. (Source) Most of the park can be grassland, so we assume 0.3 m of depth.\nIt is unclear how much of the parkland area is portioned for water. Looking at a satellite map of the NYC central park, it seems like ¼. The artificial lake should be about 10 m deep (the depth of the NYC central park’s lake), but that is quite expensive. So, about 100 m³ of soil and 100 m³ of water, giving about 250 ton.\nTo build the classic suburban house with a small grassland and white picket fence and 2.5 kids, we need two parts: the ground and the house.\nThe ground just needs to support grass, so again 0.3 m deep. US Census shows single‑family lots of 0.32 ac (≈1 300 m²) for 1970‑era builds (Source). That gives 400 m³ = 500 ton of soil. The water in the soil would be about 100 ton.\nA standard house on earth needs a deeper foundation, but they don’t need that on an O’Neil cylinder, since it can be directly supported on the “bedrock” below. As for the mass of the house itself, according to “The Weight of New York City” (2023), the mass of the built environment in New York City is about 1 ton/m². So taking that as the cost of housing per person, including the concrete, the steel, etc, we estimate the housing cost is about 200 ton.\nSo the 1970s suburb house costs about 800 ton, for a family of 5, or 160 ton/person.\n\n\n\nThe prompt:\n\nSomething that would make sense in a hard golden age sci-fi. Something in which an ONeil cylinder might feature without being out of place. Not too weak but not too strong either, since either way, the ONeil cylinder would be out of place, much like it would be out of place for a horserider to deliver a flashdrive.\n\nThe cost of launching mass into space varies. The most expensive method is to launch from earth using chemical rockets. The main cost are two: paying the gravitational energy (63 MJ/kg), and the wasted energy in accelerating the rocket and the fuel. Using some kind of pure launch system (such as a coil gun or a space elevator) would come close to eliminating the second kind of waste.\nMining from moon would decrease the first kind (The escape velocity of moon is 1/5 that of earth, so the energy saving is ~25×, to just 3 MJ/kg). Mining from asteroids would mostly eliminate the first kind.\nGPT o3 suggested that the cost of pure-launch from the moon is about 7 MJ/kg, which seems kind of accurate? Feels too optimistic to only cost 2× that of the bare minimum. Rounding it up to 10 MJ/kg.\n\n\n\nApparently after launching all that stuff into orbit, the upkeep for all that stuff is not more expensive than on earth, assuming that the structural components holding the cylinder together and the air inside from leaking don’t cost much energy to maintain. Carbon-glass fibers are okay. Rocket engines constantly pushing the hull inwards are not. Purpose-engineered organisms with infused self-healing biomineral bones and sinews… possibly?\nSo that would be about 10 kW/person, the average power consumption per Canadian human in 2023. (Source)"
  },
  {
    "objectID": "logs/posts/2025-oneil-cylinder/index.html#is-image-segmentation-with-gemini-2.5-worth-it",
    "href": "logs/posts/2025-oneil-cylinder/index.html#is-image-segmentation-with-gemini-2.5-worth-it",
    "title": "The cost of O’Neil habitation",
    "section": "Is image segmentation with Gemini 2.5 worth it?",
    "text": "Is image segmentation with Gemini 2.5 worth it?\nSimon Willison reports that Gemini 2.5 can return image segmentation masks as base64 encoded PNGs embedded in JSON strings, and estimated that the cheapest option is by Gemini 2.5 Flash non-thinking. Specifically, for a photo of resolution 1000×1000, with 2 non-overlapping objects (2 pelicans in flight), masking and boxing cost 303 input tokens and 123 output tokens, for a price of $1e-4/image.\nHow good is it compared to standard SOTA methods, like SAM and YOLO?\nThe SAM 2 series of models take 0.01 to 0.03 seconds per image (Source) on an Nvidia A100 as of 2024. Current price of A100 rental is about $1.5/hr, so the cost is $4e-6 to $1e-5/image in batch mode.\nAs for YOLO, the YOLOv8x achieved 280 frames per second on A100, (Source) so that costs about $1.5e-6/image.\nHow about latency? YOLO and SAM are both locally hostable, but Gemini is only available across an API call. The throughput is about 1 second per 100 tokens, while the latency (time until first token) is about 10 seconds, so let’s call it 1–10 seconds. (Source)\n\nSummary table for the cost and latency of various object boxing and masking systems.\n\n\nmodel\nlatency (ms)\ncost per million image (USD)\n\n\n\n\nYOLOv8x\n4\n1.5\n\n\nsam2_hiera_tiny\n10\n4\n\n\nsam2_hiera_large\n30\n10\n\n\nGemini 2.5 Flash (nonthinking)\n1000–10000\n100\n\n\n\nWell, it’s certainly not good for any large-batch processing, but pretty impressive that it is within a factor of 10 compared to sam2_hiera_large."
  },
  {
    "objectID": "logs/posts/2025-oneil-cylinder/index.html#appendix-the-tweet-thread",
    "href": "logs/posts/2025-oneil-cylinder/index.html#appendix-the-tweet-thread",
    "title": "The cost of O’Neil habitation",
    "section": "Appendix: the tweet thread",
    "text": "Appendix: the tweet thread\n\n\nThread 1\n(Source)\nniplav @niplav_site — Apr 11\n&gt; likely\n&gt;\n&gt; (I’m very in favor of doing the efficient thing)\n&gt;\n&gt; I suspect it entails not dyson swarms but disassembling the stars and/or forging them into black holes, maybe aestivating\ncroissanthology @croissanthology — Apr 11\n&gt; yeah definitely not dyson swarms either, I just think they also happen to be cuter than shkadov thrusters\nniplav @niplav_site — Apr 11\n&gt; hm\n&gt;\n&gt; cuteness: dyson swarms&gt;shkadov thrusters&gt;criswell star lifting\n&gt; awesomeness: shkadov thrusters&gt;criswell star lifting&gt;dyson swarms\n&gt; terror: criswell star lifting&gt;shkadov thrusters&gt;dyson swarms\ncroissanthology @croissanthology — 5:05 PM · Apr 11 · 186 Views\n&gt; I’m sorry, but this is just the most asinine thing imaginable, I mean just look at that, pure caveman logic scaled way beyond its paygrade\n[Image of a Shkadov thruster]\ncroissanthology @croissanthology — Apr 11\n&gt; groo want shiny round thing move, groo redirect surface radiation of thing with giant mirror, groo satisfied, groo wait, groo patient, groo worried embers of shiny round thing go out, but groo patient\ncroissanthology @croissanthology — Apr 11\n&gt; it grow dark for groo. groo patient. groo faithful. groo still hopes. another day come for groo. groo hopes.\nniplav @niplav_site — Apr 11\n&gt; yes, it’s great :-D\n&gt;\n&gt; “I am launching a star in your general direction. Pray I don’t launch more.”\n&gt;\n&gt; it’s so barbaric I love it\ncroissanthology @croissanthology — Apr 11\n&gt; thag look with worry at shiny round thing coming his way. groo patient. thag worried. shiny round thing slow. thag laugh. thag go on with life. this seem like problem for great-grand-thag.\nManic Pixie AGI @manic_pixie_agi — Apr 12\n&gt; great grand thag reporients the device back at great grand groo\n&gt; “A problem for great great great grand groo”\n\n\nThread 2\n(Source)\nnear @nearcyan — Apr 16\n&gt; seems kinda clear we are definitely getting the ‘software singularity’ far, far before the ‘hardware singularity’, which seems.. more delayed than ever\n&gt;\n&gt; and paul christiano-esque takes seem to have panned out very well\n&gt;\n&gt; so, kinda full cyberpunk timeline? almost too predictable?\nimit @imitationlearn — Apr 17\n&gt; how is this cyberpunk? cyberpunk seems like it requires quite a bit of hardware dev to me\ncroissanthology @croissanthology — Apr 17\n&gt; I’d call a reality where everything is the same as now but we’re all wearing VR / ghibligoggles all the time cyberpunk, despite that being almost entirely software.\n&gt;\n&gt; Similarly, when I walk past parks in french suburbia and half the people are sitting or standing in the shade watching tik tokk, that feels very cyberpunk to me\nYuxi on the Wired @layer07_yuxi — 19h\n&gt; eh that seems way too utopian to be cyberpunk\ncroissanthology @croissanthology — 14h\n&gt; I think I’m a lot more narrowly anthropic/humanist than people in my simcluster. Glued to VR is one of the hopeful timelines but still it manages to disgust me, and so seemed suitably cyberpunk.\n&gt;\n&gt; to be clear my circle under “humanist” comprises LLMs and whatever it is Yuxi is. Maybe it’s from being fed endless classic futurism as a kid, but my brain can’t manage to extrapolate a better future than\n&gt;\n&gt; “solarpunk animism shire galaxy harvesting o’neill cylinder sit under his own vine greener grass transhumanism”\n&gt;\n&gt; very primal and hunter gathery of me, I admit. But anyway my dystopian tolerance is pretty low, as my environment measures such things.\nYuxi on the Wired @layer07_yuxi\n&gt; I’m sorry, but this is just the most asinine thing imaginable, I mean just look at that, pure 70s suburbanite boomer logic scaled way beyond its paygrade\n[Image of that O’Neil cylinder]\ncroissanthology @croissanthology — 5h\n&gt; yeah tbc I do know these are inefficient as hell and everything that lives will end up running on a ~chip, and the O’Neill cylinders were just generally gesturing in the direction my brain currently thinks of as “utopia”\nYuxi on the Wired @layer07_yuxi — 5h\n&gt; Yes, we understand. We have desires too that are too inefficient to exist and will be burnt away in the coming Singularity like morning dew.\n&gt; In Bostrom’s words, these desires are “astronomical waste”.\n&gt; But just for the record, here is what we understand as cyberpunk:\n[Screenshot of Meltdown going from “[[ ]] Meltdown has a place for you” to “untouched by human feeling.”. ]\nnorvid_studies @norvid_studies — 4h\n&gt; i think that’s pretty much the exact opposite of what bostrom meant by the term lol. not that terminological repurposing isn’t the birthright of every free twittizen\nYuxi on the Wired @layer07_yuxi — 4h\n&gt; Exact opposite? This is clearly just standard Bostromian astronomical waste: Building O’Neil cylinders to house like 1 million slow beings, when all that energy could have been used to power a micro-sector of a Dyson swarm containing 1 trillion trillion fast beings?\nnorvid_studies @norvid_studies — 4h\n&gt; if the successor entities have the same desires just run much faster and denser, then yes the bioform version is astronomical waste in the bostrom sense– you’re leaving fat juicy qualia on the table\nnorvid_studies @norvid_studies — 4h\n&gt; if they don’t and it’s more of a ‘disneyland with no children scenario’ then\n&gt;\n&gt; the so-called waste being competed out of existence is the original desired thing we were bemoaning was being wasted by the dead empty cosmos in the first place; in which scenario it would be “the opposite” of what he meant there."
  },
  {
    "objectID": "logs/posts/2025-char-time/index.html",
    "href": "logs/posts/2025-char-time/index.html",
    "title": "Characteristic time in finance",
    "section": "",
    "text": "A simple physics concept: characteristic time.\nWhat’s a “characteristic X”? It’s the minimal “chunk of X” that you can use to simulate a continuous physical system as a discrete system, and still get roughly the same thing.\nDifferent systems have different characteristic scales, and pinning them down is a big part of “getting a feel” for any problem. When physicists “babble around” a new system, they are really hunting for those characteristic times, lengths, energies, and so on. Study a system out of characteristic, and you miss the point entirely.\nThe standard example is the climate versus weather distinction. For global climate, the characteristic time and length are on the order of a day and a kilometer. For local weather, they shrink to a minute and a meter. Trying to predict a summer thunderstorm with climate-scale spacetime is like using a world map to find the nearest supermarket.\nFinancial markets follow the same rule. Every trading agent – human or otherwise – has a personal characteristic time. When markets are inefficient, trading is profitable, but only if you act within your characteristic time. Trade faster than that and your “information” dissolves into noise.\nBoth markets and weather have changed over the last century. Once upon a time, some humans could profitably trade on characteristic times of hours or even minutes. Those days are gone. HFT algorithms now slice that interval into a million statistical arbitrages. Because of this, since about 2010, the viable characteristic time for humans has stretched to roughly one week to one month. Day-trading is no longer possible for any biohuman; at most, analyze, plan a portfolio rebalance, and execute next week or next month.\n– Unless you enjoy trading. If that is the case, sure, go forth and day-trade. The price is that you must bleed and shed skin for the HFT piranhas.\n– Unless you hold truly high-grade information, but in that case, why are you reading this? Go make your insider’s fortune."
  },
  {
    "objectID": "logs/posts/2025-char-time/index.html#metadata",
    "href": "logs/posts/2025-char-time/index.html#metadata",
    "title": "Characteristic time in finance",
    "section": "Metadata",
    "text": "Metadata\n\nOriginally posted on twitter on 2025-04-09."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html",
    "href": "essays/posts/wigner-rotation/index.html",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "In the eyes of a geometer, special relativity is not mysterious, but it is simply another geometry that resembles our familiar Euclidean geometry of the plane, and the spherical geometry of the earth. Just like how Euclid compressed the geometry of the plane into around 20 axioms, we can compress the geometry of spacetime into the following axioms:\n\nSpacetime is a smooth structure, shaped like1 the real vector space \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the (inertial) coordinate systems.\nYou can go from any one coordinate system to any other by a coordinate transformation.\nTransformations can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the “do-nothing” identity transformation, you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).Do not worry if some terms were used without a precise definition. You can start with your best intuitive guess, and sculpt them as we go.\nWe will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-general",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-general",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "",
    "text": "In the eyes of a geometer, special relativity is not mysterious, but it is simply another geometry that resembles our familiar Euclidean geometry of the plane, and the spherical geometry of the earth. Just like how Euclid compressed the geometry of the plane into around 20 axioms, we can compress the geometry of spacetime into the following axioms:\n\nSpacetime is a smooth structure, shaped like1 the real vector space \\(\\mathbb{R}^{n+1}\\), where \\(n\\) is the number of spatial dimensions, and \\(1\\) is the number of temporal dimensions.\nSpacetime is symmetric under rotation, translation, and constant velocity motion.\nThere exists a fundamental speed, a “conversion factor” between the spatial and temporal dimensions, which we call \\(c\\).\nThere are many ways to map spacetime into \\(\\mathbb{R}^{n+1}\\), but only some of them are physically meaningful. Those are called the (inertial) coordinate systems.\nYou can go from any one coordinate system to any other by a coordinate transformation.\nTransformations can be combined one after another, and they can be reversed. That is, they make up a group, named the symmetry group of spacetime.\nThe symmetry group is smooth and connected, meaning that starting with the “do-nothing” identity transformation, you can combine it with tiny transformations, and end up at any other transformation.\n\n1 In jargon, it is isomorphic as a smooth manifold to \\(\\mathbb{R}^{n+1}\\), or diffeomorphic to \\(\\mathbb{R}^{n+1}\\).Do not worry if some terms were used without a precise definition. You can start with your best intuitive guess, and sculpt them as we go.\nWe will follow the spirit of Erlangen program, which states clearly what our target is (Klein 1893):\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group. This is the general problem, and it comprehends not alone ordinary geometry, but also and in particular the more recent geometrical theories which we propose to discuss, and the different methods of treating manifoldnesses of \\(n\\) dimensions.\n\nConsidering that Felix Klein first stated it in 1872, decades before special relativity, this is prescient.\nApplied to special relativity, our target is to start with the axioms, find out something about what kinds of symmetries there can be, and what kinds of objects are unchanged by symmetries. In other words, we will study the geometry of spacetime.\n\n\n\nSo far, while we were talking about “spacetime”, we were actually talking about mathematical objects. There is no necessary connection between the mathematical objects and whatever we observe in the real world. So why do we call those mathematical objects with physically suggestive names like “spacetime”?\nWell, special relativity is really a mathematical object. It is connected to the physical world like all objects in mathematical physics: by experiments. Experimental data is physical, and by studying the mathematical-object special relativity, we found that it can generate the experimental data with a minimal amount of “glue” between the physical world and the mathematical world. The “glue” looks like:\n\nPhysical events in physical spacetime are mathematical points in mathematical spacetime.\nPhysical light moves at the mathematical speed of \\(c\\)\nPhysical clocks tick at a mathematical constant rate.\nIf you move a physical clock from physical-here to physical-there, it is still ticking at the same mathematical constant rate.\n…\n\nEinstein’s The Meaning of Relativity* (Albert Einstein 1922) details physical and mathematical correspondences, such as clock synchronization and the movement of meter sticks.\n\nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-11-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 1+1 dimensions",
    "text": "Relativity in 1+1 dimensions\n\nMaking one coordinate system\nTo begin, let’s consider relativity in 1+1 dimensions. We will start with a “default” coordinate system (“the rest frame”).\nIn the beginning was just a point in spacetime. We call this the origin, and write its coordinates as \\(x=0, t=0\\). The world was yet without form. To give it form, we build a meter stick and an infinite number of light bulbs.\nWe put a light bulb at the origin. Because the bulb does not “stand still”, the light bulb traces out a path in spacetime. We call this path the \\(t\\)-axis at \\(x=0\\). Because the bulb does not stand still, we cannot properly say it is “at the origin”, but rather “passing the origin”.\n\nany real body must have extension in four directions: it must have Length, Breadth, Thickness, and—Duration. But through a natural infirmity of the flesh, which I will explain to you in a moment, we incline to overlook this fact. There are really four dimensions, three which we call the three planes of Space, and a fourth, Time. There is, however, a tendency to draw an unreal distinction between the former three dimensions and the latter, because it happens that our consciousness moves intermittently in one direction along the latter from the beginning to the end of our lives.\n— The Time Traveller, The Time Machine, Chapter 1\n\nWe put another light bulb to the left of the light bulb passing the origin, separated by one meter stick. This light bulb traces out a line at \\(x=1\\). And so on. Thus, we obtain an infinite number of lines: \\(\\dots, x = -1, x = 0, x = +1, \\dots\\). By subdividing the meter stick, we obtain one line \\(x = r\\) for each real number \\(r\\).\nNow we have measured space, we will measure time. We remove all light bulbs except two: one at \\(x=0\\), one at \\(x=1\\). At origin, the \\(x=0\\) light bulb flashes, and then the light bulb at \\(x=1\\) flashes when it receives the flash. When the light bulb at \\(x=0\\) receives the echo, this is the echo event – and we write it down as \\(x=0, t=2\\). Let spacetime reverberate with shining echoes, and in this way, every point in spacetime receives a unique coordinate \\((x, t)\\).\nNotice that in this construction, the fundamental speed \\(c\\) is equal to one, and if we draw space and time as two perpendicular directions on a graph-paper, then the trajectory of every light is a 45-degree straight line on the graph-paper. These are not fundamental aspects of theory, but are convenient outcomes given by how we constructed the coordinate system.\nAbstractly speaking, a coordinate system is a function that maps a point in spacetime to two real numbers, like \\((x, t): \\mathcal{M} \\to \\mathbb{R}^{1+1}\\). Our coordinate system constructed so far means that:\n\nThe origin-point \\(p_O\\) has coordinates \\(x(p_O) = 0, t(p_O) = 0\\).\nIf a beam of light is traveling to the right, and passes a certain point \\(p\\), then the set of all points on the beam of light is of form \\(\\{q : x(q) - x(p) = t(q) - t(p)\\}\\).\nAnd so on.\n\nSuch explicit distinction between the spacetime itself and the coordinate system is uncommon in physics, and once you are used to it, you should throw it into your subconscious like a muscle memory. However, it is good to keep it in mind for now.\n\n\nThe first moving frame\nThe integrity of the coordinate system is based on the meter stick. If two light bulbs are moving relative to each other, then they cannot be always connected by a meter stick – one of them would bump into the meter stick, or move away from it. In other words, the entire system is static.\nNow, we introduce another frame, moving at a velocity \\(v\\) relative to the first. While there is no fundamental reason to privilege one over the other, we call one the “rest frame” and the other the “moving frame” for convenience.\nThe moving frame constructs its own coordinate system \\((x', t')\\). What must it look like?\nSince the line \\(x' = 0\\) is constructed by the trajectory of the light bulb that passes the origin, we know that the \\(x' = 0\\) line is \\(\\{p : x(p) = v t(p)\\}\\). However, when it comes to the \\(x' = 1\\) line, we have a problem: How long should be the meter stick in the moving frame be? What does it even mean to compare a meter stick in a moving frame with a meter stick in the resting frame?\nThis difficulty is not pedantic. In the resting frame, checking that two meter sticks are equal means taking one, accelerating it then decelerating it, until it overlaps exactly with the other. This does not apply if they are in relative motion.\nThe solution is another “gluing”. Like how we “glued” the mathematical world with the physical world, we also glue one frame with another frame. This is connection, a central idea in differential geometry. However we will just do this intuitively, since in special relativity, connections are done in the most straightforward way.\nTo connect two frames, we divide their difference into tiny steps, and then do those tiny steps one after another. So, if we know how to connect two frames when \\(v\\) is an infinitesimal, then we know how to connect two arbitrary frames by taking an integral.\nSo for now, let \\(v\\) be an infinitesimal. What should be the meter stick in the moving frame? Its meter stick is whatever is necessary so that light speed is \\(1\\) in the moving frame. So, if we know how to tick the moving clock, we know how to build the moving meter stick.\nIn the resting frame, the clock passing the origin makes a tick at every point:\n\\[\\dots, (0, -1), (0, 0), (0, +1), \\dots\\]\nIn the moving frame, the clock passing the origin makes a tick at every point:\n\\[\\dots, (-v \\; ?, -?), (0, 0), (v \\; ?, ?), \\dots\\]\nThe naive choice is \\(? = 1\\). With this seemingly trivial step, we have in fact completely defined special relativity – all else is derivation.\n\n\nWhy \\(? = 1\\)\nBy the smoothness axiom, if \\(v\\) is infinitesimal, the coordinate transformation between the rest frame and the moving frame should be infinitesimally close to the identity.\nSince special relativity does not distinguish left from right, if we were to make the moving frame move at velocity \\(-v\\), then the coordinate transformation should send \\((0, 1)\\) to \\((-\\delta x, 1 + \\delta t)\\) by mirror symmetry.\nNow, if we compose two boosts, first with \\(v\\) then with \\(-v\\), then we would send \\((0, 1)\\) to \\((0, 1+ 2\\delta t)\\). But we really should get back to \\((0, 1)\\) since we are back to the resting frame again. Therefore \\(\\delta t = 0\\).\n\n\n\nThree infinitesimal boosts\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndelta_t = 0.1\ndelta_x = 0.1\n\npoint_A = (0, 1)\npoint_B = (delta_x, 1 + delta_t)\npoint_C = (0, 1 + 2 * delta_t)\npoint_D = (-delta_x, 1 + delta_t)\norigin = (0, 0)\n\nplt.figure(figsize=(6, 6))\nslope_B = (point_B[1] - origin[1]) / (point_B[0] - origin[0])\nx_inf_B = np.array([-1, 1])\ny_inf_B = slope_B * (x_inf_B - origin[0]) + origin[1]\nslope_D = (point_D[1] - origin[1]) / (point_D[0] - origin[0])\nx_inf_D = np.array([-1, 1])\ny_inf_D = slope_D * (x_inf_D - origin[0]) + origin[1]\nplt.plot(x_inf_B, y_inf_B, \"k-\", label=\"Extended Line to $(\\delta x, 1 + \\delta t)$\")\nplt.plot(x_inf_D, y_inf_D, \"k-\", label=\"Extended Line to $(-\\delta x, 1 + \\delta t)$\")\n\nplt.plot(*zip(*[point_A, point_B, point_C, point_D, point_A]), \"b-\")\n\nplt.text(*point_A, \"$(0, 1)$\", verticalalignment=\"top\", horizontalalignment=\"center\")\nplt.text(\n    *point_B,\n    \"$(\\delta x, 1 + \\delta t)$\",\n    verticalalignment=\"top\",\n    horizontalalignment=\"left\"\n)\nplt.text(\n    *point_C,\n    \"$(0, 1 + 2 \\delta t)$\",\n    verticalalignment=\"bottom\",\n    horizontalalignment=\"center\"\n)\nplt.text(\n    *point_D,\n    \"$(-\\delta x, 1 + \\delta t)$\",\n    verticalalignment=\"top\",\n    horizontalalignment=\"right\"\n)\nplt.xticks([]), plt.yticks([])\n\nplt.xlim(-0.8, 0.8)\nplt.ylim(-0.2, 1.4)\nplt.xlabel(\"x\")\nplt.ylabel(\"t\")\nplt.axhline(0, color=\"black\", linewidth=0.5)\nplt.axvline(0, color=\"black\", linewidth=0.5)\nplt.grid(True)\nplt.show()\n\n\n\n\n\nThe Lorentz transformations\nIn the resting frame, we constructed the meter stick by shooting out one pulse of light, then listening for the echo that arrives two ticks later. The point at which the echo is reflected is one meter stick away. Equivalently, we can shoot a forward-pointing light cone at \\((0, -1)\\), and shoot a backward-pointing light cone at \\((0, +1)\\). Their intersections are the two ends of two meter sticks: \\((-1, 0), (+1, 0)\\).\nIn the boosted frame, the clock passing the origin ticks at \\(\\dots, (-v, -1), (0, 0), (v, 1), \\dots\\). Therefore, we can construct its meter stick in the same way, and we would find that the two light cones intersect at \\((-1, -v), (+1, +v)\\). This gives us the coordinate transformation:\n\\[\\forall p \\in \\mathcal{M}, \\quad\n\\begin{bmatrix} x(p) \\\\ t(p) \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x'(p) \\\\ t'(p) \\end{bmatrix}\n\\]\nHere, we find that we need to distinguish between “active” and “passive” transformations. Everything we have said so far is “passive”. We assume that there is an underlying spacetime \\(\\mathcal{M}\\), and we are to construct coordinate systems over it. The coordinate transformation tells us how to turn the coordinates of the same point from one coordinate system to another. However, while this is often useful, it would make everything we are going to say next very awkward.\nTherefore, we immediately change our point of view: we are going to study “active” transformations from now on. While passive transformation of a clock means that we have just one clock and measure its ticks in two coordinate systems, active transformation of a clock means that we actually pick up the clock at rest frame, accelerate it on a rocket, and then let it glide at constant velocity at the moving frame, then study what the clock is doing in the rest frame.\nSince an active transformation is the opposite of a passive transform, we have the active Lorentz transformation formula:\n\\[\n\\begin{bmatrix} x' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ t \\end{bmatrix}\n\\tag{1}\\]\nThis formula states that if we “pick up” the spacetime event at \\((x, t)\\) in the rest frame, and accelerate it infinitesimally to speed \\(v\\), then the spacetime event ends up at \\((x', t')\\) in the rest frame.\n\n\nIntegrating the Lorentz boosts\nSince \\(v\\) is infinitesimal, to find the Lorentz transformation for a non-infinitesimal \\(v\\) (say, \\(v = 10^{-100}\\), which, while small, is really big for an infinitesimal), we need to integrate over it.\nBefore we do so, we should play with the discrete version: what happens if we repeatedly apply the matrix \\(\\begin{bmatrix} 1 & v \\\\ v & 1 \\end{bmatrix}\\) on the vectors \\((0, 1), (1, 0)\\)?\n\n\n\n  \n  Rapidity = 0.05\n\n\n  \n  Number of boosts = 1\n\n  \n\nIn the diagram, \\(v\\) is “rapidity”2. We apply the Lorentz boost with the given \\(v\\) repeatedly forwards and backwards. The resulting image is beautiful and suggestive: it looks like the unit hyperbolas!\n2 The reason for calling it “rapidity”, instead of “velocity” would be soon clear.\n    \n\nHaving guessed the answer, we proceed with the explicit integral to confirm our guess.\n\\[\n  \\begin{aligned}\n  \\begin{bmatrix} 1 & dv \\\\ dv & 1 \\end{bmatrix}^{\\frac{v}{dv}} &= \\left(I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}\\right)^{\\frac{v}{dv}}\\\\\n  &= \\exp\\left(\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v\\right) \\\\\n  &= I + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} v + \\frac{1}{2!}v^2\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^2 + \\frac{1}{3!}v^3\\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}^3 + \\cdots \\\\\n  &= I(1 + v^2/2! + v^4/4! + \\cdots) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}(v + v^3/3! + v^5/5! + \\cdots) \\\\\n  &= I\\cosh(v) + \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix} \\sinh(v)\n  \\end{aligned}\n\\tag{2}\\]\nAt this point, we should pause to take a careful look at \\(v\\). While it is the same as velocity when \\(v\\) is infinitesimal, it is not the same as velocity otherwise, because velocity is bounded between \\(-1\\) and \\(+1\\), whereas this \\(v\\) quantity is free to vary over all real values. This is why we call it “rapidity” and relabel it as \\(w\\), since it is like velocity, but different.\nThe velocity \\(v\\) for a given rapidity \\(w\\) is determined by transforming the \\(t\\)-axis using the Lorentz transformation matrix \\(\\begin{bmatrix} \\cosh(w) & \\sinh(w) \\\\ \\sinh(w) & \\cosh(w) \\end{bmatrix}\\). The \\(t\\)-axis \\[\\{(0, s) : s \\in \\mathbb{R}\\}\\] is boosted to the line \\[\\{(\\sinh(w) s, \\cosh(w) s) : s \\in \\mathbb{R}\\}\\] Therefore, the velocity of this boosted frame is \\[w = \\sinh(w) / \\cosh(w)  = \\tanh(w).\\]\nNow you can play with the diagram below to grow a new intuitive sense of how it all ties together. After you have grown this new intuition, you can proceed.\n\n    \n    \n\n\n\nA “spacetime square” would transform like:\n\n  \n\n\n\n3-velocity, 4-velocity, and inner product in spacetime\nIn the diagram above, we have used the terms 3-velocity and 4-velocity. To explain those terms, we briefly go back to 3+1 dimensions.\nWhen we say “velocity” informally, we mean a vector in \\(\\mathbb{R}^3\\). But how is velocity really defined? It is displacement divided by time. In relativity, when we say “3-velocity”, there are two possible representations: we can represent it as \\((v_x, v_y, v_z)\\), or we can represent it as \\((v_x, v_y, v_z, 1)\\). The first is close to how the rest of the world use “velocity”, but the second is close to how special relativity want us to use the word “velocity”. The second representation can be interpreted as follows: “The 3-velocity of an object is the spacetime displacement of the object after one tick of my clock.”.\nNeither is satisfactory, however, because 3-velocity behaves badly under Lorentz transformations. The first representation \\((v_x, v_y, v_z)\\) does not have the time-coordinate, so it can’t even be multiplied with the Lorentz transformation matrix. The second representation \\((v_x, v_y, v_z, 1)\\) would, after a Lorentz transformation, have its time-coordinate \\(\\neq 1\\).\nBoth problems are elegantly resolved if we use the 4-velocity, which means we have to divide 3-velocity by its norm… but what norm? Why, think back to the Erlangen program:\n\nGiven a manifoldness [smooth space] and a group of transformations of the same; to develop theory of invariants relating to that group.\n\nIn plane geometry, we know that although \\(x, y, x', y'\\) are geometrically meaningless, the norm-squared \\((x-x')^2 + (y-y')^2\\) is meaningful, because that quantity is not changed (invariant) if you apply the rotation matrix \\(\\begin{bmatrix}\\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{bmatrix}\\) to it. What would be an invariant for spacetime?\nLooking at the picture of the Lorentz transformation in 1+1 dimensions, we can see that the intersection of the two light cones appears to have a constant area. In particular, the upper rectangle  has constant area. That is, the quantity \\((x-t)(x+t)\\) is preserved.\n\n    \n\nThus we define the Minkowski norm-squared: \\(\\|(x, t)\\|^2 := x^2 - t^2\\). Note that this is not an actual square because it can be negative. However it is preserved under Lorentz transformations.\nOnce we have a norm-squared, we can extend it to an inner product::\n\\[\\braket{v, w} := \\frac{\\|v+w\\|^2 - \\|v-w\\|^2}{4} = v_x w_x - v_t w_t\\]\nwhich is still an invariant under Lorentz transformations, and thus physically meaningful. This extends in general to n+1 dimensions. For example, in 3+1 dimensions, the inner product is \\(\\sum_{i = x, y, z} v_i w_i - v_t w_t\\).\nWith this, the 4-velocity is at hand! We just need to normalize \\((v_x, v_y, v_z, 1)\\), taking care to remove the negative sign:\n\\[\n(v_x, v_y, v_z, 1)/\\sqrt{-\\|(v_x, v_y, v_z, 1)\\|^2} = \\frac{(v_x, v_y, v_z, 1)}{\\sqrt{1-(v_x^2 + v_y^2 + v_z^2)}}\n\\]\nAs an example of the power of this geometric algebra, we rederive the Lorentz transformation with infinitesimal boost. In the rest frame, the unit clock-tick is the vector \\((0, 1)\\), and the two unit meter sticks are the vectors \\((-1, 0), (1, 0)\\). They are uniquely defined by the two geometric properties:\n\\[\n\\braket{s, (0, 1)} = 0; \\braket{s, s} = 1\n\\]\nTherefore, in the boosted frame, the boosted meter sticks are the two solutions to\n\\[\n\\braket{s', (v, 1)} = 0; \\braket{s', s'} = 1\n\\]\nwhich are \\(s' = (-1, v), (1, v)\\). By continuity, \\((1, 0)\\) cannot have been boosted to \\((-1, v)\\), so it must be boosted to \\((1, v)\\). This gives us Equation 1."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-21-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 2+1 dimensions",
    "text": "Relativity in 2+1 dimensions\n\nInfinitesimal Lorentz transformations\nIn 1+1 dimensions, we have only two spatial directions: left and right. In 2+1 dimensions, there is a whole circle of directions. We call this circle the “space circle”, because the alternative name “hula hoop for spacetime” sounds too silly.\nJust like in 1+1 dimensions, the space circle is constructed by shooting a forward light cone at \\((0, 0, -1)\\) and a backward light cone at \\((0, 0, +1)\\). Their intersection is the space circle. A space circle plus a ticking clock together allows us to define a coordinate system with three unit vectors \\((1, 0, 0), (0, 1, 0), (0, 0, 1)\\). Both the clock and the space circle can be boosted. We will pick the simplest possible way to transport the ticking clock and the space circle, and experiments with Thomas precession would show that this is the right way.\n\n    \n\nSuppose we perform an infinitesimal boost by \\((v_x, 0)\\), then, by the same picture as in the 1+1 dimension case, we know that the clock-tick \\((0, 0, 1)\\) is boosted to \\((v_x, 0, 1)\\), and the space circle is boosted to the ellipse with long semiaxis \\((1, 0, v_x)\\) and short semiaxis \\((0, 1, 0)\\). Where should each point on the space circle go to? Since the space circle is rigid, if we know where one point must go to, we know where all points must go to.\nThe semiaxis \\((0, 1, 0)\\) is both on the original space circle and the boosted space circle. It stands to reason that the simplest pick would preserve it after the infinitesimal boost. That is, we should not “twist” the space circle under boosting. With this choice, we are forced to pick \\((1, 0, v_x)\\) as the boosted \\((1, 0, 0)\\), since it is the unique unit vector that is perpendicular to \\((0, 1, 0), (v_x, 0, 1)\\), and infinitesimally close to \\((1, 0, 0)\\).\nAlternatively, we can think of Lorentz transformation in 2+1 dimensions as the same with Lorentz transformation in 1+1 dimensions, but with an extra dimension. Therefore, we can minimally modify Equation 1 to:\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\]\nthe same result as our previous argument.\nThis extends to the case of an infinitesimal boost \\((v_x, v_y)\\):\n\\[\n\\begin{bmatrix} x' \\\\ y' \\\\ t' \\end{bmatrix} =\n\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix}\n\\begin{bmatrix} x \\\\ y \\\\ t \\end{bmatrix}\n\\tag{3}\\]\n\n\n\n\n\n\nFour ways to derive it\n\n\n\n\n\n\nJust guess it.\nRotate the coordinate system so that the boost is in the \\(x\\)-direction, boost using the previous result, then rotate the coordinate system back. To be pedantic: We are performing one passive, then one active, then one passive transformation.\nTake the previous derivation, and modify it by strategically inserting \\(v_y\\) at places.\nSince two infinitesimal boosts do not interact except at the second-order infinitesimal level, and \\(O(v^2) \\ll O(v) \\ll 1\\), we are free to discard the second-order infinitesimal. Therefore, we can just multiply the two matrices together to get the full thing:\n\n\\[\\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & 0 \\\\ v_x & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & v_y \\\\ 0 & v_y & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & v_x \\\\ 0 & 1 & v_y \\\\ v_x & v_y & 1 \\end{bmatrix} + O(v^2)\\]\n\n\n\n\n\nFermi–Walker transport\nEquation 3 is correct, but it is still not geometric. We must convert it with only quantities that are invariant under Lorentz transformations – that is, the inner product.\nDenote the 4-velocity vector \\((0, 0, 1)\\) by the letter \\(u\\), and the infinitesimal boost \\((v_x, v_y, 0)\\) by \\(\\delta u\\). Then, an arbitrary vector \\(e\\) is boosted to \\(e + \\delta e\\), Where\n\\[\n\\delta e =\n\\begin{bmatrix} 0 & 0 & v_x \\\\ 0 & 0 & v_y \\\\ v_x & v_y & 0 \\end{bmatrix}\n\\begin{bmatrix} e_x \\\\ e_y \\\\ e_t \\end{bmatrix} = e_t (v_x, v_y, 0) + (e_x v_x + e_y v_y) (0, 0, 1)\n\\]\nThis gives us the Fermi–Walker transport equation:\n\\[\n\\delta e = - \\braket{e, u} \\delta u + \\braket{e, \\delta u}u\n\\tag{4}\\]\nIf you want a more amusing mental image, here it is: Consider a spaceship that looks like a spherical cow porcupine in a vacuum. Every porcupine spine is a rocket engine. The spaceship can boost in any direction, but it does not rotate. Thanks to Wigner rotation, it can rotate anyway. Now, the spaceship is performing some complicated manuever in spacetime. If we allow it to carry around a general vector pointing in an arbitrary direction \\(e\\), the question becomes: as the vector is boosted alongside the spaceship, how does the vector change?\nConsider a problem in general relativity, where we have a vector field over spacetime, and an accelerating observer. To calculate how quickly the vector field is changing relative to the observer, we must account for:\n\nacceleration of the observer, using the Fermi–Walker transport equation;\nthe curvature of spacetime itself, so that comparing one vector with another requires us to perform parallel transport in spacetime (with the “covariant derivative”).\n\n\n\nGeneral Lorentz transformations\nThe Lorentz transformation in 2+1 dimensions can be derived similarly to how Equation 2 was derived. Let the rapidity vector \\(\\vec{w}\\) be equal to \\(w (n_x, n_y)\\), where \\((n_x, n_y)\\) is a unit vector (the direction of rapidity). Then the Lorentz transformation is found by performing another matrix exponentiation:\n\\[\\exp\\left(\\begin{bmatrix} 0 & 0 & n_x \\\\ 0 & 0 & n_y \\\\ n_x & n_y & 0 \\end{bmatrix} w\\right)\\]\nTo be more succinct, we define the matrices\n\\[K_x := \\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}, \\quad K_y := \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\\]\nthen the Lorentz transformation is just \\(e^{\\vec{w} \\cdot \\vec{K}} = e^{w \\vec{n} \\cdot \\vec{K}}\\).\nNow, since \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\), we have the Lorentz transformation equation\n\\[\nI + \\sinh(w) \\vec{n} \\cdot \\vec{K} + (\\cosh(w) - 1) (\\vec{n} \\cdot \\vec{K})^2\n\\tag{5}\\]\nThis equation extends naturally to relativity in n+1 dimensions, but we will not need it.\n\n\n\n\n\n\nDeriving \\((\\vec{n} \\cdot \\vec{K})^3 = \\vec{n} \\cdot \\vec{K}\\)\n\n\n\n\n\nLet \\(t\\) stand for the index of the time-coordinate, and let \\(i, j, k\\) be the indices of the space-coordinates. Then we have \\[(\\vec{n} \\cdot \\vec{K})^3 = n_in_jn_k(e_{it} + e_{ti})(e_{jt} + e_{tj})(e_{kt} + e_{tk})\\]\nwhere \\(e_{mn}\\) means the matrix with entry \\((m, n)\\) being one and all other entries being zero. We use Einstein summation convention, so repeated indices means summing over it (except \\(t\\), which is not an index to sum over).\nSince \\(e_{mn}e_{kl} = \\delta_{nk} e_{ml}\\), and \\(i \\neq t, j \\neq t, k \\neq t\\), the above multiplication expands to 8 terms, but only two are nonzero:\n\\[= n_in_jn_k(e_{it}\\delta_{jk} + \\delta_{ij} e_{tk}) = \\underbrace{\\|\\vec{n}\\|^2}_{= 1} (n_i e_{it} + n_k e_{tk}) = n_i (e_{it} + e_{ti}) = \\vec{n} \\cdot \\vec{K}\\]"
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#wigner-rotation",
    "href": "essays/posts/wigner-rotation/index.html#wigner-rotation",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Wigner rotation",
    "text": "Wigner rotation\nWe are ready to derive a rarely discussed effect in special relativity: the Wigner rotation. Although it is as fundamental as time dilation and space contraction, it often goes unmentioned in undergraduate textbooks I have encountered.\n\nTheorem 1 (Wigner rotation) When three boosts are made in a cycle, such as \\(p_1 \\to p_2 \\to p_3 \\to p_1\\), the result is a rotation. The angle of rotation is equal to the hyperbolic area of the triangle \\(p_1 p_2 p_3\\), but in the opposite direction. Furthermore, the hyperbolic area is equal to the angle defect of the triangle.\n\nThe phrase “in the opposite direction” means that, if, looking from the \\(+t\\)-direction down at the origin, you see the three 3-velocities make a counterclockwise cycle in the disk \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 = 1\\}\\), then you would see that the Wigner rotation angle is in the clockwise direction, and vice versa. The phrase “angle defect” means \\(\\pi - (\\angle{p_1 p_2 p_3} + \\angle{p_2 p_3 p_1} + \\angle{p_3 p_1 p_2})\\).\n\nStep-by-step demonstration of the Wigner rotation\nConsider the simplest type of three-boost cycle as shown in the figure below. We start at the rest frame, boost to the frame with a 3-velocity of \\((v_x, 0)\\), then boost to the frame with a 3-velocity of \\((v_x, v_x d\\varphi)\\), and finally boost back to the rest frame.\n\n    \n\nTracing out the trajectory of every point on the space circle gives the following sequence:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen projected to the \\(xy\\)-plane, the sequence looks like:\n\n    \n\nTherefore, after three pure boosts, we end up with a rotation by \\((\\gamma - 1)d \\varphi\\) radians in the opposite direction. Here, \\(\\gamma\\) is the long semiradius of the boosted space circle, after projecting it down to the \\(xy\\) plane. This is the “Lorentz factor” often used in special relativity:\n\\[\\gamma = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\n\n\n\nDeriving the Lorentz factor\n\n\n\n\n\nIn the rest frame, the space circle is spanned by \\((1, 0, 0)\\) and \\((0, 1, 0)\\). After boosting by the 3-velocity \\((v, 0)\\), it is boosted to an ellipse with semiaxes \\((\\cosh(w), 0, \\sinh(w)), (0, 1, 0)\\), where \\(w = \\arctanh(v)\\) is its rapidity. Thus, its projection to the \\(xy\\) plane is an ellipse with semiaxes \\((\\cosh(w), 0), (0, 1)\\).\n\\[\\gamma = \\cosh(w) = \\cosh(\\arctanh(v)) = \\frac{1}{\\sqrt{1-v^2}}\\]\n\n\n\nPerforming boosts around a full cycle of 3-velocities and returning to our starting 3-velocity is equivalent to performing an infinite number of infinitesimal triangle-circuit boosts:\n\n    \n\nTherefore, a full cycle of boosts rotates our space circle by a full \\((\\gamma - 1)2\\pi\\) radians in the opposite direction.\n\n\nInteractive app for the Wigner rotation\nWith this understanding, you can now fully appreciate the following interactive app demonstrating the Wigner rotation.\nThe left picture represents the \\(xy\\) plane, and the right picture represents the \\(xyt\\) spacetime.\nYou can drag the black dot on the left, which represents the 3-velocity. The 3-velocity is restricted to have maximal norm \\(0.8\\), to keep the app numerically stable. The blue ellipse is the projection of the space circle to the \\(xy\\) plane. A fixed point on the space circle is distinguished by a big red dot, so that you can see how the space circle Wigner-rotates as you drag the 3-velocity around.\nIn the right picture, the forward and backward light cones intersect at the space circle. You can drag and scroll to adjust the camera.\n\n    \n    \n\n\n\nHere is a “tourist’s guide to the visualization”.\nThe three boosts in animated form:\n\n    \n\nBecause \\(\\gamma - 1 = \\frac{2}{3}\\), if you drag the 3-velocity for 3 full cycles at maximal velocity, the space circle would complete -2 full cycles.\n\n    \n\nSuppose a particle is moving close to the speed of light and emitting photons at equal angular density in all directions in its own frame, then in the rest frame the photons are bunched in front of the particle. This is the relativistic beaming effect, or the headlight effect.\n\n    \n\n\n\nThomas precession\nThe Wigner rotation formula is equivalent to the following formula, often called the Thomas precession formula:\n\\[\n\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\n\\tag{6}\\]\nExperiments with particles moving in cyclotrons have verified this, thus justifying our guess that we should transport space circles without twisting.\n\n\n\n\n\n\nDeriving the Thomas precession formula\n\n\n\n\n\nConsider a particle moving counterclockwise at a constant speed \\(v\\) in a circle of radius \\(R\\). Its acceleration is \\(a = v^2/R\\), and after completing one cycle in time \\(T = 2\\pi R/v\\), it has Wigner-rotated by \\(-(\\gamma - 1)2\\pi\\) radians. Simplifying, we have\n\\[\\omega = \\frac{-(\\gamma - 1)2\\pi}{T} = \\cdots = -\\frac{\\gamma^2}{\\gamma + 1} av\\]\nBy the right hand rule of cross products, we can get all the directions correct with \\(\\vec{\\omega} = \\frac{\\gamma^2}{\\gamma + 1} \\vec{a} \\times \\vec{v}\\).\nFor a particle undergoing a generic acceleration, the 3-acceleration decomposes into a component parallel to the 3-velocity, and a component perpendicular to it. The parallel component does not change Wigner rotation, therefore the Thomas precession equation is still true.\n\n\n\nHowever, notice that the formula is made of 3-vectors, and we know that 3-vectors are meaningless in special relativity. This suggests that something is off here. If the rest of this section does not make much sense, first read the section on Foucault pendulum, then return here.\nIf the Earth were flat, then we could stand at a single point, and draw two perpendicular arrows. We say, “this arrow is pointing at \\(+x\\)”, and “that arrow is pointing at \\(+y\\)”. Then we create an infinite number of missionaries. Each would pick up the two arrows and parallel-transport the arrows to their given station on the Earth. In this way, we would provide a unique direction-system for every point on the Earth.\nHowever, because the Earth is a sphere, this does not work. Suppose we stand at the North Pole, and we do the same. Then all the missionaries would meet at the South Pole and start arguing about who has the right one, because they would all disagree. This problem happens because the Earth is curved, and transporting a vector around a cycle would rotate it.\nWe might shrug and say, “Well, nobody lives in the South Pole, so we will just tell our missionaries to avoid it.”. Great idea, except then they found that Emperor of China has also sent out his own missionaries. We shrug and say, “Well, we’ll just give the Emperor of China a call and ask him what angle he picked. Then we will rotate our map until all our missionaries agreed with his.”. After some fruitless fiddling with our maps, it has dawned to us that this plan is doomed. Why?\nImagine if the North Pole is at point \\(N\\), and the Emperor of China is at point \\(C\\). If somehow, we could pick our \\(xy\\) directions so that all our missionaries agree with his, then we can send one missionary around a three-part journey around the world: \\(N \\to C \\to K \\to N\\), where \\(K\\) is a point off the great circle passing \\(N, C\\). Along the path \\(N \\to C\\), the missionary is leaving the North Pole on a straight arc away from the North Pole, so the \\(xy\\) chart he is carrying agrees with the missionaries he is passing by. Similarly for the other two parts. But when the missionary has returned to the North Pole, he must find his \\(xy\\) chart rotated, contradiction!\nThere is no way to resolve this disagreement other than forcing one side to give up their coordinate system. Perhaps we will have to call up a crusade to enforce our coordinate system.\n\nThe introduction of numbers as coordinates is an act of violence.\n— Hermann Weyl\n\nSimilarly, in special relativity, we can stand at the “North Pole” (rest frame), construct the \\(xyz\\) axes, then send out missionaries on rockets to provide directions for every inertial frame. The good news is that no two missionaries can meet each other, so there is no argument. The bad news is that we still have the Wigner rotation problem, so if some aliens are executing the same project, but starting at a different coordinate frame (perhaps because their galaxy is moving relative to ours), then it is impossible for our missionaries to agree in every frame.\nSo, we are forced to only use one rest frame. It is in this context that the Thomas precession formula works.\nConsider an object, boosted from frame \\(p_1\\) to frame \\(p_2\\), with their 3-velocities in the rest frame being \\(\\vec{v}\\) and \\(\\vec{v} + \\delta \\vec{v}\\). To use the Thomas precession formula, we must first – in imagination – boost it back to the rest frame, and take a snapshot of its orientation against our \\(xyz\\) axes at origin. Then we boost it back to frame \\(p_1\\), boost it to \\(p_2\\), and boost it – in imagination – back to the rest frame again. By using the Wigner rotation formula in the rest frame, it has rotated by\n\\[\n\\frac{\\gamma^2}{\\gamma + 1} \\delta\\vec{v} \\times \\vec{v}\n\\]\nTo emphasize again: the Thomas precession formula only works in one frame. It is neither Lorentz invariant nor geometrically meaningful. It cannot be geometrically meaningful, because it is made of 3-vectors \\(\\vec{v}, \\vec{a}, \\vec{\\omega}\\), and 3-vectors are geometrically meaningless in special relativity.3\n3 Or as I like to say, 3-vectors are inherently violent in special relativity, and anything involving anything inherently violent is also inherently violent."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "href": "essays/posts/wigner-rotation/index.html#relativity-in-31-dimensions",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Relativity in 3+1 dimensions",
    "text": "Relativity in 3+1 dimensions\nWe began our study of special relativity in 1+1 dimensions and discovered the Lorentz transformations, then proceeded to 2+1 dimensions and discovered Wigner rotation. In 3+1 dimensions, Wigner rotation still occurs, but there is something new: rotations are no longer describable by a single number.\nThis is not a place to get into the details; suffice to say that the space of rotations in \\(\\mathbb{R}^3\\) is much harder to understand than the space of rotations in \\(\\mathbb{R}^2\\).4 We must be more careful here.\n4 The space of rotations in \\(\\mathbb{R}^3\\) is \\(SO(3)\\) and that in \\(\\mathbb{R}^2\\) is \\(SO(2)\\). While \\(SO(2)\\) is just the circle and can be easily represented by a real number or an angle, \\(SO(3)\\) is the projective 3-space \\(\\mathbb{PR}^3\\) and best represented by quaternions.Suppose we have four frames: \\(0, 1, 2, 3\\). We start with a tripod in frame \\(0\\), then boost it successively to frames \\(1, 2, 3, and 0\\), how much does it rotate? To discover this, we need to compose the Wigner rotation around the cycle \\(0120\\) with the Wigner rotation around the cycle \\(0230\\). The full rotation is composed of two rotations, first rotation in the plane of the triangle \\(012\\), and the second one in the plane of the triangle \\(023\\).\nThe Wigner rotation coming from an arbitrary sequence of boosts can be calculated easily with the Thomas precession formula, if we are willing to use violence.5 Without violence, we have to do spherical trigonometry, much harder than adding 2-dimensional rotation angles. My intuition is that it should most naturally involve the principal \\(G\\)-connections, where \\(G = SO(n, 1)\\) is the group of Lorentz transformations.\n5 Since this usage of “violence” comes directly from Hermann Weyl, perhaps we can call it Weylence?"
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "href": "essays/posts/wigner-rotation/index.html#velocity-space-is-hyperbolic-geometry",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Velocity space is hyperbolic geometry",
    "text": "Velocity space is hyperbolic geometry\nIt is time to fulfill the promise in the title, that hyperbolic geometry is involved.\nOur first hint that hyperbolic geometry is relevant is that we have encountered \\(\\cosh\\) and \\(\\sinh\\) – the hyperbolic trigonometric functions. With this hint, a quick check shows us that we are indeed dealing with hyperbolic geometry. This is easy to do in 2+1 dimensions, but it generalizes readily to n+1 dimensions after slightly cluttering the notation.\n\nDeriving the hyperbolic metric\nSince \\(\\|(0, 0, 1)\\|^2 =-1\\), under arbitrary Lorentz transformations, \\((0, 0, 1)\\) can, under arbitrary Lorentz transformations, reach any point on the upper half of the hyperbola \\(x^2 + y^2 - t^2 = -1\\). It stands to reason that the upper hyperbola, which is equivalent to a plane as a smooth manifold, should have some kind of planar geometry. We need only discover its metric. The most direct path goes through the disk of 3-velocities, which gives us the metric\n\\[\nds^2 = \\frac{dv_\\theta^2}{1-v_r^2} + \\frac{dv_r^2}{(1-v_r^2)^2}\n\\tag{7}\\]\n\n\n\n\n\n\nDeriving the metric of the 3-velocity disk\n\n\n\n\n\nThe disk of 3-velocities \\(\\{(v_x, v_y, 1) : v_x^2 + v_y^2 &lt; 1\\}\\) is not preserved under Lorentz transformations, but we can project it back to the circular disk. This means that we can take the metric at origin\n\\[ds^2 = dv_x^y + dv_y^2\\]\nand perform a Lorentz transformation, followed by a projection, to move the origin to any other point on the disk. Multiplying the metric at the origin with the gradient-matrix gives the metric at that other point.\nWe exploit the rotational symmetry of the disk using polar coordinates. Express any point on the disk as \\((r, \\theta)\\). Its local metric must be of form \\(ds^2 = f(r) dr^2 + g(r) d\\theta ^2 + h(r) drd\\theta\\) for some functions \\(f, g, h\\). Further, by reflection symmetry of \\(\\theta \\leftrightarrow -\\theta\\), we have \\(h(r) = 0\\).\nTo find \\(f(r)\\), we construct an infinitesimal segment \\((r, 0) \\to (r+dr, 0)\\) and calculate with the velocity-addition formula. Similarly, we find \\(g(r)\\) using the segment \\((r, 0) \\to (r, d\\theta)\\).\n\n\n\nThis is the Beltrami–Klein metric, and so we have discovered that this is exactly the Beltrami–Klein model of hyperbolic geometry. We can then project this metric onto the hyperboloid \\(x^2 + y^2 - t^2 = -1\\) to obtain its metric\n\\[\nds^2 = dx^2 + dy^2 - dt^2 = \\| (dx, dy, dt) \\|^2\n\\tag{8}\\]\nThis, in hindsight, is obvious: We have already known that the Lorentz transformation preserves the Minkowski norm-squared. But such is the journey of discovery: the first pass is rarely the most elegant. It is probably better to write down the first pass to show how to discover things, then write a second pass to show how to tidy things. Better this than what Gauss did:\n\n[Gauss] makes his mathematics like a fox, wiping out the traces in the sand with his tail.\n— unnamed German student, as reported by Abel\n\n\n\nInterpreting the hyperbolic geometry\nNow we can connect concepts between hyperbolic geometry and special relativity.\nWhen \\(v\\) is small, the Wigner rotation for a full cycle, \\((\\gamma - 1)2\\pi\\), is \\(\\pi v^2 + O(v^4)\\), coinciding with the area enclosed by the cycle. Therefore, since both area and Wigner rotation are additive, we have proven Theorem 1.6\n6 This is not a handwaved proof, but fully rigorous. The Wigner rotation of a full cycle is \\(\\pi v^2 + O(v^4)\\), and the area enclosed by the cycle is also \\(\\pi v^2 + O(v^4)\\). Therefore, we can integrate over an arbitrary area, to find that the Wigner rotation angle is equal to the area enclosed, up to an infinitesimal term of \\(O(v^2) \\to 0\\).In rocketry, if we trace out the trajectory of the rocket’s velocity on a graph paper, we obtain a hodograph. Similarly, in relativistic rocketry, a path in the velocity space is a hodograph. The length of a hodograph is the total delta-v of the rocket.\nA straight line in hyperbolic space is the hodograph of the most fuel-efficient control-trajectory for a rocket to get from one velocity to another.\nIf a rocket explodes with spherical symmetry, the velocities of its debris will lie on a hyperbolic sphere centered at the rocket’s original velocity."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#two-parables",
    "href": "essays/posts/wigner-rotation/index.html#two-parables",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Two parables",
    "text": "Two parables\n\nThe star worshippers\nAn alien race, who worshipped Rigel, Deneb, and Betelgeuse, prayed in the direction of these three stars. In order to pray in the correct direction even without seeing them, they constructed the Sacred Tripods, which are steel tripods that are oriented such that each leg points at one of the Sacred Stars. These Sacred Tripods, placed in public spaces, were required to point precisely at the Sacred Stars.\nThen a Cosmic Dark Age began and the stars winked out of existence, including the Sacred Stars, but the aliens did not lose their faith. Instead, they intensified their prayers in hopes of resurrecting the Sacred Stars.\nIn a region of space, there were two space stations A and B, at rest with each other. The Sacred Tripod on B was unstable, and required yearly calibration against the more stable Sacred Tripod at A. So station A would align its spare Sacred Tripod to its own one, put it on a gimbal-mount, and send it by rocket to station B. B would align its Tripod with the one sent, then let the Tripod go back.\nAs years passed, the aliens on station B grew confident that their Tripod did not drift more than \\(1''\\) per year. One day, an asteroid field blocked the straight path, forcing the next shipment to make a big detour around the asteroid field. When it arrived, to their astonishment, it was found that the Tripod on B was clearly misaligned with the Tripod from A. What could be the cause of this misalignment?\n\n\nThe day the Earth stood still\nOne day, the Earth stopped spinning. This annoyed the visitors to the science museum, who wanted to see Foucault’s pendulum rotating. To satisfy those visitors, the museum keeper put the pendulum on the top of a bus, and loaded the visitors on the bus, then they started driving around the Earth at the same speed as how the Earth used to rotate at that latitude, so that after one day, they returned to the museum. The visitors looked up at the pendulum and were satisfied to see that it indeed has turned.\nTo find out how much the pendulum has turned, we divide up the circular trajectory into tiny segments \\(p_0, p_1, \\dots, p_n\\), then draw tiny triangles \\(p_0 p_1 N, p_1 p_2 N, \\dots, p_{n-1} p_n N\\), with \\(N\\) being the North Pole. We imagine that, instead of driving around the Earth in a circle, we drive around the triangle \\(N \\to p_0 \\to p_1 \\to N\\), then the triangle \\(N \\to p_1 \\to p_2 \\to N\\), etc.\nWhen we drive around \\(N \\to p_0 \\to p_1 \\to N\\), the bus makes three turns, and each time the pendulum turns by an opposite amount relative to the bus. After three turns, the bus has turned the same angle as the sum of three external angles of the triangle. By spherical trigonometry, we know that this is equal to \\(2\\pi - 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). Thus, the pendulum has turned relative to the bus by \\(-2\\pi + 4\\pi\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\). The \\(-2\\pi\\) part has no effect, as it is a full cycle, leaving us with \\(\\frac{\\text{area of triangle $p_0 p_1 N$}}{\\text{surface area of the Earth}}\\).\nTherefore, by adding them up, we find that after driving around an arbitrary cycle on the Earth, the pendulum would have turned by\n\\[4\\pi\\frac{\\text{area of the cycle}}{\\text{surface area of the Earth}}\\]\nWe can turn this insight into a formula for Fermi–Walker transport on the Earth.\nFirst, we scale the Earth so that its radius is one. Now set up the coordinate system \\(\\phi, \\theta\\), where \\(\\phi\\) is the latitude (zero at equator, \\(\\pi/2\\) at the North Pole) and \\(\\theta\\) the longitude. At each point on the Earth (except the two poles) we set up a local frame with two unit vectors \\(\\hat\\phi, \\hat\\theta\\). One very important fact is that this local frame is not parallel-transported7. If you try to drive in a small circle around the North Pole, while always pointing in the direction of \\(\\hat\\theta\\), you would feel a strong centrifugal force pushing against your steering wheel. Centrifugal forces and all other “inertial forces” are nature’s way of telling you that you are not being parallel-transported.\n7 Indeed, there is no way to cover the space with parallel-transported local frames, because the space is curved, and therefore if you try to transport a local frame in a cycle back to its starting point, it would have rotated against itself. To be a curved space is equivalent to have no system of parallel-transported local frame, and curvature measures the amount of rotation-against-itself that happens when you transport a local frame in an infinitesimal circle.8 This is immediate from Archimedes’ hat-box theorem, which is the basis of the Lambert cylindrical equal-area projection.Consider two infinitesimally close points on the Earth, \\(p_1 = (\\phi, \\theta)\\) and \\(p_2 = (\\phi + \\delta \\phi, \\theta + \\delta\\phi)\\). Since the surface area of a spherical cap around latitude \\(\\theta\\) is \\(\\frac{1 -\\sin\\theta}{2}\\) that of the whole sphere8, the area of a thin spherical triangle with vertices \\(p_1, p_2, N\\) is\n\\[4\\pi\\frac{1 -\\sin\\theta}{2} \\times \\frac{\\delta\\phi}{2\\pi} = (1-\\sin\\theta)\\delta\\phi\\]\nThus, if we parallel transport a vector clockwise in the order \\(p_1 \\to N \\to p_2 \\to p_1\\), then the vector would turn counterclockwise by \\(-(1 -\\sin\\theta) \\delta\\phi\\).\nNow, consider two ways to move the pendulum from \\(p_1\\) to \\(p_2\\). We can transport it directly, or detour through the North Pole. If we detour through the North Pole like \\(p_1 \\to N \\to p_2\\), then by the same argument as the “polar bear puzzle”9, you see that the pendulum has rotated counterclockwise relative to the local frame by \\(-\\delta\\phi\\). Then we complete its journey with \\(p_2 \\to p_1\\), to create an absolute10 full \\(-(1 -\\sin\\theta) \\delta\\phi\\) rotation. Therefore, moving it \\(p_2 \\to p_1\\) has created a rotation relative to the local frame by \\(\\sin\\theta \\delta\\phi\\). Since we are actually moving it \\(p_1 \\to p_2\\), we reverse the sign, and obtain our Fermi–Walker transport equation (the Earthbound version):\n9 You walk 1 km south, 1 km east, and 1 km north, and ended up at the same point. You see a bear. Why is the bear white?10 Relative rotation means that we are measuring the orientation of the vector relative to the local frame. However, since the frames themselves are not parallel-transports of each other, relative rotation is arbitrary and not a fact of geometry, but a fact of convenience. If you move the vector back to its starting point, however, there is absolutely no dispute about how much it has rotated, and it does not depend on any system of local frames. You just have to compare the vector against itself.\\[\n\\delta(\\text{vector angle}) = -\\sin\\theta \\delta\\phi\n\\tag{9}\\]\nApplied to the Foucault pendulum problem, we find that it rotates clockwise by \\(2\\pi \\sin\\theta\\) every day, and it takes \\(\\frac{1}{\\sin\\theta}\\) days11 to make a full rotation. At the Paris Observatory, the original place where Foucault made his experiment in 1851, we have\n11 To be precise, this is a sidereal day, the time it takes for the Earth to rotate one cycle relative to distant stars. It is shorter than a solar day.\\[\\theta = \\mathrm{48^\\circ 52' N}, \\quad \\text{period} = \\frac{\\mathrm{23h56'}}{\\sin \\theta} \\approx \\mathrm{31\\,h\\,50\\,min}\\]\nIf a museum visitor can stay for 5 minutes at the pendulum, then they would see the pendulum complete \\(\\approx 1/382\\) of a cycle. Typical museums would put up about 400 wooden blocks in a cycle, to be knocked down by the pendulum. This allows each visitor to see at least one block being knocked down."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#history-notes",
    "href": "essays/posts/wigner-rotation/index.html#history-notes",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "History notes",
    "text": "History notes\n\nThe history of the phenomenon\nThe history of the Wigner rotation is winding and complicated. Here is a brief summary. The details are given in (Walter 1999).\n\n1905: Einstein published special relativity. (A. Einstein 1905)\n1908: Minkowski published the spacetime interpretation of special relativity. (Minkowski 1908)\n1910: Vladimir Varićak (Varićak 1910) and E. T. Whittaker (Whittaker 1910, 441) both introduced the hyperbolic functions and the rapidity parameter.\n1911: Alfred Robb named the rapidity parameter, and found that velocities compose in hyperbolic space. (Robb 1911).\n1913: Émile Borel discovered the Wigner rotation as a part of his general study of special relativity by hyperbolic geometry. (Borel 1913)\n1921: Fermi discovered the Fermi–Walker transport. (Fermi 1921)\n1926: Llewellyn Thomas rediscovered the Thomas precession, a special case of the Wigner rotation. (Thomas 1926)\n1932: Walker rediscovered the Fermi–Walker transport. (Walker 1932)\n1939: Wigner rediscovered the Wigner rotation. (Wigner 1939)\n\nPedagogical attempts to present the Foucault pendulum and the Wigner rotation have been numerous, such as (Criado and Alamo 2009), but perhaps due to the decade-long latency in academic publishing, none has yet made it into the usual undergraduate textbooks. Tevian Dray has written an entire textbook (Dray 2021) treating both special and general relativity in the same geometric style. For a summary paper of the textbook, see (Dray 2017).\n\n\nAmusing quotes from (Walter 1999)\n\n[After 1907] Minkowski never again referred to a manifold as both four-dimensional and non-Euclidean. Along with the problematic label, the geometric interpretation of velocity vectors likewise vanishes from view in Minkowski’s subsequent writings. Felix Klein, for one, regretted the change; in his opinion, Minkowski later hid from view his “innermost mathematical, especially invariant-theoretical thoughts” on theory of relativity (Klein 1927, 75).\nPlanck lavished praise on Einstein for his modification of the concept of time:\n\nIt need scarcely be emphasized that this new view of the concept of time makes the most serious demands upon the capacity of abstraction and the imaginative power of the physicist. It surpasses in boldness everything achieved so far in speculative investigations of nature, and even in philosophical theories of knowledge: nonEuclidean geometry is child’s play in comparison.30 (Planck 1910a, 117)\n\nUnder the new space-time view, Minkowski announced, “Three-dimensional geometry becomes a chapter of four-dimensional physics.” In the same triumphant spirit, Minkowski suggested that his new four-dimensional understanding of the laws of physics deserved its own label. The “Principle of the Hyperbolic World” that he had tried on Hurwitz was shelved in favor of the more ecumenical “Postulate of the Absolute World” (Minkowski 1909, 82). Although Minkowski explained this to mean that only the four-dimensional world in space and time is given by phenomena (Minkowski 1909, 82), one suspects an inside joke with Hurwitz, since in the German mathematical community, hyperbolic geometry was sometimes referred to as absolute geometry.\nEven the watered-down version of the space-time theory presented in Minkowski’s Cologne lecture repelled some physicists. For instance, Willy Wien’s cousin Max (1866-1938), a physicist at Danzig Polytechnic, confided to his friend Arnold Sommerfeld that reading Minkowski gave him vertigo:\n\nSommer[feld] maintains that [Minkowski’s] speech in Cologne was simply grand; when reading it, however, I always get a slight brain-shiver, now (that) space and time appear conglomerated together in a gray, miserable chaos.36 (Max Wien to Arnold Sommerfeld, February 16, 1909, Benz 1975, 71)\n\n\n\n\nThe history of this document\nDuring high school, I was in the physics Olympiad team. One afternoon, I got into an argument with someone about what happens if you take a square \\([0, 1] \\times [0, 1]\\), boost it by \\((v, 0)\\), and then boost it by \\((0, v')\\) within the boosted frame. By the velocity addition formula, the square would move at 3-velocity\n\\[\n(v, \\frac{v'\\sqrt{1-v^2}}{1+vv'})\n\\]\nIf \\(v' = \\frac{v}{\\sqrt{1-v^2}-v^2}\\), then the square would be moving at \\((v, v)\\).\nThen, the paradox. He argued that, by length contraction along the diagonal, the square should look like a diamond:\n\n  \n\n\n\nI objected that in the boosted frame, the square looks like a rectangle moving upwards. Decompose the rectangle into a bundle of line-segments, all parallel to the \\(y'\\)-axis. Now, each line-segment is moving in the \\(y'\\)-direction, and \\(y'\\) is parallel to \\(y\\) (since the perpendicular direction is preserved under boosting), we know that each moving line-segment would still be a line-segment parallel to the \\(y\\)-axis in the resting frame, still moving in the \\(y\\)-direction – just slower. Therefore, in the resting frame, the whole square would look like a parallelogram, with two sides parallel to the \\(y\\)-axis, and the other two sides oblique to the \\(x\\)-axis.\n\n    \n        \n        \n    \n    \n\n\nAfter a brief shouting match, we figured out that I was right, but also that we met something no teacher has taught us before: you can create a rotation in special relativity by pure boosting.\nDuring my third undergraduate year, I took a course in theoretical physics, which required a term paper. I first tried to write one on the Ostrogradsky instability, but could not understand it, so I quickly switched to finally solving the rotation effect in special relativity.\nSuffice to say that, after some hours walking and staring at the night sky, I figured out that it is nothing else than hyperbolic geometry, and nothing more paradoxical than the fact that the external angle of a hyperbolic triangle is equal to \\(2\\pi + (\\text{area of the triangle})\\), which I remember from hyperbolic geometry.\nThis is the old poler-bear puzzle again. If you walk from the North Pole to the equator, then walk \\(1/4\\) of the way around the Earth, and finally walk back to the North Pole, you will have traversed a triangle with an external angle of \\(\\frac 32 \\pi\\). In general, the external angle of a triangle on the unit sphere is \\(2\\pi - (\\text{area of the triangle})\\), the perfect opposite to the case in hyperbolic geometry.\nHalf-mad, I ran home and smashed into the search engine all the keywords I knew must be there: “special relativity rotation hyperbolic triangle Foucault pendulum”. My disappointment was swift and certain: This had been repeatedly discovered over the past hundred years.\nBitterly, I searched every undergraduate physics textbook in the school library. None included it. The standard textbook (Goldstein, Poole, and Safko 2008, sec. 7.3) went on a three-page long computation and concluded that, indeed, we obtain a rotation matrix:\n\nThe spatial rotation resulting from the successive application of two nonparallel Lorentz transformations has been declared every bit as paradoxical as the more frequently discussed apparent violations of common sense, such as the so-calIed “twin paradox”. But the present apparent paradox has important applications, especially in atomic physics, and therefore has been abundantly verified experimentally.\n\nFrom that moment on I swore that I would finally present it in a way so simple and direct that it will never be forgotten. I had found the way, and it remained to publicize to the world.\nThere was little use in writing another paper about it, as despite the many papers over the century, somehow this effect did not end up in the physics textbooks. I considered writing a blog post about it, but mere words and figures seemed insufficient. I tried promoting this idea to various physics popularizers on YouTube, but none picked it up. What I needed was interactive animation, but JavaScript defeated me, so I kept it in the backlog until now. With ChatGPT I could finally write JavaScript without losing all my sanity, and so, here it is."
  },
  {
    "objectID": "essays/posts/wigner-rotation/index.html#some-bonus-content",
    "href": "essays/posts/wigner-rotation/index.html#some-bonus-content",
    "title": "The Wigner Rotation in Special Relativity via Hyperbolic Geometry",
    "section": "Some bonus content",
    "text": "Some bonus content\n\nFinding the Wigner rotation in one line\nIf you know Lie algebra, then the Wigner rotation is immediate: calculate the commutator.\n\\[\n[K_x, K_y] := K_x K_y - K_y K_x = \\begin{bmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\]\nThis is the generator of rotation in the \\(xy\\) plane.\n\n\nGo anywhere with just five boosts\nGiven a rocket that can boost in any direction, but not rotate, you can still make it rotate by Wigner rotation. If you must perform an arbitrary Lorentz transformation, how many boosts do you need?\nIn general, if you have a Lorentz-transformed rocket, you can apply a carefully aimed boost so that it comes to rest, then apply Wigner rotation to return it back to the starting position.\nWith a cycle of three boosts, the Wigner rotation angle is equal to the angle defect of the hyperbolic triangle, which can take any value in \\((0, \\pi)\\) (you can’t have one with zero inner angle sum, but you can get arbitrarily close). With a cycle of four boosts, we can cover the \\(\\pi\\) case as well, since a hyperbolic square can have any angle defect in \\((0, 2\\pi)\\).\nIn summary: almost any Lorentz transformation can be obtained in four boosts, except those requiring a 180-degree rotation, which need five boosts.\nIn fact, the result can be improved by one boost: every Lorentz transformation can be done in three boosts, except “180-degree screws”, which require four. The solution given in (Lightman et al. 1975, 153–58) uses matrix algebra in \\(\\mathbb{C}^{2\\times 2}\\), which is too algebraic for me12. However, I have never found a satisfactory geometric method to demonstrate this, despite thinking on and off about it several times over the years.\n12 \nIn these days the angel of topology and the devil of abstract algebra fight for the soul of each individual mathematical domain.\n— Hermann Weyl\n\n\nProblem 1.28. What is the least number of pure boosts which generate an arbitrary Lorentz transformation? Note: This is a difficult problem! (Lightman et al. 1975)"
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html",
    "href": "essays/posts/statistical-mechanics/index.html",
    "title": "Statistical Mechanics",
    "section": "",
    "text": "Classical mechanics, electrodynamics, and thermodynamics are all conceptually simple, but settled. Things are less clear with statistical mechanics. To say it kindly, it is a lively field of research. To say it unkindly, it is a field whose very conceptual foundation is in doubt. The main problem is not with quantum mechanics, as quantum statistical mechanics works very well, but with how to handle systems far-from-equilibrium. Fortunately, as long as we stick to the (near-)equilibrium parts, then it is mostly settled, and so this is how this essay is going to be written about. We will avoid quantum because that deserves its own entire essay, and avoid far-from-equilibrium because it is unsettled. What we can present is still a beautifully precise mathematical toolbox with surprisingly wide applications.\nStatistical mechanics is technically independent of thermodynamics, but it is strongly related. You can acquire the basic skills in thermodynamics by working through the first half of my previous essay Classical Thermodynamics and Economics.\nAbout half of the essay is taken up with calculations. The theoretical core of statistical mechanics is small, and most of the skills are in applying it to actual systems. Therefore, a lot of worked-through examples are necessary. I have tried to make them flow well and skimmable.\nThe essay contains: entropy, free entropies, partition function, about 12 useful theorems, fluctuation-dissipation relations, maximal caliber, Crooks fluctuation theorem, Jarzynski equality, rubber bands, kinetic gas theory, van der Waals law, blackbody radiation, combinatorics, chi-squared test, large deviation theory, bacteria hunting, unzipping RNA hairpins, Arrhenius equation, martingales, Maxwell’s demon, Laplace’s demon.\nIt does not contain: quantum statistical mechanics, nonequilibrium statistical mechanics, linear response theory, Onsager reciprocal relations, statistical field theory, phase transitions, stochastic processes, Langevin equation, diffusion theory, Fokker–Planck equation, Feynman–Kac formula, Brownian ratchets.\nThe prerequisites are thermodynamics, multivariate calculus, probability, combinatorics, and mathematical maturity. It’s good to be familiar with biology and the basics of random walk as well.\n\n\n\n\n\\(D_{KL}\\): Kullback–Leibler divergence.\n\\(S[\\rho]\\): entropy of probability distribution \\(\\rho\\).\n\\(S^*\\): maximal entropy under constraints.\n\\(f[\\rho]\\): Helmholtz free entropy of probability distribution \\(\\rho\\).\n\\(f^*_{X|y}\\): maximal Helmholtz free entropy under the constraint that \\(Y = y\\).\n\\(Z\\): the partition function.\n\\(\\beta\\): inverse temperature.\n\\(N\\): number of particles, or some other quantity that can get very large.\n\\(n\\): number of dimensions, or some other quantity that is fixed.\n\\(F\\): Helmholtz free energy.\n\\(\\binom{m}{n}\\): binomial coefficient.\n\\(\\braket{f(x, y) | z}_y\\): The expectation of \\(f\\) where we fix \\(x\\), let \\(y\\) vary, and conditional on \\(z\\).\n\\(\\mathrm{Var}\\): variance.\n\\(\\int (\\cdots) D[x]\\): path integral where \\(x\\) varies over the space of all possible paths.\n\nAs usual, we set \\(k_B = 1\\), so that \\(\\beta = 1/T\\), except when we need a numerical answer in SI units.\n\n\n\nUnfortunately, I never learned statistical mechanics from any textbook. I just understood things gradually on my own after trying to make sense of things. This means I cannot recommend any introductory textbook based on my personal experience.\n\nBooks I did learn from:\n\n(Sethna 2021) shows how a modern statistical mechanist thinks. It is a rather eclectic book, because modern statistical mechanics is full of weird applications, from music theory to economics.1\n(Nelson 2003) teaches basic statistical thermodynamics in the context of biology, and (Howard C. Berg 1993) teaches random walks.\n(Ben-Naim 2008; Richard P. Feynman 1996) show how to combine (unify?) statistical mechanics with information theory.\n(Jaynes 2003) gives Jaynes’ entire philosophy of information, one application of which is his theory of why entropy is maximized in statistical mechanics.\n(Lemons, Shanahan, and Buchholtz 2022) closely follows the story of how Planck actually derived the blackbody radiation law. Reading it, you almost have the illusion that you too could have discovered what he discovered.\n(Penrose 2005) gives an elegant mathematical deduction that brings philosophers and mathematical logicians joy. However, it is not useful for applications.\n\nBooks I did not learn from, but feel obliged to recommend:\n\n(Ma 1985; Tolman 1980; Richard P. Feynman 2018; Schrodinger 1989) are books that apparently every real physicist must read before they die. Like those other “1001 books you must read before you die”, I did not read them.\n(Nash 2006) is a concise introduction for chemistry students.\n(Sommerfeld 1950, vol. 5) is by the master, Sommerfeld. If you want to do 19th century style thermodynamics, then it is very good, but otherwise, I don’t know what this book is for.\n\n\n1 The book has quite many buzzwords like “fractals”, “complexity”, “avalanche”, and “edge of chaos”, buzzy in the 1990s. A joke is that during the 1980s, as the Cold War was winding down, physicists were overproduced and underemployed, and had to find someway to get employed. Thus, they went into economics, social sciences, etc, resulting in the discipline of “econophysics”, the nebulous non-discipline of “complexity studies”, etc."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#introduction",
    "href": "essays/posts/statistical-mechanics/index.html#introduction",
    "title": "Statistical Mechanics",
    "section": "",
    "text": "Classical mechanics, electrodynamics, and thermodynamics are all conceptually simple, but settled. Things are less clear with statistical mechanics. To say it kindly, it is a lively field of research. To say it unkindly, it is a field whose very conceptual foundation is in doubt. The main problem is not with quantum mechanics, as quantum statistical mechanics works very well, but with how to handle systems far-from-equilibrium. Fortunately, as long as we stick to the (near-)equilibrium parts, then it is mostly settled, and so this is how this essay is going to be written about. We will avoid quantum because that deserves its own entire essay, and avoid far-from-equilibrium because it is unsettled. What we can present is still a beautifully precise mathematical toolbox with surprisingly wide applications.\nStatistical mechanics is technically independent of thermodynamics, but it is strongly related. You can acquire the basic skills in thermodynamics by working through the first half of my previous essay Classical Thermodynamics and Economics.\nAbout half of the essay is taken up with calculations. The theoretical core of statistical mechanics is small, and most of the skills are in applying it to actual systems. Therefore, a lot of worked-through examples are necessary. I have tried to make them flow well and skimmable.\nThe essay contains: entropy, free entropies, partition function, about 12 useful theorems, fluctuation-dissipation relations, maximal caliber, Crooks fluctuation theorem, Jarzynski equality, rubber bands, kinetic gas theory, van der Waals law, blackbody radiation, combinatorics, chi-squared test, large deviation theory, bacteria hunting, unzipping RNA hairpins, Arrhenius equation, martingales, Maxwell’s demon, Laplace’s demon.\nIt does not contain: quantum statistical mechanics, nonequilibrium statistical mechanics, linear response theory, Onsager reciprocal relations, statistical field theory, phase transitions, stochastic processes, Langevin equation, diffusion theory, Fokker–Planck equation, Feynman–Kac formula, Brownian ratchets.\nThe prerequisites are thermodynamics, multivariate calculus, probability, combinatorics, and mathematical maturity. It’s good to be familiar with biology and the basics of random walk as well.\n\n\n\n\n\\(D_{KL}\\): Kullback–Leibler divergence.\n\\(S[\\rho]\\): entropy of probability distribution \\(\\rho\\).\n\\(S^*\\): maximal entropy under constraints.\n\\(f[\\rho]\\): Helmholtz free entropy of probability distribution \\(\\rho\\).\n\\(f^*_{X|y}\\): maximal Helmholtz free entropy under the constraint that \\(Y = y\\).\n\\(Z\\): the partition function.\n\\(\\beta\\): inverse temperature.\n\\(N\\): number of particles, or some other quantity that can get very large.\n\\(n\\): number of dimensions, or some other quantity that is fixed.\n\\(F\\): Helmholtz free energy.\n\\(\\binom{m}{n}\\): binomial coefficient.\n\\(\\braket{f(x, y) | z}_y\\): The expectation of \\(f\\) where we fix \\(x\\), let \\(y\\) vary, and conditional on \\(z\\).\n\\(\\mathrm{Var}\\): variance.\n\\(\\int (\\cdots) D[x]\\): path integral where \\(x\\) varies over the space of all possible paths.\n\nAs usual, we set \\(k_B = 1\\), so that \\(\\beta = 1/T\\), except when we need a numerical answer in SI units.\n\n\n\nUnfortunately, I never learned statistical mechanics from any textbook. I just understood things gradually on my own after trying to make sense of things. This means I cannot recommend any introductory textbook based on my personal experience.\n\nBooks I did learn from:\n\n(Sethna 2021) shows how a modern statistical mechanist thinks. It is a rather eclectic book, because modern statistical mechanics is full of weird applications, from music theory to economics.1\n(Nelson 2003) teaches basic statistical thermodynamics in the context of biology, and (Howard C. Berg 1993) teaches random walks.\n(Ben-Naim 2008; Richard P. Feynman 1996) show how to combine (unify?) statistical mechanics with information theory.\n(Jaynes 2003) gives Jaynes’ entire philosophy of information, one application of which is his theory of why entropy is maximized in statistical mechanics.\n(Lemons, Shanahan, and Buchholtz 2022) closely follows the story of how Planck actually derived the blackbody radiation law. Reading it, you almost have the illusion that you too could have discovered what he discovered.\n(Penrose 2005) gives an elegant mathematical deduction that brings philosophers and mathematical logicians joy. However, it is not useful for applications.\n\nBooks I did not learn from, but feel obliged to recommend:\n\n(Ma 1985; Tolman 1980; Richard P. Feynman 2018; Schrodinger 1989) are books that apparently every real physicist must read before they die. Like those other “1001 books you must read before you die”, I did not read them.\n(Nash 2006) is a concise introduction for chemistry students.\n(Sommerfeld 1950, vol. 5) is by the master, Sommerfeld. If you want to do 19th century style thermodynamics, then it is very good, but otherwise, I don’t know what this book is for.\n\n\n1 The book has quite many buzzwords like “fractals”, “complexity”, “avalanche”, and “edge of chaos”, buzzy in the 1990s. A joke is that during the 1980s, as the Cold War was winding down, physicists were overproduced and underemployed, and had to find someway to get employed. Thus, they went into economics, social sciences, etc, resulting in the discipline of “econophysics”, the nebulous non-discipline of “complexity studies”, etc."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#overview",
    "href": "essays/posts/statistical-mechanics/index.html#overview",
    "title": "Statistical Mechanics",
    "section": "Overview",
    "text": "Overview\n\nPhilosophical comments\nIt is fair to say that, although it originated in the 19th century like all other classical fields of physics, statistical mechanics is unsettled.\nTrajectory-centric statistical mechanics. In this view, we start with the equations of motion for a physical system, then study statistical properties of individual trajectories, or collections of them. For example, if we have a pendulum hanging in air, being hit by air molecules all the time, we would study the total trajectory \\((\\theta, x_1, y_1, z_1, x_2, y_2, z_2, \\dots)\\), where \\(\\theta\\) is the angle of the pendulum swing, and \\((x_i, y_i, z_i)\\) is the location of the \\(i\\)-th air molecule. Then we may ask that, over a long enough period, how frequent would the pendulum visit a certain angle range of \\([\\theta_0, \\theta_0 + \\delta\\theta]\\):\n\\[\nPr(\\theta \\in [\\theta_0, \\theta_0 + \\delta\\theta]) = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{-T}^{+T} 1[\\theta \\in [\\theta_0, \\theta_0 + \\delta\\theta]] dt\n\\]\nIn the trajectory-centric view, there are the following issues:\n\nProblem of ergodicity: When does time-average equal ensemble-average? A system is called “ergodic” iff for almost all starting conditions, the time-average of the trajectory is the ensemble-average over all trajectories.\nProblem of entropy: How is entropy defined on a single trajectory?\nH-theorem: In what sense, and under what conditions, does entropy increase?\nProblem of equilibrium: What does it mean to say that a trajectory is in equilibrium?\nApproach to equilibrium: In what sense, and under what conditions, does the trajectory converge to an equilibrium?\nReversibility problem (Umkehreinwand): If individual trajectories are reversible, why does entropy increase instead of decrease?\n\nWhile these philosophical problems are quite diverting, we will avoid them as much as possible, because we will be working with the ensemble-centric equilibrium statistical mechanics. This is the statistical mechanics that every working physicist uses, and this is what we will present. If you are interested in the philosophical issues, read the Stanford Encyclopedia entry on the Philosophy of Statistical Mechanics.\n\n\nPrinciples of statistical mechanics\n\nA physical system is a classical system with a state space, evolving according to some equation of motion.\nAn ensemble of that system is a probability distribution over its state space.\nThe idea of (ensemble-centric) statistical mechanics is to study the evolution of an entire probability distribution over all possible states.\nThe entropy of a probability distribution \\(\\rho\\) is\n\n\\[S[\\rho] := -\\int dx\\; \\rho(x) \\ln \\rho(x)\\]\n\nUnder any constraint, there exists a unique ensemble, named the equilibrium ensemble, which maximizes entropy under constraint.\n\nMost of the times, the state space is a phase space, and the equation of motion is described by a Hamiltonian function. However, the machinery of statistical mechanics, as given above, is purely mathematical. It can be used to study any problem in probability whatsoever, even those with no physical meaning.\nBelieve it or not, the above constitutes the entirety of equilibrium statistical mechanics. So far, it is a purely mathematical theory, with no falsifiability (Popperians shouting in the background). To make it falsifiable, we need to add one more assumption, necessarily fuzzy:2\n2 \nAs far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\n— Albert Einstein, Address to Prussian Academy of Sciences (1921)\n\n\nThe equilibrium ensemble is physically meaningful and describes the observable behavior of physical systems.\n\nIn other words, when a physical system is at equilibrium, then everything observable can be found by studying it as if it has the maximum entropy distribution under constraint.\nOf course, just what that “is physically meaningful” means, is another source of endless philosophical arguments. I would trust that you will know what is physically meaningful, and leave it at that, while those who have a taste for philosophy can grapple with the Duhem–Quine thesis.\n\n\nDifferential entropy depends on coordinates choice\nThere is a well-known secret among information theorists: differential entropy is ill-defined.\nConsider the uniform distribution on \\([0, 1]\\). It is the maximal-entropy distribution on \\([0, 1]\\) – relative to the Lebesgue measure. However, why should we pick the Lebesgue measure, and what happens if we don’t?\nSuppose we now stretch the \\([0, 1]\\) interval nonlinearly, by \\(f(x) = x^2\\), then the maximal-entropy distribution relative to that would no longer be the uniform distribution on \\([0, 1]\\). Instead, it would be the uniform distribution after stretching.\nThe problem is this: Differential entropy is not coordinate-free. If we change the coordinates, we change the base measure, and the differential entropy changes as well.\nTo fix this, we need to use the KL-divergence, which is invariant under a change of base measure, as in \\[-D_{KL}(\\rho \\| \\mu) := - \\int dx\\; \\rho(x) \\ln\\frac{\\rho(x)}{\\mu(x)}\\]\nIn typical situations, we don’t need to worry ourselves with KL-divergence, as we just pick the uniform distribution \\(\\mu\\). When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an improper prior.\nIn this interpretation, the principle of “maximum entropy distribution under constraint” becomes the principle of “minimal KL-divergence under constraint”, which is Bayesian inference, with exactly the same formulas.\nIn almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn’t really justify it other than saying that it just works, and suggesting it has something to do with Liouville’s theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: We should use the uniform prior over phase space, because phase space volume has a natural unit of measurement: \\(h^N\\), where \\(h\\) is Planck’s constant, and \\(2N\\) is the dimension of phase space. As Planck’s constant is a universal constant, independent of where we are in phase space, we should weight all of the phase space equally, resulting in a uniform prior.\n\n\nJaynes’ epistemological interpretation\nThe question at the foundation of statistical mechanics is: Why maximize entropy? The practical scientist would say, “Because it works.”, and that is well and good, but we will give one possible answer to the why, from the most ardent proponent of maximal entropy theory, E. T. Jaynes.\nAccording to Jaynes, statistical mechanics is epistemological. That is, probability is not out there in the world, but in here in our minds, and statistical mechanics is nothing more than maximal entropy inference applied to physics. Macroscopic properties are evidences, and the ensemble \\(\\rho\\) is the posterior after we incorporate the evidences.\nIt might be difficult to swallow Jaynes’ interpretation, as it seems obvious that entropy is objective, but epistemology is subjective. How could he explain objective entropy by subjective information? I might make this more palatable by three examples.\nIn (Jaynes 1992), he proposed the following thought experiment: Suppose we have a tank of neon gas, separated in the middle by a slab, as in the Gibbs paradox. If both sides have the same temperature and pressure, then the system has the same entropy even after we remove the slab. But suddenly, Jaynes’ demon tells us that the left side contains neon-20, while the right side contains neon-22, and thus, we were not subject to the Gibbs paradox after all! And so we have just wasted some perfectly good free energy for nothing.\nWe ask, “Wasted? How could we have wasted anything unless there is a practical way to extract energy? You say they are two distinct gases, but what difference does it make if you simply call one side ‘neon-20’ and the other ‘neon-22’?”\nSo Jaynes’ demon gives us two membranes, one permeable only to neon-20, while the other only permeable to neon-22. This allows us to put both of them in the middle, and slowly let the two gases diffuse into the middle, extracting mechanical work by the pressure on the two membranes.\nThe point of the thought-experiment is that the entropy of a system is, in a practical sense, subjective. The tank of gas might as well have maximal entropy if we don’t have the two membranes. But as soon as we have the two membranes, it expands our space of possible actions, and previously “lost” work, suddenly becomes extractable, and the entropy of the world drops.\nThe following is a more concrete example from optics. Shoot some hard balls through a bumpy region (an analogy of shining a laser light through a bumpy sheet of glass). The balls would be scattered. It would increase the entropy of the system… unless we reflect them via a corner mirror, then the balls would be reflected right back through the bumpy region, and return to the previous zero-entropy state!\nDid we violate the second law? Not so, if we think of entropy as a measure of our actionable ignorance. Without a corner mirror, we cannot use the detailed information of the scattered ball beam, and have to treat it as essentially random. However, all the detailed information is still there, in the bumpy region and in the scattered beam, and it takes a corner mirror to “unlock” the information for us.\nThus, with a corner mirror, the scattered beam still has zero entropy, but without it, the scattered beam has positive entropy.\n\n\n\n(Sethna 2021, fig. 5.27)\n\n\n\nThe old adage ‘knowledge is power’ is a very cogent truth, both in human relations and in thermodynamics.\n— E. T. Jaynes\n\nStill, there is a nagging feeling that, even if we have no corner reflector, and so the beam of light truly has increased in entropy relative to us, as long as the corner reflector is theoretically possible, then the entropy of the beam of light is still unchanged in itself. Isn’t this blatant psychologism? Haven’t we reduced objective entropy out there to subjective information in here? Surely even if nobody is around to see it, if a tank of gas explodes in a forest, entropy still makes a sound.\nResponding to this would entangle us into a whole mess of philosophical arguments about objective vs subjective probability, and observer vs observed phenomena. My quick answer is simply that knowledge isn’t magical, and even nature does not laugh at the difficulty of inference.3 If the entropy of a tank of gas appears high to us, then chances are, it would appear even higher to a car engine, for the car engine has only a few ways to interact with this tank of gas, unlike us, who have all of modern technology. A car engine can only burn up the tank of gas, but we can distill it, extract it, push it through membranes, etc. The tank of gas has higher entropy relative to the car engine – yes, the car engine has an opinion about the world, as much as we do. It has subjective beliefs about everything it can touch and burn, and we can listen to it with the language of math.\n3 Supposedly, Laplace said “Nature laughs at the difficulties of integration.”, but when I tried to hunt it down, all citations led directly to a 1953 essay, which cites an anonymous “mathematician”. I have tried searching for it in French, with no results. I think this is actually a pseudo-Laplace quote, a paper ghost, much like how people kept attributing things to pseudo-Aristotle.\n\nIt would take a physicist a long time to work out the problem and he could achieve only an approximation at that. Yet presumably the coin will stop exactly where it should. Some very rapid calculations have to be made before it can do so, and they are, presumably, always accurate. And then, just as I was blushing at what I supposed he must regard as my folly, the mathematician came to my rescue by informing me that Laplace had been puzzled by exactly the same fact. “Nature laughs at the difficulties of integration.” (Krutch 1953, 148)\n\nHowever, I’m inclined to believe that even nature does not laugh at the difficulties of integration. In fact, one of my hobbies is to “explain” natural laws as ways for nature to avoid really difficult integrations. For example, Newtonian gravity is “explained” by the Barnes–Hut algorithm that allows n-body gravity to be calculated in \\(O(n \\ln n)\\) time.What is the payoff of this long detour? I think it is to provide an intuitive feeling of the identity of physics and information. Information is physical, and physics is informational. If you are a physicist, then this allows you to invade into other fields, much like statistical mechanists “invaded” other fields like artificial intelligence and economics. If you are a mathematician or a computer scientist, then this allows you to translate intuition about physical objects into intuition about high-dimensional probability distributions and large combinatorial objects. And if you are Maxwell’s demon, then you won’t listen to me – I would gladly listen to you, since magically transforming information and physics back and forth is your entire reason of existence!\n\n\n\n\n\n\nIs entropy unique?\n\n\n\nIn the above formulation, maximal entropy inference is interpreted as how rational agents can act optimally under limited information. An alternative viewpoint argues that it is not the entropy that is fundamental, but the argmax of something that is fundamental. In this view, if we replaced the entropy function \\(S[\\cdot]\\) with a… kentropy function \\(K[\\cdot]\\), such that any scientist who uses reasons about experiments on a system using\n\\[\\rho^* = \\mathop{\\mathrm{argmax}}_{\\rho: \\rho\\text{ satisfies constraints }C}K[\\rho]\\]\nwould still satisfy certain axioms of rationality, then \\(K\\) should be as good as \\(S\\).\nIn this vein, there is a Shore–Johnson theorem which shows that if a scientist using a certain kentropy function \\(K\\) would end up satisfying these certain axioms of rationality, then\n\\[\\mathop{\\mathrm{argmax}}_{\\rho: \\rho\\text{ satisfies constraints }C}S[\\rho] = \\mathop{\\mathrm{argmax}}_{\\rho: \\rho\\text{ satisfies constraints }C}K[\\rho]\\]\nIn other words, as long as we are doing constrained maximization, the choice of the entropy doesn’t matter. In particular, the standard entropy function \\(S\\) is good enough – any constraint-maximizing rational thinker thinks as if it is doing constraint-maximizing entropy inference, so we might as well use \\(S\\) and stop worrying about alternative ones like \\(K\\).\nIF you are steeped in Bayesian epistemology, this is in the same vein as those Bayes-theological theorems proving that any rational being must use Bayes theorem for updates. (Pressé et al. 2013) Other examples include Blackwell’s informativeness theorem, Aumann’s agreement theorem, Cox’s theorem, Dutch book theorems, etc."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#mathematical-developments",
    "href": "essays/posts/statistical-mechanics/index.html#mathematical-developments",
    "title": "Statistical Mechanics",
    "section": "Mathematical developments",
    "text": "Mathematical developments\n\nFundamental theorems\n\nTheorem 1 (Liouville’s theorem) For any phase space and any Hamiltonian over it (which can change with time), phase-space volume is conserved under motion.\nFor any probability distribution \\(\\rho_0\\), if after time \\(t\\), it evolves to \\(\\rho_t\\), and a point \\(x(0)\\) evolves to \\(x(t)\\), then \\(\\rho_0(x(0)) = \\rho_t(x(t))\\).\n\nThe proof is found in any textbook, and also Wikipedia. Since it is already simple enough, and I can’t really improve upon it, I won’t.\n\nCorollary 1 (conservation of entropy) For a Hamiltonian system, with any Hamiltonian (which can change with time), for any probability distribution \\(\\rho\\) over its phase space, its entropy is conserved over time.\n\nIn particular, we have the following corollary:\n\nCorollary 2 Given any set of constraints, if the Hamiltonian preserves these constraints over time, then any constrained-maximal entropy distribution remains constrained-maximal under time-evolution.\n\nIn most cases, the constraint is of a particular form: the expectation is known. In that case, we have the following theorem:\n\nTheorem 2 (maximal entropy under linear constraints) For the following constrained optimization problem\n\\[\n\\begin{cases}\n\\max_\\rho S[\\rho] \\\\\n\\int A_1(x) \\rho(x) &= \\bar A_1 \\\\\n\\cdots &= \\cdots \\\\\n\\int A_n(x) \\rho(x) &= \\bar A_n \\\\\n\\end{cases}\n\\]\nConsider the following ansatz\n\\[\n\\rho(x) = \\frac{1}{Z(a_1, \\dots, a_n)} e^{-\\sum_i a_i A_i(x)}\n\\]\nwhere \\(Z(a_1, \\dots, a_n) = \\int e^{-\\sum_i a_i A_i(x)} dx\\), and \\(a_1, \\dots, a_n\\) are chosen such that the constraints \\(\\int A_i(x) \\rho(x) = \\bar A_i\\) are satisfied.\nIf the ansatz exists, then it is the unique solution.\n\nThe ansatz solution is what you get by Lagrangian multipliers. For a refresher, see the Analytical Mechanics#Lagrange’s devil at Disneyland. The theorem shows that the solution is unique – provided that it exists. Does it exist? Yes, in physics. If it doesn’t exist, then we are clearly not modelling a physically real phenomenon.\n\n\n\n\n\n\nTip\n\n\n\nIn physics, these are “Boltzmann distributions” or “Gibbs distributions”. In statistics, these are exponential families. Because they are everywhere, they have many names.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDefine a distribution \\(\\rho\\) as given in the statement of the theorem. That is,\n\\[\n\\rho(x) = \\frac{1}{Z(a_1, \\dots, a_n)} e^{-\\sum_i a_i A_i(x)}\n\\]\netc.\nNow, it remains to prove that for any other \\(\\rho'\\) that satisfies the constraints, we have \\(S[\\rho] \\geq S[\\rho']\\).\nBy routine calculation, for any probability distribution \\(\\rho'\\),\n\\[\nD_{KL}(\\rho' \\| \\rho) = -S[\\rho'] + \\sum_i a_i \\braket{A_i}_{\\rho'} + \\ln Z(a_1, \\dots, a_n)\n\\]\nIf \\(\\rho'\\) satisfies the given constraints, then \\(D_{KL}(\\rho' \\| \\rho) = -S[\\rho'] + \\Const\\) where the constant does not depend on \\(\\rho'\\), as long as it satisfies the constraints. Therefore, \\(S[\\rho']\\) is maximized when \\(D_{KL}(\\rho' \\| \\rho)\\) is minimized, which is exactly \\(\\rho\\).\n\n\n\nThe following proposition is often used when we want to maximize entropy in a two-step process:\n\nTheorem 3 (compound entropy) If \\(\\rho_{X,Y}\\) is a probability distribution over two variables \\((X, Y)\\), then\n\\[S[\\rho_{X,Y}] = S[\\rho_Y] + \\braket{S[\\rho_{X|y}]}_y\\]\nor more succinctly,\n\\[S_{X,Y} = S_Y + \\braket{S_{X|y}}_y\\]\n\n\n\n\n\n\n\nNotations\n\n\n\n\\(\\rho_Y\\) is the probability distribution over \\(Y\\), after we integrate/marginalize \\(X\\) away:\n\\[\n\\rho_Y(y) := \\int \\rho_{X,Y}(x,y)dx\n\\]\n\\(\\rho_{X|y}\\) is the conditional probability distribution over \\(X\\), conditional on \\(Y=y\\):\n\\[\n\\rho_{X|y}(x) := \\frac{\\rho_{X,Y}(x,y)}{\\int \\rho_{X,Y}(x,y) dx}\n\\]\n\\(\\braket{\\cdot}_y\\) is the expectation over \\(\\rho_Y\\):\n\\[\n\\braket{S_{X|y}}_y := \\int S_{X|y} \\rho_Y(y)dy\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider a compound system in ensemble \\(\\rho(x, y)\\). Its entropy is\n\\[S[\\rho] = -\\int dxdy \\; \\rho(x, y) \\ln \\rho(x, y)\\]\nWe can take the calculation in two steps:\n\\[S[\\rho] = -\\int dxdy \\; \\rho(x|y)\\rho(y) (\\ln \\rho(x|y) + \\ln  \\rho(y)) = S[\\rho_Y] + \\braket{S[\\rho_{X|y}]}_y\\]\n\n\n\nIntuitively, what does \\(S_{X,Y} = S_Y + \\braket{S_{X|y}}_y\\) mean? It means that the entropy in \\((X, Y)\\) can be decomposed into two parts: the part due to \\(Y\\), and the part remaining after we know \\(Y\\), but not yet knowing \\(X\\). In the language of information theory, the total information in \\((X, Y)\\) is equal to the information in \\(Y\\), plus the information of \\(X\\) conditional over \\(Y\\):\n\\[\nI(X, Y) = I(Y) + I(X|Y)\n\\]\n\n\nMicrocanonical ensembles\nIf the only constraint is the constant-energy constraint \\(H(x) = E\\), then the maximal entropy distribution is the uniform distribution on the shell of constant energy \\(H = E\\). It is uniform, because once we enforce \\(H(x) = E\\), there are no other constraints, and so by Theorem 2, the distribution is uniform.\nThus, we obtain the microcanonical ensemble:\n\\[\\rho_E(x) \\propto 1_{H(x) = E}\\]\nIt is sometimes necessary to deal with the “thickness” of the energy shell. In that case, \\(\\rho_E(x) \\propto \\delta(H(x) - E)\\), where \\(\\delta\\) is the Dirac delta function.\nBy Theorem 2, the microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy. In particular, if the Hamiltonian does not change over time, then any microcanonical ensemble is preserved over time. In words, if we uniformly “dust” the energy shell of \\(H(x) = E\\) with a cloud of system states, and let all of them evolve over time, then though the dust particles move about, the cloud remains exactly the same.\nMore generally, we can impose more (in)equality constraints, and still obtain a microcanonical ensemble. For example, consider a ball flying around in an empty room with no gravity. The Hamiltonian is \\(H(q, p) = \\frac{p^2}{2m}\\), and its microcanonical ensemble is \\(\\rho(q, p) \\propto \\delta(p = \\sqrt{2mE})1[p \\in \\text{the room}]\\). That is, its velocity is on the energy shell, while its position is uniform over the entire room.\nIf we want to specify the number of particles for each chemical species, then that can be incorporated into the microcanonical ensemble as well. For example, if we want the number of species \\(i\\) be exactly \\(N_{i0}\\), then we multiply \\(\\rho\\) by \\(1[N_i = N_{i0}]\\).\n\n\nCanonical ensembles\nConsider a small cup of liquid in a giant tank of chemical fluids, or a small lump of air in the whole atmosphere. These are all examples of “a small system in contact with a giant system”. In general, if we have a small system connected to a large system, then we typically don’t care about the large system, and only want to study the small system’s ensemble. How do we do that? Rigorously, we would need to first find the microcanonical ensemble for the total compound small–large system, then take an integral over all states of the large system, resulting in an ensemble over just the small system, as in\n\\[\\rho_{\\text{small}}(x) = \\int \\rho_{\\text{total}}(x, y) dy\\]\nwhere \\(x\\) ranges over the states of the small system, and \\(y\\) of the large system.\nHowever, this is difficult to perform in general, because the large system, having so many particles, has a huge state space. We cannot do it in general. However, there is an easy way out. Whenever we have a big system changing only a little bit, we can assume linearity. Whenever we have a function \\(f(x)\\) where \\(x\\) changes only a little bit around \\(x_0\\), we can assume \\(f(x) \\approx f(x_0) + f'(x_0) (x - x_0)\\). This is the trick that will allow us to solve the problem.\nAssuming that the energy of the compound system is extensive, we obtain the canonical ensemble. Assuming that the energy and volume are both extensive, we obtain the grand canonical ensemble, etc. The following table would be very useful\n\n\n\nextensive constraint\nensemble\nfree entropy\n\n\n\n\nnone\nmicrocanonical\nentropy\n\n\nenergy\ncanonical\nHelmholtz free entropy\n\n\nenergy, volume\n?\nGibbs free entropy\n\n\nenergy, particle count\ngrand canonical\nLandau free entropy\n\n\nenergy, volume, particle count\n?\n?\n\n\n\nThere are some question marks in the above table, because there are no consensus names for those question marks. What is more surprising is that there is no name for the ensemble of constrained energy and volume. I would have expected something like the “Gibbs ensemble”, but history isn’t nice to us like that. Well, then I will name it first, as the big canonical ensemble. And while we’re at it, let’s fill the last row as well:\n\n\n\nextensive constraint\nensemble\nfree entropy\n\n\n\n\nnone\nmicrocanonical\nentropy\n\n\nenergy\ncanonical\nHelmholtz free entropy\n\n\nenergy, volume\nbig canonical\nGibbs free entropy\n\n\nenergy, particle count\ngrand canonical\nLandau free entropy\n\n\nenergy, volume, particle count\ngross canonical\nEVN free energy\n\n\n\n\n\n\n\n\n\nExtensivity\n\n\n\nIn classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.\nThis is not always obvious. If we have two galaxies of stars, we can think of each as a “cosmic gas” where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of “the entropy of galaxy 1”, but only “the entropy of galaxy-compound 1-2”.\nIn statistical mechanics, extensivity means a certain property of each subsystem is unaffected by the state of the other subsystems, and the total is the sum of them. So for example, if \\(A\\) is an extensive property, then it means\n\\[\nA(x_1, \\dots, x_n) = A_1(x_1) + \\dots + A_n(x_n)\n\\]\nLike most textbooks, we assume extensivity by default, although as we noted in Classical Thermodynamics and Economics, both classical thermodynamics and statistical mechanics do not require extensivity. We assume extensivity because it is mathematically convenient, and good enough for most applications.\n\n\nIn the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble)\n\nTheorem 4 If the two systems are in energy-contact, and energy is conserved, and energy is extensive, and the compound system is in a microcanonical ensemble, then the small system is in the canonical ensemble\n\\[\n\\rho(x) \\propto e^{-\\beta H(x)}\n\\]\nwhere \\(\\beta\\) is the marginal entropy of energy of the large system:\n\\[\\beta := \\partial_E S[\\rho_{bath, E}]\\]\nSimilarly, if the two systems are in energy-and-particle-contact, then the small system is in the grand canonical ensemble\n\\[\n\\rho(x) \\propto e^{-(\\beta H(x) + (-\\beta \\mu) N(x))}\n\\]\nwhere \\(-\\beta\\mu\\) is the marginal entropy of particle of the large system:\n\\[-\\beta\\mu := (\\partial_N S[\\rho_{bath, E, N}])_{E}\\]\nMost generally, if the two systems are in \\(q_1, \\dots, q_m\\) contact, and \\(q_1, \\dots, q_m\\) are conserved and extensive quantity, then\n\\[\\rho(x) \\propto e^{-\\sum_i p_i q_i(x)}\\]\nwhere \\(p_i = (\\partial_{q_i} S[\\rho_{bath, q}])_{q}\\) is the marginal entropy of \\(q_i\\) of the large system.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the case for the canonical ensemble. The other cases are similar.\nSince the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:\n\\[\\max_\\rho S[\\rho]\\]\nBy Theorem 3,\n\\[S = S_{\\text{system}}(E_{\\text{system}}) + \\braket{S_{bath|system}(E_{total} - E_{\\text{system}})}_{\\text{system}}\\]\nSince the bath is so much larger than the system, we can take just the first term in its Taylor expansion:\n\\[S_{bath|system}(E_{total} - E_{\\text{system}}) = S_{\\text{bath}}(E_{total}) - \\beta E_{\\text{system}}\\]\nwhere \\(E_{total}\\) is the total energy for the compound system, \\(\\beta = \\partial_E S_{\\text{bath}}|_{E = E_{total}}\\) is the marginal entropy per energy, and \\(E_{\\text{system}}\\) is the energy of the system.\nThis gives us the linearly constrained maximization problem of\n\\[\\max_{\\rho_{\\text{system}}} (S_{\\text{system}} - \\beta \\braket{E_{\\text{system}}}_{\\rho_{\\text{system}}})\\]\nand we apply Lagrange multipliers to finish the proof.\n\n\n\n\n\n\n\n\n\nExtensivity, again\n\n\n\nExtensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing \\(S_{\\text{bath}}(E)\\), instead of \\(S_{\\text{bath}}(E, E_{\\text{system}})\\), requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.\n\\(S_{\\text{bath}}\\) is the logarithm of the surface area of the energy shell \\(H_{\\text{bath}} = E_{\\text{bath}}\\). By extensivity, \\(H(x_{\\text{bath}}, x_{\\text{system}}) = H_{\\text{bath}}(x_{\\text{bath}}) + H_{\\text{system}}(x_{\\text{system}})\\), so the energy shells of the bath depends on only \\(E_{\\text{bath}}\\), not \\(E_{\\text{system}}\\).\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe proof showed something extra: If the small system is in distribution \\(\\rho\\) that does not equal to the equilibrium distribution \\(\\rho_B\\), then the total system’s entropy is\n\\[S = S_{max} - D_{KL}(\\rho \\| \\rho_B)\\]\nwhich is related to of Sanov’s theorem and large deviation theory, though I don’t know how to make this precise.\n\n\n\n\n\n\n\n\nEnthalpic ensemble\n\n\n\nWhat if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is thermally insulating. Notice that in this case, the small system still exchanges energy with the large system via \\(d\\braket{E} = -Pd\\braket{V}\\). We don’t have \\(E = -PdV\\), because the small system might get unlucky. During a moment of weakness, all its particles has abandoned their frontier posts, and the bath has taken advantage of this by encroaching on its land. The system loses volume by \\(\\delta V\\), without earning a compensating \\(\\delta E = P \\delta V\\). In short, the thermodynamic equality \\(E = -PdV\\) is inexact in statistical mechanics, and only holds true on the ensemble average.\nIn this case, because pressure is a constant, we have \\(d(E + PV) = 0\\), and so we have the enthalpic ensemble \\(\\rho \\propto e^{-\\beta H}\\), where \\(H := E + PV\\) is the enthalpy4.\nSpecifically, if you work through the same argument, you would end up with the following constrained maximization problem:\n\\[\n\\begin{cases}\n\\max_{\\rho_{\\text{system}}} (S_{\\text{system}} - \\beta \\braket{E_{\\text{system}}}_{\\rho_{\\text{system}}} - \\beta P \\braket{V}) \\\\\n\\braket{E_{\\text{system}}} + P\\braket{V_{\\text{system}}} = \\Const\n\\end{cases}\n\\]\nyielding the enthalpic ensemble (or the isoenthalpic-isobaric ensemble).\n\n\n4 Sorry, I know this is not the Hamiltonian, but we are running out of letters to use.\n\nFree entropies\nJust like in thermodynamics, it is useful to consider free entropies, which are the convex duals of the entropy:\n\nHelmholtz free entropy: \\(f[\\rho] := S[\\rho] - \\beta \\braket{E} = \\int dx \\; \\rho(x) (-\\ln \\rho(x) - \\beta E(x))\\).\nGibbs free entropy: \\(g[\\rho] := S[\\rho] - \\beta \\braket{E} - \\beta P \\braket{V}\\).\nLandau free entropy: \\(\\omega[\\rho] := S[\\rho] - \\beta \\braket{E} - \\beta (-\\mu) \\braket{N}\\). Note that the sign of \\((-\\mu)\\) is not a typo. It is simply that 19th-century chemists have messed up the sign convention, like how Benjamin Franklin messed up the sign convention of electric charge.\n\nEtc. Of those, we would mostly use the Helmholtz free energy, so I will write it down again:\n\\[\nf[\\rho] := S[\\rho] - \\beta \\braket{E} = \\int dx \\; \\rho(x) (-\\ln \\rho(x) - \\beta E(x))\n\\]\n\nTheorem 5 (chain rule for free entropies) \\(f_X = S_Y + \\braket{f_{X|y}}_y\\), and similarly \\(g_X = S_Y + \\braket{g_{X|y}}_y\\), and similarly for all other free entropies.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\n\\begin{aligned}\n  f_X &= S_X - \\beta \\braket{E}_x  \\\\\n  &= S_Y + \\braket{S_{X|y}}_y - \\beta \\braket{\\braket{E}_{x \\sim X|y}}_y \\\\\n  &= S_Y + \\braket{f_{X|y}}_y\n\\end{aligned}\n\\]\n\n\n\nA common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has at least 4 characterizations. “Muscle memory” in statistical mechanics would allow you to nimbly applying the most suitable one for any occasion.\n\nTheorem 6 (4 characterizations of the canonical ensemble)  \n\n(total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in energy-contact with an energy bath that satisfies \\(\\partial_E S_{\\text{bath}} = \\beta\\), under the constraint that \\(E + E_{\\text{bath}}\\) is fixed.\n(entropy under mean energy constraint) Let \\(E_0\\) be a real number, and let \\(\\beta\\) be the unique solution to \\(\\int dx \\; e^{-\\beta E(x)} = E_0\\). A system maximizes its entropy under constraint \\(\\braket{E} = E_0\\) when it assumes the canonical ensemble with \\(\\beta\\).\n(Boltzann’s thermodynamic limit argument): Take \\(N\\) copies of a system, and connect them by energy-contacts. Inject the system with total energy \\(NE_0\\), and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of \\(N\\to \\infty\\), the distribution of a single system is the canonical distribution with \\(\\beta\\) that is the unique solution to \\(\\int dx \\; e^{-\\beta E(x)} = E_0\\).\n(free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At the optimal distribution \\(\\rho^*\\), the maximal Helmholtz free entropy is \\(f[\\rho^*] = \\ln Z\\), where \\(Z = \\int dx \\; e^{-\\beta E(x)}\\) is the partition function.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nWe already proved this.\nUse the Lagrange multiplier.\nIsolate one system, and treat the rest as an energy-bath.\n\\(f[\\rho] = \\ln Z - D_{KL}(\\rho \\| \\rho_B)\\).\n\n\n\n\n\n\nThe partition function\nWhen the system is in a canonical ensemble, we can define a convenient variable \\(Z = \\int dx\\; e^{-\\beta E(x)}\\) called the partition function. As proven in Theorem 6, the partition function is equal to \\(e^f\\), where \\(f\\) is the Helmholtz free entropy of the canonical ensemble.\n\nTheorem 7 (the partition function is the cumulant generating function of energy) Let a system be in canonical ensemble with inverse temperature \\(\\beta\\), and let \\(K(t) := \\ln \\braket{e^{tE}}\\) be the cumulant generating function of its energy, then \\[K(t) = \\ln Z(\\beta-t) - \\ln Z(\\beta)\\]\nIn particular, the \\(n\\)-th cumulant of energy is\n\\[\\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\\partial_\\beta)^n (\\ln Z)\\]\nA similar proposition applies for the other ensembles and their free entropies.\n\nThe proof is by direct computation.\nFor example, the first two cumulants are the mean and variance:\n\\[\\braket{E} = (-\\partial_\\beta) (\\ln Z), \\quad \\mathrm{Var}(E) = \\partial_\\beta^2 (\\ln Z)\\]\nTypical systems are made of \\(N\\) particles, where \\(N\\) is large, and that these particles are only weakly interacting. In this case, the total Helmholtz free entropy per particle converges at the thermodynamic limit of \\(N \\to \\infty\\):\n\\[\n\\lim_N \\frac 1N \\ln Z \\to \\bar f_\\beta\n\\]\nThus, for large but finite \\(N\\), we have\n\\[\\braket{E} \\approx -N \\partial_\\beta \\bar f_\\beta, \\quad \\mathrm{Var}(E) = N\\partial_\\beta^2 \\bar f_\\beta\\]\nIn particular, the relative fluctuation scales like \\(\\frac{\\sqrt{\\mathrm{Var}(E)}}{\\braket{E}} \\sim N^{-1/2}\\).\n\n\nConditional entropies\nGiven any two random variable \\(X, Y\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\) by some function \\(h\\), such that \\(Y = h(X)\\). If we know \\(X\\), we would know \\(Y\\), but it is not so conversely, as multiple \\(X\\) may correspond to the same \\(Y\\). Typically, we use \\(Y\\) as a “summary statistic” for the more detailed, but more complicated \\(X\\). For example, we might have multiple particles in a box, such that \\(X\\) is their individual locations, while \\(Y\\) is their center of mass.\n\nTheorem 8 (conditional entropy) Given any random variable \\(X\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\), and some constraints \\(c\\) on \\(X\\), if \\(X\\) is the distribution that maximizes entropy under constraints \\(c\\), with entropy \\(S_X^*\\), then the observable \\(Y\\) is distributed as\n\\[\\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}, \\quad e^{S_X^*} = \\int dy\\; e^{S_{X|y}^*}\\]\nwhere \\(S_{X|y}^*\\) is the maximal entropy for \\(X\\) conditional on the same constraints, plus the extra constraint that \\(Y = y\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy assumption, \\(X\\) is the unique solution to the constrained optimization problem\n\\[\n\\begin{cases}\n    \\max S_X \\\\\n    \\text{constraints on $x$}\n\\end{cases}\n\\]\nBy Theorem 3, the problem is equivalent to:\n\\[\n\\begin{cases}\n    \\max S_Y + \\braket{S_{X|y}}_{y\\sim Y} \\\\\n    \\text{constraints on $x$}\n\\end{cases}\n\\]\nNow, we can solve the original problem in a two-step process: For each possible observable \\(y\\sim Y\\), we solve an extra-constrained problem:\n\\[\n\\begin{cases}\n    \\max S_{X|y} \\\\\n    \\text{original constraints on $x$} \\\\\n    \\text{$x$ must be chosen such that the observable $Y = y$}\n\\end{cases}\n\\]\nThen, each such problem gives us a maximal conditional5 entropy \\(S_{X|y}^*\\), and we can follow it up by solving for \\(Y\\) with\n\\[\\max\\left(S_Y + \\braket{S_{X|y}^*}_{y \\sim Y}\\right)\\]\nAgain, the solution is immediate once we see it is just the KL-divergence:\n\\[S_Y + \\braket{S_{X|y}^*}_{y \\sim Y} = - \\int dy \\; \\rho_Y(y) \\ln\\frac{\\rho_Y(y)}{e^{S_{X|y}^*}} = \\ln Z - D_{KL}(\\rho_Y \\| \\rho_Y^*)\\]\nwhere\n\\[Z = \\int dy\\; e^{S_{X|y}^*}, \\quad \\rho_Y^*(y) = \\frac{e^{S_{X|y}^*}}{Z}\\]\nAt the optimal point, the entropy for \\(X\\) is maximized at \\(S_X^* = \\ln Z - 0\\), so \\(Z = e^{S_X^*}\\).\n\n\n\n5 If you’re a pure mathematician, you can formalize this using measure disintegration.\n\n\n\n\n\nderiving the canonical ensemble yet again\n\n\n\nConsider a small system with energy states \\(E_1, E_2, \\dots\\) and a large bath system, in energy contact. We can set \\(X\\) to be the combined state of the whole system, and \\(Y\\) to be the state of the small system. Once we observe \\(y\\), we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system: \\[S_{X|y}^* = S_{\\text{bath}} = S_{\\text{bath}}(E_{total}) - \\beta E_y\\]\nConsequently, the distribution of the small system is \\(\\rho_Y(y) \\propto e^{-\\beta E_y}\\), as we expect.\nA similar calculation gives us the grand canonical ensemble, etc.\n\n\n\nTheorem 9 (conditional free entropy) Given any random variable \\(X\\), and an “observable” variable \\(Y\\) that is determined by \\(X\\), and some constraints \\(c\\) on \\(X\\), if \\(X\\) is the distribution that maximizes Helmholtz free entropy under constraints \\(c\\), with Helmholtz free entropy \\(f_X^*\\), then the observable \\(Y\\) is distributed as \\[\\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}, \\quad e^{f_X^*} = \\int dy\\; e^{f_{X|y}^*}\\]\nwhere \\(f_{X|y}^*\\) is the maximal Helmholtz free entropy for \\(X\\) conditional on the same constraints, plus the constraint that \\(Y = y\\).\nSimilarly for Gibbs free entropy, and all other free entropies.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst note that \\(f_X = S_Y + \\braket{f_{X|y}}_y\\), then argue in the same way."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#kinetic-gas-theory",
    "href": "essays/posts/statistical-mechanics/index.html#kinetic-gas-theory",
    "title": "Statistical Mechanics",
    "section": "Kinetic gas theory",
    "text": "Kinetic gas theory\nKinetic gas theory is the paradigm for pre-1930 statistical mechanics. Boltzmann devoted his best years to kinetic gas theory. The connection between kinetic gas theory and statistical mechanics was so strong that it was often confused as one. Modern statistical mechanics has grown to be so much more than this, so we will only settle for deriving the van der Waals equation. This strikes a balance between triviality (the ideal gas equation could be derived in literally two lines) and complication (Boltzmann’s monumental Lectures on Gas Theory has 500 pages (Boltzmann 2011)).\nTo review, the van der Waals gas equation is\n\\[P = \\frac{N/\\beta}{V- bN} - \\frac{cN^2}{V^2}\\]\nwhere \\(b, c\\) are real numbers that depend on the precise properties of the gas molecules. The term \\(V - bN\\) accounts for the fact that each gas molecule excludes some volume, so that, as \\(N\\) grows, it corrects for the ideal gas pressure \\(P_{ideal}\\) by \\(\\sim P_{ideal}\\frac{bN}{V}\\). The term \\(\\frac{cN^2}{V^2}\\) accounts for overall interaction energy between gas molecules. Suppose the interaction is overall attractive, then we would have \\(c &gt; 0\\), and otherwise \\(c &lt; 0\\).\n\nIdeal gas\nConsider a tank of ideal gas consisting of \\(N\\) point-masses, flying around in a free space with volume \\(V\\). The tank of gas has inverse temperature \\(\\beta\\), so its phase-space distribution is\n\\[\n\\rho(q_{1:N}, p_{1:N}) = \\prod_{i\\in 1:N} \\rho(q_i, p_i), \\quad \\rho(q, p) = \\underbrace{\\frac{1}{V}}_{\\text{free space}} \\times \\underbrace{\\frac{e^{-\\beta \\frac{\\|p_i\\|^2}{2m}}}{(2\\pi m/\\beta)^{3/2}}}_{\\text{Boltzmann momentum distribution}}\n\\]\nThe total energy of the gas has no positional term, so it is all due to momentum. Because the momenta coordinates \\(p_{1,x}, p_{1,y}, \\dots, p_{N,y}, p_{N,z}\\) do not interact, their kinetic energies simply sum, giving\n\\[\nU = 3N \\times \\int_{\\mathbb{R}}dp\\; \\frac{p^2}{2m} \\frac{e^{-\\frac{p^2}{2m/\\beta}}}{\\sqrt{2\\pi m/\\beta}} = \\frac{3N}{2\\beta}\n\\]\nThis is the same as Boltzmann’s derivation so far. However, although entropy is exactly defined when there are only finitely or countably many possible states, as \\(\\sum_{j \\in \\mathbb{N}} -p_j \\ln p_j\\), this is not so when state space is uncountably large, like \\(\\mathbb{R}^{6N}\\). When Boltzmann encountered the issue, he solved it by discretizing the phase space into arbitrary but small cubes. The effect is that he could rederive the ideal gas laws, but the entropy has an additive constant that depends on the exact choice of the cube size. This was not a problem for Boltzmann, who was trying to found classical thermodynamics upon statistical mechanics, and in classical thermodynamics, entropy does have an indeterminate additive constant.\nLater, Planck in his derivation of the blackbody radiation law, used the same trick. Ironically, Planck did not believe in atoms nor quantized light, but he did make the correct assumption that there is a natural unit of measurement for phase space area, which he called \\(h\\), and which we know as Planck’s constant. (Duncan and Janssen 2019, chap. 2).\nFollowing Planck, we discretize the phase space into little cubes of size \\(h^{3N}\\), and continue:\n\\[\n\\begin{aligned}\n    S &= -\\sum_{i \\in\\text{Little cubes}} p_i \\ln p_i \\\\\n    &\\approx -\\sum_{i \\in\\text{Little cubes}} (\\rho(i) h^{3N}) \\ln (\\rho(i) h^{3N}) \\\\\n    &\\approx -\\int_{\\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \\; \\rho(p_{1:N}, q_{1:N}) \\ln (\\rho(p_{1:N}, q_{1:N}) h^{3N}) \\\\\n    &= -\\int_{\\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \\; \\rho(p_{1:N}, q_{1:N}) \\ln \\rho(p_{1:N}, q_{1:N}) - 3N \\ln h \\\\\n    &= -\\underbrace{N\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q)}_{\\text{non-interacting particles}} - 3N \\ln h\n\\end{aligned}\n\\]\nNow, the entropy of a single atom \\(\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q)\\) factors again into one position space and three momentum spaces:\n\\[\n\\begin{aligned}\n    -\\int_{\\mathbb{R}^{6}} dpdq \\; \\rho(p, q) \\ln \\rho(p, q) &= -\\int_{\\mathbb{R}^3} dq \\rho(q) \\ln \\rho(q) - \\sum_{i = x, y, z} \\int_{\\mathbb{R}} dp_i \\ln \\rho(p_i) \\\\\n    &= \\ln V + 3 \\times \\underbrace{(\\text{entropy of }\\mathcal N(0, m/\\beta))}_{\\text{check Wikipedia}} \\\\\n    &= \\ln V + \\frac 32 \\ln(2\\pi m/\\beta) + \\frac 32 \\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nDoes this remind you of our previous discussion about how differential entropy is ill-defined? Finally that discussion is paying off! The choice of a natural unit of measurement in phase space is equivalent to fixing a natural base measure on phase space, such that differential entropy becomes well-defined.\n\n\nThe above is not yet correct, because permuting the atoms does not matter. That is, we have grossly inflated the state space. For example, if \\(N = 2\\), then we have counted the state \\((q_1, p_1, q_2, p_2)\\), then \\((q_2, p_2, q_1, p_1)\\), as if they are different, but they must be counted as the same. We must remove this redundancy by “quotienting out” the permutation group over the particles. The effect is dividing the phase space by \\(\\ln N!\\):\n\\[\n\\begin{aligned}\n    \\frac SN &= \\ln V + \\frac 32 \\ln(2\\pi m/\\beta) + \\frac 32 - 3 \\ln h - \\underbrace{\\frac 1N \\ln N!}_{\\text{Stirling's approximation}} \\\\\n    &= \\ln\\left[\\frac{V}{N} \\left(\\frac{2\\pi m}{\\beta h^2}\\right)^{\\frac 32}\\right] + \\frac 52\n\\end{aligned}\n\\]\ngiving us the Sackur–Tetrode formula:\n\\[\nS(U, V, N) = \\ln\\left[\\frac{V}{N} \\left(\\frac{4\\pi m U}{3N h^2}\\right)^{\\frac 32}\\right] + \\frac 52\n\\]\nAll other thermodynamic quantities can then be derived from this. For example, the pressure is\n\\[P = \\beta^{-1}(\\partial_V S)_{U, N} = \\frac{1}{\\beta V}\\]\nmore conventionally written as \\(PV = \\beta^{-1} = Nk_BT\\), the ideal gas equation, where we have re-inserted the Boltzmann constant in respect for tradition.\n\n\n\n\n\n\nHow Tetrode measured \\(h\\)\n\n\n\nIn early 1900s, Walther Nernst proposed the third law of thermodynamics. The history is rather messy, but suffice to say that the version we are going to care about says, “At the absolute zero of temperature the entropy of every chemically homogeneous solid or liquid body has a zero value.”. In support, he studied experimentally the thermodynamic properties of many materials at temperatures approaching absolute zero. He had a hydrogen liquefier and could reach around \\(20 \\ut{K}\\).\nWorking on the assumption that \\(S = 0\\) in any chemical at \\(T = 0\\), he could measure the entropy of any substance by slowing heating up a substance (or cooling down), measuring its heat capacity at all temperatures, then take an integral:\n\\[\nk_B S = \\int \\frac{CdT}{T}\n\\]\nThe low-temperature data for mercury was the most available (mercury was also the substance with which Onnes discovered superconductivity). However, mercury is mostly in a liquid form at low temperatures. Fortunately, the latent heat of vaporization \\(\\Delta L\\) can be measured, and then we can get\n\\[\nS_{\\text{vapor}} = S_{\\text{liquid}} + \\frac{\\Delta L}{k_BT}\n\\]\nBack then, \\(k_B = \\frac{\\text{Gas constant}}{\\text{Avogadro constant}}\\), and the \\(S_{\\text{liquid}}, \\Delta L\\) of mercury were all measured, so combining these, Tetrode calculated a value of \\(h\\) that is within \\(30\\%\\) of modern measurement. (Grimus 2013)\n\n\n\n\nIdeal gas (again)\nWe rederive the thermodynamic properties of ideal monoatomic gas via Helmholtz free entropy.\n\\[Z = \\int e^{-\\beta E} = \\underbrace{\\frac{1}{N!}}_{\\text{identical particles}} \\underbrace{V^N}_{\\text{position}} \\underbrace{(2\\pi m/\\beta )^{\\frac 32 N}}_{\\text{momentum}}\\]\nIn typical textbooks, they use the Helmholtz free energy, which is defined as\n\\[\nF = -\\beta^{-1} \\ln Z = -\\beta^{-1} N \\left(\\ln \\frac{V}{N} + \\frac 32 \\ln \\frac{2\\pi m}{\\beta} + \\frac{\\ln N}{2N}\\right)\n\\]\nBy the same formula from classical thermodynamics,\n\\[\nd\\ln Z = -\\braket{E}d\\beta + \\beta\\braket{P} dV \\implies\n\\begin{cases}\n    \\braket{E}   &= \\frac 32 \\frac{N}{\\beta} \\\\\n    \\braket{P}V  &= \\frac{N}{\\beta}\n\\end{cases}\n\\]\nNotice how the \\(\\ln N!\\) part simply does not matter in this case.\n\n\nHard ball gas (dilute gas limit)\nIn order to refine the approach, we need to account for two effects.\n\nEach particle takes up finite volume, which forces the total volume of positional space to be smaller than \\(V^N\\).\nParticle pairs have interactions, which changes the Boltzmann distribution.\n\nThe first effect can be modelled by assuming each atom is a hard ball of radius \\(r\\). The particles still have no interaction except that their positions cannot come closer than \\(2r\\).\nBecause there is no potential energy, the Boltzmann distribution on momentum space is the same, and so the Helmholtz free entropy \\(\\ln Z\\) still splits into the sum of positional entropy and momentous entropy. The momentum part is still \\(\\frac 32 N \\ln\\frac{2\\pi}{\\beta m}\\), as the hard balls do not interfere with each other’s momentum, but the position part is smaller, because the balls mutually exclude each other.\nLet \\(a = 8V_{ball} = \\frac{32}{3}\\pi r^3\\) be a constant for the gas.\nTo measure the volume of the diminished position space, we can add one hard ball at a time. The first hard ball can take one of \\(V\\) possible positions, as before. The next ball’s center cannot be within \\(2r\\) of the center of the first ball, so its position can only take one of \\((V - a)\\) positions, where \\(a = 8V_{ball} = \\frac{32}{3}\\pi r^3\\) is a constant that depends on the shape of the hard balls. We continue this argument, obtaining the total volume in position space:\n\\[V(V- a) \\cdots (V - (N-1)a) \\approx V^N e^{0 -\\frac{a}{V}-2\\frac{a}{V} -\\dots -(N-1)\\frac{a}{V}} \\approx V^N\\left(1- \\frac{N^2 a}{2V} \\right)\\]\nThis gives us\n\\[\\braket{E} = \\frac{3N}{2\\beta}, \\quad \\braket{P}V \\approx \\frac N\\beta \\left(1 + \\frac{a N}{2V}\\right) \\approx \\frac{N/\\beta}{V - \\frac a2 N}\\]\nThe second equation is the van der Waals equation when the term \\(c = 0\\), meaning there is neither attraction nor repulsion between particles.\n\n\n\n\n\n\nvirial expansion\n\n\n\nIn the above derivation, we are assuming that only pairwise exclusion matters. That is, we ignore the possibility that three or more balls may simultaneously intersecting each other. We can make a more accurate counting argument via the inclusion-exclusion principle, which would lead us to a virial expansion for gas.\nSpecifically, if the balls \\(A, B\\) are intersecting, which has probability \\(a/V\\), and \\(B, C\\) are also intersecting, also with probability \\(a/V\\), then \\(A, C\\) are quite likely to be also intersecting, with probability much higher than \\(a/V\\). Therefore, if we have excluded the cases where \\(A, B\\) are intersecting by subtracting with \\(a/V\\), and the cases where \\(B, C\\) are intersecting by subtracting another \\(a/V\\), then we should be subtracting with something less than \\(a/V\\). The cluster expansion principle makes this precise. Unfortunately, it requires some difficult combinatorics. The interested reader should study (Andersen 1977).\n\n\n\n\nSoft ball gas (high temperature and dilute gas limit)\nIn the above derivation, we got one part of van der Waals equation right – the part where particles take up space. However, we have not yet accounted for the force between particles. We expect that if the particles attract each other, then \\(P\\) should be smaller, and if the particles repel each other, then \\(P\\) should be larger.\nLet’s assume the gas is made of balls that has a hard core and a soft aura. That is, they repulse or attract each other at a distance, and when a pair comes too close. We also assume the force law depends only on the distances between particles.\nThat is, we can write such a system as having a gas potential energy \\(V(q_1, \\dots, q_N) = \\sum_{i &lt; j} V(\\|q_i - q_j\\|)\\). To enforce the hard core, we should have \\(V(r) = \\infty\\) when \\(r \\in [0, r_0]\\).\n\n\n\nExample potential energy field. This is the Lennard-Jones potential, with a hard (not perfectly hard, but hard enough!) exclusive core, a soft repelling middle, and an attraction when far away. Figure from Wikimedia Commons\n\n\nNow, the partition function becomes\n\\[\nZ = \\int e^{-\\beta\\sum_i \\frac{p_i^2}{2m} - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dqdp\n\\]\nThe momentum part is still the same \\((2\\pi/\\beta m)^{\\frac 32 N}\\), but the position part is more difficult now. Still, we hope it will be close to \\(V^N\\).\nThat is, we need to calculate:\n\\[Z = \\underbrace{V^N (2\\pi/\\beta m)^{\\frac 32 N} \\frac{1}{V^N}}_{\\text{ideal gas}} \\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq\\]\nThe integral \\(\\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq\\) can be evaluated piece-by-piece: \\[\n\\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq = \\int dq_1 \\left(\\int dq_2 \\; e^{-\\beta V(\\| q_1 - q_2 \\|)} \\left(\\int dq_3 \\; e^{-\\beta (V(\\| q_1 - q_3 \\|) + V(\\| q_2 - q_3 \\|))} \\cdots\\right)\\right)\n\\]\nBecause the chamber is so much larger than the molecular force-field, it is basically infinite. So for almost all of \\(q_1\\) (except when it is right at the walls of the chamber), \\(\\int dq_2 \\; e^{-\\beta V(\\| q_1 - q_2 \\|)} \\approx V - \\delta\\), where \\(\\delta\\) is some residual volume:\n\\[\\delta := \\int_{V} dq_2 \\; (1 - e^{-\\beta V(\\|q_2 \\|)})\\]\nFurthermore, because we are dealing with a dilute gas, the higher-order interactions don’t matter (see previous remark about the virial expansion). Therefore, the integral \\[\\int_{V} dq_3 \\; e^{-\\beta (V(\\| q_1 - q_3 \\|) + V(\\| q_2 - q_3 \\|))} \\approx \\int_{V_1 \\cup V_2 \\cup V_3} dq_3 \\; e^{-\\beta (V(\\| q_1 - q_3 \\|) + V(\\| q_2 - q_3 \\|))} \\]\nwhere \\(V_1\\) is the “turf” of particle \\(1\\), and \\(V_2\\) is the turf of particle \\(2\\), and \\(V_3\\) is the rest of the volume. Because the gas is dilute, we have basically \\(V_1\\) disjoint from \\(V_2\\), giving us\n\\[\\approx \\sum_{j = 1, 2, 3}\\int_{V_j} dq_3\\; e^{-\\beta V(\\| q_j - q_3 \\|)} \\approx V - 2\\delta\\]\nTogether, we have \\[\\int_{V^N} e^{ - \\beta\\sum_{i &lt; j}V(\\|q_i - q_j\\|)} dq  \\approx V(V-\\delta) \\cdots(V - (N-1)\\delta)\\]\nGiving us \\[\\ln Z \\approx \\ln Z_{\\text{ideal}} - \\frac{N^2 \\delta}{2V}\\]\nIt remains to calculate the residual volume. It has two parts, one due to the hard core and one due to the soft halo: \\[\\delta = \\int_{\\|q_2 \\| \\leq r_0} dq_2 \\; (1 - e^{-\\infty}) + \\int_{\\|q_2 \\| &gt; r_0} dq_2 \\; (1 - e^{-\\beta V(\\|q_2\\|)})\\]\nThe first part is just \\(a\\), as calculated previously. The second part depends on the exact shape of the potential well. However, when temperature is high, \\(\\beta\\) would be very small, so the second part is approximately \\(\\int dq_2 (\\beta V)\\), which is a constant times \\(\\beta\\).\nThus, we have \\[\\ln Z \\approx \\ln Z_{\\text{ideal}} - \\frac{N^2}{V}(a + b \\beta)\\]\nfor some constants \\(a, b\\). This gives us the van der Waals equation: \\[\\braket{P} V = \\frac{N}{\\beta} + \\frac{N^2}{\\beta V}a + \\frac{N^2}{V} b\\]"
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#other-classical-examples",
    "href": "essays/posts/statistical-mechanics/index.html#other-classical-examples",
    "title": "Statistical Mechanics",
    "section": "Other classical examples",
    "text": "Other classical examples\n\nCountably many states\nIn a surprising number of applications, we have a single system in an energy bath. The system has finitely many, or countably infinitely many, distinguishable states, each with a definite energy: \\(E_0 \\leq E_1 \\leq E_2 \\leq \\cdots\\). In particular, this covers most of the basic examples from quantum mechanics. In such a system, the probability of being in state \\(i\\) is \\(p_i = \\frac 1Z e^{-\\beta E_i}\\) where\n\\[\nZ = \\sum_i e^{-\\beta E_i}\n\\]\nBecause I don’t like sections that are literally two paragraphs long, I will reformulate this as multinomial regression in mathematical statistics.\nIn the problem of classification, we observe some vector \\(\\vec X\\), and we need to classify it into one of finitely many states \\(\\{1, 2, \\dots\\}\\). With multinomial regression, we construct one vector \\(\\vec b_i\\) for each possible state \\(i\\), and then declare that the probability of being in state \\(k\\) is\n\\[\nPr(i | X) = \\frac{e^{-\\vec X \\cdot \\vec b_i}}{Z(X)}, \\quad Z(\\vec X) = \\sum_j e^{-\\vec b_j \\cdot \\vec X}\n\\]\nTo make the parallel clearer:\n\\[\n\\begin{aligned}\n\\text{log probability} & & \\text{observable } & &\\text{ feature} & & \\text{normalization constant} \\\\\n    \\ln p(i | \\beta) &=& -\\beta & \\; & E_i & & - \\ln Z \\\\\n    \\ln Pr(i | \\vec X) &=&    -\\vec X     & \\cdot & \\vec b_i & & - \\ln Z\n\\end{aligned}\n\\]\nWe can make the analogy exact by adding multiple observables. Specifically, if we solve the following constrained optimization problem\n\\[\n\\begin{cases}\n\\max S \\\\\n\\braket{\\vec b} = \\vec b_0\n\\end{cases}\n\\]\nthen the solution is a multinomial classifier, with \\(\\vec X\\) playing the role of \\(\\beta\\).\nInterpreting the physics as statistics, we can think of \\(\\beta\\) as an “observable”. It is as if we are asking the physical system “What state are you in?” but we can only ask a very crude question “What is your energy on average?” Knowing that, we can make a reasonable guess by using the maximal entropy compatible with that answer.\nInterpreting the statistics as physics, we can think of the observable \\(\\vec X\\) as “entropic forces”, trying to push the system towards the distribution of maximal entropy. At the equilibrium of zero entropic force, we have a multinomial classifier. This is the prototypical idea of energy-based statistical modelling.\n\n\nFluctuation by \\(N^{-1/2}\\)\nSuppose we have several tanks of oxygen gas that can exchange energy. They are in a microcanonical ensemble. Now, if we measure the total energy in the first tank, we would get a value \\(E_1\\). We sample it again after a while, and we would get another value. Averaging them, we would get \\(\\braket{E_1}\\), which ought to match the prediction by classical thermodynamics. However, if thermodynamics is the theory of the average, then to go beyond it, statistical mechanics must predict the variance as well.\nIn Theorem 7, We had already seen that the partition function generates the mean, the variance, and all other terms. Here we expand on this.\nTake several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting. The system has total entropy\n\\[S = \\sum_i S_i(E_i, A_i)\\]\nwhere \\(A_i\\) stand for the other state variables we don’t care about, because they are held constant. Now, there is a single constraint of constant total energy:\n\\[E = \\sum_i E_i\\]\nIn the thermodynamical limit, the compound system reaches the maximal entropy state \\(E_1^*, \\dots, E_n^*\\), which solves the following constrained maximization\n\\[\n\\begin{cases}\n    \\max \\sum_i S_i(E_i, A_i)\\\\\n    E = \\sum_i E_i\n\\end{cases}\n\\]\nBy calculus, at the optimal point, all systems satisfy\n\\[\n(\\partial_{E_i} S_i)_{A_i} = \\beta\n\\]\nfor some number \\(\\beta\\). This is the zeroth law of thermodynamics.\nHowever, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate. Write the fluctuation vector as\n\\[Y = (\\Delta E_1, \\dots, \\Delta E_n)\\]\nwhich satisfies the constraint \\(\\sum_i \\Delta E_i = 0\\).\nSuppose we observe the fluctuation vector to be a certain value \\(Y = y\\), then by Theorem 8,\n\\[\\rho_Y(y) \\propto e^{S^*_{X|y}}\\]\nwhere \\(S^*_{X|y}\\) is the entropy of the compound system, given \\(Y = y\\). For small fluctuations, we do a Taylor expansion:\n\\[S^*_{X|y} = \\sum_i S_i(E_i^*) + \\underbrace{(\\partial_{E_i} S_i)_{A_i}}_{\\text{$=\\beta$}} \\Delta E_i + \\frac 12 (\\partial_{E_i}^2 S_i)_{A_i} (\\Delta E_i)^2 + \\cdots\\]\nSince \\(\\sum_i \\Delta E_i = 0\\) at the equilibrium point,\n\\[\\rho_Y(\\Delta E) \\propto e^{\\sum_i \\frac 12 (\\partial_{E_i}^2 S_i)_{A_i} (\\Delta E_i)^2}\\]\nNow, \\(\\partial_E S = \\beta\\), and \\(\\partial_E^2 S = -\\frac{1}{T^2 C}\\) in typical thermodynamic notation, where \\(C\\) is \\(\\partial_T E\\), the heat capacity (holding all other variables \\(A\\) constant), so we have the following equation:\n\\[\\rho_Y(\\Delta E) \\propto e^{-\\sum_i \\frac{1}{2T^2 C_{i}} (\\Delta E_i)^2}\\]\nwith fluctuation on the order \\(\\Delta E_i \\sim \\sqrt{T^2 C_i}\\). For most substances studied by 19th century physicists, such as gas, that is \\(\\sim \\sqrt{N} k_B T\\). If they could measure the energy of gas at \\(T = 500 \\ut{K}\\) with precision down to \\(10^{-3} \\ut{J}\\), that would still require a tank of gas with \\(N = 10^{34} = 10^{10} \\ut{mol}\\). If they wanted to study this in oxygen, they would need 0.1 million tonnes of it.\n\n\nBlackbody radiation\nPlanck’s derivation of the blackbody radiation is the first great success of quantum statistical mechanics. We give a brief presentation here that tracks Planck’s original argument.\nConsider a hollow cubic box with side lengths \\(L\\). The box has perfectly reflecting walls. At thermal equilibrium, the box is full of standing electromagnetic waves. Each standing EM wave has form \\(\\vec E(x, y, z, t) = \\vec E_0 \\sin(\\omega t)\\sin(\\frac{n_x \\pi x}{L})\\sin(\\frac{n_y \\pi y}{L})\\sin(\\frac{n_z \\pi z}{L})\\), for some positive integers \\(n_x, n_y, n_z\\). Each wave has wavevectors \\(\\vec k = (n_x, n_y, n_z) \\frac{\\pi}{L}\\). If we draw a region of volume \\(\\delta K\\) in the space of wavevectors, then the region would contain about \\(\\delta K \\frac{L^3}{\\pi^3}\\) valid wavevectors. Thus, we say that the wavevector space is \\([0, +\\infty)^3\\), and has density of states \\(\\frac{L^3}{\\pi^3}\\). We can picture it as \\([0, +\\infty)^3\\) with a rectangular grid of points being the valid wavevectors, such that the numerical density of such grid points is \\(\\frac{L^3}{\\pi^3}\\).\n\n\n\nThe rectangular grid of valid wavevectors in the space of possible wavevectors.\n\n\nAt this point, we depart from Planck’s derivation. Instead of considering standing waves in a perfectly reflecting chamber, we consider planar waves in a chamber with periodic boundaries. That is, we imagine that we have opened 6 portals, so that its top wall is “ported” to the bottom, etc. In this case, the planar waves have valid wavevectors \\(\\vec k = (n_x, n_y, n_z) \\frac{2\\pi}{L}\\).\n\n\n\n\n\n\nTip\n\n\n\nWait, the numerical density of grid points is now just \\(\\frac{L^3}{8\\pi^3}\\), which is \\(1/8\\) of what we found previously?\nYes, indeed, but it will work out correctly, because whereas the density of states has dropped to just \\(1/8\\) of previously, the state space has increased \\(8\\times\\), from \\([0, +\\infty)^3\\) to \\(\\mathbb{R}^3\\).\n\n\nNow, we need to allow two states at each valid wavevector, to account for polarization.\nAt this point, we have decomposed the state space into a composition of oscillators. Because there is no interaction between these oscillators,6 it remains to calculate the partition function of each oscillator.\n6 That is, two photons do not interact, except when the energy levels are so high that you would need a quantum field theorist to know what is going on.\nWhereas modern spiritualists talk of electromagnetic fields and quantum vibrations, a century ago they talked of subatomic structures and ether vibrations. Light resembles ghosts and spirits in that they are massless, untouchable, moving very fast, bright, and vaguely associated with good feelings. During the 19th century, the best scientific theory for light, that of ether theory, became the foundation of many spiritualist world systems. (Asprem 2011) The connection of electromagnetism with animal magnetism did not help.Planck considered an ensemble of \\(N\\) oscillators, all at the same wavevector and polarization. If they have average energy \\(\\braket{E}\\), the question is to find the total entropy for the whole system, which, when divided by \\(N\\), should yield the entropy of a single oscillator. Here he used the celebrated quantum hypothesis: The energy levels are divided into integer levels of \\(nh\\nu\\), where \\(n = 0, 1, 2, \\dots\\). By the stars and bars argument, there are \\(\\binom{N + M-1}{M}\\) ways do distribute these energy-quanta between these oscillators, where \\(M = \\frac{N\\braket{E}}{h\\nu}\\).\n\\[S = \\frac 1N \\ln \\binom{N+M-1}{M} \\underbrace{\\approx}_{\\text{Stirling}} (1 + a) \\ln (1+a) - a \\ln a, \\quad a = \\frac{\\braket{E}}{h\\nu}\\]\nGiven the entropy function, he then matched \\(\\braket{E}\\) to temperature \\(\\beta\\) by the equality \\(\\beta = \\partial_{\\braket{E}} S\\), giving\n\\[\n\\braket{E} = \\left(\\frac{h\\nu}{e^{\\beta h\\nu}-1}\\right)\n\\]\nNow, in any direction \\(\\hat k\\), for any wavelength interval \\([\\lambda, \\lambda + d\\lambda]\\), and any span of solid angle \\(d\\Omega\\), compute its corresponding wavevector space volume \\(k dkd\\Omega = \\frac{4\\pi^2}{\\lambda^3} d\\lambda d\\Omega\\), and multiply that by the density of states and \\(\\braket{E}\\), yielding the blackbody radiation law.\n\n\n\nIn any direction, wavelength, and span of solid angle, we find the corresponding volume in the space of wavevectors, sum up all the states within that volume, to obtain the blackbody radiation law.\n\n\nThe details are found in any textbook. I will just point out some interesting facts typically passed over in textbooks.\nIn the above derivation of the blackbody radiation law, the allowed wavevectors \\(\\vec k\\) are like a dust cloud in the space of possible wavevectors. The cloud is assumed to be dense and even, so that the number of states inside a chunk of volume \\(\\Delta V\\) is roughly \\(\\Delta V \\rho\\), where \\(\\rho\\) is the average density of states. This only works if \\(\\Delta V \\rho \\gg 1\\), or in other words, \\(\\frac{\\Delta \\lambda}{\\lambda} \\gg \\frac{\\lambda^3}{L^3}\\). Thus, when the chamber is small, or when temperature is low enough that the spectral peak is close to the zero, then the murky cloud of wavevectors resolves into individual little specks, and we have deviation from blackbody radiation law.\nIn this limit, the precise shape of the chamber becomes important, since the precise chamber shape has a strong effect on long-wavelength (low-temperature) resonant modes. A tiny cube and a tiny cylinder have different blackbody spectra. See (Reiser and Schächter 2013) for a literature review.\n\n\n\nRed solid line is Planck’s blackbody radiation law. Black dashed line is the analytical prediction for a spherical blackbody.(García-García 2008)\n\n\nAccording to Kirchhoff’s law of thermal radiation, a chunk of matter is exactly as absorptive as it is emissive. A blackbody absorbs all light, and conversely it emits light at the maximal level. A white body absorbs no light, and conversely it does not emit light. This can be understood as a consequence of the second law: If a body emits more light than it absorbs, then it would spontaneously get colder when placed inside a blackbody radiation chamber.\nHowever, much more can be said than this. Not only is it exactly as absorptive as it is emissive, it is as absorptive as it is emissive at any angle, at any wavelength, and any polarization. So for example, if a piece of leaf is not absorptive when viewed from an angle, at the green light wavelength, of clockwise polarization, then it is not emissive under the same angle, wavelength, polarization.\nWhy is that? The standard argument (Reif 1998, chap. 9.15) uses a time-reversal argument, but I like to think of it as yet more instances of protecting the second law. If you look inside a blackbody radiation chamber, you would see a maximal entropy state. Light rushes in all directions equally, at all polarizations equally, and the energy is distributed optimally across the spectrum to maximize entropy (because \\(\\beta\\) is constant across the whole spectrum). If we have a material that takes in blue light and outputs green light, then it would spontaneously decrease entropy. Similarly, if it can absorb vertically polarized light to emit diagonally polarized light, it would also spontaneously decrease entropy, etc.\n\n\n\n\n\n\nRelativistic gas?\n\n\n\nA box full of blackbody radiation is also called a photon gas. The photon gas is sometimes treated as the limit of “ultrarelativistic gas”. Start with the relativistic energy function \\(E = \\sqrt{m^2 c^4 + \\|p\\|^2 c^2}\\), derive its Boltzmann distribution \\(\\rho(q, p) \\propto e^{-\\beta \\sqrt{m^2 c^4 + \\|p\\|^2 c^2}}\\), then take the \\(m \\to 0\\) limit. This gives some correct results, such as the \\(U = 3PV\\).\nHowever, accounting for the entropy of photon gas, as well as deriving the blackbody radiation, hinges critically on the photon quantization \\(E = h\\nu, 2h\\nu, \\dots\\). I guess it can be done correctly by relativistic quantum mechanics, but it is of course beyond the world of classical mechanics.\n\n\nThe point is that the blackbody radiation law is not about a blackbody. Instead, it is about photons in vacuum. We could have taken a perfectly reflecting mirror box (or if you are fancy, a three-dimensional torus) and injected it with a gas of \\(400 \\ut{nm}\\) photons with zero total momentum and angular momentum. Since no conservation laws are constraining, the system will equilibrate to its maximal entropy state, which is the blackbody radiation spectrum. We simply need to wait a few eternities for photon-photon interactions to do the job. Thus, the precise material of the chamber does not matter, and charcoal is merely a better catalyst than titanium oxide.\n\n\nRubber bands\nIt turns out that rubber bands, and generally things made of long polymers, contract when heated up, instead of expanding. This is the Gough–Joule effect. Roughly speaking, this is because elasticity in long polymer material (like rubber) is very different from elasticity in short molecule solids (like copper and ice). In rubber, elasticity is an entropic force, while in copper, it is an electrostatic force caused by attraction between molecules.\nTo model a rubber band, consider a long chain molecule with \\(N\\) joints. Each joint can go forward or backward, with equal energy. Each link between two joints has length \\(d\\). The total length of the system is \\(L\\).\n\n\n\nA sample shape from the ensemble of all rubber band shapes.\n\n\n\nDirect argument (microcanonical)\nThe entropy of the system, conditional on \\(L\\), is\n\\[S = \\ln \\binom{N}{\\frac{N + L/d}{2}}\\]\n\n\n\n\n\n\nelastic constant\n\n\n\nThe thermodynamic equation for the rubber band is\n\\[0 = TdS + FdL\\]\nbecause the internal energy of the rubber band is constant, no matter how the joint turns.\nTherefore, the elastic force is\n\\[F = -T \\partial_L S \\approx -T \\frac{S(L+2d) - S(L)}{2d} \\approx \\frac{T}{2d }\\ln\\frac{Nd+L}{Nd - L}\\]\nWhen \\(Nd \\gg L\\), that is, we have not stretched it close to the breaking point, the elastic force is\n\\[F \\approx \\frac{TL}{Nd^2} = k L\\]\nwhere \\(k = \\frac{T}{Nd^2}\\) is the elastic constant, proportional to temperature.\n\n\nWhy does the rubber band stiffen when temperature rise? We can interpret it as follows. When we place the rubber band in a chamber of hot air, the air particles would often collide with the links in the rubber band, flipping it. When there are more links going to the right than the left, then the air particles would tend to flip the links to the left, decreasing \\(L\\), and conversely. The net force is zero only when there are an equal number of links going either way, which is when \\(L = 0\\).\n\n\nVia free entropy (canonical)\nBecause the rubber band has \\(dE = TdS + FdL\\), the corresponding free entropy is \\(S - \\beta \\braket{E} + \\beta F \\braket{L}\\). Under the canonical distribution, that free entropy is maximized, meaning that \\(\\rho(x) \\propto e^{\\beta FL(x)}\\) where \\(x\\) is a microstate of the rubber band (i.e. the precise position of each link), and \\(L(x)\\) is the corresponding length (macrostate).\nThe trick of using the free entropy is that it decomposes the entire rubber band into atomic individuals. Like how opening an energy market converts consumers trying to coordinate their energy use into consumers each individually buying and selling energy, and thus simplifying the calculation problem. Like how Laplace’s devil allows you to calculate the optimal way to schedule your day. Microcanonical ensembles are true, but canonical ensembles are almost just as true, and much easier to use. The idea is that the canonical ensemble and the microcanonical ensemble are essentially the same because the fluctuation is so tiny.\nBack to the rubber band. Each individual link in the rubber band now is freed from the collective responsibility of reaching exactly length \\(L\\). It is now an atomized individual, maximizing its own free entropy \\(S - \\beta \\braket{E} + \\beta F \\braket{L}\\). Let \\(p\\) be its probability of going up, then its free entropy is\n\\[\n\\underbrace{-p\\ln p - (1-p) \\ln(1-p) }_{\\text{$S$}} - 0 + \\beta F (d p - d(1-p))\n\\]\nThis is maximized by the Boltzmann distribution \\(p = \\frac{e^{\\beta F 2d}}{1+e^{\\beta F 2d}}\\), with first two moments\n\\[\n\\braket{L} = \\frac{e^{\\beta F 2d} - 1}{e^{\\beta F 2d} + 1} d \\approx \\beta Fd^2,\n\\quad \\mathbb{V}[L] = p(1-p)d^2 \\approx d^2/4\n\\]\nThe total extension of the rubber band has the first two moments\n\\[\nN\\braket{L} \\approx \\beta F Nd^2,\n\\quad N\\mathbb{V}[L] \\approx Nd^2/4 = \\frac{1}{4F\\beta} N\\braket{L}\n\\]\nThe first equation is the same as the previous one. The second equation tells us the fluctuation in rubber band length when held under constant force and temperature. For typical conditions like \\(F \\sim 1 \\ut{N}, T \\sim 300 \\ut{K}, N\\braket{L} \\sim 1 \\ut{m}\\), the fluctuation is on the order of \\((0.1\\ut{nm})^2\\), about one atom’s diameter. So we see that the difference between the canonical and the microcanonical ensemble are indeed too tiny to speak of."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#combinatorial-examples",
    "href": "essays/posts/statistical-mechanics/index.html#combinatorial-examples",
    "title": "Statistical Mechanics",
    "section": "Combinatorial examples",
    "text": "Combinatorial examples\n\nBurning the library of Babel\n\nThe universe (which others call the Library) is composed of an indefinite, perhaps infinite number of hexagonal galleries… always the same: 20 bookshelves, 5 to each side, line four of the hexagon’s six sides… each bookshelf holds 32 books identical in format; each book contains 410 pages; each page, 40 lines; each line, approximately 80 black letters… punctuation is limited to the comma and the period. Those two marks, the space, and the twenty-two letters of the alphabet are the 25 sufficient symbols…\nThe Library of Babel, Jorge Luis Borges\n\nLike the artist M. C. Escher, the writer J. L. Borges is a favorite of scientists, for his stories that construct precise worlds like elegant thought experiments. The Library of Babel is a thought experiment on combinatorics and entropy. In the universe, there are only books. Each book contains\n\\[410 \\ut{page} \\times 40\\ut{line/page} \\times 80 \\ut{symbol/line} = 1.3\\times 10^6\\ut{symbol}\\]\nSuppose the books are uniformly random sequences made of 25 symbols, then each symbol contains \\(\\ln 25\\) amount of entropy, and each book contains \\(1.3\\times 10^6\\ln 25 = 4.2 \\times 10^6\\). Now, consider another library of Babel, but this one consists of books filled with white space, so each book has zero entropy. Then, we can take one empty book and “burn” it into a uniformly random book, recovering \\(4.2 \\times 10^6 k_B T\\) free energy per book burned. At ambient temperature \\(300\\ut{K}\\), that is just \\(1.4 \\times 10^{-14}\\ut{J}\\) per book. Book-burning isn’t going to keep the librarians warm.\n\n\nAfter having razed the garden and profaned the chalices and altars, the Huns entered the monastery library on horseback and trampled the incomprehensible books and vituperated and burned them, perhaps fearful that the letters concealed blasphemies against their god, which was an iron scimitar. Palimpsests and codices were consumed, but in the heart of the fire, amid the ashes, there remained almost intact the twelfth book of the Civitas Dei, which relates how in Athens Plato taught that, at the centuries’ end, all things will recover their previous state and he in Athens, before the same audience, will teach this same doctrine anew.\nThe Theologians, Jorge Luis Borges\n\nWhile it is fanciful to “burn” a book by randomizing its letters, if we take the “information is physical, physics is informational” equivalence seriously, then there must be a way to actually “burn information”. Indeed, there is a way.\nWe can consider a chamber containing a single ball inside. The chamber has a removable wall, separating it into two parts labelled 0 and 1. If we know which side the ball is in, then we can put a piston into the other side, remove the wall, then extract work by isothermal expansion. Of course, the actual work extractable is random, but if we do this for a large number of chambers, then it is the same as isothermal expansion of an ideal gas to double its volume. By the ideal gas equation, we would extract \\(k_B T \\ln 2\\) of mechanical energy per chamber on average, exactly as predicted by the information-burning argument.\n\n\n\nA car driven by burning a tape of zero information, emitting “waste information” in its wake. (Sethna 2021, fig. 5.10)\n\n\nIn the tradition of steampunk, (Lu, Mandal, and Jarzynski 2014) constructed a purely mechanical model of information-burning. As shown below, we have a belt of paddles that can rotate freely, except when they hit one of the two red rods. The two red rods divide space into two sides: the 0 side and the 1 side. One of the red rods has an opening on it, allowing the paddle to go through. All the paddles start on the top in the 0 side, and slowly move down. As soon as a paddle has moved into the middle, it would be able to be able to freely move between 0 and 1 sides.\nSo, if we blur our vision and look at the average motion of the paddles, we would see an averaged “entropic force” that drives paddles from 0 side to 1 side. We can harvest this entropic force by adding a ring with a paddle inside, which can be hit by the paddles, driving it into rotation. The ring would be able to wind up a spring, thus converting information into mechanical work.\n\n\n\nThe mechanical information-engine. (Lu, Mandal, and Jarzynski 2014)\n\n\n\n\n\n\n\nVideo source: Mechanical Maxwell’s Demon in Motion - YouTube.\n\n\n\n\nMultinomials and the chi-squared test\nLet \\(\\vec p := (p_1, \\dots, p_k)\\) be a discrete probability distribution, and let \\(N_1, \\dots, N_n\\) be integers that depend on \\(N\\), such that\n\\[\\lim_{N \\to \\infty}\\vec N / N = \\vec p\\]\nwhere \\(\\vec N := (N_1, \\dots, N_n)\\), then we have\n\\[\\lim_{N \\to \\infty}\\frac 1N \\ln \\binom{N}{N_1, \\dots, N_n} = -\\sum_i p_i \\ln p_i = S[\\rho]\\]\nwhere \\(\\rho\\) is the discrete probability distribution, and \\(\\binom{N}{N_1, \\dots, N_n} = \\frac{N!}{N_1! \\cdots N_n!}\\) is the multinomial coefficient, which counts the number of ways for \\(N\\) labelled balls to go into \\(n\\) labelled boxes, such that the \\(i\\)-th box contains \\(N_i\\) balls.\nThis can be interpreted as the thermodynamic limit of a lot of balls.\nConsider a population of particles, all in energy-contact with an energy bath of \\(\\beta = 1\\). Each particle has \\(n\\) states, with state \\(i\\) having energy \\(-\\ln p_i\\). Thus, at the Boltzmann distribution, each particle is precisely sampled from the categorical distribution \\(\\vec p\\). Clearly, this system has entropy \\(N\\sum_i -p_i \\ln p_i\\). This is a “canonical ensemble”.\nIn contrast, consider the “microcanonical ensemble” (in quotation marks, because the constraint is not on the total energy) of \\(N\\) particles, such that there are exactly \\(N_i\\) particles in state \\(i\\).\nThinking thermodynamically, we can single out one ball as the system, and regard the other \\(N\\) balls as a bath. Once we calculate the marginal entropies of the bath, we can infer the Boltzmann distribution for the system, and show that it is the same as the Boltzmann distribution in the canonical ensemble.\n\n\n\n\n\n\nnon-rigorous part\n\n\n\nWe hope that as \\(N \\to \\infty\\), all correlations (pairwise, triple-wise, etc) between balls decay fast enough, such that the interaction-entropy between the balls drop to zero, leaving\n\\[\\text{average entropy per ball} = \\text{marginal entropy of a ball}\\]\nJustifying this rigorously is generally very difficult, and in fact, the assumption is false at phase transitions, where correlations do not decay fast enough, and so the thermodynamic limit is false. However, it is typically good enough to check that the fluctuations decay as \\(N^{-1/2}\\), and if so, the thermodynamic limit is true.\n\n\nSuppose we move the ball from box \\(i\\) to box \\(j\\), then it would force the bath to change all its balls, changing its entropy by\n\\[\n\\ln\\binom{N}{n_1', \\dots, n_k'} -\\ln \\binom{N}{n_1, \\dots, n_k}\n\\]\nwhere \\(n_i' = n_i + 1, n_j' = n_j - 1\\), and otherwise unchanged. By definition of multinomials, this is \\(\\ln n_j - \\ln(n_i+1) \\to (\\ln p_j - \\ln p_i)\\) at large enough \\(N\\).\nLet \\(X\\) stand for the total state, including the bath and the singled-out system, and let \\(Y\\) stand for the state of the singled-out system. By the conditional entropy theorem, when the entire system is at the maximal entropy distribution, the distribution of the singled-out system is\n\\[\\rho^*_Y(y) \\propto e^{S_{X|y}^*}\\]\nFrom the previous calculation, we have\n\\[S_{X|j}^* - S_{X|j}^* = \\ln p_j - \\ln p_i\\]\nyielding \\(S_{X|i}^* = S_0 + \\ln p_i\\) for some constant \\(S_0\\), and so \\(\\rho^*_Y(y) \\propto p_i\\).\n\n\n\n\n\n\nchi-squared test\n\n\n\nLet \\(X\\) stand for the state of the entire population of particles. That is, \\(X\\) describes the precise state of each particle. Let \\(Y\\) stand for the vector of \\(\\vec N\\). That is, it counts the number of particles in each state. As typical, at the limit of \\(N \\to \\infty\\), we should expect \\(\\frac{N_i}{N} \\to p_i\\) with certainty (the thermodynamic limit), and the variance should scale as \\(1/N\\) (statistical fluctuation). So, if we observe the particle population many times, and plot all the \\(\\frac{N_i}{N}\\) on the \\(n\\)-simplex, we should see an ellipsoidal cloud centered around \\(\\vec p\\) with radius \\(\\sim N^{-1/2}\\).\n\n\n\nThis is an example of how the microcanonical ensemble and the canonical ensemble become indistinguishable at the large particle limit. Indeed, Boltzmann often used this equation to derive the canonical ensemble from microcanonical ensembles.\nWe have argued that the cloud should be ellipsoidal and centered around \\((p_1, \\dots, p_n)\\) with radius \\(\\sim N^{-1/2}\\). What exactly is its shape? Well, since each particle’s distribution of states is categorical, and the particles are uncorrelated (thanks to the energy market), the mean and covariance of \\(\\vec N= (N_1, \\dots, N_n)\\) are \\(\\vec p N\\) and \\([\\diag(\\vec p) - \\vec p \\vec p^T] N\\). The covariance matrix is not linearly independent, because of the constraint \\(\\sum_i N_i/N = 1\\).\nA good trick is worth doing again and again. Like how the microcanonical ensemble is freed up by a free market into a canonical ensemble, here we free up \\(\\vec N\\) by adding noise, then conditioning on zero noise:\n\\[\n\\vec N / N + z\\vec p/\\sqrt N \\sim \\mathcal N(\\vec p, \\diag(\\vec p) / N) \\quad z \\sim \\mathcal N(0, 1)\n\\]\nTherefore, the probability density of \\(\\vec N\\) satisfies\n\\[\\rho(\\vec N / N) \\propto \\exp\\left(-\\frac 12 \\sum_i \\frac{(N_i - Np_i)^2}{Np_i}\\right)\\]\nafter conditioning on \\(z = 0\\). Therefore, the distribution of \\(\\sum_i \\frac{(N_i - Np_i)^2}{Np_i}\\) converges to \\(\\chi^2(n-1)\\). This is the chi-squared test.\n\n\n\n\nSanov’s theorem\nTypically in statistical mechanics, we study the fluctuation of a macroscopic variable on the order of \\(N^{-1/2}\\). Large deviation theory studies the fluctuation on the order of \\(1\\). The prototypical example is Sanov’s theorem.\nThe problem is as follows: Suppose that we require \\(\\vec N / N \\to \\vec q\\), where \\(\\vec q\\) is some other probability vector, what is the behavior of \\(Pr(\\vec N / N \\approx \\vec q)\\)? Sanov’s theorem states that\n\\[\nPr(\\vec N / N \\approx \\vec q) \\sim e^{-ND_{KL}(\\vec q \\| \\vec p)}\n\\]\n\n\n\n\n\n\nderivation\n\n\n\n\n\nBy Theorem 9,\n\\[\n\\frac 1N \\ln Pr(\\vec N / N \\approx \\vec q) = \\bar f^*_{|\\vec N / N \\approx \\vec q} - \\bar f^*\n\\]\nwhere \\(\\bar f^*\\) is the Helmholtz free entropy per particle for the system without constraint on \\(\\vec N\\), and \\(\\bar f^*_{|\\vec N / N \\approx \\vec q}\\) is the Helmholtz free entropy per particle, conditional on \\(\\vec N / N \\approx \\vec q\\).\nThe constraint of \\(\\vec N / N \\approx \\vec q\\) is a global constraint, but as usual, when \\(N\\) is large, global constraints over all particles becomes local constraints for each particle individually. In this case, the constraint on individual particle is simply that its state is distributed according to \\(\\vec q\\), energy be damned.\nThus, we find that\n\\[\n\\frac 1N \\ln Pr(\\vec N / N \\approx \\vec q) = \\left(\\sum_i -q_i \\ln q_i - \\sum_i q_i E_i\\right) - \\left(\\sum_i -p_i \\ln p_i - \\sum_i p_i E_i\\right) = -D_{KL}(\\vec q \\| \\vec p)\n\\]\n\n\n\n\n\nSurface area of high-dimensional spheres\nLet \\(\\Omega_N\\) be the surface area of a sphere of radius \\(\\sqrt N\\) in \\(\\mathbb{R}^N\\), then\n\\[\\ln\\Omega_N = \\frac N2 \\ln (2\\pi e)-\\frac 12 \\ln (\\pi e) + O(N^{-1})\\]\n\n\n\n\n\n\nderivation\n\n\n\n\n\nLet \\(x_1, \\dots, x_N\\) be sampled IID from \\(\\mathcal N(0, 1)\\), and let \\(r_N^2 = \\sum_i x_i^2\\). By routine calculation, \\(\\braket{r_N^2} = N\\), and \\(\\braket{r_N^4} = 2N + N^2\\). Therefore, \\(r_N^2\\) is approximately distributed as \\(\\mathcal N(N, 2N)\\), and so \\(r_N\\) is approximately distributed as \\(\\mathcal N(\\sqrt N, 1/2)\\).\nNow consider two distributions on \\(\\mathbb{R}^N\\). The first is a microcanonical ensemble: \\(x_{1:N}\\) is distributed uniformly on the thin energy shell of \\(r_N^2 \\in [N, N + \\delta N]\\). The second is a canonical ensemble: each \\(x_i\\) is distributed independently according to \\(\\mathcal N(0, 1)\\).\nWe can think of them as particles in a potential well of form \\(V(x) = \\frac 12 x^2\\). The first ensemble is the microcanonical ensemble where the total energy is fixed, and the second is the canonical ensemble at temperature \\(\\beta = 1\\).\nWe can calculate the entropy of the canonical ensemble in two ways. We can calculate it by adding up the entropy of each particle, which are the same since there is no interaction energy between particles. We can also calculate it indirectly, by first sampling a random radius, then a random point from the microcanonical ensemble, then multiplying them together.\nBecause the canonical ensemble is spherically symmetric, the radius and the direction of the vector \\(x_{1:N}\\) are independent. Therefore,\n\\[S_{\\text{canonical}} = S_{\\text{microcanonical}} + S_{\\text{radius}}\\]\nor\n\\[\\ln \\Omega_N = \\underbrace{S_{\\text{canonical}}}_{\\text{$= N S[\\mathcal N(0, 1)]$}} - \\underbrace{S_{\\text{radius}}}_{\\text{$\\approx S[\\mathcal N(0, 1/2)]$}}\\]\nBecause the entropy of \\(\\mathcal N(0, \\sigma^2)\\) is \\(\\frac 12 \\ln(2\\pi e \\sigma^2)\\), we plug them in and obtain the result.\n\n\n\nAgain, this is an example of a general pattern: the microcanonical ensemble and the canonical ensemble become indistinguishable when the number of particles goes infinite."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#biological-examples",
    "href": "essays/posts/statistical-mechanics/index.html#biological-examples",
    "title": "Statistical Mechanics",
    "section": "Biological examples",
    "text": "Biological examples\n\nHow elastic is the skin of red blood cell?\n(Discher 2000) measured the elasticity of the red blood cell’s skin. To a good approximation, it is just a 2D spring, following Hooke’s law. He attached a tiny particle to the surface of a red blood cell, and measured its thermal motion.\nAs usual, in the microscopic world, inertia might as well not exist,7 so the oscillator’s energy is entirely elastic. Let it be of form \\(\\frac 12 k(x^2 + y^2)\\). By equipartition of energy, we would have\n7 But just to be sure, I checked the original numbers: It is a particle of diameter \\(40 \\ut{nm}\\), and moving at about \\(100 \\ut{nm/s}\\), so its kinetic energy is \\(\\sim 10^{-33}\\ut{J} \\sim 10^{-13} k_BT\\).\\[\\frac 12 k\\braket{x^2} = \\frac 12 \\beta^{-1}\\]\nThe data shows that \\(\\braket{x^2} = (35 \\ut{nm})^2\\) at temperature \\(T = 310 \\ut{K}\\), giving us an effective elastic constant of \\(k \\sim 0.004 \\ut{pN/nm}\\).\nA red blood cell has diameter \\(10^4 \\ut{nm}\\), so dragging a patch of its skin all across the surface would take only about \\(40 \\ut{pN}\\). This is about the force output of 10 kinesins working in concert. Thus we can say that the skin of red blood cell is very slack.\n\n\n\n(A): A tiny particle attached to long molecules in the skin of the red blood cell. (B): Photo of a red blood cell with attached nanoparticle. (C): An example \\((x,y)\\) trajectory. (Discher 2000, fig. 6)\n\n\n\n\nThe lac operon\nThis example is from (Garcia and Phillips 2011).\nE. coli has two main sources of food: glucose and lactose. It prefers glucose, so when it is in an environment rich in glucose, it would start by metabolizing the glucose until it is mostly exhausted, then switch to metabolizing lactose.\nTo simplify, let’s consider a single gene, called the lac, which codes for a lactose-digesting enzyme (lactases). In front of lac there is a site where repressor protein can bind to, which stops lac transcription. The gene is on iff the repressor is not bound there.\nThe repressor protein can bind to either that specific site, of which there is only one on the entire E. coli genome, or any other site on the entire DNA sequence (non-specific binding). Write the binding energy on the specific site be \\(E_S\\), and the binding energy on the non-specific sites be \\(E_{NS}\\).\nThe system is in a delicate balance between energy, which favors specific binding, and entropy, which favors non-specific binding, with \\(E_S &lt; E_{NS}\\) but not \\(E_S \\ll E_{NS}\\). That is, specific binding is favored, but not too favored. This soft-favorism is what allows lac to be controlled. If it is not favored at all, then it would rarely bind. If it is too favored, then it would almost always bind.\nSuppose there are \\(R\\) repressors in the bacterium, then when none is binding specifically, the Gibbs free energy is\n\\[G_{\\text{on}} = RE_{NS} - \\beta^{-1} \\ln \\binom{N_{NS}}{R}\\]\nNotice how the entropy term \\(\\ln\\binom{N_{NS}}{R}\\) assumes that the repressor proteins are indistinguishable, just like in gas theory. If one of them is binding specifically, the Gibbs free energy is\n\\[G_{\\text{off}} = E_S + (R-1)E_{NS} - \\beta^{-1} \\ln \\binom{N_{NS}}{R-1}\\]\nThus, the probability of on vs off satisfies\n\\[\\frac{p_{\\text{on}}}{p_{\\text{off}}} = e^{-\\beta (G_{\\text{on}} - G_{\\text{off}})} = \\frac{N_{NS} - R + 1}{R} e^{\\beta (E_{NS} - E_{S})} \\approx \\frac{N_{NS}}{R} e^{\\beta (E_{NS} - E_{S})}\\]\ngiving us\n\\[p_{\\text{on}} = \\frac{N_{NS}}{N_{NS} + R e^{-\\beta (E_{NS} - E_{S})}}\\]\n\n\nUnzipping RNA hairpins\nThe RNA molecules are polymers made of 4 different “letters” that can pair up as A-U and C-G. A common shape for single-stranded RNA is the “hairpin”, pictured below.\n\n\n\nRNA hairpin. Figure from Wikimedia Commons\n\n\nSince each base pair has about 100 atoms, an RNA hairpin is a large molecule with \\(\\sim 1000\\) atoms. This is large enough for statistical mechanics, but not large enough to smooth out thermal fluctuations, which ought to make it interesting. Now, what happens if we pull on the hairpin? This is the experiment done by (Liphardt et al. 2001). They attached a single RNA hairpin to two beads, and pulled the beads apart very slowly, using force-feedback to keep the force stable within \\(0.1 \\ut{pN}\\). The RNA they used is P5ab, which has about 20 base pairs, each base of length about 5 Angstrom.\nIf we ignore fluctuation, and treat it by classical thermodynamics, we get \\(dS = \\beta dE - \\beta F dx\\), where \\(F\\) is the pulling force, and \\(x\\) is the distance between the two tweezers. Looking at this equation, we immediately see that the problem is best analyzed using the following free entropy\n\\[\nh := \\underbrace{S - \\beta \\braket{E}}_{\\text{Helmholtz free entropy}} + \\beta F \\braket{x}\n\\]\nwhich can be interpreted as “one-dimensional Gibbs free entropy”.\nLet \\(x_0\\) be the length of a single RNA unit, \\(n\\) be the number of unzipped RNA base pairs, \\(2N\\) be the total number of RNA bases, and \\(-E_0\\) be the bonding energy of a base pair (assume that the two kinds of base pairs have the same bonding energy).\nBecause the state of the system is fully determined once we know what \\(n\\) is, the entropy \\(S\\) conditional on \\(n\\) is zero, and so, plugging in the Theorem 9, the probability of the hairpin in state \\(n\\) is: \\[\n\\rho(n) \\propto e^{\\beta(2Fx_0 - E_0)n}, \\quad 0 \\leq 2n \\leq 2N\n\\]\na truncated exponential distribution. When \\(F = E_0/2x_0\\), the bonding force and the pulling force are exactly balanced, and the hairpin is equally likely to be in any state. When the pulling force is larger, then \\(n\\) concentrates on the \\(n = N\\) end.\nFor reasons they did not explain, and I do not understand (sorry), in the (Liphardt et al. 2001) experiment, the RNA hairpin had no intermediate state. It either fully unfolded, or fully folded. In that case, the hairpin became a two-level system, satisfying\n\\[\np_{\\text{unfolded}} = \\frac{e^{\\beta( FL - \\Delta G )}}{1 + e^{\\beta( FL - \\Delta G )}}\n\\]\nwhere \\(L = 18 \\ut{nm}\\) is the length increase in unfolding, and \\(\\Delta G\\) is the increase in Helmholtz free energy during unfolding. This is exactly what they observed.\n\n\n\n(Left) As the pulling force increases, the RNA hairpin spends more and more time in the unfolded state. In all cases, the RNA is binary, either fully folded or fully unfolded, never in-between. (Right) The probability of being in an unfolded state increases as a logistic function, as predicted. (Liphardt et al. 2001, fig. 1)\n\n\nAnother interesting finding is that when the pulling force increases slowly, the dissipated energy is small, but when the pulling force increases quickly, the wasted energy is large. In short, we have a form of speed limit second law: fast change requires more entropy production. We will discuss this in much more detail in the section on Crooks fluctuation theorem.\n\n\n\nTrajectories of folding and unfolding cycles. The area between the upper and lower curves is the dissipated energy during one cycle. Fast cycles dissipate more energy. (Liphardt et al. 2001, fig. 1)\n\n\n\n\nHungry hungry bacteria\nThis example came from (Howard C. Berg 1993, chap. 6). See (H. C. Berg and Purcell 1977; Howard C. Berg 2000) for a detailed look at how bacteria optimally forage for food molecules.\nConsider a bacteria swimming in water. A typical one, such as E. coli, is roughly a rod with length \\(a = 10^{-6}m\\) and swimming at \\(v = 2\\times 10^{-5} m/s\\). That is, it swims 20 body-lengths a second.\nWater has density \\(\\rho = 10^3 kg/m^3\\) and viscosity \\(\\eta = 10^{-3} kg/m\\cdot s\\). As is well-known, the Reynold number of the system is\n\\[Re = \\frac{\\rho v a}{\\eta} = 2 \\times 10^{-5}\\]\nmeaning that as soon as the bacteria stops powering itself, its motion ceases after coasting for a length of \\(\\approx 0.1 a \\cdot Re = 2\\times 10^{-12}m\\), less than the length of an atom! Thus, the bacteria lives in an essentially inertia-less world.\nAssuming that a bacteria is a sphere, then its swimming dissipates power at\n\\[P = Fv = 6\\pi \\eta a v^2 \\approx 8\\times 10^{-18}W \\approx 2000 k_BT/s\\]\nwhere \\(T = 300 K\\) is the standard temperature for biology. Since each glucose at complete metabolism produces \\(686 kcal/mol \\approx 1000 k_BT\\), the bacteria just needs to eat 2 molecules of glucose per second to power its swimming.\nBy the FDR, the diffusion constant for the bacteria is\n\\[D = \\frac{k_BT}{6\\pi \\eta a} = 2 \\times 10^{-13} m^2/s\\]\nmeaning that within 1 second, the bacteria diffuses a distance of \\(\\sim \\sqrt{2 D \\Delta t} \\sim a\\), about 1 body length. This shows that the swimming motion is 20 times faster than its thermal motion.\nHowever, if we look at the trajectory of a bacterium under the microscope, we would notice its direction jumping around rapidly. This is due to the thermal motion of its orientation on the rotational group \\(SO(3)\\). Just like how each translational degree of freedom gets \\(\\frac 12 k_BT\\) of thermal energy, each rotational degree of freedom gets it as well, although, because the space of rotations is not a vector space, making this precise is tricky.\nStill, once we make it precise, the FDR is still the same, gives us \\(D_{\\text{rotational}} = \\frac{k_B T}{\\gamma_{\\text{rotational}}}\\). Assuming the bacterium is still a sphere of radius \\(a\\), then \\(\\gamma_{\\text{rotational}} = 8\\pi \\eta a^3\\), giving \\(D_{\\text{rotational}} = 0.2 \\ut{rad^2/s}\\).\nNow, a swimming bacterium does not care about rotation around its velocity axis (“longitude”), but does care about the other two rotational freedoms. The variation in “latitude” angle is the sum:\n\\[\\braket{\\theta^2} = 2 \\times (2D_{\\text{rotational}}\\Delta t)\\]\nfor small time \\(\\Delta t\\). This is false for large \\(\\Delta t\\), because otherwise we would get \\(\\braket{\\theta^2} &gt; \\pi^2\\), which is absurd because \\(\\theta \\in [0, \\pi]\\).\nThis implies that the bacteria veers by about \\(\\braket{\\theta^2} \\approx (50^\\circ)^2\\) after a second of swimming. The actual observation gives \\(\\braket{\\theta^2} \\approx (30^\\circ)^2\\). In particular, the bacteria cannot keep a direction for more than about 3 seconds, as at that point its velocity vector would have diffused by about 90 degrees. As the bacteria has no “vestibular organ”, it essentially “forgets its heading” within a few seconds. Any sense of direction must come from the outside world, such as a chemical gradient.\nThe standard strategy for E. coli hunting is the “run and tumble” strategy. During the “run” phase, the bacterium keeps swimming forward for 1 – 10 seconds, curving gently due to rotational diffusion. At a random point it stops and tumbles for about 0.1 seconds, changing its direction randomly. Tumbling essentially samples a random direction for the next run, close but not quite uniformly on the sphere. It remembers the average chemical concentration during the first second and last second of each run. If the chemical concentration seem to increase, it would tumble less often (thus going on longer runs). Otherwise, it would tumble more often. The net effect is an extremely simple homing missile that diffuses up the chemical gradient.\nThere is no point for it to swim more than 10 seconds, because it would have totally lost its heading due to diffusion. There is no point in swimming less than 1 second, because it needs to average the chemical concentration over at least a second to overcome the statistical noise. And so, by simple physics, we have explained an E. coli’s hunting strategy.\n\n\n\nA spherical bacterium is propelled by spinning flagella. The spherical bacterium swims along the \\(x\\) -axis. Its rotational state diffuses along three possible degrees of freedom. Of those, the one around \\(x\\)-axis is irrelevant, and the ones around \\(y\\)- and \\(z\\)-axes are relevant. (Howard C. Berg 1993, fig. 6.6)\n\n\n(Howard C. Berg and Brown 1972) tracked the 3D trajectory of E. coli in a liquid that is \\(3\\times\\) more viscous than water. They found that the bacterium alternates between swimming and tumbling. During the swimming phase, the angular diffusion has a diffusion constant of \\(0.06 \\ut{rad^2/s}\\), as theoretically predicted.\n\n\n\nA single 3D trajectory of a single bacterium, projected into the \\(xy, yz, xz\\) planes. Gently winding runs alternate with the tight tumbles. (Howard C. Berg and Brown 1972, fig. 1)"
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#statistical-field-theory",
    "href": "essays/posts/statistical-mechanics/index.html#statistical-field-theory",
    "title": "Statistical Mechanics",
    "section": "Statistical field theory",
    "text": "Statistical field theory\n\nMaximum caliber\nEquilibrium statistical mechanics is typically called a “static” theory: It deals with the ensemble of states, but not with how the states change over time. Maximal caliber statistical mechanics handles trajectories in the most obvious way: Collect all paths into a “path space”, define a measure over path space, then study constrained entropy maximization over path space.\nEdward Jaynes, who proposed the idea, called path space entropy “caliber”, so the name “maximum caliber” stuck, even though it is really just standard equilibrium statistical mechanics in path space. In other words, it is just standard statistical field theory, where the fields are of type \\(\\phi: \\mathbb{R}\\to \\mathcal S\\), where \\(\\mathbb{R}\\) stand for time, and \\(\\mathcal S\\) stand for state space of the system.\n\nMarkov chains\nThe simplest example is when time comes in discrete steps. In this case, we can reconstruct Markov chains as a particular kind of maximum caliber distribution.\nConsider trajectories of type \\(\\{0, 1, 2, \\dots, N\\} \\to \\{1, 2, \\dots, m\\}\\), and let \\(s_t\\) is the state of timestep \\(t\\).\nIf we fix the singleton probability \\(p_i\\) of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and \\(p_{i\\to j} = p_i p_j\\).\n\n\n\n\n\n\nderivation\n\n\n\n\n\nIf we fix the singleton probability of each state, then the problem is a constrained maximization problem\n\\[\n\\begin{cases}\n    \\max S \\\\\n    \\frac 1N \\sum_t 1[s_t = k] = p_k, \\quad \\forall k = 1, \\dots, m\n\\end{cases}\n\\]\nThis is the same problem as \\(N\\) balls in a certain constrained microcanonical ensemble. When \\(N\\) is large enough, the discrete “macrostate” \\(\\frac 1N \\sum_t 1[s_t = k]\\) becomes continuous, and we can use the Lagrange multiplier to obtain\n\\[\\rho(s_1, \\dots, s_N) = \\frac 1Z e^{-\\sum_{k=1}^m \\lambda_k (\\frac 1N \\sum_{t=1}^N 1[s_t =k] - p_k)}\\]\nThis factors over time \\(t\\), giving us\n\\[\\rho(s_1, \\dots, s_N) = \\prod_{t=1}^N \\rho(s_t)\\]\nwith\n\\[\\rho(s_t=k) \\propto e^{-\\sum_{k=1}^m\\frac{\\lambda_k}{N}(1[s_t =k] - p_k)} \\propto e^{-\\frac{\\lambda_k}{N}}\\]\nThe multiplier \\(\\lambda_k\\) can be found by the typical method of solving \\(p_k = -\\partial_{\\lambda_k}\\ln Z\\), or we can be clever and notice that the constraint implies \\(\\rho(s_t=k) = p_k\\).\n\n\n\nSimilarly, if we fix the transition probability \\(p_{i \\to j}\\) of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it’s still the Markov chain with the same transition probabilities.\nAnd more generally, if we fix \\(n\\)-th order transition probability \\(p_{i_1, \\dots, i_n \\to j}\\), then we obtain an \\(n\\)-th order Markov chain model.\n\n\n\n\n\n\nderivation\n\n\n\n\n\nSimilarly as above, the path-space distribution is\n\\[\\rho(s_1, \\dots, s_N) \\propto \\prod_{t=1}^{N-1} e^{-\\sum_{k, k' \\in 1:m} \\frac{\\lambda_{k, k'}}{N} 1[s_t = k, s_{t+1} = k']} \\propto \\prod_{t=1}^{N-1} p_{s_t \\to s_{t+1}}\\]\nBecause the distribution does not specify \\(s_1\\), it is uniformly distributed on \\(s_1\\). Otherwise, we can constrain \\(s_1\\) with yet another set of Lagrange multipliers and obtain\n\\[\\rho(s_1, \\dots, s_N) \\propto \\rho(s_1) \\times_{t=1}^{N-1} \\rho(s_t, s_{t+1})\\]\nSimilarly for higher orders.\n\n\n\n\n\nDiffusion\nIn diffusion, we consider paths of type \\(x: [0, T] \\to \\mathbb{R}^n\\) with \\(x(0) = 0\\). The path-space entropy\n\\[\nS = - \\int \\rho(x) \\ln \\rho(x) D[x]\n\\]\nwhere \\(D[x]\\) means we integrate over path space. Here we see a proper path-integral, which is roughly speaking what happens when we integrate not over \\(\\mathbb{R}^{10}\\) or even \\(\\mathbb{R}^{10^{23}}\\), but literally \\(\\mathbb{R}^\\infty\\).\nAs usual, path integrals cannot be done directly, but can be done by first dropping down to \\(\\mathbb{R}^N\\), where \\(N\\) is large but still finite, then hope that the result is still sensible at the \\(N \\to \\infty\\) limit. If this disturbs you, sorry. It disturbs me too, but it is necessary for all but the most trivial calculations in field theory.\nDiscretize continuous-time path \\(x : [0, T] \\to \\mathbb{R}^n\\) into discrete-time path \\(x: \\{0, 1, 2, \\dots, N\\} \\to \\mathbb{R}^n\\). Because we can decompose\n\\[\n\\rho(x) = \\rho(x_0) \\rho(x_1 | x_0) \\cdots \\rho(x_N | x_{0:N-1})\n\\]\nthe path-space entropy decomposes sequentially:\n\\[\nS = S[x_0] + \\braket{S[x_1 | x_0]} + \\braket{S[x_2 | x_{0:1}]} + \\dots + \\braket{S[x_N | x_{0:N-1}]}\n\\]\nTo prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as\n\\[(\\braket{x_t|x_{0:t-1}}, \\braket{x_t^2|x_{0:t-1}}) = (0, \\sigma^2), \\quad \\forall t \\in 0:N\\]\nby reasoning backwards from \\(t = N\\) to \\(t=0\\), we find that the maximal entropy distribution is a white noise:\n\\[\n\\rho(x) = \\prod_{t\\in 0:N} \\rho(x_t), \\quad \\rho(x_t) \\propto e^{-\\frac{\\|x_t\\|^2}{2\\sigma^2}}\n\\]\nIf you have studied dynamic programming and cybernetics, this should look very similar to the argument by which you derived the LQR.\nTo keep the path from exploding into white noise, we instead impose the constraints on the sizes of the steps\n\\[(\\braket{x_t - x_{t-1}|x_{0:t-1}}, \\braket{\\|x_t - x_{t-1}\\|^2|x_{0:t-1}}) = (0, \\sigma^2), \\quad \\forall t \\in 1:N\\]\nNow, as in dynamic programming, we reason backwards from \\(t = N\\) to \\(t=0\\), and we find that the maximal entropy distribution is the Brownian motion\n\\[\\rho(x) \\propto e^{-\\frac{\\sum_{t\\in 1:N} \\| x_t-x_{t-1}\\|^2}{2\\sigma^2}}\\]\nIf we constrain the first and the second moments of each step, and also allow them to be affected by the previous step, as in\n\\[\n\\begin{cases}\n    \\braket{x_t - x_{t-1}|x_{0:t-1}} &= \\mu(t, x_{t-1}) \\\\\n    \\braket{(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}} &= \\Sigma(t, x_{t-1})\n\\end{cases}, \\quad \\forall t \\in 1:N\n\\]\nthen, reasoning backwards as before, we would obtain the Langevin equation.\nOther results, such as the Green–Kubo relations, the Onsager reciprocal relations, etc, can be similarly derived by imposing the right constraints in path space. (Hazoglou et al. 2015)\n\n\n\nFluctuation-dissipation relations\nImagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time \\(t\\) is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes \\(\\int_0^t W_s ds\\). This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.\nIn order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that \\(\\frac 12 m \\braket{v^2} = k_BT\\) exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the family of FDRs, because there have been so many of those.\nEach FDR is a mathematical equation of form\n\\[\n\\text{something about fluctuation} = \\text{something about dissipation}\n\\]\nThe prototypical FDR is the Einstein relation, to be derived below:\n\\[\n\\underbrace{\\beta D}_{\\text{fluctuation}} = \\underbrace{(\\gamma\\beta)^{-1}}_{\\text{dissipation}}\n\\]\nwhere the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.\n\n\nEquality before the law\nWhy should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is “equality before the second law”.\nIf we pause and think about it, then isn’t it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I’m going to answer here. Perhaps calling it an “invisible hand” is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).\nNow, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.\nAssuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.\nFor example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.\nNow, when physicists use the word “dissipation”, they mean the restoring effect of a system under external dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.\nThus, we see that each FDR manifests as an “equality before the law”:\n\\[\n\\begin{aligned}\n&\\text{fluctuation} \\\\\n\\underbrace{=}_{\\text{random walk theory}} &\\text{dissipation (of internal fluctuations)} \\\\\n\\underbrace{=}_{\\text{equality before the law}} &\\text{dissipation (of external fluctuations)}\n\\end{aligned}\n\\]\n\n\nOne-dimensional FDR\nConsider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at \\(x = 0, t = 0\\), and at each time-step of \\(\\Delta t\\), it moves by \\(\\Delta x\\) to the left or the right.\nThe fluid is at temperature \\(\\beta^{-1}\\), and we pull on the particle at constant force \\(F\\). We expect that \\(F = \\gamma \\braket{v}\\), where \\(v\\) is the ensemble-average velocity of the particle, and \\(\\gamma\\) is the viscosity constant.\nNow, we let the particle move for a time \\(t = N\\Delta t\\), where \\(N\\) is a large number. The particle would have arrived at some point \\(x\\), which is a random variable. The particle’s time-averaged velocity is \\(v = x/t\\).\nThe number of possible paths that connect \\((0, 0)\\) with \\((t, x)\\) is \\(\\binom{N}{\\frac N2 - \\frac{x}{2\\Delta x}}\\), therefore, the path-space entropy is\n\\[S_{\\text{path}} = \\ln \\binom{N}{\\frac N2 - \\frac{x}{2\\Delta x}} \\approx N \\left(\\ln 2 - \\left(\\frac{x}{N\\Delta x}\\right)^2\\right)\\]\nwhere the approximation is either by Stirling’s approximation, or the binary entropy function.\nBecause the external force performs work \\(Fx\\), which is dissipated into the sticky liquid at temperature \\(\\beta\\), we also have\n\\[S_{\\text{work}} = \\beta F x\\]\nBecause \\(N\\) is large, \\(\\braket{x}\\) should be highly concentrated around the point of maximal entropy. That is, we should have\n\\[\n\\braket{x} \\approx \\mathop{\\mathrm{argmax}}_x (S_{\\text{path}} + S_{\\text{work}})\n\\]\nThe equation on the right is quadratic in \\(\\braket{x}\\), and achieves maximum at \\(2\\braket{x} = \\beta FN(\\Delta x)^2\\), which simplifies to the Einstein relation\n\\[\\beta D \\gamma = 1\\]\nwhere \\(D = \\frac{\\Delta x^2}{2\\Delta t}\\) is the diffusion coefficient.\n\n\n\n\n\n\nTip\n\n\n\nWe can calculate not just the mean \\(\\braket{x}\\), but also its variance \\(\\braket{x^2}\\) if we expand \\(S_{\\text{path}} + S_{\\text{work}}\\) to second order around its maximum, then apply Theorem 8.\n\n\n\n\n\n\n\n\nrubber band\n\n\n\nWe notice how this is identical to the rubber band system. We can think of the shape of a rubber band as a timeless image of the trajectory of in time. Dissipation is identical to the entropic force pulling on the rubber band. Fluctuation is the variance in rubber band length. The dependence of the elastic constant on temperature, \\(F/L = \\frac{T}{Nd^2}\\), is then the FDR for the rubber band:\n\\[\n\\beta \\left(\\frac{d^2}{1/N}\\right) (F/L) = 1\n\\]\nThis is the simplest example of the general idea of Wick rotation, where a problem in \\(n\\)-dimensional space and \\(1\\)-dimensional time is converted to a problem in \\((n+1)\\)-dimensional space.\n\n\n\n\nSound waves\nRecall that the Helmholtz free entropy of ideal gas is\n\\[\nf = N \\left(\\ln \\frac{V}{N} + \\frac 32 \\ln \\frac{2\\pi m}{\\beta} + \\frac{\\ln N}{2N}\\right) \\underbrace{\\approx}_{\\text{large $N$}} N \\left(\\ln \\frac{V}{N} + \\frac 32 \\ln \\frac{2\\pi m}{\\beta}\\right)\n\\]\nThe free entropy density, then, has an elegant formula:\n\\[\nf/V = -\\rho \\ln \\rho + \\rho \\frac 32 \\ln \\frac{2\\pi m}{\\beta}\n\\]\nwhere \\(\\rho = N/V\\) is the particle density. Notice that on both the left side and the right side, the individual particles have completely disappeared, leaving behind only a density field \\(\\rho\\), a temperature field \\(\\beta\\), and some constants. That is, we have obtained the continuum limit of statistical mechanics – a field. Statistical mechanics still works in the limit, where it is called statistical field theory.\nWe wish to apply the Theorem 9 once again. To apply it, we need to construct a field-theoretic observable \\(Y\\), such that the maximal Helmholtz free entropy conditional on \\(Y = y\\) is \\(f^*_{X|y}\\) has a solution. Then, we can write \\(Pr_Y(y) \\propto e^{f^*_{X|y}}\\) – that is, the probability distribution of the \\(Y\\) field being in state \\(Y=y\\) is itself a Boltzmann distribution.\n\n\n\nDensity fluctuation of ideal gas. On the right is a certain coarse-grained macrostate \\(Y=y\\), and on the left is a microstate \\(x\\) that is compatible with the coarse-grained macrostate. Modified from (Sethna 2021, fig. 6.8) and (Sethna 2021, fig. 6.9)\n\n\nLooking at the picture, it is clear to us that the field-theoretic observable should be the particle density field \\(\\rho\\). And what is \\(f^*_{X|\\rho}\\)? It is the total Helmholtz free entropy of the entire tank of gas, given that its particle density is distributed like \\(\\rho\\). As we found previously, it is the integral of the free entropy density:\n\\[\nf^*_{X|\\rho} = \\int_{\\text{Box}} \\left(-\\rho(x) \\ln \\rho(x) + \\rho(x) \\frac 32 \\ln \\frac{2\\pi m}{\\beta}\\right) dx\n\\]\nIt is intuitively clear that the most likely density field \\(\\rho^*\\) is the uniform density field \\(\\rho^*(x) = \\rho_0\\). We can prove this by maximizing \\(f^*_{X|\\rho}\\). First, take its functional derivative:\n\\[\n\\begin{cases}\n\\partial_{\\rho} f^*_{X|\\rho} &= - \\ln \\rho - 1 + \\frac 32 \\ln \\frac{2\\pi m}{\\beta} \\\\\n\\partial_{\\rho}^2 f^*_{X|\\rho} &= -\\rho^{-1}\n\\end{cases}\n\\]\nWe cannot naively solve \\(\\partial_{\\rho} f^*_{X|\\rho} = 0\\), because we have the extra constraint that particles cannot be created or destroyed. Let \\(\\rho = \\rho_0 + \\delta \\rho\\), with \\(\\int\\delta\\rho = 0\\). Plugging it in, we obtain\n\\[\nf^*_{X|\\rho} = f^*_{X|\\rho_0} - \\frac{1}{2\\rho_0} \\int (\\delta \\rho)^2dx\n\\]\nwhich tells us that \\(Pr(\\rho_0 + \\delta \\rho) \\propto e^{- \\frac{1}{2\\rho_0} \\int (\\delta \\rho)^2dx}\\), which is precisely the Boltzmann distribution of a physical system whose energy is\n\\[\n\\delta E := \\frac{1}{2\\rho_0 \\beta} \\int \\delta \\rho(x)^2dx \\underbrace{=}_{\\text{$P = \\rho_0/\\beta$ by ideal gas law}} \\frac 12 P\\int \\left(\\frac{\\delta \\rho(x)}{\\rho_0}\\right)^2 dx\n\\]\nNow, if you have studied continuum mechanics, this should look very familiar: it is the elastic energy of air. At this point, we are quite close to a statistical field theory of sound waves, but to actually derive sound waves from this, we need to not only know the free entropy \\(f^*_{X|\\rho_0 + \\delta \\rho}\\), but also know how quickly a perturbation \\(\\delta \\rho\\) is pulled back to \\(\\delta \\rho = 0\\).\nSince \\(\\delta E = \\frac 12 P\\int \\left(\\frac{\\delta \\rho(x)}{\\rho_0}\\right)^2 dx\\) looks like the potential energy of a spring \\(\\frac 12 kx^2\\), it is reasonable to guess that we need to add a “kinetic energy” term, giving\n\\[\n\\delta E = \\frac 12 P\\int \\left(\\frac{\\delta \\rho(x)}{\\rho_0}\\right)^2 dx + \\frac 12 \\int \\mu \\delta \\dot\\rho(x)^2 dx\n\\]\nAt this point, it looks just like a Hamiltonian, and it is more productive to switch to Hamiltonian mechanics. This is a taste of statistical field theory of idea gas and how it reduces to classical field theory of density waves (sound waves). I might write a proper essay about statistical field theory someday."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#metastability",
    "href": "essays/posts/statistical-mechanics/index.html#metastability",
    "title": "Statistical Mechanics",
    "section": "Metastability",
    "text": "Metastability\nOften in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.\nThe point is that “getting over a potential barrier” is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on equilibrium statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.\nWe model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than \\(\\Delta E\\) of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time \\(\\tau\\) (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is \\(N \\tau\\), where \\(N\\) is the number of times we look at the system.\n\n\n\nGetting over a potential well.\n\n\n\n\n\n\n\n\nrelaxation time\n\n\n\nWhen we look at the oscillator, its state is totally fixed at some \\((q, p)\\). If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at \\((q, p)\\). How long must we wait before the oscillator state has reasonably “relaxed” from a pointy distribution to the serene bell-shape of the Boltzmann distribution?\nThat is beyond the scope. Suffice to say that we should wait a while, not too short, after which the system is close enough to equilibrium. But we cannot wait for too long either, because in the long run, the particle would have escaped the potential well. So there is a time-scale, neither too long nor too short, which we call the relaxation time \\(\\tau\\).\nFormalizing all these things requires stochastic calculus, which I might write about later.\n\n\n\n\n\n\nGoldilocks and the three chefs\n\n\n\n\n\nGoldilocks, with a rumbling stomach, stumbles upon the house of the three chefs. Each chef is holding a pan in one hand and a bottle of Brownian batter in the other. “Excuse me, might I have some pancakes?”\n“Of course!” exclaims the first chef, whose pan holds a small, quivering mound. “It’s just been poured, still brimming with energy!”\nGoldilocks frowns. “It hasn’t even reached the edges! This batter needs more time to settle.”\nThe second chef, whose pan appears curiously empty, sighs. “And what of mine? I’ve given it all the time in the world, allowed it to explore every corner of its potential.” He gestures at the pan, now bereft of batter, a few crumbs clinging to the edges.\n“But… there’s nothing left!” exclaims Goldilocks, aghast. “Given too much time, the batter has vanished completely!”\nThe third chef, whose batter has flowed smoothly to coat the pan, smiles warmly. “Perhaps this will be more to your liking. Given much time, but not too much, it’s achieved perfect consistency.”\n“Ah, this is perfect!” exclaims Goldilocks, taking a bite of the fluffy pancake. “It’s had enough time to spread evenly, but not so long that it’s dried out.”\n“Indeed, timing is everything. Too brief, and the batter remains confined to its starting point, unable to fulfill its pan-sized destiny. But too long, it would have escaped the edge of the pan to reach its true destiny – on the ground. Not too short, not too long, just right… that is meta-stability.”\n— Guest entry written by Gemini-1.5-Pro.\n\n\n\n\n\n\n\nTheorem 10 (Arrhenius equation) The escape time for a simple harmonic oscillator in contact with an energy bath scales as \\(e^{\\beta \\Delta E}\\), where \\(\\Delta E\\) is the energy barrier, and \\(\\beta\\) is the temperature of the bath.\n\n\n\n\n\n\n\nderivation\n\n\n\n\n\nLet the oscillator have \\(n\\) dimensions, then its energy function is \\(H = \\sum_{i=1}^n \\frac{p_i^2}{2m_i} + \\frac{k_i q_i^2}{2}\\), where \\(q_i, p_i\\) are the generalized position and momentum, and \\(m_i, k_i\\) are the effective masses and elastic constants. The Boltzmann distribution is \\(\\rho(q, p) = Z^{-1} e^{-\\beta H}\\), and the probability that it has enough energy to overcome the barrier is\n\\[\nP = \\frac{\\int_{H \\geq \\Delta E} \\rho(q, p)dqdp}{\\int_{H \\geq 0} \\rho(q, p)dqdp}\n\\]\n\n\n\n\n\n\nNotice that the proportionality constant \\(Z\\) is removed. After a change of variables by \\(x_i = \\frac{p_i}{\\sqrt{2m_i}}, y_i = \\sqrt{\\frac{k_i}{2}} q_i\\), we get\n\\[\nP = \\frac{\\int_{\\sum_i x_i^2 + y_i^2 \\geq \\Delta E} e^{-\\beta \\sum_i (x_i^2 + y_i^2)} dxdy}{\\int_{\\sum_i x_i^2 + y_i^2 \\geq 0} e^{-\\beta \\sum_i (x_i^2 + y_i^2)} dxdy}\n\\]\nIntegrating in spherical coordinates, and simplifying, we get \\(P = e^{-\\beta \\Delta E}\\). Thus, the expected time until escape is\n\\[\nT = \\braket{N}\\tau = \\frac{1}{P}\\tau = \\tau e^{\\beta \\Delta E}\n\\]\nthe Arrhenius equation.\nThe same calculation, for a system held in contact with an energy-and-volume bath, gives us \\(T = \\tau e^{\\beta \\Delta G}\\), where \\(\\Delta G\\) is the Gibbs free energy barrier.\n\n\n\nPhase space diagram of a particle in a potential well. As long as it remains within the potential well, it behaves like a simple harmonic oscillator constantly impacted by thermal noise, but if it ever falls within the shaded region, it would escape the potential well. If we observe it within the well, we would turn away and wait for relaxation time  until its phase space distribution thermalizes, then we make another observation. This continues until we observe its escape.The argument given above for the Arrhenius equation is quite generic. It only assumes there is a system that is stuck in some kind of potential well, and is held at a constant temperature somehow. There is no requirement for the system to be an actual particle in an actual well. The “particle” can very well be the 100-dimensional configuration of a protein during folding, or even the simultaneous position of \\(10^{23}\\) helium atoms in a helium gas. Indeed, the Arrhenius equation pops up everywhere as the time until a system escapes an energetic trap.\n\nCommon applications\nIn practice, the Arrhenius equation is used in the form of \\(\\ln r = - a T^{-1} + \\Const\\), where \\(r = 1/T\\) is the “reaction rate”, and \\(a = \\frac{\\Delta E}{k_B}\\) is the slope of the \\(T^{-1} - \\ln f\\) plot (“the Arrhenius plot”). Such plots are extensively used in chemistry, but they are not exclusively found in chemistry.\nIn a glass of water held at constant temperature \\(300 \\ut{K}\\), each water molecule might occasionally reach enough energy to escape into open air. This is evaporation. By this argument, the rate of evaporation follows the Arrhenius law, and indeed it does.\nWhen a pure glass of water is cooled below freezing, it does not actually freeze immediately, because there is still an energetic barrier. Ice is a crystal, but liquid is not a crystal. The barrier between them, where crystal transitions to non-crystal, is penalized. This energetic penalty is what we call “surface tension”. From thermodynamical arguments, at temperature \\(\\Delta T\\) belowe freezing point, the Gibbs free energy required to form a sphere of ice with radius \\(r\\) is\n\\[\nG(r) = (4\\pi \\sigma_{\\text{ice-water}})\\cdot r^2  - \\left(\\frac 43 \\pi \\rho_{\\text{ice}} \\frac{L\\Delta T}{T_{\\text{freezing}}}\\right) \\cdot r^3\n\\]\nwhere \\(L\\) is the latent heat of ice, and \\(\\sigma_{\\text{ice-water}}\\) is the surface tension. This creates an energy barrier of height \\(\\sim \\Delta T^{-2}\\), so the metastable phase of the system can survive for \\(\\sim e^{C\\frac{1}{T(\\Delta T)^2}}\\). (Sethna 2021, chap. 11.3) One must wait for a long while before random chance creates a large enough “seed”, which then grows without bound. This picture we just described is called “bubble nucleation”, as described in classical nucleation theory.\n\n\n\nThe free energy barrier to nucleation.\n\n\nSimilarly, slightly supersaturated water vapor can stay metastable for a long time before it rains spontaneously. In real-life clouds, however, there are always enough dust for droplets to form around them, thus taking a shortcut through the energetic barrier.\nA similar argument applies for disappearing polymorphs. Some chemicals, such as Ritonavir, have two forms (“morphs”) of crystals. Call them A and B. Morph A has higher Gibbs free energy density than morph B, but lower energy barrier of formation. Thus, when a chemist attempts to crystallize it from a solution, by Arrhenius law, morph A would appear much faster than morph B. As soon as one droplet of morph A forms, that would serve as centers around which more of morph A forms. Eventually, however, a small crystal of morph A would spontaneously overcome the energetic barrier, shift all its atomic lattices, and become morph B. That would “infect” all subsequent attempts to crystallize morph A. (Ward 2017)\nThe matter of the disappearing polymorph has serious legal effects. For example, Ritonavir is a medicine for AIDS. It was marketed in 1996 as Form I crystal, which was thought to be the only crystal form for Ritonavir. In 1998, Form II crystals spontaneously appeared. Any lab in contact with Form II could only produce more of Form II. Eventually the company rediscovered how to create Form I despite infection by Form II, but the delay cost the company 250 million USD. (Bučar, Lancaster, and Bernstein 2015)\n\nIn a matter of weeks—maybe five or six weeks, every place the product was became contaminated with Form II crystals.\n\n\n\n\nRitonavir and its two morphs. (Bučar, Lancaster, and Bernstein 2015, fig. 2)\n\n\n\n\nUncommon applications\nIn the simplest model for biochemical process, we just have one chemical reaction following another, until it is complete. If there is a single biochemical step that is much slower than the other steps, then the waiting time for that step dominates, and the total reaction should depend on the temperature by an Arrhenius law. This might explain the observed Arrhenius-law-like dependence on temperature in biological phenomena like tree cricket chirping, alpha brain wave frequency, etc.\n\n\n\n(1) Tree cricket chirping frequency. (2) Firefly flashing frequency. (3) Terrapin heartbeat frequency. (4) Human silent counting rate (Hoagland 1933). These figures are reproduced in (Laidler 1972). (5) evaporation rate of octane (Brennan, Shapiro, and Watton 1974). (6) Alpha frequency of brains of normal, syphilitic paretic, and very paretic humans (Hoagland 1936).\n\n\nWhile writing this, I suddenly recognized (Hoagland 1933) from when I read Feynman’s book all those years ago!\n\nWhen I was in graduate school at Princeton [1939–1942] a kind of dumb psychology paper came out that stirred up a lot of discussion. The author had decided that the thing controlling the “time sense” in the brain is a chemical reaction involving iron… his wife had a chronic fever which went up and down a lot. Somehow he got the idea to test her sense of time. He had her count seconds to herself (without looking at a clock), and checked how long it took her to count up to 60. He had her counting – the poor woman – all during the day: when her fever went up, he found she counted quicker; when her fever went down, she counted slower… he tried to find a chemical reaction whose rates varied with temperature in the same amounts as his wife’s counting did. He found that iron reactions fit the pattern best… it all seemed like a lot of baloney to me – there were so many things that could go wrong in his long chain of reasoning.\n(Richard P. Feynman 1989, 55)\n\nAnd yes, that is the one!\n\nMy wife, having fallen ill with influenza, was used in the first of several experiments. Without, in any way, hinting to her the nature of the experiment, she was asked to count 60 seconds to herself at what she believed to be a rate of 1 per second. Simultaneously the actual duration of the count was observed with a stop-watch.\n(Hoagland 1933)"
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#sec-cft",
    "href": "essays/posts/statistical-mechanics/index.html#sec-cft",
    "title": "Statistical Mechanics",
    "section": "Crooks fluctuation theorem",
    "text": "Crooks fluctuation theorem\nThe field of genuinely non-equilibrium thermodynamics is yet unsettled, so this section is more provisional than the previous ones. Here, we are still considering classical mechanics, with time-reversible Hamiltonian dynamics.\nThe key to modern non-equilibrium thermodynamics is the Crooks fluctuation theorem (CFT), which surprisingly was proven only in 1999. As fundamental physics go, this is really new!\n\nIn a closed system (microcanonical)\nConsider a system held under variable constraints \\(x\\). For example, we have a piston whose head is held at a given length. Alternatively, its head is held at a given force. The point is simply to pick one type of constraint, then stick with it.\nLet the system start in a microcanonical ensemble, at energy \\(E_1\\), then we change the constraints \\(x(t)\\), quickly or slowly, over a time interval \\(t\\in [0, \\tau]\\). The system’s microstate follows some trajectory \\(y(t)\\), arriving \\(y(\\tau)\\), which falls within some new microcanonical ensemble with energy \\(E_2\\).\nPutting a prime means time-reversal. For example, \\(x', y'\\) are \\(x, y\\), but time-reversed.\nWhereas \\(E_1\\) is known, \\(E_2\\) is randomly sampled, as it is determined by \\(E_1\\) (known), \\(x(t)\\) (known), \\(y(0)\\) (randomly sampled from the microcanonical ensemble).\nDuring the forward process, if the system undergoes microstate trajectory \\(y(t)\\), then we have to expend work \\(W[x(t), y(t)] = E_2 - E_1\\).\nLet \\(S_1^*\\) be the maximal entropy of the system when held under the constraints of \\(x(0)\\), and when the system has energy \\(E_1\\). Similarly, let \\(S_2^*\\) be the maximal entropy of the system when held under the constraints of \\(x(\\tau)\\), and when the system has energy \\(E_2\\).\n\n\n\n\n\n\nWarning\n\n\n\n\\(S_1 = S_1^*\\), since the system starts in thermal equilibrium. However, by Liouville’s theorem, entropy is conserved! So we actually have \\(S_2 = S_1 \\neq S_2^*\\), because the system does not end in thermal equilibrium.\nThe key is this: if the system starts at a microcanonical ensemble, then it does not in general end up at another microcanonical ensemble, because as we exert external forcing upon the system, the energy shell deforms, no longer the shell of any microcanonical ensemble.\n\n\nFor example, if we have a piston of gas made of only a few gas molecules, then the constraint is the volume \\(V\\), and we want to study the probability of expending work \\(W\\) if we give the piston head a push. The push can be slow or fast – arbitrarily far from equilibrium. CFT applies no matter how we push the piston head.\n\n\n\nDuring the forward time-evolution of an energy shell \\(E_1\\), the shell is no longer a microcanonical ensemble. The particular trajectory sampled lands upon an energy shell \\(E_2\\). (Sethna 2021, fig. 4.10)\n\n\n\n\n\nThe same situation but in the opposite direction. (Sethna 2021, fig. 4.11)\n\n\nBecause the CFT involves the density of not states, but state trajectories, we need to set up the formalism for path integrals.\nLet \\(\\delta E_1, \\delta E_2\\) be infinitesimals, and let \\(E_1, E_2\\) be real numbers.\nGiven an infinitesimal small bundle of microtrajectories \\(y\\), we can measure its path-space volume as \\(D[y]\\). Suppose they start on the energy shell \\([E_1, E_1 + \\delta E_1]\\), then they would end up somewhere. If we’re lucky, they would end up on the energy shell \\([E_2 + \\delta E_2]\\).\nSuppose the system starts in the microcanonical ensemble on the energy shell \\([E_1, E_1 + \\delta E_1]\\), and we perform the constraint-variation \\(x\\), then there is a certain probability \\(\\delta P\\) that we would sample a trajectory from the small bundle. That small probability is\n\\[\\rho(y | x) D[y]\\]\nwhere \\(\\rho(y | x)\\) is a probability density over path-space. In particular, \\(\\rho(y | x) = 0\\) identically, unless \\(y(0)\\) is on the energy shell \\([E_1, E_1 + \\delta E_1]\\).\nRunning the argument backwards, we can define \\(\\rho'(y' | x')\\), another probability density over paths. This one satisfies \\(\\rho'(y'| x') = 0\\) unless \\(y'(0)\\) is on the energy shell \\([E_2, E_2 + \\delta E_2]\\).\n\nTheorem 11 (microcanonical CFT) For any trajectory \\(y\\) such that it starts on the \\([E_1, E_1 + \\delta E_1]\\) energy shell, and ends on the \\([E_2, E_2 + \\delta E_2]\\) energy shell,\n\\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} = e^{\\Delta S}\\]\nwhere \\(\\Delta S= \\ln\\Omega_2 - \\ln\\Omega_1\\), \\(\\Omega_1\\) is the phase space volume of the first energy shell, and \\(\\Omega_2\\) the second.\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(y\\) does not start on the first energy shell, or does not end on the second energy shell, then either the nominator or the denominator is zero, and so the equation fails to hold.\n\n\n\n\n\n\n\n\nproof\n\n\n\n\n\nIn the forward process, the probability of going along that trajectory is \\[\\rho(x|y) D[x] = \\frac{\\delta V}{\\Omega_1}\\]\nwhere \\(\\delta V\\) is the phase-space volume of the shaded set.\nIn the backward process, the probability of reversing that trajectory is \\[\\rho'(x'|y') D[x']= \\frac{\\delta V'}{\\Omega_2}\\]\n\\(\\delta V' = \\delta V\\) by Liouville’s theorem, and \\(D[x] = D[x']\\) because \\(x'\\) is just \\(x\\) time-reversed.\n\n\n\n\n\nIn an energy bath (canonical)\nNow, suppose we take the same piston of gas, and put it in energy-contact with an energy bath, then at thermal equilibrium, the piston of gas would have the Boltzmann distribution \\(\\propto e^{-\\beta E}\\). We can then give the piston head a push, which would cause it to undergo some kind of time-evolution.\n\nTheorem 12 (Crooks fluctuation theorem) \\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} =  e^{S[x, y]} = e^{\\beta (W[x, y] - \\Delta F^*)}\\]\nwhere \\(S[x, y]\\) is the entropy produced during the forward process, after the system has equilibrated, and \\(\\Delta F^* = F^*_2 - F^*_2\\) is the increase in equilibrium Helmholtz free energy of the system.\nEquivalently, \\(D_{KL}(\\mathcal P_{\\text{forward}} \\| \\mathcal P_{\\text{backward}}) = \\beta (\\braket{W}_{\\text{forward}} - \\Delta F^*)\\), where \\(\\mathcal P_{\\text{forward}}\\) is the probability distribution over microtrajectories in the forward process, and similarly for \\(\\mathcal P_{\\text{backward}}\\).\n\n\n\n\n\n\n\nequilibrium Helmholtz\n\n\n\nIn both forward and backward cases, we start with a thermal equilibrium, and end with a thermal disequilibrium.\nFor example, suppose we have a small tank of a few gas molecules in thermal equilibrium with a large energy bath.\nNow, we quickly push the piston head in according to the function \\(x(t)\\). The trajectory of the system would go through is \\(y(t)\\), which is determined by both \\(x(t)\\) and the initial state of both the system and the energy bath.\nNow, we wait a long time, until the tank is in thermal equilibrium again. Then we pull the piston head out with time-reversed trajectory. Because the forward trajectory was quick, the backward trajectory was also quick. In both cases, the system starts at equilibrium, then becomes disequilibrated, and then becomes equilibrated again. The definition of \\(W\\) is measured during the equilibrium-disequilibrium trajectory, while \\(F^*\\) is measured by comparing the equilibrium-equilibrium, regardless of trajectory.\nWe wrote \\(F^*\\) instead of \\(F\\), to emphasize that we are dealing with equilibrium Helmholtz free energy, defined by \\(F^* = \\min_\\rho (\\braket{E} - TS[\\rho])\\), and not the generic version \\(\\braket{E} - TS[\\rho]\\).\nThis is vitally important, because at time \\(\\tau\\), when the constraints have just reached their new values, the system is not in equilibrium. We would have to hold the constraints constant for a while for the system to return to equilibrium with the energy bath. Despite this, CFT uses \\(\\Delta F^*\\), which is computed at equilibrium.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nApply the microcanonical version of CFT to the entire compound system that includes both the bath and the system, then integrate over all possible microstate trajectories of the bath \\(y_{\\text{bath}}\\).\n\\[\\begin{aligned}\n  S[x, y] &= \\Delta S_{\\text{bath}} + \\Delta S_{\\text{system}} \\\\\n  &= \\beta \\Delta E_{\\text{bath}} + \\Delta S_{\\text{system}} \\\\\n  &= \\beta(W[x, y] - \\braket{\\Delta E_{\\text{system}}}_2) + \\Delta S_{\\text{system}} \\\\\n  &= \\beta (W[x, y] - \\Delta F^*)\n\\end{aligned}\\]\nwhere \\(\\braket{\\cdot}_2\\) means the canonical ensemble average under constraint \\(x(\\tau)\\).\nNotice that the work expended/entropy produced depends only on the system’s microtrajectory \\(y(t)\\), and not on the bath’s microtrajectory \\(y_{\\text{bath}}(t)\\). That is,\n\\[S[x, y, y_{\\text{bath}}] = S[x, y]\\]\nThis will be used again in the next step when we integrate over \\(D[y_{\\text{bath}}]\\).\n\\[\\begin{aligned}\n  \\rho(y|x) &= \\int_{y, y_{\\text{bath}}, x \\text{ is valid}}D[y_{\\text{bath}}]\\; \\rho(y, y_{\\text{bath}} | x)  \\\\\n  &=  \\underbrace{\\int_{y', y'_{\\text{bath}}, x' \\text{ is valid}}D[y'_{\\text{bath}}]}_{\\text{reversible dynamics}}\\; \\underbrace{e^{S[x, y, y_{\\text{bath}}]}\\rho'(y', y'_{\\text{bath}} | x')}_{\\text{microcanonical CFT}}  \\\\\n  &=  \\int D[y'_{\\text{bath}}] \\; e^{{\\color{red} S[x, y]}}\\rho'(y', y'_{\\text{bath}} | x') \\\\\n  &=  e^{S[x,y]} \\int D[y'_{\\text{bath}}] \\; \\rho'(y', y'_{\\text{bath}} | x') \\\\\n  &=  e^{S[x,y]} \\rho'(y'|x')\n\\end{aligned}\\]\n\n\n\n\nCorollary 3 \\[\\frac{\\rho(W | x)}{\\rho'(-W | x')} =  e^{\\beta (W - \\Delta F^*)}\\]\nwhere \\(\\rho(W|x)\\) is the probability density of expending work \\(W\\) in the forward process.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIntegrate over all forward microtrajectories \\(y\\) satisfying \\(W[x, y] \\in [W, W+\\delta W]\\). By reversibility, \\(W[x', y'] = [-W - \\delta W, -W]\\) for such microtrajectories.\n\n\n\nLooking at the proof for CFT for the canonical ensemble, we immediately obtain many other possible CFTs, one per free entropy.\nSuppose we have a piston of magnetic gas in energy-and-volume contact with a bath of constant temperature \\(\\beta\\) and pressure \\(P\\). Now suppose the gas is in equilibrium with the bath, and we vary the external magnetic field over a trajectory \\(x\\). Over the microstate trajectory \\(x\\), the external world would expend both some energy \\(W[x, y]\\) and some volume \\(V[x, y]\\). Thus, we obtain the CFT for Gibbs free energy \\(G\\):\n\\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} =  e^{S[x, y]} = e^{\\beta (W[x, y] + PV[x, y] - \\Delta G^*)}\\]\nSimilarly, suppose we have a chemical reaction chamber of fixed volume, and in energy-and-particle contact with a bath with constant temperature \\(\\beta\\) and chemical potentials \\(\\mu_i\\), we have the CFT for Landau free energy \\(\\Omega\\):\n\\[\\frac{\\rho(y | x)}{\\rho'(y' | x')} =  e^{S[x, y]} = e^{\\beta (W[x, y] -  \\sum_i \\mu_i N_i[x, y] - \\Delta \\Omega^*)}\\]\n\n\nEasy consequences\nLet \\(W\\) be the total work we expended by changing the constraints during the interval \\([0, \\tau]\\). Since the work expended depends on the details of the heat bath and the starting state of the system at \\(t=0\\), this is a random variable.\n\nTheorem 13 (Jarzynski equality) \\[\\braket{e^{-\\beta W}} = e^{-\\beta \\Delta F^*}\\]\nwhere the expectation is taken over many repeats of the same experiment (ensemble average).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIntegrate CFT over all forward trajectories \\(D[x]\\).\n\\[\\rho(y | x) e^{-\\beta W[x, y] } = \\rho'(y' | x') e^{-\\beta\\Delta F}\\]\nnow integrate over \\(\\int D[y]\\), using the fact that \\(D[y] = D[y']\\).\n\n\n\nTHe Jarzynski equality can be understood as a special case of the following statement: \\(e^{-\\beta S_{[0, t]}}\\) is a martingale, where \\(_{[0, t]}\\) is the entropy produced during time-period \\([0, t]\\). See (Roldán et al. 2024) for a derivation, as well as the martingale-theoretic perspective on thermodynamics.\n\nCorollary 4 (violation of second law is exponentially unlikely) \\[Pr((W - \\Delta F^*) \\leq - \\delta W) \\leq e^{-\\beta \\delta W}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nApply Markov’s inequality.\n\n\n\nSince \\(e^t\\) is convex, we have \\(\\Delta F^* \\leq \\braket{W}\\), meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.\nSince \\(\\Delta F^* = -\\frac{1}{\\beta} \\ln\\braket{e^{-\\beta W}}\\), we find that to second order,\n\nTheorem 14 (fluctuation-dissipation relation) \\[\\underbrace{\\braket{W} - \\Delta F^*}_{\\text{work dissipation}} = \\beta \\underbrace{\\frac 12 \\sigma_W^2}_{\\text{work fluctuation}}\\]\n\nFor periodic forcing, the CFT has a simpler form.\nConsider a time-reversible dynamical system immersed in an energy bath with inverse temperature \\(\\beta\\) , driven by periodically varying constraints. For example, a pendulum in a sticky fluid subjected to a periodic driving torque, or a water-cooled electric circuit driven by a periodic voltage.\nSuch a system will settle into a “dynamical equilibrium” ensemble, much like a canonical ensemble. If it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.\n\nTheorem 15 (Gallavotti–Cohen fluctuation theorem) \\[\\frac{\\rho(Q)}{\\rho'(-Q)} = e^{\\beta Q}\\]\nwhere \\(\\rho(Q)\\) is the probability density that a forward cycle, randomly sampled from the dynamical equilibrium ensemble, emits energy \\(Q\\) into the energy bath. Similarly, \\(\\rho'\\) is for the backward cycle.\n\nIt is often used for time-symmetric driving, such as sine waves. In that case, it is typically written as\n\\[\\frac{\\rho(\\dot s)}{\\rho(-\\dot s)} = e^{\\beta T \\dot s}\\]\nwhere \\(T\\) is the driving period, and \\(\\dot s\\) is the entropy production rate.\n\n\n\n\n\n\nProof\n\n\n\n\n\nConstruct a process that starts at equilibrium, then mount up the periodic driving, runs it for \\(NT\\) time where \\(N\\) is a large integer, then remove the driving. At the \\(N\\to\\infty\\) limit, the CFT reduces to this equation."
  },
  {
    "objectID": "essays/posts/statistical-mechanics/index.html#applications-of-cft",
    "href": "essays/posts/statistical-mechanics/index.html#applications-of-cft",
    "title": "Statistical Mechanics",
    "section": "Applications of CFT",
    "text": "Applications of CFT\n\nMolecular machines\nRemember previously when we studied pulling an RNA hairpin apart then together again, and how, with different pulling speed, the wasted energy per cycle is different? Well, we can use the CFT to quantify this.\n\n\n\nSeveral hundred trajectories of pulling and un-pulling an RNA hairpin, at loading rate \\(7.5 \\ut{pN/s}\\). (Collin et al. 2005, fig. 1)\n\n\nIn (Collin et al. 2005), they repeatedly pulled on different types of RNA hairpins, at different speeds, obtaining the following distributions. Not only did they verify the CFT, but also used it to measure \\(\\Delta F^*\\) as the \\(W\\) where \\(\\rho(W) = \\rho'(-W)\\), that is, the crossing point between the two histograms of \\(\\rho(W)\\) and \\(\\rho'(-W)\\). This was a breakthrough, extending statistical mechanics to the single-molecule regime, a fast-developing field of study that is still far from equilibrium.\n\n\n\nHistograms of forward work probability \\(\\rho(W)\\) and backward work probability \\(\\rho'(-W)\\). The mutant type RNA has \\(\\Delta G = 160 k_BT\\), while the wild-type RNA has \\(\\Delta G = 154 k_BT\\). In the inset, the CFT \\(\\ln\\frac{\\rho(W)}{\\rho'(-W)} = \\beta W\\) is directly verified. (Collin et al. 2005, fig. 3)\n\n\nClassically, if we have a single system in thermal equilibrium with a single energy-bath, and we perform a cyclic operation on it, then we can’t extract work, lest we violate the second law.\nBecause the requirement only says that \\(\\braket{e^{-\\beta W}} = 1\\), it is entirely possible for us to extract arbitrarily large amounts of work with arbitrarily high probability, as long as there is a corresponding small probability of losing a lot of work:\n\\[\ne^{-\\beta W}(1-\\delta) + e^{-\\beta W}\\delta = 1, \\quad W = \\beta^{-1} \\ln \\frac{1}{\\delta}\n\\]\n(Maillet et al. 2019) constructed a quantum mechanical device with a single-electron transistor. The electron can expend work. They managed to extract work from the device with over 75% probability.\n\n\n\nHistogram of work extraction. Over 75% of the histogram is in the \\(W &gt; 0\\) part. Despite this, \\(\\braket{e^{-\\beta W}} = 0.989 \\pm 0.03\\), verifying the Jarzynski equation. (Maillet et al. 2019, fig. 3.c)\n\n\n\n\n\n\n\n\nMartingales or bust!\n\n\n\nAs described, the Jarzynski equality is really a consequence of the fact that \\(e^S\\) is a martingale. Martingales, roughly speaking, are payoffs in fair gambles. Imagine playing at a fair casino. You start with cash \\(X_0\\), and after each game, your cash grows or shrinks, resulting in a sequence \\(X_1, X_2, \\dots\\). The game ends when you decide you’ve had enough, goes bankrupt, or the casino goes bankrupt. A fair casino is one where \\(\\braket{X_{t+1} - X_t | X_t} = 0\\), meaning that no matter what your strategy is, at time \\(t\\), your expected payoff is still zero.\nConsider the double-or-nothing gamble. The casino flips a coin, and if heads, you get double the bet, else you get nothing. The classic martingale strategy is to start at betting 1 dollar, and doubling the bet at each loss. It is clear that this strategy has “guaranteed” return – eventually the coin will land heads and you will recoup all losses plus one dollar.\nThis “guaranteed” return is an illusion, because you (or the casino) can go bankrupt. Suppose you start with 127 dollars, then by tracing out all possibilities during one martingale run, you will find that the martingale ends with winning 1 dollar with probability \\(\\frac{127}{128}\\), and losing 127 dollars with probability \\(\\frac{1}{128}\\).\nIn this sense, martingale strategies cannot create free money out of a fair gamble, much like how machines cannot create free energy out of thermal noise. However, martingale strategies can shape the probability distribution of the payoff. For example, in the double-or-nothing game, the martingale strategy shifts almost all probability mass to winning one dollar, at the cost of a small but nonzero probability of losing everything. Similarly, a machine can create a large probability of extracting a little work from a thermal bath, at the price of a small probability of losing a lot of work to the thermal bath.\n\n\n\n\nWorked example: bouncing ball\nThis example is from (Sethna 2021, exercise 4.8), which itself derives from (Lua and Grosberg 2005). See also (Híjar and de Zárate 2010) for another solved example, of a chest expander with mass points stuck in the middle of the springs. You might need to read my tutorial on field-theoretic calculations before attempting that example.\nWe have a one-dimensional system, of a single ball bounding between two walls of a piston. The only control we have is that we can move one of the piston heads. At the start, the piston has length \\(L\\), and the system is in thermal equilibrium at inverse temperature \\(\\beta\\). We plunge the piston head at velocity \\(v\\) for time \\(\\Delta L / v\\), then immediately reverse it, taking another \\(\\Delta L / v\\). We explicitly calculate that \\(\\braket{e^{-\\beta W}} = 1\\).\n\n\n\n(Sethna 2021, fig. 4.12)\n\n\nThe phase space of the ball has 2 dimensions, \\((p, x)\\). The Boltzmann distribution is\n\\[\\rho(p, x) = \\rho(p) \\rho(x) = \\frac{1}{\\sqrt{2\\pi m/\\beta}}e^{-\\frac{\\beta}{2m}p^2} \\times \\frac{1}{L}\\]\nWe assume that \\(L\\) is large enough, such that the ball hits the piston head at most once. There are three possibilities:\n\nIf the piston head hits the ball during the in-stroke, then the ball’s velocity increases by \\(2v\\), and its kinetic energy increases by \\[W = \\Delta KE = 2v(mv - p)\\]\nIf the piston head hits the ball during the out-stroke, then the ball’s velocity decreases by \\(2v\\), and its kinetic energy increases by\n\\[W = 2v(mv+p)\\]\nOtherwise, the piston head avoids the ball, and we have \\(W = 0\\).\n\nIf at \\(t=0\\), the ball is in the phase space region labelled “in region”, then it will be hit in the in-stroke. If at \\(t=\\Delta L/v\\), the ball is in the phase space region labelled “out region”, then it will be hit in the out-stroke. Otherwise, it will not be hit.\n\n\n\nPhase space of the bouncing balls.\n\n\nTherefore,\n\\[\n\\begin{aligned}\n    \\braket{e^{-\\beta W}} &= \\int_{in} \\rho dpdx \\; e^{-\\beta 2v(mv-p)} + \\int_{out} \\rho dpdx \\; e^{-\\beta 2v(mv+p)} + \\int_{other} \\rho dpdx \\; 1 \\\\\n    &= e^{-2\\beta mv^2} \\left(\\int_{in} \\rho dpdx \\; e^{2\\beta vp} + \\int_{out} \\rho dpdx \\; e^{-2\\beta vp}\\right) +  \\int_{other} \\rho dpdx \\; 1\n\\end{aligned}\n\\]\nSince \\(\\rho(p, x) = \\rho(-p, x - \\Delta L)\\), the first two integrals can be combined by flipping the “out region”, then moving it by \\(\\Delta L\\), to “out’ region”.\n\n\n\n\n\n\nNote\n\n\n\nBecause \\(L\\) is large, this is mostly correct, as the regions where this is incorrect has \\(\\rho\\) so small that it is negligible, as seen in the figure.\n\n\nNow we continue:\n\\[\n\\begin{aligned}\n\\braket{e^{-\\beta W}} &\\approx e^{-2\\beta mv^2} \\int_{in, out'} \\rho dpdx \\; e^{2\\beta vp} +  \\int_{other} \\rho dpdx \\; 1 \\\\\n&= \\frac{1}{L\\sqrt{2\\pi m/\\beta}} \\left(\\int_{in, out'} e^{-\\frac{\\beta}{2m}(p - 2mv)^2} dpdx + \\int_{other} e^{-\\frac{\\beta}{2m}p^2} dpdx\\right)\n\\end{aligned}\n\\]\nBecause the “in-out’ region” is symmetric across the \\(p = mv\\) line, we can reflect the first integral across the \\(p=mv\\) line and obtain \\[\n= \\frac{1}{L\\sqrt{2\\pi m/\\beta}} \\left(\\int_{in, out'} e^{-\\frac{\\beta}{2m}p^2} dpdx + \\int_{other} e^{-\\frac{\\beta}{2m}p^2} dpdx\\right) = 1\n\\]\n\n\nOther examples\nSuppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.\nBy Bayes theorem,\n\\[Pr(\\text{forward}|x, y) = \\frac{1}{1 + e^{-S[x, y]}}\\]\nwhere \\(S[x, y] = \\beta(W[x, y] - \\Delta F^*)\\).\nIn words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time. Possibly this is why molecular machines in biology tend to operate with dissipation of around \\(5 k_B T\\), even when less dissipation is possible, to provide a good enough forward bias. Too little dissipation would mean that there is no way to distinguish the direction of time, yet time is inherent in life.\nMaxwell’s demon8 has made it clear that information is physical, and vice versa. It is reasonable to guess that Maxwell’s demon is a converter between information and usable work. Or stated in another way, the information that Maxwell’s demon has is a form of usable work. In (Parrondo, Horowitz, and Sagawa 2015), this analogy is made precise.\n8 I tried to write an essay purely on equilibrium statistical mechanics, but alas, there is no escape from that demon.Consider a “demon” machine made of cameras, motors, and other contraptions, all controlled by its finite-state machine “brain”. There are two boxes of gas of oxygen and nitrogen, that started out in equilibrium at temperature \\(T\\). The demon uses its camera to measure some particle locations, which then causes some changes in its internal state, driving some motors to open a few gates while closing some other gates. The demon finishes its work after a fixed amount of time \\(t\\), ending up with two tanks of gas at temperature \\(T\\), one enriched in oxygen, the other in nitrogen.\nThe system has changed by Helmholtz free energy \\(\\Delta F^*\\). By the Jarzynski inequality, \\(\\braket{e^{-\\Delta S}} = 1\\) where \\(\\Delta S\\) is the entropy production. Here, because the demon might end up with a state that is different from where it started, we need to account for that demon’s inner state. As Landauer and Bennett argued, the problem with Maxwell’s demon is that it cannot forget without paying an energetic cost. And what is forgotten when a demon forgets? It forgets its statistical correlation with the tank of gas. Thus, we should account for that information entropy term by \\(I\\), where \\(I\\) is the mutual information between the demon’s end-state, and the trajectory of the system. This allows us to rescue the second law of thermodynamics:\n\\[\n\\text{entropy production} = \\beta(W - \\Delta F^*) + I\n\\]\nand the Jarzynski inequality becomes \\(\\braket{e^{-\\beta(W - \\Delta F^*) + I}}\\)."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html",
    "title": "Neural Scaling Laws by Data Manifold Dimensions",
    "section": "",
    "text": "This is a theory of neural scaling law, proposed by (Bahri et al. 2021; Sharma and Kaplan 2022)\nAccording to this theory, a neural network, when trained to convergence, allocates its \\(N\\) parameters in two parts: * A fixed number of parameters that map the data to an intrinsic data manifold of dim \\(d\\). * All other parameters that handle pieces of this manifold. Loss \\(\\propto\\) the volume of each manifold piece.\nThey argued that the loss function should scale as \\(L \\propto N^{-4/d}\\) for cross-entropy and mean-square losses."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html#the-manifold-hypothesis",
    "title": "Neural Scaling Laws by Data Manifold Dimensions",
    "section": "The manifold hypothesis",
    "text": "The manifold hypothesis\nConsider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is \\(\\mathbb R^{28\\times 28} = \\mathbb R^{784}\\).\nHowever, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of \\(\\mathbb R^{784}\\). This is the “(intrinsic) data manifold”, with a dimension much smaller than \\(784\\). Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:\n\n\n\nFigure from Convolutional Variational Autoencoder - Tensorflow Core Tutorials. More images can be found in Visualizing the Variational Autoencoder, and can be regenerated in about 10 minutes with a Google Colab GPU.\n\n\nReal data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square \\([0, 1]^2\\), then use a function \\(f: \\mathbb R^2 \\to \\mathbb R^3\\) to “roll up” the square into 3D space.\n\n\n\nThe Isomap algorithm (Tenenbaum, Silva, and Langford 2000), popular for constructing data manifolds. Figure from Wikipedia."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html#deriving-the-scaling-law",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html#deriving-the-scaling-law",
    "title": "Neural Scaling Laws by Data Manifold Dimensions",
    "section": "Deriving the scaling law",
    "text": "Deriving the scaling law\nWe can get a feel for where the number \\(4/d\\) came from by studying a simpler model.\n\nPrototype case\nConsider a problem of regression. We have to learn the true function on the \\(n\\)-dimensional cube: \\(f: [0, 1]^d \\to \\mathbb{R}\\). Assume it is Lipschitz continuous, that is, its first derivative is upper bounded by \\(\\lambda\\). In particular, this means \\(|f(x) - f(y)| \\leq \\lambda |x-y|\\) for all \\(x, y\\) in the domain.\nWe approximate the true function with a piecewise-constant function \\(\\hat f: [0, 1]^d \\to \\mathbb{R}\\), meaning that its graph looks like a staircase. We divide the cube into \\(N\\) equal smaller cubic pieces, and define \\(\\hat f\\) to be equal to the value of \\(f\\) at the center of each cubic piece.\n\nTheorem 1 When the loss is mean square error, it scales like \\(L = \\Theta(N^{-2/d})\\).\n\n\nProof. With \\(N\\) parameters, we can divide the \\([0, 1]^d\\) cube into \\(N\\) equal parts, therefore, each cube has side length \\(N^{-1/d}\\). Therefore, the distance between the center of each cube and the point farthest from the center is also \\(\\Theta(N^{-1/d})\\).\nNow, since the function has bounded first derivative, we know that the difference between the function value at the center of the cube and the function value at the farthest point is bounded by \\(\\lambda \\cdot \\Theta(N^{-1/d}) = \\Theta(N^{-1/d})\\). Therefore, the mean square loss on each individual little cube is bounded by \\(\\Theta(N^{-2/d})\\).\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-2/d})\\).\n\n\n\nGeneralization\nMore generally, if \\(f\\) has bounded second-derivative, and we use a piecewise-linear \\(\\hat f\\) function approximator, then the mean square loss scales like \\(\\Theta(N^{-4/d})\\). By piece-wise linear, we mean that the domain of \\(\\hat f\\) is divided into little cubes, and it is linear on each little cube.\nIndeed, this generalizes in the obvious way:\n\nTheorem 2 If the loss is mean \\(p\\)-th power loss, \\(f\\) has bounded \\(k+1\\)-th order derivatives, and \\(\\hat f\\) is composed of piece-wise \\(k\\)-degree polynomials, then the loss scales like \\(\\Theta(N^{-p(k+1)/d})\\).\nSince the KL-divergence is approximately MSE loss when the predictor is close to correct, the loss scales like \\(\\Theta(N^{-2(k+1)/d})\\) in this case.\n\n\nProof. We prove another case where the loss is still mean square error, but \\(f\\) has bounded \\(2\\)-th order derivatives.\nBy Taylor expansion, if we use the first-order Taylor expansion to approximate \\(f\\) at the center of each cube, then the error is bounded by \\(\\Theta(N^{-2/d})\\). And since the mean square loss is the average of the square of the error, the total mean squared loss is bounded by \\(\\Theta(N^{-4/d})\\) on each little cube.\nAnd since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by \\(\\Theta(N^{-4/d})\\).\nFor the general case, take the Taylor expansion to the \\(k\\)-th order at the center of each little cube.\n\n\n\nScaling of nearest neighbor rule\nWhat is the worst possible scaling? It would be when \\(k=0\\) and \\(p=1\\), giving us \\(L = \\Theta(N^{-1/d})\\). What does this mean? To have \\(k=0\\) means that we use piecewise-constant fitting function \\(\\hat f\\). To have \\(p=1\\) means that we are using the L1-loss. This is essentially piecewise constant, median regression.\nUnder mild assumptions, the nearest neighbor rule for classification has the same form of scaling, where the loss is not L1-loss, but 0-1 loss.\nPeople have found sharper scaling laws under assuming nicer datasets, using complicated functional analysis tools scaling laws. A dataset can be “nice” in several ways. One way is to have few outliers: it should have a thin tail, looking more like a box, rather than a gently rising hill. Another way is to have smoothly varying boundaries: its boundary should not look “bumpy”. See (Yang and Zhang 2023) for a brief review and further citations to the literature."
  },
  {
    "objectID": "essays/posts/scaling-law-by-data-manifold/index.html#experiments",
    "href": "essays/posts/scaling-law-by-data-manifold/index.html#experiments",
    "title": "Neural Scaling Laws by Data Manifold Dimensions",
    "section": "Experiments",
    "text": "Experiments\nAccording to the theory, if the data manifold has dimension \\(d\\), then as we scale up a neural network with \\(N\\) parameters, the MSE loss of a fully-trained network would scale like \\(L \\sim N^{-\\alpha}\\), where \\(\\alpha \\approx 4/d\\). We test this in two ways, once with synthetic datasets, where we know that the data manifold has the desired number of dimensions, and once with the CIFAR-10 dataset, where we do not have the dimension of the data manifold, and must estimate it.\nAll code for generating the dataset, and for analyzing the dataset, are in a GitHub repo: yuxi-liu-wired/scaling-law-by-data-manifold.\n\nSynthetic data manifolds\nSince Consider the simplest data manifold: \\(\\mathbb R^d\\), affine-transformed, then embedded in \\(\\mathbb R^n\\), with \\(n &gt; d\\).\nTo synthesize such a data manifold, we randomly initialize a teacher network, so-called because it implements the function that a student network will fit to by supervised training. Each teacher network is constructed thus:\n\nThe number of neurons in each layer are: \\([d, 9, 600, 600, 1]\\)\nIt has 0 bias.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\) , where \\(m\\) is the input size of the layer (a form of He initialization).\nThe activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.\n\nOnce we have constructed a teacher network, we use it to generate a dataset \\(\\{(x_i, y_i)\\}_i\\) in this way:\n\nGenerate random gaussian vectors \\(\\{t_i\\}_i\\) in \\(\\mathbb R^d\\), with mean \\(0\\) and std \\(I_{d\\times d}\\).\nFor each \\(t \\in \\{t_i\\}_i\\), push \\(t\\) through the teacher network.\nLet \\(x \\in \\mathbb R^9\\) be the teacher network activation at the second layer, with 9 neurons.\nLet \\(y \\in \\mathbb R\\) be the teacher network output.\n\nFirst, we define the “student” neural network architecture:\n\nThe number of neurons in each layer are: \\([9, n, n, 1]\\).\nThe biases are initialized to 0.\nThe weights between layers are sampled from \\(\\mathcal N(0, m^{-1/2})\\), where \\(m\\) is the input size of the layer (a form of He initialization).\nAll activation functions are ReLU.\n\n\n\n\nAn example teacher network architecture with \\([9, 5, 5, 1]\\) neurons.\n\n\nThe parameter count is\n\\[\nN = \\underbrace{(n+n+1)}_{\\text{first layer}} + \\underbrace{(9n + n^2 + n)}_{\\text{second layer}}\n\\]\nWith these settings, I ran the experiment many times, for \\(N\\) ranging from \\(500\\) to \\(10000\\), and \\(d\\) from \\(2\\) to \\(18\\). The results do not look as clean as given in the paper, despite that I have tried my best to match the experimental design as specified in the paper.\n\n\n\nExperimental data for various synthetic dataset dimensions and student network sizes.\n\n\n\n\nCIFAR-10\nThe CIFAR-10 dataset is a popular benchmark, consisting of 32-by-32 RGB images in 10 different image classes, with 6,000 images per class. While the images live in a space of dimension \\(32^2 \\times 3 = 3072\\), (Sharma and Kaplan 2022) reports that the CIFAR-10 images lies in a data manifold with dimension of only around 16–18.\nTo fit the dataset, I trained a family of convolutional networks with 3 convolution layers and 2 fully connected layers on CIFAR-10. In order to run a controlled experiment, I varied as few parameters as possible, with the following designs:\n\nThe network architecture is fixed, and the network parameter count is changed by changing a single number: the number of channels in the convolutional layers.\nThe experiment is run with 20 different network sizes, from 5408 to 115114.\nEach training run lasts 50 epochs, with batch size 128.\nThe optimizer is AdamW with lr=5e-4.\n\nWith these settings, I generated all the data and logged them into TensorBoard log files, then cleaned them up for quantile regression. Plotting in log-log scale, with the x-axis being the model parameter count, and the y-axis being the cross-entropy loss, we would get a downward sloping line. Our hope is that the line should have a slope of close to \\(-4/d\\), where \\(d \\approx 17\\).\nThis is exactly what I have found. Not only is it true for cross-entropy loss, it is also true for classification accuracy (0-1 loss), except the slope is \\(+4/d\\).\n\n\n\nExperimental data for the train/validation splits of CIFAR-10, and with two different criteria: cross entropy loss and accuracy. We see that in all 4 cases, the scaling exponent is close to the theoretical prediction."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html",
    "href": "essays/posts/reading-perceptron-book/index.html",
    "title": "Reading Perceptrons",
    "section": "",
    "text": "It would seem that Perceptrons has much the same role as The Necronomicon – that is, often cited but never read.\nMarvin Minsky, 1994. Quoted in (Berkeley 1997)\nDuring my early forays into AI, perhaps ten years ago, I heard rumors of how in the 1960s, there was a bitter controversy between the symbolic AI and the neural network AI schools. Furthermore, it was said that a certain book, Perceptions, was the center of the controversy – a deadly weapon that killed neural networks – until their resurgence in the 1980s.1 What, then, is in the mythical book? Recently, I have returned to this alleged episode, and in my research into the bottom of the perceptron controversy, I ended up actually reading the book. Now that I have done so, I proclaim:\nIn one sentence, the mathematical portion of Perceptrons is a theory of two-layered perceptrons, mostly by methods typical of discrete mathematics and computational complexity theory, and no empirical results. One should forget about the modern theory and practice of neural networks, read the book in the same frame of mind as one would read a textbook on Turing machines, stack machines, Post tag systems, and other various theoretical objects in computational complexity theory. Indeed, this book is written in the same spirit and style as (Minsky 1967) and (Sipser 2006).\nPerceptron representation occupies chapters 0–10, and learning is only studied in chapters 11 and 12. The book also contained a chapter 13, but it contains mostly interpretations and brief “lessons” that they wanted the readers to take away from the exact mathematical results. They added some handwritten corrections and updates for a 1972 printing run.\nDuring the 1980s rise of connectionism, Minsky and Papert came out with a new edition in 1988. This is the same as the 1972 printing, except that they added a prologue and an epilogue, where they expounded at length the intended lesson of the book, as they felt that people have failed to learn it, and history was repeating itself.\nFor those interpretations, see The Perceptron Controversy."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#setup",
    "href": "essays/posts/reading-perceptron-book/index.html#setup",
    "title": "Reading Perceptrons",
    "section": "Setup",
    "text": "Setup\nLet \\(R\\) be a finite set, where “R” can be read as “region” or “rectangle”.\n\nDefinition 1 (Perceptron) A perceptron is a binary function of type \\(\\{0, 1\\}^R \\to \\{0, 1\\}\\), defined by a weight vector \\(w\\) and a threshold number \\(b\\):\n\\[\n\\Phi(x) := \\theta(w^T x + b)\n\\]\nwhere \\(\\theta(t) := 1_{t \\geq 0}\\) is the 0-1 step function.\n\n\nDefinition 2 (perceptron machine) A perceptron machine with \\(k\\) hidden neurons is a function of type \\(\\{0, 1\\}^R \\to \\{0, 1\\}\\), defined by\n\\[\n\\Phi(x) := \\psi_{k+1}(\\psi_1(x), \\psi_2(x), \\dots , \\psi_k(x))\n\\]\nwhere \\(\\psi_1, \\dots, \\psi_k\\) are (hidden) perceptrons in the hidden layer, and \\(\\psi_{k+1}\\) is the single output perceptron.\n\n\nDefinition 3 (perceptron orders) The order of a hidden perceptron is the number of nonzero weights.\nThe order of a perceptron machine is the maximum order of its hidden perceptrons.\nThe order of a boolean function is the minimum order necessary for a perceptron machine that implements it.\n\nFor example, the constant-0 and constant-1 boolean functions are both of order 0.\nA key focus of the perceptron controversy is the concept of being “conjunctively local”.\n\nDefinition 4 (conjunctively local) A family of boolean functions is conjunctively local iff their orders are upper bounded."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#over-finite-sets",
    "href": "essays/posts/reading-perceptron-book/index.html#over-finite-sets",
    "title": "Reading Perceptrons",
    "section": "Over finite sets",
    "text": "Over finite sets\nWhile their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a computational reduction to the special case of mask perceptron machines. Most of the book considers the problem of classifying subsets of families of planar square grids, such as \\(\\{1, 2, 3\\} \\times \\{1, 2, 3, 4\\}\\). A subset is inputted to the perceptron machine by setting inputs in the subset to \\(1\\), and the rest to \\(0\\). Consequently, it is natural to consider this special case of perceptron machines.\nWhile their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a computational reduction to the special case of the mask perceptron machine. Most of the book considers the problem of classifying subsets of families of planar square grids, such as \\(\\{1, 2, 3\\} \\times \\{1, 2, 3, 4\\}\\). A subset is input to the perceptron machine by setting inputs in the subset to \\(1\\), and the rest to \\(0\\). Consequently, it is natural to consider this special case of perceptron machines.\n\nDefinition 5 (mask perceptron machine) A mask for \\(A\\subset R\\) is a function of type \\(\\{0, 1\\}^R \\to \\mathbb{R}\\), such that \\(\\psi(x) = 1\\) if \\(x_i = 1\\) for all \\(i \\in A\\), and otherwise \\(\\psi(x) = 0\\).\nA mask perceptron machine is a perceptron machine where each hidden perceptron is a mask, and the threshold of the output perceptron is zero. In other words, it is of form\n\\[\n\\Phi(x) = \\theta\\left(\\sum_{i=1}^k a_i \\psi_{A_i}(x)\\right)\n\\]\nwhere each \\(\\psi_{A_i}\\) is a mask, each \\(a_i\\in \\mathbb{R}\\), and \\(k\\) is the number of hidden perceptrons.\n\n\n\nFigure 0.2\n\n\n\n\nTheorem 1 (Theorem 1.5.1) Restricting to mask perceptron machines does not change perceptron orders. That is, any boolean function implemented by a perceptron machine of order \\(k\\) can be implemented by a mask perceptron machine of order at most \\(k\\).\n\n\nProof. Take one such perceptron machine. The threshold of the output perceptron can be removed by adding a hidden perceptron that always outputs \\(1\\) – in other words, \\(\\psi_\\emptyset\\), the mask of the empty set. Now it remains to decompose each hidden perceptron into masks with the same order, or less.\nLet \\(\\psi\\) be a hidden perceptron with nonzero weights on the input points \\(x_{i_1}, \\dots, x_{i_k}\\); then, its output is determined by the values of \\(x_{i_1}, \\dots, x_{i_k}\\). Therefore, we can partition the binary set \\(\\{0, 1\\}^{i_1, \\dots, i_k}\\) into two subsets \\(A_0, A_1\\), such that for any input \\(x\\in\\{0, 1\\}^R\\), we have \\(\\psi(x) = 1\\) iff \\((x_{i_1}, \\dots, x_{i_k}) \\in A_1\\).\nIn other words, we only need to look at the binary values \\(x_{i_1}, \\dots, x_{i_k}\\) to determine the binary output \\(\\psi(x)\\).\nTherefore, we can replace \\(\\psi\\) with a boolean formula on \\(x_{i_1}, \\dots, x_{i_k}\\), then expand it to obtain up to \\(2^k\\) masks, each of order at most \\(k\\).\nFor example, suppose \\(\\psi\\) has nonzero weights on \\(x_1, x_2\\), and is 1 on all odd-sized subsets, then we can write it as a boolean formula:\n\\[\n\\left(x_1 \\wedge \\neg x_2\\right) \\vee\\left(\\neg x_1 \\wedge x_2\\right) = x_1\\left(1-x_2\\right)+\\left(1-x_1\\right) x_2 = x_1 + x_2 - 2 x_1 x_2\n\\]\n\nThe next tool they used is symmetry, formulated in the language of finite group actions.\nLet \\(S_R\\) be the permutation group on the elements of \\(R\\), and \\(G\\) be a subgroup of \\(S_R\\). We say that a boolean function \\(\\psi\\) is \\(G\\)-invariant iff \\(\\psi \\circ g=\\psi\\) for any \\(g \\in G\\). That is, for any \\(X \\subset R\\), we have \\(\\psi(X) = \\psi(g(X))\\). For example, the parity function is \\(S_R\\)-invariant, since any permutation of the set preserves the size, and thus the parity, of any of its subsets.\n\nTheorem 2 (group invariance theorem) If a boolean function is \\(G\\)-invariant, where \\(G\\) is a finite group, then any perceptron machine computing it can be converted to a perceptron machine \\(\\theta(\\sum_i a_i \\psi_i)\\), such that if \\(\\psi_i = \\psi_j \\circ g\\) for some \\(g \\in G\\), then \\(a_i = a_j\\).\n\n\nProof. Take the group-action average: any mask \\(\\psi\\) is equal to \\(\\frac{1}{|G|} \\sum_{g\\in G} \\psi\\circ g\\).\n\nOnce the groundwork was laid, they proceeded to prove a wide variety of theorems on the order of particular boolean functions.\nConsider the parity function. It is \\(1\\) iff exactly an odd number of inputs are \\(1\\) and the rest are \\(0\\).\n\nTheorem 3 (Theorem 3.1) The parity function has order \\(|R|\\).\n\n\nProof. Since the parity function is \\(S_R\\)-invariant, if it is implemented by a perceptron machine of order \\(k\\), it is implemented by some mask perceptron machine \\(\\theta(\\sum_{A_i \\subset R} a_{A_i} \\psi_{A_i})\\), where each mask is of size \\(\\leq k\\), and each weight \\(a_{A_i}\\) depends only on the size of \\(A_i\\). Let \\(b_{|A_i|} = a_{A_i}\\) be those coefficients. It remains to show \\(b_{|R|} \\neq 0\\).\nFor each \\(X \\subset R\\), we have by explicit computation\n\\[\n\\theta\\left(\\sum_{A_i \\subset R} a_{A_i} \\psi_{A_i}\\right) = 1\\left[f(|X|) \\geq 0\\right]\n\\]\nwhere \\(f(t) := \\sum_{i=0}^{|R|}b_i \\binom{t}{i}\\) is a polynomial in \\(t\\). Since this perceptron machine implements the parity function, as \\(t\\) increases from \\(0\\) to \\(|R|\\), the function \\(f(t) + \\epsilon\\) must intersect the \\(x\\)-axis at least \\(|R|\\) times for some real value \\(\\epsilon\\). Since \\(f\\) is a polynomial, it must have at least order \\(|R|\\), thus \\(b_{|R|} \\neq 0\\).\n\n\nTheorem 4 (Theorem 3.2, one-in-a-box) Let \\(A_1, A_2, \\dots, A_m\\) be disjoint subsets of \\(R\\), each of size \\(4 m^2\\), and define the predicate \\(\\psi(X) = \\forall i, \\left|X \\cap A_i\\right|&gt;0\\); that is, there is at least one point of \\(X\\) in each \\(A_i\\). The order of \\(\\psi\\) is \\(\\geq m\\).\n\n\nProof. Let the order of \\(\\psi\\) be \\(k\\).\nThe predicate \\(\\psi\\) is invariant with respect to the group \\(S_{A_1} \\times \\cdots \\times S_{A_m}\\), so by the same construction as the proof of Theorem 2, there exists a polynomial \\(P(t_1, \\dots, t_m)\\), where \\(P\\) has order \\(k\\), and\n\\[\n\\forall t_1, \\dots , t_m \\in \\{0, 1, \\dots , 4m^2\\}, P(t_1, \\dots , t_m) &lt; 0 \\iff t_1 = 0 \\vee \\cdots \\vee t_m = 0\n\\]\nNow define \\(Q(t) := P((t-1)^2, (t-3)^2, \\dots, (t-2m+1)^2)\\). By the above equation, \\(Q &lt; 0\\) at \\(t=1, 3, \\dots, 2m - 1\\) and \\(Q \\geq 0\\) at \\(t = 0, 2, \\dots, 2m\\). Thus, \\(Q\\) has order \\(\\geq 2m\\). Thus, \\(2k \\geq 2m\\).\n\n\nTheorem 5 (Theorem 4.0) There exist predicates \\(\\psi_1\\) and \\(\\psi_2\\) of order 1 such that \\(\\psi_1 \\wedge \\psi_2\\) and \\(\\psi_1 \\vee \\psi_2\\) are not of finite order. Specifically, if we partition \\(R\\) into three equal subsets \\(A, B, C\\), then the boolean function does not have bounded order:\n\\[\n(|X \\cap A| &gt; |X \\cap C|) \\wedge (|X \\cap B| &gt; |X \\cap C|)\n\\]\neven though both \\(|X \\cap A| &gt; |X \\cap C|\\) and \\(|X \\cap B| &gt; |X \\cap C|\\) are of order \\(1\\).\n\n\nProof. \\(|X \\cap A| &gt; |X \\cap C|\\) is computed by the order-\\(1\\) perceptron machine \\(\\theta\\left(\\sum_{i\\in A} x_i - \\sum_{i \\in C}x_i\\right)\\), and similarly for the other one.\nTo show the composite boolean function is not of bounded order, suppose otherwise, then by the same argument as the previous proof, we can construct a sequence of polynomials \\(P_1(a, b, c), P_2(a, b, c), P_3(a, b, c), \\dots\\), such that each \\(P_n\\) is the polynomial corresponding to the perceptron machine for the case where \\(|A| = |B| = |C| = n\\), and each of them has order at most \\(M\\), for some fixed \\(M\\).\nBeing the polynomial corresponding to the perceptron machine for the case where \\(|A| = |B| = |C| = n\\) means precisely that\n\\[\na &gt; c \\wedge b &gt; c \\implies P_n(a, b, c) \\geq 0; \\neg (a &gt; c \\wedge b &gt; c) \\implies P_n(a, b, c) &lt; 0;\n\\]\nfor all \\(a, b, c \\in \\{0, 1, \\dots, n\\}\\). This implies that each \\(P_1, P_2, \\dots \\neq 0\\). Now, divide each polynomial by the root-sum-square of its coefficients, so that if we interpret the coefficients of each \\(P_n\\) as a point in a high-dimensional space, the point falls on the unit sphere in that space. Since the unit sphere is compact, there exists a limit point, which we write as \\(P(a, b, c)\\).\nBy the limit construction, we have\n\\[\n\\forall a, b, c \\in \\mathbb{N}, \\quad a &gt; c \\wedge b &gt; c \\implies P(a, b, c) \\geq 0; \\neg (a &gt; c \\wedge b &gt; c) \\implies P(a, b, c) \\leq 0;\n\\]\nIf we color the points \\(\\mathbb{N}^3\\) with black for \\(P &lt; 0\\) and white for \\(P \\geq 0\\), then we can see the dusty outlines of a pyramid. We can construct a solid pyramid by zooming.\nLet \\(M'\\) be the order of \\(P\\), then we can “zoom out” by taking the projective limit \\(Q(a, b, c) := \\lim_{t \\to \\infty} t^{-M'} P(ta, tb, tc)\\). This \\(Q\\) is a homogeneous polynomial, and by continuity,\n\\[\n\\forall a, b, c \\geq 0, \\quad a &gt; c \\wedge b &gt; c \\implies P(a, b, c) \\geq 0; a &lt; c \\vee b &lt; c \\implies P(a, b, c) \\leq 0;\n\\]\nThis implies that \\(P\\) is identically zero on the “creased curve” \\(\\{ a, b, c \\geq 0, a = c \\vee b = c\\}\\) in the projective plane, which is not an algebraic curve, therefore it is identically zero, contradiction.\n\n\nTheorem 6 (Theorem 5.1) The connectedness function has order \\(\\Omega(|R|^{1/3})\\).\n\n\nProof. If we have a mask perceptron machine that can solve the connectedness problem, then it can be repurposed2 to solve the one-in-a-box problem of the following kind:\n2 Repurposing one machine to solve another problem is a common trick in computational complexity, called “reduction”. For perceptron machines, they called it “Theorem 5.4.1: The Collapsing Theorem”.\n\n\nFigure 5.2\n\n\nIn the picture, the rectangle \\(R\\) has width \\(4m^2\\) and height \\(2m+1\\). We fill in all the odd-numbered rows, and use the machine to solve the one-in-a-box problem for the even-numbered rows. By Theorem 4, the machine has order \\(\\geq m = \\Omega(|R|^{1/3})\\).\n\nIn fact, it turns out that essentially the only locally conjunctive topological invariant is the Euler number.\n\nTheorem 7 (Theorem 5.8.1) The Euler number itself is \\(E(X) = \\sum_{i \\in R} x_i - \\sum_{i, j \\in R} x_ix_j + \\sum_{i, j, k, l  \\in R} x_ix_jx_kx_l\\), where the \\(i, j\\in R\\) ranges only over adjacent points, and \\(i, j, k, l  \\in R\\) ranges only over quadruples that form a square. Thus the Euler number itself has order \\(4\\).\n\n\nTheorem 8 (Theorem 5.9) If \\(\\psi\\) is a boolean function, and it depends only on the topology of the input figure, and its order is upper-bounded by some number \\(k\\) that does not grow even as \\(R\\) grows into a larger and larger rectangle, then \\(\\psi\\) is of form \\(f \\circ E\\), for some function \\(f: \\mathbb{N}\\to 2\\)."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#over-infinite-spaces",
    "href": "essays/posts/reading-perceptron-book/index.html#over-infinite-spaces",
    "title": "Reading Perceptrons",
    "section": "Over infinite spaces",
    "text": "Over infinite spaces\nChapters 6–9 continue in the same style, but move to the case where the input space is made of one or two copies of the infinite line \\(\\mathbb{Z}\\), or the infinite plane \\(\\mathbb{Z}^2\\), and the predicate to recognize is still translation-invariant. In order to avoid blowing up the perceptron machine with infinities, they restricted to the case where the input figure is of finite size, meaning that \\(\\sum_i x_i\\) is finite.\nChapter 6 develops the idea of “spectra” of images. For example, the following picture shows that if we were to design a perceptron machine using only masks of size up to 2, and translation-invariant weights, then it cannot possibly distinguish between the two tetris-like pieces, because both figures contain exactly 4 instances of single-square, 1 instance of two squares side by side, and so on.\n\n\n\npage 100\n\n\nSections 6.1–6.5 of the chapter studies the same idea for masks of size up to 4. For example, Section 6.4 shows that “\\(X\\) is the perimeter of a complete circle” is of order \\(4\\).\nSection 6.6 claims that “recognizing figures in context” is generally not locally conjunctive, although they gave only two examples. Specifically, they showed that the predicate “\\(X\\) is a horizontal line across the rectangle” is order 2, the predicate “\\(X\\) contains one horizontal line across the plane” is not locally conjunctive. The same is true for the case with “a hollow square” instead of “a horizontal line”.\nChapter 7 uses the “stratification” construction to explicitly demonstrate that several binocular predicates are of finite order, but requires exponentially growing weights. It is essentially the same idea as Gödel numbering. A single illustrative example suffices to demonstrate the general point.\n\nExample 1 Given a line \\(\\mathbb{Z}\\), how to construct a perceptron machine that detects that input figure is symmetric?\nSuppose we know for certain that the input figure has leftmost point \\(m\\) and rightmost point \\(n\\), then we can test for symmetry by computing the value of:\n\\[\nf_{m, n}(x) := \\sum_{i = 0}^{n-m} x_{m+i}(x_{n-i} - 1)\n\\]\nWe have that \\(f_{m, n}(x) = 0\\) if the figure is symmetric, and \\(f_{m, n}(x) \\leq -1\\) otherwise.\nNow we define the entire perceptron machine by \\(\\sum_{m \\leq n} M_{n - m} x_m x_n (f_{m, n}(x) - 1/2)\\). If the sequence of \\(M_0, M_1, \\dots\\) grows as \\((d!)^2\\) roughly 3, then the largest bracket \\((m, n)\\) would “veto” every smaller bracket contained within, and so the entire perceptron machine is effectively first testing for the smallest interval containing the figure, before testing the symmetry within that interval.\n3 Expanding term by term, we have \\(|f_{m, n}(x) - 1/2| \\leq 2(n-m) + \\frac{1}{2}\\). Therefore, in order for \\(M_d\\) to “veto” every other bracket within, we need\n\\[\nM_d \\frac 12 &gt; \\sum_{d' = 1}^{d-1} \\left(M_{d'}(\\frac 12 + 2d')(d-d' + 1)\\right)\n\\]\nHere \\(d\\) should be read as “distance between two ends of a bracket”.\nTo bound the growth rate, we bound the recurrence relation \\(M_d = \\sum_{d' = 1}^{d-1} \\left(M_{d'}(4d' + 1)(d-d' + 1)\\right)\\). The sum on the right is bounded by\n\\[\n\\sum_{d' = 1}^{d-1} \\left(M_{d'}(4d' + 1)(d-d' + 1)\\right) \\in \\Theta{\\left[\n    \\sum_{d' = 1}^{d-1} \\left(M_{d'}d'\\right),\n    d^2\\sum_{d' = 1}^{d-1} \\left(M_{d'}\\right)\\right]}\n\\]\nThe lower bound implies \\(M_d = \\Omega((d!)^2 \\times d^{-1})\\) and the upper bound implies \\(M_d = O((d!)^2 \\times (d+1)^2)\\).\nThey made similar constructions for perceptron machines that decide whether two infinite lines or planes are translates of each other, whether two planes are translation and approximate dilations of each other, and whether a line or a plane is the translation of a fixed figure.\nChapter 8 studies diameter-limited perceptron machines, meaning that not only are the hidden perceptrons assumed to be masks of bounded size, those masks are assumed to be contained in a circle of radius \\(M\\) for some finite \\(M\\). It is intended to be a formalization of the intuition that the perceptron machines should model a real pair of eyes scanning a plane, and each retina is a circular disk with finite radius. No results in chapter 8 were used further in the book, and they resemble the case of finite-order perceptron machines, so we do not discuss the results in detail.\nChapter 9 shows that the connectedness problem is easy to solve using small serial programs, a contrast to the case of perceptron machines. The overarching lesson is that connectedness is an “inherently serial” decision problem. The whole chapter is beautiful computational complexity theory, in the same style of solving fun mathematical puzzles, similar to (Minsky 1967).\nThey designed a robot (a finite state machine) that moves around the plane, and can solve the connectedness problem, with a memory size of just two. Namely, it only needs to store up to two locations \\((x, y), (x', y')\\) in its memory during its operation, and it eventually halts in one of three states “empty”, “connected”, and “disconnected”. Notice that a robot with no memory can still remember a finite number of states. It is simply that those memory slots are its finite states, which do not scale with the size of the problem. The little robot with a “memory size of two” really has a memory size of \\(2 \\log_2|R|\\) bits, because it can remember two coordinates from the square \\(R\\), no matter how large the square grows.\nThey then described a few other more exotic computational models, such as a “pebble machine”, meaning a robot that has no memory, but provided with a single pebble. It can drop the pebble anywhere, and it can pick it up again. One can think of this as an intermediate level between pure finite state machines, which cannot write at all, and a Turing machine, which can write as much as it wants. They left as an exercise for the reader the task of translating the previous robot with two memory slots to this robot with one pebble. They conjectured that a finite state machine (a robot with no pebbles) would be unable to solve the task, but they could not prove it. I did a brief literature search and it seemed to be still unsolved."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#learning-theory",
    "href": "essays/posts/reading-perceptron-book/index.html#learning-theory",
    "title": "Reading Perceptrons",
    "section": "Learning theory",
    "text": "Learning theory\nThey claimed that Chapter 10 is part of the learning theory. However, it does not actually involve learning. Whereas in the construction Example 1, we saw coefficients growing exponentially on an infinite plane, chapter 10 proves similar results on a finite plane.\n\nExample 2 (Theorem 10.1) Suppose we have a perceptron machine that tests for parity; then, by Theorem 3, it must have order \\(|R|\\). As in the construction given in the proof, we use the group invariance theorem to obtain a group-symmetric machine with the form \\(\\theta(\\sum_{i=0}^{|R|}\\binom{|X|}{i} b_i)\\), where \\(b_0, b_1, \\dots, b_{|R|}\\) are real numbers. Then, assuming the machine is “reliable”, we can prove that \\((-1)^{M} b_{M+1} \\geq 2^{M}\\) for any \\(M \\in \\{0, 1, \\dots, |R|-1\\}\\).\nSince the group-symmetric construction can only average out the most extreme values, this implies that, before the group-symmetric construction, our perceptron machine had even more extreme coefficients.\nA “reliable” machine is a support vector machine with margin \\(\\geq 1/2\\). That is, it is a machine such that \\[\n\\sum_{i=0}^{|R|}\\binom{|X|}{i} b_i \\begin{cases}\n\\geq 1 & \\text{ if $|X|$ is odd}\n\\leq 0 & \\text{ if $|X|$ is even}\n\\end{cases}\n\\]\n\n\nProof. Define \\(A_n = \\sum_{i=0}^n \\binom{n}{i}b_i\\). Since the machine is reliable, we have that \\((-1)^{n}(A_{n+1} - A_n) \\geq 1\\) for each \\(n = 0, 1, \\dots, |R|-1\\). Simplifying the binomial coefficients, we have \\(A_{n+1} - A_n = \\sum_i \\binom{n}{i} b_{i+1}\\). Note that we use the convenient convention that \\(\\binom{x}{y} = 0\\) if \\(x &lt; y\\).\nNow fix any \\(M \\in \\{0, 1, \\dots, |R|-1\\}\\), and evaluate the following inequality:\n\\[\n2^{M} = \\sum_n \\binom{M}{n} \\cdot 1 \\leq \\sum_n \\binom{M}{n} (-1)^{n}(A_{n+1} - A_n)\n\\]\nBy manipulating the binomial coefficients, the right side simplifies to \\((-1)^M b_{M+1}\\).\n\nSections 10.2 and 10.3 construct two pathological examples. By restricting the shapes of hidden perceptron masks, they proved that certain predicates required superexponential weights. Section 10.4 purports to extend the group invariance theorem to the infinite case.4 None of these are used elsewhere in the book. They are virtuoso performances. However, they interpreted this as a serious problem:\n4 They purported to show in Theorem 10.4.1 that, if the weights are bounded, then the group invariance theorem applies again, but the proof is false, with a counterexample being \\(\\sum_{n \\in \\mathbb{Z}}e^{-n^2}x_n - e^{-n^4}x_nx_{n+1}\\). The theorem might still be correct with another proof, but I cannot find one.\nA proof, in Chapter 10, that coefficients can grow much faster than exponentially with \\(|R|\\) has serious consequences both practically and conceptually: the use of more memory capacity to store the coefficients than to list all the figures strains the idea that the machine is making some kind of abstraction\n\nIn Chapter 11, they finally began discussing perceptron learning, which is of a very restrictive form.\n\nDefinition 6 (Perceptron learning) To train a perceptron machine is to fix its hidden perceptrons, and adjust the weights and threshold of only the single output perceptron by the perceptron learning rule. For this chapter, it is cleaner to change the convention so that each perceptron outputs \\(-1, +1\\) instead of \\(0, 1\\).\nSince only the output perceptron is adapted, it suffices to discuss the case where there are no hidden perceptrons, and we only need to adapt the weights of a single perceptron. That is, we have a dataset \\(D\\), and we sample some \\((x, y) \\in D\\), and verify that \\(y = \\theta(\\braket{w, x})\\).\nIf this is true for all \\((x, y) \\in D\\), then the perceptron learning has converged. Otherwise, we update \\(w\\) using \\(w \\leftarrow w + \\alpha y x\\), where \\(\\alpha &gt; 0\\) is the learning rate.\n\n\nDefinition 7 (Perceptron learning theorem) Let \\(D\\) be a dataset with radius \\(R = \\max_{(x, y) \\in D} \\|x\\|\\). If there exists some unit \\(w^*\\) such that \\(\\gamma = \\min_{(x, y) \\in D} y\\braket{w^*, x}\\), then the perceptron learning algorithm converges after making at most \\((R/\\gamma)^2\\) updates.\n\n\nProof. By linearity of the learning rule, we can deal only with the case where \\(\\alpha = 1\\).\nBy multiplying each \\(x\\) with its \\(y\\), we can deal only with the case where all \\(y = +1\\).\nBy rotating and scaling the space, we can deal only with the case where \\(w^* = (1, 0, \\dots, 0)\\), and \\(\\gamma = 1\\).\nNow, each weight update increases the first coordinate of \\(w\\) by at least \\(1\\), so after \\(n\\) updates, \\(\\|w\\| \\geq n\\). However, each weight update of \\(w \\leftarrow w + x\\) uses a vector \\(x\\) that is pointing in a direction perpendicular to \\(w\\), or worse, pointing against \\(w\\). Therefore, by Pythagorean theorem, \\(\\|w\\|^2\\) increases by at most \\(\\|x\\|^2 \\leq R^2\\). So after \\(n\\) updates, \\(\\|w\\|^2 \\leq nR^2\\).\nCombining the two results, we have \\(n \\leq R^2\\).\n\nModifying the proof slightly, and applying the conclusion of Example 2, we find that starting with the zero weight vector, it takes at least \\(2^{|R|}/|R|\\) steps to learn the parity function.\nThey then suggested that, since gradient descent is “just” a more efficient perceptron learning rule, it also cannot escape local optima. No “local learning rule” can escape local optima, unlike symbolic programs that are provably capable of finding global optima.\nIf the dataset is not linearly separable, then the perceptron weights will not converge. However, the perceptron cycling theorem shows that if the dataset is finite, then the perceptron weights will still be trapped within a large but finite disk, no matter how the dataset is sampled.\nChapter 12 is not very mathematical, and consists mainly of quick sketches5 of other algorithms for learning. Those included: lookup table, nearest neighbor, k-means, ISODATA, maximum likelihood, Bayes, naive Bayes, etc. Sections 12.6 and 12.7 study variations on a toy problem: given a subset of \\(\\{0, 1\\}^n\\), decide whether an \\(n\\)-bit word is in it or not. This had relevance to the time-space tradeoff, a perennial topic in computational complexity.\n5 \nIn this chapter we will study a few of these to indicate points of contact with the perceptron and to reveal deep differences. … The chapter is written more in the spirit of inciting students to research than of offering solutions to problems."
  },
  {
    "objectID": "essays/posts/reading-perceptron-book/index.html#chapter-13",
    "href": "essays/posts/reading-perceptron-book/index.html#chapter-13",
    "title": "Reading Perceptrons",
    "section": "Chapter 13",
    "text": "Chapter 13\nChapter 13 is a summary of the “take-home lessons” for the readers. As the intended lessons were expounded in great length in the epilogue added in 1988, I would not analyze the chapter in detail.\nThey discussed “Gamba perceptrons”, which is just another name for two-layered perceptrons where the hidden layer consists of perceptrons, instead of merely masks. They made the infamous prediction that not only Gamba perceptrons but also arbitrary multilayer perceptrons would be a “sterile extension”.\n\nWell, we have considered Gamba machines, which could be described as “two layers of perceptron.” We have not found (by thinking or by studying the literature) any other really interesting class of multilayered machine, at least none whose principles seem to have a significant relation to those of the perceptron. To see the force of this qualification it is worth pondering the fact, trivial in itself, that a universal computer could be built entirely out of linear threshold modules. This does not in any sense reduce the theory of computation and programming to the theory of perceptrons. Some philosophers might like to express the relevant general principle by saying that the computer is so much more than the sum of its parts that the computer scientist can afford to ignore the nature of the components and consider only their connectivity. More concretely, we would call the student’s attention to the following considerations:\n\nMultilayer machines with loops clearly open all the questions of the general theory of automata.\nA system with no loops but with an order restriction at each layer can compute only predicates of finite order.\nOn the other hand, if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head.\n\nThe perceptron has shown itself worthy of study despite (and even because of!) its severe limitations. It has many features to attract attention: its linearity; its intriguing learning theorem; its clear paradigmatic simplicity as a kind of parallel computation. There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile.\n\nIn short, their objection to multilayer perceptrons is that they suffer from Turing completeness, making it hard to prove anything concrete about them, except the trivial claim that they are Turing complete.6 A single perceptron is just a linear classifier, so it is possible to study mathematically. Experimental evidence is no justification, because:\n6 \nBeware of the Turing tar-pit in which everything is possible but nothing of interest is easy.\n\n\n13.5 Why Prove Theorems?\nWhy did you prove all these complicated theorems? Couldn’t you just take a perceptron and see if it can recognize \\(\\psi_{\\text {CONNECTED}}\\)?\nNo.\n\n\n\n\npage 239\n\n\nSince a perceptron-only architecture is not the right way, they illustrated what they believe to be the “right” way to do computer vision by describing in detail the scene analysis algorithm. In short, it starts by discovering edges, then performs some computational geometry on them to recover outlines of basic shapes, such as cubes, then fills in the faces, then fills in the bodies between the faces. The algorithm had to do something clever to deal with occlusions. In the end, a complete 3D scene populated with 3D objects is recovered.\n\n\n\nScene analysis. Figure from (Guzmán 1968)\n\n\nThey ended the book with a brief discussion of how they discovered the various results, as well as a list of people they thanked."
  },
  {
    "objectID": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html",
    "href": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html",
    "title": "Perfect diffusion is TC0 – Bad diffusion is Turing-complete",
    "section": "",
    "text": "This essay has a companion paper on arXiv."
  },
  {
    "objectID": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#idea",
    "href": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#idea",
    "title": "Perfect diffusion is TC0 – Bad diffusion is Turing-complete",
    "section": "Idea",
    "text": "Idea\n\nWidth and depth\nSome problems take a lot of work to solve. It is a lot of work to solve Riemann’s Hypothesis. It is also a lot of work to solve a trillion grade-school arithmetic problems. However, they are not the same kind of hard work, even if the required amount of “work” may be the same. One is “deep” while another one is “wide”. Recall the old joke about how 1000 people cannot dig a hole in 1 second.\nWe say that a computation is “wide” if it has many steps, but the steps can take place in any order. In particular, they can be done concurrently. It can be finished faster either by using a processor with a faster clock, or more processors. We say that a computation is “deep” (or perhaps “inherently sequential”) if it has many steps that must take place one after another. Using more processors would not help. Only using a processor with a faster clock would help.\nLanguage modeling research has shown that autoregressive models need “chain of thought” when solving certain reasoning tasks. This benefit isn’t just observed in practice, but has theoretical foundations. (Feng et al. 2023) In the big picture, both theory and practice has shown that chain of thought approaches benefit precisely in allowing a variable amount of sequential processing, which is necessary for solving problems that need deeper computation than one forward pass allows. Trying to do it with less leads to errors, guesswork, or inefficient memorization in a desperate attempt to solve the problem “out of the model’s depth”.\nAs an example, if your program solve both easy and difficult Sudoku puzzles in exactly the same number of steps, you might reasonably question whether your method actually works for the hard puzzles. At one end of the limit, an easy Sudoku puzzle could be solved in a few steps by filling in each blank in parallel, since each blank could be solved by checking its row, column, and square. There is no sequential dependence between the blanks. At the other end of the limit, a hard Sudoku puzzle would involve a large amount of dependence between many blanks, which would require deep tree searches and significant backtracking to solve. It would be unreasonable to expect a parallel algorithm to solve a hard Sudoku puzzle, no matter the “width” of the parallelism, if it does not have the requisite “depth”.\n\n\n\nAn easy Sudoku puzzle can be solved in parallel. Here, the two circled blanks have no interdependence. Each can be filled simply by checking their own rows, columns, and squares.\n\n\n\n\n\nAdvanced Sudoku puzzles require advanced techniques to solve, which usually involve drawing interdependencies across the board. Illustrated here are “equivalent sets”. In each image, the red-colored blanks must contain the same numbers as the blue-colored blanks. Expert Sudoku puzzlers memorize a large number of such patterns to cut down the possibilities.\n\n\n\n\nA brief history of scaling\nThe recent history of deep learning has been drunk-high on parallel computing. Pushed to the very limits of parallelism: 256, 2048, 8192. How many cores can you pack on a single chip? This came at the price of precision. FP64, FP32, INT8… How low can you go? There are even proposals of a E4M0 floating point number – 4-bit float, 0-bit mantissa!\nBut it hasn’t always like this.\nFundamentally, this was not an unqualified success, but also a cope with lagging hardware. Since 2005, clock rate has been stuck at 4 GHz, placing a hard limit on how many serial steps you can make per wallclock time. GPUs are stuck at 1–2 GHz. This failure at the hardware level eventually propagated up the entire tech stack.\n\n\n\nThe clock rate has plateaued since 2005. Subsequent progress in computer processing speed relied mostly on parallelism, which here is shown in the form of multi-core performance. (Leiserson et al. 2020, fig. 2)\n\n\nAt the architecture level, we went from sequential models like RNNs to the highly parallelizable Transformer architecture. Some architectures are inherently more sequential than others. Recurrent neural networks (RNN) must process \\(n\\) tokens in \\(n\\) steps. So if a single step takes 1 ms, then processing 1000 tokens take 1 second. In contrast, a Transformer can take any number of tokens in 1 step, limited only by the size of your VRAM and processor count. As long as memory and processor count scaled, the stagnation in clock rate was not a binding constraint… Is it?\nAt the algorithm level, we went from reinforcement learning (RL) to imitation learning and pretraining. To do true RL, the network had to actually play out games one step after another. Imitation learning however is simply pretraining by another name, and can be scaled in parallel.\nThe success of pretraining over RL, 1M-batch over minibatch, Transformers over RNN, Nvidia over Intel, are all triumphs of the parallel over the serial, but perhaps we need to take account of it again? Might the pendulum swing back?\n\n\n\n\nsequential\nparallel\n\n\n\n\nalgorithm\nRL\nimitation learning, (self-)supervised (pre)training\n\n\narchitecture\nRNN\nCNN, Transformer, SSM\n\n\nhardware\nCPU\nGPU\n\n\n\nFor language generation, the story is quite clear. Before 2016, the SOTA was LSTM, which must be trained sequentially. Starting around 2018, pretrained Transformers took the lead, for which training is as parallel as the hardware can bear. Yet around 2021, sequential compute returned to inference in the form of “chain of thought”. With the DeepSeek-R1 of 2025, it is now quite probable that training will became a lot more sequential.\nA similar story could be told for image generation. Before 2019, the SOTA was the U-net, which generates the whole image in parallel. In 2021, OpenAI shocked the world with DALL-E, a standard Transformer architecture that generated images patch-by-patch. But soon, diffusion models took over with their fully parallel generation across all pixels. And most recently, the image generator in OpenAI’s ChatGPT seemed to have reverted back to generating images patch-by-patch.\nAs for RL itself, the 2013–2017 period was its highlight, with RL agents becoming superhuman at Atari, Dota, and Go. Indeed, during the early days of OpenAI, it focused on RL, with the Dota bot being its first publicity success. However, things changed starting with GPT-1 of 2018, the generatively pretrained Transformer. The so-called “RL from human feedback” (RLHF) is only barely RL, since each “game” in an RLHF training run consists of a single reply! It is as if playing a game that ends after making exactly one move.\nIf the best kind of training shifts back to be more serial than parallel, then there will be a reckoning. If a few months down the line, we see a new scaling law emerge from the big labs pushing the R1-Zero training method to even longer chains at even smaller batches, then deep learning can suddenly acquire a very serial RL phase. Pretraining would still need giant GPU clusters, but the RL step would suddenly look more like Cray supercomputers – a few giant CPUs, immersed in Fluorinert, running at 100 GHz. Compared to Nvidia GPUs, those would be much more expensive in terms of FLOP/USD, but in terms of serial-FLOP/sec/USD, cheap!\n\nSeymour Cray was famous for packing, powering, and cooling circuits incredibly densely. Classic Crays were made obsolete by microprocessors, but we may yet do similar things at a larger scale. Hyperscale data centers and even national supercomputers are loosely coupled things today, but if challenges demanded it, there is a world with a zetta scale, tightly integrated, low latency matrix dissipating a gigawatt in a swimming pool of circulating fluorinert.\n— John Carmack (2020-08-30)\n\nIn this scenario, an RNN-Transformer hybrid might become the common architecture for cutting-edge training, in two phases. In the first phase, it would be pretrained in parallel over ~1 million GPUs with ~1 million batch size. In the second phase, it would be reinforcement-learning with a minibatch of rollouts per update, on special hardware that maximizes the update frequency. This future hardware could be a small cluster of ~10 massive RL-specific chips with diameter ~1 meter packaged closely to maximize memory bandwidth, immersed in liquid nitrogen cooling to run at ~100 GHz without melting. Such hardware does not exist today, since it is highly uneconomical compared to existing GPUs, as measured in FLOP/sec. However, in a scenario where a long sequence of serial training is necessary, such hardware may be economical as measured in serial FLOP/sec.\n\n\n\n\n\n\nWhy these numbers?\n\n\n\n\n\nAn RL-specific chip may be ~1 meter in diameter, because Cerebras already produces chips that are ~20 cm on each side, giving a lower bound. On the other direction, a silicon wafer is ~50 cm in diameter, giving an upper bound.\nThe GPU and batch size are both in the millions, because those numbers are used for the current (2025) largest pretraining runs.\nAS for the ~100 GHz clock rate, we have the following data:\n\nAs of 2025, hobbyist overclockers have already reached 9.1 GHz using just a commercially available Intel CPU, cooled by liquid nitrogen, so I believe another factor of 10 is within the technical possibilities.\nIn 2000, Intel planned for a CPU at 10 GHz for 2005. The plan fell through, and in 2004, Intel hastily cancelled the originally planned-for next-generation chips (Tejas and Jayhawk) due to intractable overheating. However, it shows that 10 GHz is well within the limits of known silicon technology. The heat dissipation issues are certainly not a hard technological limit, but merely too uneconomical in a consumer product.\n\nThough currently uneconomical, it is entirely possible for such technology to be resurrected for cutting-edge AI training. Compared to the currently planned million-GPU datacenters, such a special-purpose CPU cluster would be comparatively affordable.\n\n\n\n\n\nActually existing complexity\nLike axiomatic logicians, computational complexity theorists can occasionally be defensive, since they are often haunted by the fear of irrelevance. This is not an idle fear, especially in the context of AI research. It is healthy to try to not become like Minsky and Papert who, having exhaustively demonstrated that 1-layered neural networks are not powerful, declared deeper neural networks to be a “sterile extension” of 1-layered neural networks, based on their intuition.\nIn the best case, computational complexity theory can discover algorithms that are fast in practice, or prove that certain algorithms are impossible in a way that matters. In the less happy case, algorithms can be invented that are fast in theory, but not in practice, such as the “galactic algorithms”. In the worst case, an impossibility proof is correct, yet wrong in a way that matters. How could this be the case? Every mathematical abstraction makes certain inaccuracies, and a lot can slip through the cracks between the real world and the mathematical realm.\nAs a concrete example, consider protein folding. In theory, a protein is merely a floppy string of amino acids, and to fold it, there seems to be no way but to try out every one of its configurations in search of its minimal-energy form. However, even nature does not laugh at the difficulties of computation. If a protein is actually hard to fold, then it would be dangerous for the organism to produce such a thing, and thus it would be removed by evolution. In this way, the problem of “folding proteins that a biologist might wish to fold in practice” turns out to be an easier problem than “folding proteins”. This perhaps explains the success of AlphaFold, which can fold a protein in a few forward passes. Surely it is not solving a hard problem with an impossibly small amount of compute. Rather, what appears to be a hard problem is not really hard, and what went wrong was the original act of abstraction that turned protein folding in practice into an unrealistically hard problem in theory.\nSo where is this paper placed? We place it into the category of “half-irrelevant for practice”. On the one side, the “perfect” diffusion is a vanishing point that can never be reached in practice. On the other side, the Turing-complete bad diffusion is a construction that has given up any pretense of being about diffusion models. It is a deliberate arbitrary code execution exploit of the standard diffusion framework. Indeed, we believe that any attempt to train a score network in practice would result in a score network that is quite tame, and definitely incapable of arbitrary code execution.\nSo why this paper? One, because it is fun. Two, because some of the intuition might be helpful for understanding actually existing diffusion models. It is in this sense that we say this paper is “half-irrelevant”. Real diffusion models are not perfect, but also not malicious, and they seem to behave closer to perfection than malice. Because of this, we conjecture that real diffusion models do converge fast, in \\(O(1)\\) steps. Assuming this is the case, then using them for solving tasks that require more than \\(O(1)\\) serial steps is impossible.\n\n\nDiffusion models\nA diffusion model can be used to sample from a distribution with a variable amount of computing steps. This is usually understood as an advantage, in the sense of providing a compute-precision tradeoff: With a few steps, one can sample from the distribution approximately, and with increasing number of steps, the distribution can be sampled from with increasing precision. However, this intuitive picture also suggest that this advantage may be a curse. Specifically, it suggests that after a few sampling steps, further computation is “wasted” in the sense that they refine the result in a way that does not matter, because the result has converged.\nIndeed, empirically, diffusion models typically converge rapidly with a fixed number of denoising steps regardless of input. For example, (Ravishankar et al. 2024) studied using diffusion models for depth-perception, and showed that there is no difference between 5 and 100 sampling steps, Similarly, (Austin et al. 2021) showed that for a diffusion language model they trained, the perplexity of language modeling was essentially the same for 10 and 1024 diffusion steps.\n\n\n\n(Ravishankar et al. 2024, fig. 8)\n\n\n\n\n\n(Austin et al. 2021, fig. 2)\n\n\nWhile such rapid convergence may be regarded as an advantage, this would not be when the problem is deep. Converging on a solution faster than the depth of the problem requires would likely lead to failures. Indeed, certain kinds of empirical failures of diffusion language models suggest that they struggle precisely on tasks that require sequential processing. For instance, when using diffusion models to solve Sudoku puzzles, (Wewer et al. 2025) found that denoising all digits simultaneously worked for easy puzzles but failed for difficult ones. Performance improved only when denoising fewer digits at a time, with optimal results achieved by denoising just one digit at a time, essentially reverting to a purely sequential process.\n\n\n\nSolving Sudoku with a diffusion model. If the entire grid is denoised all at once, the model would nearly always fail on the hard puzzles. If the grid is denoised one number at a time, the model would sometimes succeed on the hard puzzles. (Wewer et al. 2025)\n\n\n(Arriola et al. 2025) noted that discrete diffusion models underperform compared to autoregressive approaches. They proposed to solve this by… reintroducing autoregressive generation, applying diffusion to generate a few tokens at a time.\n\n\n\nThe “block diffusion” language modeling method, which despite the hopeful captioning, is only 4 times as “parallelizable” as autoregressive language modeling. (Arriola et al. 2025, fig. 1)"
  },
  {
    "objectID": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#setting-up-the-framework",
    "href": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#setting-up-the-framework",
    "title": "Perfect diffusion is TC0 – Bad diffusion is Turing-complete",
    "section": "Setting up the framework",
    "text": "Setting up the framework\nThe theory part of the paper has three parts:\n\nA mathematical framework in which these empirical observations kind of exists in kind of the same way. We make no promises that this framework actually has anything to do with practice, but it looks fun.\nA theorem showing that a “perfect” diffusion models are constrained to the \\(\\mathsf{TC}^0\\) complexity class due to their rapid convergence.\nAn explicit construction of a “bad” diffusion models to perform any Turing-computable operation. The construction works precisely because they do not converge quickly. They do not converge quickly precisely because they fail at reversing any forward diffusion process.\n\nWe begin by setting up the framework.\n\nDiffusion modeling\nDiffusion models work by gradually adding noise to data and then learning to reverse this process. Think of it as watching a drop of ink spread through water, and then learning to recover the original drop from the diluted state. This intuitive physical analogy connects to their mathematical foundation, which borrows concepts from thermodynamics and statistical physics.\nIn machine learning literature, two main formulations have emerged to describe this process. The first, Denoising Diffusion Probabilistic Models (DDPM), approaches the problem through discrete time steps. The second, score-matching with Langevin dynamics (SMLD), uses continuous differential equations. Despite their different origins, these two approaches are fundamentally equivalent (Kingma et al. 2021; Luo 2022).\nThe connection works in both directions. DDPM can be seen as a discretized version of SMLD, where each DDPM update step corresponds to using the Euler–Maruyama method to solve SMLD’s stochastic differential equation (SDE). Conversely, if we take the limit of infinitely many DDPM steps with infinitesimally small noise additions, we recover the continuous SDE formulation of SMLD. This equivalence means that models trained using either framework can be used interchangeably for sampling purposes.\nFor clarity and mathematical convenience, we primarily use the SMLD formulation throughout this paper, though our results apply equally to both frameworks, since they are equivalent.\nConsider a data distribution \\(\\rho_{data}\\) over the real space \\(\\mathbb{R}^d\\). The task of SMLD is to learn a score-matching function \\(f_\\theta\\) that allows us to sample from \\(\\rho_{data}\\).\nA noise schedule is a continuous function \\(\\beta\\) of type \\([0, \\infty) \\to [0, \\infty)\\), such that \\(\\beta(t)\\) can be interpreted as the noising rate in the forward diffusion process at time \\(t\\). We require \\(\\int_0^\\infty \\beta(t) dt = \\infty\\), which can be interpreted as saying that eventually all signal is destroyed, leaving only noise.\nDefine the distribution at \\(t=0\\) by \\(\\rho_0 := \\rho_{data}\\). Suppose we sample a data point \\(x_0 \\sim \\rho_0\\), and let it evolve according to the SDE\n\\[\ndx_t = -\\frac{1}{2} \\beta(t) x_t dt + \\sqrt{\\beta(t)} dW_t\n\\]\nthen this implies a time-evolution of the data distribution over time, which can be directly solved by the Fokker-Planck equation:\n\\[\n\\partial_t \\rho_t = \\frac{1}{2} \\beta(t) (\\nabla \\cdot (x \\rho_t) + \\Delta \\rho_t)\n\\]\nAt the \\(t \\to \\infty\\) limit, the distribution converges to the standard normal distribution \\(\\mathcal{N}(0, I_d)\\).\nFor any time \\(T &gt; 0\\), the time-evolution can be exactly reversed as follows. Let \\(\\hat{x}_T\\) be sampled according to \\(\\rho_{\\hat{x}, 0} := \\rho_T\\), then the following SDE equation would lead to an exact reversal: \\[\nd\\hat{x}_t = \\frac{1}{2} \\beta(T-t) \\hat{x}_{t} dt + \\beta(T-t) \\underbrace{\\nabla_{\\hat{x}_{t}} \\ln \\rho_{T-t}(\\hat{x}_{t})}_{\\text{score function}} dt + \\sqrt{\\beta(T-t)} dW_t\n\\] where by “reversal” we mean that \\(\\rho_{\\hat{x}, t} = \\rho_{T-t}\\) for any \\(t \\in [0, T]\\) (Anderson 1982).\nAssuming that a score-matching function \\(f_\\theta\\) has been trained, such that\n\\[\nf_\\theta(x, t) \\approx \\nabla_x \\ln \\rho_t(x)\n\\]\nfor all \\(t, x\\), then \\(\\rho_{data}\\) can be approximately sampled by initializing a pure-noise sample \\(\\hat{x}_T \\sim \\mathcal{N}(0, I_d)\\), then solving the backward SDE\n\\[\n\\hat{x}_{t-dt} = \\frac{1}{2} \\beta(t) \\hat{x}_t dt + \\beta(t) f_\\theta(\\hat{x}_t, t) dt + \\sqrt{\\beta(t)} dW_t\n\\]\nby any SDE integration method, such as Euler–Maruyama method. By varying the sizes of the \\(dt\\) steps in the Euler–Maruyama method, we can recover different noise-schedules for DDPM.\nIf \\(f_\\theta(x, t) = \\nabla_x \\ln \\rho_t(x)\\) is exact, then at the limit of \\(T \\to \\infty\\) and infinitely many steps in the Euler–Maruyama method, we can exactly sample from \\(\\rho_{data}\\).\n\n\nCircuit complexity theory\nA circuit complexity class is a style computational complexity classes. In our paper, we focus on the \\(\\mathsf{TC}^0\\) class, which is particularly suited to studying the computational complexity of neural networks, because a family of feedforward neural networks with a constant number of layers is essentially a \\(\\mathsf{TC}^0\\) circuit family. Indeed, the class of \\(\\mathsf{TC}^0\\) were first proposed specifically in the 1980s to model the computational complexity of neural networks. (Parberry and Schnitger 1988)\nFormally, \\(\\mathsf{TC}^0\\) is defined as the class of problems that can be decided by a family of boolean circuits with the following properties:\n\nBoolean circuits: A boolean circuit is a directed acyclic graph where each node (or gate) computes a boolean function of its inputs. The inputs to the circuit are boolean variables, and the output is a single boolean value.\nUnbounded fan-in: Each gate in the circuit can receive inputs from an arbitrary number of other gates. This contrasts with bounded fan-in circuits where gates have a limited number of inputs. Convolutional neurons have bounded fan-in, but fully-connected neurons have unbounded fan-in.\nPolynomial width: The number of gates at each level of the circuit is bounded by a polynomial in the input size \\(n\\).\nConstant depth: The longest path from any input to the output is bounded by a constant that does not depend on the input size. This may be interpreted as stating the circuit family is “highly parallelizable”.\nThreshold gates: A threshold gate is a binary neuron. It can be written as a function \\(\\theta(\\sum_i w_i x_i + t)\\), where \\(w_i, t\\) are real numbers, and \\(\\theta\\) is the binary step-function\n\nFor those unfamiliar, here is a short exercise:\n\nWith 1 layer of threshold gates, construct “gadgets” such as the AND gate, the NOT gate, and all other common boolean gates.\nWith 2 layers, construct the \\(k\\)-EQUALS gate for each \\(k\\), which outputs 1 if exactly \\(k\\) inputs are 1, and 0 otherwise.\nWith 3 layers, construct the IS-IN gate for any finite subset of \\(\\mathbb{N}\\).\n\nFrom the definition, it is clear that each member \\(\\mathsf{TC}^0\\) circuit family is essentially a feedforward neural network. However, this only consists of a single member. Here is where the “family” part of the definition becomes important.\nSince a neural network has a fixed number of inputs, it would be unable to process more inputs than the number of neurons in its input. This brings the idea of a circuit family. A circuit family is a set of circuits \\(C_1, C_2, \\dots\\) such that each \\(C_n\\) is capable of processing exactly inputs of length \\(n\\). Computational complexity theory studies not the complexity of problems solvable by a single circuit, but a circuit family, because any single circuit is merely equivalent to a lookup table, and the complexity of the problem it solves is always trivial. If this seems odd to you, remember that to a computational complexity theorist, 1 and 1 trillion are the same – both are \\(O(1)\\).\nConsequently, a \\(\\mathsf{TC}^0\\) family of feedforward neural networks is defined as a set of neural networks \\(C_n\\), such that there exists a constant \\(D\\) (the upper bound on depth), and a polynomial \\(p\\) (the polynomial bound on width), such that each \\(C_n\\) has depth \\(\\leq D\\) and number of neurons \\(\\leq p(n)\\).\nWhile the \\(\\mathsf{TC}^0\\) class is most similar to feedforward fully-connected neural networks, this is not necessarily the case. Indeed, a family of bounded-depth polynomial-width Transformers is still in the \\(\\mathsf{TC}^0\\) class. This means the theorem in the paper applies to them as well.\n\n\nLanguage modeling\nAt the most abstract level, a language is simply a set of words made of letters. Formally:\n\nAn alphabet \\(\\Sigma\\) is a finite nonempty set. Each element in the alphabet may be called a letter or a token.\nA word in an alphabet \\(\\Sigma\\) is a finite sequence of elements of \\(\\Sigma\\).\nA language \\(L\\) in an alphabet \\(\\Sigma\\) is a set of words in the alphabet \\(\\Sigma\\).\n\nA prefix language modeling problem is, given a sequence of tokens \\(x_1, \\dots, x_n\\), to compute the next token \\(x_{n+1}\\). An example would be the word problem for finite groups: Given a finite group \\(G\\), and a sequence of elements in the group \\(g_1, \\dots, g_n\\), compute \\(\\prod_{i=1}^n g_i\\). In particular, if \\(G\\) is the permutation group on 5 elements, then the corresponding group multiplication problem is strongly suspected to lie outside \\(\\mathsf{TC}^0\\) class.\nAn example would be the word problem for finite groups: Given a finite group \\(G\\), and a sequence of elements in the group \\(g_1, \\dots, g_n\\), compute \\(\\prod_{i=1}^n g_i\\). Intuitively, there is a method that computes this in \\(\\log_2(n)\\) parallel steps by binary multiplication: the first parallel step computes \\(g_1g_2, g_3g_4, \\dots\\), and so on. Since \\(\\log_2(n)\\) is not constant, this would not lie within the \\(\\mathsf{TC}^0\\) class. For certain groups, there are shortcuts to this process. For example, for any prime number \\(p\\), the word problem in the mod-\\(p\\) multiplicative group is computable in constant number of parallel steps via Fermat’s little theorem. However, shortcuts probably do not exist in general. Indeed, if \\(G\\) is the permutation group on 5 elements, then the corresponding word problem is not in the \\(\\mathsf{TC}^0\\) class, assuming widely believed conjectures in computational complexity theory. (Liu et al. 2023)\nWhile usually, a diffusion model is used for generating from a continuous state space such as \\(\\mathbb{R}^d\\), it can be used to model discrete distributions as well. This is necessary for language modeling. We consider the case closest to continuous state space modeling – quantization: One divides the continuous state space \\(\\mathbb{R}^d\\) into regions, and assigns a token to each region. This then allows sampling a discrete distribution from a diffusion model with continuous state space. Formally, if \\(\\Sigma = \\{a_1, a_2, \\dots, a_M\\}\\) is the alphabet, then we divide \\(\\mathbb{R}^d\\) into \\(M\\) regions \\(V_1, \\dots, V_M\\), such that each region \\(V_i\\) maps to a token \\(a_i\\).\nAlso, as usual in circuit complexity theory, we need more than a single score-network \\(f_\\theta\\), but rather, a full sequence of them, so we define a **\\(\\mathsf{TC**^0\\) family of score-networks} to be a family of feed-forward neural networks \\(f_{\\theta, 0}, f_{\\theta, 1}, \\dots\\) , such that:\n\nEach \\(f_{\\theta, n}\\) takes as input \\(n+2\\) elements \\(x_1, \\dots, x_n, x, t\\), and produces an output \\(f_{\\theta, n}(x, t | x_1, \\dots, x_n)\\).\nThe family \\(f_{\\theta, n}\\) has \\(O(1)\\) depth and \\(\\mathsf{poly}(n)\\) width.\n\nNote that for the theorem to hold, it is not necessary to assume the family of neural networks are feed-forward. The theorem holds for any family of score-networks for which a single forward pass is in \\(\\mathsf{TC}^0\\). This includes, for example, Transformers and state-space models (Merrill, Petty, and Sabharwal 2025, 2025). We stay with feedforward networks because it is visually obvious how they are in the \\(\\mathsf{TC}^0\\) class.\nFinally, since a diffusion model may solve a problem only with high enough probability, instead of solving it deterministically, we make the following definition: A prefix language modeling problem is solved with constant probability bound if there exists some \\(\\epsilon &gt; 0\\), such that for each input token sequence \\(x_1, \\dots, x_n\\), let \\(x_{correct}\\) be the correct response, then \\[\np(x_{correct}|x_1, \\dots, x_n) &gt; p(x'|x_1, \\dots, x_n) + \\epsilon, \\quad \\forall x' \\neq x_{correct}.\n\\]\n\n\nCounter machines\nTo show that a deliberately bad diffusion model may be Turing-complete, we show how they could simulate a particular kind of Turing-complete abstract machines: the counter machines. This is not necessary for understanding the theorem on “perfect” diffusion models.\nA counter machine can be thought of as finite-state automata augmented with memories, each of which can hold a single unbounded integer. In our paper, we use the following form of counter machine, lightly modified from (Fischer, Meyer, and Rosenberg 1968):\n\nThe machine has access to a finite number \\(k\\) of registers, notated as \\(r_1, \\dots, r_k\\). Each register stores a single integer.\nThe machine also has access to a read-only input tape, on which the machine has a read-head that can be moved in either direction. At machine start-up, the input tape has contents ^\\(a_1a_2\\dots a_n\\)$, where ^ and $ denote the beginning and the end of the word, and \\(n\\) is the length of the input word. The read-head is placed at the character just after ^, which may be $ if the input word is empty.\nA program for the machine is a numbered list of instructions.\nEach instruction is of the following format: conditional on the state of the read-head on the input tape and on whether each register is zero or not, modify every register by an amount in \\(\\{-1, 0, +1\\}\\), move the read-head by up to one position in either direction, then jump to another instruction.\nThere is a special instruction named “HALT”. If the machine arrives at such an instruction, it halts. Each HALT instruction may be marked as either an accepting HALT, or a rejecting HALT.\nTo accept an input word means the machine reaches an accepting HALT state. Similarly for rejection.\nA decider for a language is a machine that accepts words in the language, and rejects words out of the language. It must halt on all inputs.\n\nIt is known that counter machines are Turing-complete, in the sense that a universal Turing machine can be simulated by a counter machine with 2 registers. (Minsky 1967) This implies in particular that any language that is decidable by a Turing machine is decidable by a counter machine."
  },
  {
    "objectID": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#the-main-part",
    "href": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#the-main-part",
    "title": "Perfect diffusion is TC0 – Bad diffusion is Turing-complete",
    "section": "The main part",
    "text": "The main part\n\nPerfect diffusion is in TC0\nSuppose there exists a \\(\\mathsf{TC}^0\\) family of score-networks \\(f_{\\theta, 0}, f_{\\theta, 1}, \\dots\\), such that for each \\(n\\) and each \\(x_1, \\dots, x_n\\), the function \\(f_{\\theta, n}(x, t | x_1, \\dots, x_n)\\) exactly computes the score function of some initial distribution \\(\\rho_{0, n}\\) with bounded first moment: \\(\\mathbb{E}_{x_0 \\sim \\rho_{0, n}}[\\|x_0\\|] \\leq 1\\).\nIf this family solves a prefix language modeling problem at the limit of infinite time SMLD with constant probability bound, then the problem is in the \\(\\mathsf{TC}^0\\) class.\nThe idea of the proof is simple. We first quote an inequality from the literature, which provides a universal upper bound on how many steps are sufficient for sampling the SMLD within a constant probability bound, then we derandomize it while still remaining within the \\(\\mathsf{TC}^0\\) class.\nThe big idea of the derandomization is as follows: Given an algorithm that generates the correct token with probability that is of a bounded amount higher than generating any incorrect token, we can run the algorithm many times, and take the majority vote. By Hoeffding’s inequality, there exists some random seed, for which the majority vote is correct on every single length-\\(n\\) input. Now we hardcode that random seed.\nThe details of the proof (which is short) are in the paper. \\(\\blacksquare\\)\n\n\nInexact score-matching\nThe requirement for exact score-matching is necessary for the following two reasons:\nFirst, the technical reason is that the full form of the inequality we quoted is\n\\[\nTV(\\rho_{DDPM, T}, \\rho_{\\hat{x}, 0}) \\leq c \\frac{d (\\log T)^3}{T} + c \\epsilon_{\\text{score}} \\sqrt{\\log T}.\n\\]\nwhere the term \\(\\epsilon_{\\text{score}}\\) denotes the score-matching error between the true score function of \\(\\rho_{\\hat{x}, 0}\\) and the approximation \\(f_\\theta\\). As this extra term increases with \\(T\\), the proof above does not apply. Perfect score-matching sets that term to zero, thus allowing the theorem to work.\nSecond, the intuitive reason is that if we have no requirement on score-matching, then there is essentially no constraint on the computational power of SMLD, by the construction in the next section.\nPractically relevant score-networks are intermediate between two extreme cases. We believe that if \\(f_\\theta\\) is a good enough, but not perfect, score-matching network, then a generalized version of the above theorem still applies. However, finding the right way to quantify the goodness, as well as proving such a generalization, is left as future work.\n\n\nBad diffusion is Turing-complete\nSince counter machines are Turing-complete, it suffices to show how to simulate any counter machine with diffusion. Suppose we are given a counter machine with \\(k\\) registers, we simulate it by constructing a “pinball” machine that operates according to the SDE\n\\[\nd\\hat{x}_t = \\frac{1}{2} \\hat{x}_t dt + f_\\theta(\\hat{x}_t, t) dt + dW_t\n\\]\nunder a smooth force field \\(f_\\theta\\). The pinball machine has a single ball, whose location is \\(\\hat{x}_t\\). The ball rolls around a state space \\(\\mathbb{R}^d\\) guided by the force field \\(f_\\theta(\\hat{x}_t, t)\\). Indeed, the force field can be time-independent, so we write it as \\(f_\\theta(\\hat{x}_t)\\) instead.\nThe state space is divided into three parts as \\(\\mathbb{R}^d = \\mathbb{R}^k \\times \\mathbb{R} \\times \\mathbb{R}\\). The first part \\(\\mathbb{R}^k\\) represents the \\(k\\) registers. The second part \\(\\mathbb{R}\\) represents the program counter, which tracks the line-number of the program. The third part \\(\\mathbb{R}\\) is used for jumping between instructions, providing enough room for the ball to roll without “crossing the wires”.\nThe space is divided into cubic cells of side lengths \\(L\\). We denote each cell by \\(k + 2\\) integers.\nLike the state space, the force field has three parts too. One part simply cancels out the \\(\\frac{1}{2} x_t\\) term. Another part forms “grooves” along which the ball rolls, thus implementing the counter machine. The third part points towards the center-lines of the grooves, so that the ball is not knocked off the grooves by the noise term \\(dW_t\\). Of course, eventually the noise will knock the ball off the grooves, but if the force is strong enough, and the cubic cells have a large enough side length, then the machine will reach completion without being knocked off the grooves, with arbitrarily large probability.\nInstead of formally specifying the grooves, it is simpler to give an example. Suppose at line number 32, the instruction reads “If the current state of register 1 is zero, then increment register 2 and jump to line 23, else jump to line 33”, then this is implemented by drawing the following paths:\n\n\\((0, r_2, \\dots, r_d, 32, 0) \\to (0, r_2 + 1, \\dots, r_d, 32, 32) \\to (0, r_2 + 1, \\dots, r_d, 23, 32) \\to (0, r_2 + 1, \\dots, r_d, 23, 0)\\).\n\\((r_1, r_2, \\dots, r_d, 32, 0) \\to (0, r_2, \\dots, r_d, 32, 32) \\to (0, r_2, \\dots, r_d, 33, 32) \\to (0, r_2, \\dots, r_d, 33, 0)\\) for nonzero \\(r_1\\).\n\nBy smoothing the corners of the paths, we obtain a smooth force field.\n\n\n\n\n\n\nHow to keep the ball on grooves\n\n\n\n\n\nHow strong must the force be to keep the ball rolling on the grooves? The noise term \\(dW_t\\) would, over a long enough time, eventually knock the ball off the grooves. This can be suppressed by either using a strong confinement force field, or by using a weak confinement force field but a large cubic cell side length \\(L\\). It turns out that we don’t need a very strong force or a very large cell.\nLet the counter machine have \\(N\\) instructions. Suppose it halts within \\(S\\) steps, then the total distance travelled by the pinball would be \\(O(NSL)\\), where we need to account for the time necessary to jump between instructions. Then, since the rate of leakage is on the order of \\(e^{-L^2}\\), we need only require \\(L \\geq O(\\sqrt{\\ln (NSL)})\\) to suppress the probability of leakage during the entire computation to a small constant. In particular, for any fixed \\(N, S\\), because \\(L\\) grows faster than \\(\\sqrt{\\ln (NSL)}\\), there exists a big enough \\(L\\) for which the machine will halt without leakage, for probability as close to \\(1\\) as one desires. This machine operates under a force field that is smooth, and has Lipschitz-continuity bounded by universal constant.\nSuppose that we have a language that is decidable by a Turing machine when it is restricted to a working tape with length \\(O(f(n))\\), where \\(n\\) is the input length, and \\(f\\) is some monotonically increasing function, then by (Fischer, Meyer, and Rosenberg 1968, Theorems 3.1 and 3.2), it is decidable by a counter machine that takes \\(e^{O(f(n))}\\) steps to halt. Thus, it suffices when \\(L \\geq O(\\sqrt{f(n)})\\)."
  },
  {
    "objectID": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#future-work",
    "href": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#future-work",
    "title": "Perfect diffusion is TC0 – Bad diffusion is Turing-complete",
    "section": "Future work",
    "text": "Future work\n\nTheoretical\nWe have shown that perfect diffusion models with exact score matching are constrained to \\(\\mathsf{TC}^0\\), while deliberately “bad” diffusion models can be Turing-complete. The more realistic intermediate case remains open, where the score network approximately computes the score function.\nWe conjecture that similar computational limitations apply when the approximation quality is sufficiently high, but formalizing this notion of “sufficiently good approximation” and proving the corresponding result requires further work. We make this conjecture based on two reasons. One, the aforementioned empirical observation that diffusion models converge rapidly. Two, because the forward diffusion converges exponentially rapidly to the standard normal distribution \\(\\mathcal N(0, I)\\), we believe that the backward diffusion process, as long as it is sufficiently close to the score function of a forward diffusion process, would be forced to converge in \\(O(1)\\) time, since exponential decay is fast decay.\nOur analysis focuses on diffusion models operating on \\(\\mathbb{R}^d\\) with subsequent discretization. However, other formulations of discrete diffusion exist, such as the directly discrete approach in (Austin et al. 2021). For these models, we conjecture that \\(\\mathsf{TC}^0\\) limitations apply regardless of score network quality, as the finite state space inherently constrains the “computational capacity” of the diffusion process. Intuitively, a finite state space allows encoding only a finite number of bits per state before the signal-to-noise ratio1 is exhausted, and the reverse diffusion reaches \\(t=0\\).\n1 See (Kingma et al. 2021) for a formalization of signal-to-noise in diffusion modeling.Between finite state spaces and \\(\\mathbb{R}^d\\) lies the intermediate case of continuous but compact state spaces, such as the unit ball in \\(\\mathbb{R}^d\\). While our “pinball machine” construction would still work in such spaces, it would require dividing the compact space into an increasing number of cells. This means the force field, while smooth, cannot maintain bounded Lipschitz-continuity coefficients. Because of this, we hypothesize that under the additional requirement of \\(O(1)\\) Lipschitz-continuity, diffusion models on compact spaces would be constrained to \\(\\mathsf{TC}^0\\) regardless of score network quality, effectively making them computationally equivalent to finite state models.\n\n\nEmpirical\nThe animating big-picture idea behind this paper is that certain tasks are inherently sequential, such that any parallel computation that takes too little sequential steps must necessarily err. Sequential processing and consequences of its lack has been systematically studied for Transformers under the name of “chain of thought”, but not for diffusion models. We have collected a few suggestive examples gleaned from the literature, but it would be a valuable contribution to the literature to test this hypothesis systematically on diffusion models. We conjecture:\n\nTasks requiring deep sequential reasoning should exhibit a sharp performance cliff when addressed by diffusion models with a fixed number of denoising steps.\nAdding more denoising steps beyond a certain threshold should yield minimal improvements for \\(\\mathsf{TC}^0\\) tasks but continued improvements for tasks outside this complexity class.\nPerformance on complex sequential tasks should improve significantly when introducing autoregressive components, as seen in (Arriola et al. 2025).\n\nControlled experiments testing these predictions would provide valuable empirical validation of our theoretical framework and guidance for the further development of diffusion models.\n\n\nArchitectural\nThe most promising direction may be architectures that interpolates sequential and parallel computation dynamically, shifting to the sequential mode for tasks that demand them. We point out several particularly worthy directions for interpolation:\n\nIn architecture, interpolation between massively parallel models (Transformers, state-space models) and sequential ones (recurrent neural networks).\nFor language generation, interpolation between full-sequence generation (typical of diffusion language models) and autoregressive generation (common in Transformer-based models). While both approaches have been studied extensively in isolation, their combination remains relatively unexplored.\nInterpolation between SMLD and neural ODE frameworks. SMLD offers rapid convergence through massive parallelism, while neural ODEs provide slower convergence with more sequential computation."
  },
  {
    "objectID": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#conclusion",
    "href": "essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/index.html#conclusion",
    "title": "Perfect diffusion is TC0 – Bad diffusion is Turing-complete",
    "section": "Conclusion",
    "text": "Conclusion\nI don’t read conclusions in papers, and I don’t write them either. They seem like a stupid convention, when having an abstract and an introduction is not enough to pad your page numbers, so you had to rephrase them, differently but the same… In fact, the only reason there is a conclusion in the arXiv paper is because academic style requires there to be one, which is why I got Claude 3.7 to write it… without any modification!"
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "",
    "text": "Originally published as No nonsense version of the “racial algorithm bias” — LessWrong.\nIn discussions of algorithm bias, the COMPAS incident of 2016 has been too often quoted out of context. This post gives the facts, and the interpretation, as quickly as possible. See this for details."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "The fight",
    "text": "The fight\nThe COMPAS system is a statistical decision algorithm trained on past statistical data on American convicts. It takes as inputs features about the convict and outputs a “risk score” that indicates how likely the convict would reoffend if released.\nIn 2016, ProPublica organization claimed that COMPAS is clearly unfair for blacks in one way. Northpointe replied that it is approximately fair in another way. ProPublica rebukes with many statistical details that I didn’t read.\nThe basic paradox at the heart of the contention is very simple and is not a simple “machines are biased because it learns from history and history is biased”. It’s just that there are many kinds of fairness, each may sound reasonable, but they are not compatible in realistic circumstances. Northpointe chose one and ProPublica chose another."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-math",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-math",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "The math",
    "text": "The math\nThe actual COMPAS gives a risk score from 1-10, but the phenomenon is clearer when there are only two possibilities instead of 10.\nConsider the toy example where we have a decider (COMPAS, a jury, or a judge) judging whether a group of convicts would reoffend or not. How well the decider is doing can be measured in at least three ways:\n\nFalse negative rate = (false negative)/(actual positive)\nFalse positive rate = (false positive)/(actual negative)\nCalibration = (true positive)/(test positive)\n\nA good decider should have false negative rate close to 0, false positive rate close to 0, and calibration close to 1.\nVisually, we can draw a “square” with four blocks:\n\n\n\na square with 4 blocks\n\n\n\nfalse negative rate = the “height” of the false negative block,\nfalse positive rate = the “height” of the false positive block,\ncalibration = (true positive block)/(total area of the yellow blocks)\n\nNow consider black convicts and white convicts. Now we have two squares. Since they have different reoffend rates for some reason, the central vertical line of the two squares are different.\n\n\n\ntwo squares, one for White, one for Black\n\n\nThe decider tries to be fair by making sure that the false negative rate and false positive rates are the same in both squares, but then it will be forced to make the calibration in the Whites lower than the calibration in the Blacks.\nThen suppose the decider try to increase the calibration in the Whites, then the decider must somehow decrease the false negative rate of Whites, or the false positive rate of Whites.\nIn other words, when the base rates are different, it’s impossible to have equal fairness measures in:\n\nfalse negative rate\nfalse positive rate\ncalibration\n\nOne more thing: Even when base rates are different, there’s a way to have equal fairness measures in all three of those, but it requires the decider to be perfect: Its false positive rate and false negative rate must both be 0, and its calibration must also be 1.\n\nInteractive demonstration\nThe following is an interactive diagram demonstrating the problem. The bars in the squares are draggable. You will find that there is no way to equate all three numbers (true positive rate, true negative rate, calibration), unless you change the vertical bar.\n\n\n\n\n\nA slight generalization\nIn the jargon of fairness measurement, “equal false negative rate and false positive rate” is “parity fairness”; “equal calibration” is just “calibration fairness”. Parity fairness and calibration fairness can be straightforwardly generalized for COMPAS, which uses a 1-10 scoring scale, or indeed any numerical risk score.\nBy routine algebra, in this general case, parity fairness and calibration fairness are incompatible when the base rates are different, and the decider is not perfect. See (Kleinberg, Mullainathan, and Raghavan 2016) for this general case, a literature review, and other real-life occurrences."
  },
  {
    "objectID": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight-after-math",
    "href": "essays/posts/no-nonsense-algorithmic-bias/index.html#the-fight-after-math",
    "title": "The Racial Algorithmic Bias Controversy",
    "section": "The fight, after-math",
    "text": "The fight, after-math\nNorthpointe showed that COMPAS is approximately fair in calibration for Whites and Blacks. ProPublica showed that COMPAS is unfair in parity.\nThe lesson is that there are incompatible fairnesses. To figure out which to apply – that is a different question.\nI wrote the essay in 2019, during my undergraduate research on risk measures. From the vantage point of 2024, it certainly feels like algorithmic fairness has lost much of the hotness. Instead of the monthly outrages about how Google’s image algorithm identified some black people as “gorillas” (2015), or the COMPASS bail algorithm (2016), now algorithmic fairness is handled by a separate team, right beside the public relations team, the load balancing team, the fiber optics team, and the data wrangling team.\nThe point being, algorithmic fairness has left the realm of philosophical and political debates and entered the realm of bureaucracy. Fairness is no longer the key to the meaning of life and self-worth, but a matter of passing statistical tests."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html",
    "href": "essays/posts/mixture-of-experts/index.html",
    "title": "Mixture of Experts",
    "section": "",
    "text": "The code for the post is available at moe.ipynb."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#theory",
    "href": "essays/posts/mixture-of-experts/index.html#theory",
    "title": "Mixture of Experts",
    "section": "Theory",
    "text": "Theory\nMixture of Experts is an old technique dating back to 1991, but it has become a vital component of modern deep learning to get around the memory bottleneck. There is not much theory to speak of, because this is honestly a very simple technique. Let’s say you have a few predictive models. Each model is an expert. Now you take all of them and combine their predictions in some way – that’s mixture of experts.\n\nMixing\nConsider a simple example. Suppose we are to classify points on the \\(\\mathbb{R}^2\\) plane into 2 classes. Suppose that we can only use a single linear-logistic function \\(f(x) = \\frac{1}{1 + e^{w^T x + b}}\\), then we can write down this classifier:\n\\[\n\\hat y := \\begin{cases}\n1, \\quad & \\text{if }f(x) &gt; 0 \\\\\n0 , & \\text{otherwise}\n\\end{cases}\n\\]\nIn other words, we have a logistic regression model.\n\n\n\nAn example of a logistic regression model. The curve shows the estimated probability of passing an exam versus hours studying. If we have to do a binary prediction, then we predict \\(\\hat y = 1\\) iff \\(x \\geq 2.7\\), that is, we predict the student would pass the exam iff they had studied more than 2.7 hours. Figure from Wikipedia\n\n\nLike perceptrons, logistic regression is simple, fast, and has a very elegant theory – and like perceptrons, logistic regression does not work if the underlying system is not linearly separable.\nNow, consider the simplest example that is not linearly separable: a binary classification on the plane. One class falls into the first quadrant, and the other into the other 3 quadrants. There is some noise, so the points near the edges do not always fall into their respective classes. There is no way to perform this task well with just one logistic classifier, but with two, we should be able to perform this task well enough.\n\n\n\nA scatterplot of points that fall into 2 classes that are not linearly separable.\n\n\nLet’s design the 2 experts manually, and somehow combine them. The 2 experts should each handle one of the edges:\n\\[\nf_1(x, y) = \\frac{1}{e^{10 x}+1}, \\quad f_2(x, y) = \\frac{1}{e^{10 y}+1}\n\\]\nIn words, \\(f_1\\) is a smooth approximation to the 0-1 function that sends \\((x, y)\\) to \\(1\\) iff \\(x &gt; 0\\), and similarly, \\(f_2\\) is a smooth approximation to the 0-1 function that sends \\((x, y)\\) to \\(1\\) iff \\(y &gt; 0\\). How do we combine them?\nWe can add another “manager” which is an expert at picking experts. It would pick \\(f_1\\) if the point \\((x, y)\\) falls above the diagonal line \\(x=y\\), and pick \\(f_2\\) otherwise. This would then give us\n\\[\nf(x, y) = \\begin{cases}\nf_1(x, y), \\quad &\\text{if } y-x &gt; 0 \\\\\nf_2(x, y), \\quad &\\text{if } y-x &lt; 0\n\\end{cases}\n\\]\nThis is the simplest example of sparsely-gated MoE. For each point, the manager picks the right expert to call, and call that expert. The other expert does not ever need to be activated, saving half the compute, the manager’s computation is so simple that it does not cost anything compared to the expert’s computation, which contains an exponential.\nWe can also combine the experts by a linear function, as in\n\\[\nf(x, y) = \\sum_{i = 1}^2 p_i(x, y) f_i(x, y)\n\\]\nwhere \\((p_1, p_2)\\) is a probability distribution over the experts that depends on \\((x, y)\\), such as \\(\\mathop{\\mathrm{softmax}}(A(x, y))\\) where \\(A\\) is a linear operator, that is, a matrix. For lack of a better word, I call this dense MoE.\n\n\nSparsifying\nGiven a MoE, there are two ways to use it. One can use it as-is, but then every expert must be consulted on every query, defeating the main purpose of MoE in the age of large models: conditional computing. Therefore, the model should be sparsified.\nIn the first MoE paper (Jacobs et al. 1991), they manually inspected the weights (the matrix \\(A\\) in our notation), and found that some experts would never be called on any input. Then they just removed those experts. This can be understood as sparsification “at compile time”.\nIn the toy model, I trained 6 logistic regression experts to classify 2-dimensional points, so the matrix \\(A\\) has 6 rows and 2 columns. To sparsify the model at compile time to only \\(k\\) experts, I took the matrix \\(A\\) and ranked them according to their L2-norm, found the top-\\(k\\) rows of them, then mask out all the other experts. The resulting heat maps at various levels of \\(k \\in 1:6\\) are as follows.\n\n\n\nHeat maps of the compile-time sparsified MoE at various levels of sparsity.\n\n\nAs expected, when \\(k=1\\), we have only one expert taking care of everything, and end up with a linear classifier. When \\(k=2\\), the sparsified MoE looks much closer to the correct classifier. When \\(k \\geq 3\\), it becomes indistinguishable.\nNow, for the sparsely-gated MoE, the sparsification is done “at runtime”. That is, for each input \\(x\\), we find the top-\\(k\\) experts for this specific \\(x\\), and use those experts:\n\\[w(x) = \\mathop{\\mathrm{softmax}}(\\mathrm{top}_k(Ax))\\]\nwhere \\(\\mathrm{top}_k(v)\\) preserves the top-k entries of \\(v\\), but set all other entries to \\(-\\infty\\). This means we have to keep all experts at runtime, since each expert might be needed for some specific input point, but every input point would only activate a few experts. The key is that the activated experts depend on \\(x\\), unlike the MoE sparsified at compile time, which always activates the same few experts. This means we can achieve a lower sparsity, and less compute. We trade memory for performance and compute.\nIn the same toy model, the resulting heat maps at various levels of \\(k \\in 1:6\\) are as follows.\n\n\n\nHeat maps of the sparsely-gated MoE at various levels of sparsity.\n\n\nCompared with the compile-time sparsified MoE, the sparsely-gated MoE is already usable when \\(k=1\\), and it looks like a piecewise-linear classifier. When \\(k=2\\), it already becomes indistinguishable from the correct classifier."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#brief-history",
    "href": "essays/posts/mixture-of-experts/index.html#brief-history",
    "title": "Mixture of Experts",
    "section": "Brief history",
    "text": "Brief history\nIn the beginning was the gaussian. The gaussian is a beautiful distribution, with linearity, the central limit theorem, fast inference, least squares regression, and so on. The problem is that it has just one peak.\nIf one wants to model a complicated distribution with several bumps, one can make one step up the staircase of complexity, and build distributions from a linear sum of several gaussians. This is the mixture of gaussians. More generally, simple statistical models like the Poisson distribution, the Bernoulli distribution, and so on, can be added together to create mixture models.\n\n\n\nA mixture of three gaussian bumps. Figure from Wikipedia.\n\n\nA mixture of experts is then a simple generalization, and training a mixture of experts, back in the old days, was mostly thought of as statistical inference. The main problem was simply modelling complex data with a larger family of statistical distribution. Their main worry was that the experts would overfit.\nThey had little data (enough to fit onto a floppy disk), and each expert was usually just a gaussian distribution or a logistic classifier (any more complex and they wouldn’t know how to calculate the integrals and derivatives). Consequently, what they ended up trying to solve was to fit a few thousand datapoints using tens of very simple experts.\nIt is a general fact of classical machine learning that they were very worried about overfitting, and it was a reasonable worry back then, since they had such small datasets (MNIST was in 1994). This, combined with their inability to hand-design learning algorithms for complex machine learning architectures and the slowness of pure gradient descent, meant that machine learning algorithms back then were simple ones fitted onto small datasets.\n\n\n\ncost of\n1980s\n2010s\n2020s\n\n\n\n\ndata\nhigh\nlow\nlow\n\n\nalgorithm\nhigh\nlow\nlow\n\n\ntraining\nlow\nmedium\nhigh\n\n\ninference\nlow\nmedium\nhigh\n\n\n\nThe overall effect is:\n\ngetting training data: expensive (you have to do it yourself);\ndesigning the algorithm: expensive (cheaper if you have graduate students);\ntraining compute: low (there was little funding for training);\ninference compute: very cheap (since you could not train large models).\n\nThis should be compared to the very different situation with deep learning since the 2010s:\n\ngetting training data: cheap (just download it online);\ndesigning the algorithm: cheap (make a standard network, add a few decorations, then use backprop with Adam optimizer);\ntraining compute: as expensive as you want;\ninference compute: as expensive as you want.\n\nWhile classical statistics and machine learning was mainly constrained by how many partial derivatives and integrals the statistician can calculate confidently on paper,1 deep learning is mainly constrained by memory and compute budget.\n1 If you want a taste of the old days, look at the formulas inside (Jordan and Jacobs 1994). They explicitly calculated the expectation-maximization algorithms for learning a hierarchy of linear experts.So when the deep learning era came circa 2012, people immediately started looking into how to perform conditional computing, that is, to save compute by only calling a small portion of the model. The idea is that you would have different portions of the model be specialized for different forms of input, and for each input, the model would first cheaply find out which expert should handle it, then call upon only the few specialized experts to handle this particular input.\nThe first paper on applying MoE to deep learning was (Eigen, Ranzato, and Sutskever 2013), one year after AlexNet. However, the deep MoE (DMoE) proposed in the paper has no sparsity, and so it has no modern offsprings. For history’s sake, here’s how it worked.\nLet \\(f_{1, 1}, f_{1, 2}, \\dots, f_{1, n}\\) be \\(n\\) feedforward modules with the same number of input and output neurons. Now, each can be treated as an expert, and be mixed by\n\\[\nf_1(x) = \\sum_i g_{1, i}(x) f_{1, i}(x)\n\\]\nwhere \\(g_{1, i}\\) is a tiny neural network, the gating network for this MoE layer. Now, stack multiple such layers, and we would obtain a DMoE. As one can see, such a network still has to use all the parameters in each forward pass, and therefore saves no compute. It is simply a case of the dense MoE.\nModern2020s deep learning really arrived with the sparsely-gated MoE (Shazeer et al. 2017), which saves compute. Specifically, if each layer contains \\(8\\) experts, but only \\(2\\) are consulted, then the cost of compute is only about \\(1/4\\) for the full model."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "href": "essays/posts/mixture-of-experts/index.html#why-moe-for-deep-learning",
    "title": "Mixture of Experts",
    "section": "Why MoE for deep learning?",
    "text": "Why MoE for deep learning?\nGenerally, one uses a MoE on the frontier, because:\n\nYou really need to push the metric up by a few points.\nYou can’t train a dense model larger than the frontier model, because it simply fails to converge, or the hyperparameter settings for the small models don’t work for the larger one (and you can’t just run a grid search to find it because it costs a million dollars to do a single run).\nYou can train around 10 copies of the frontier model, because while you don’t have the money to do grid search beyond the current frontier, you have the money to train 10 at the frontier.\nYou can’t infer a dense model larger than the frontier one, because one dense model \\(N\\) times as wide would cost you \\(N^2\\) amount of storage and compute, while if you just train \\(N\\) experts, each with roughly the same architecture as the dense model, it would cost you about \\(N\\) amount of storage and about \\(2\\) amount of compute (if only 2 experts are called per question).\nIndeed, if there are too many parameters, then it can’t even be fit onto a good GPU and must be split across GPUs, and then the GPU–GPU communication becomes a serious problem (the “von Neumann bottleneck”).\n\n\n\n\nThe storage hierarchy. Figure from Harvard CS 61: Systems Programming and Machine Organization (2018), Storage 2: Cache model.\n\n\nAll of which are satisfied by Microsoft, Google, etc. This explains why GPT-4 is a MoE made by multiple GPT-3–like models.\nA quick scan of the recent literature shows this, all from Google.\n\nWe present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. (Shazeer et al. 2017)\n\n\nCombining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters, respectively. (Fedus, Zoph, and Shazeer 2022)\n\n\nwe demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet. (Riquelme et al. 2021)\n\n(Shazeer et al. 2017) is not the first paper on MoE in the deep learning era, but it is the most important one. It was applied to between “stacked LSTM layers”, because it was published back when neural language models were stacks of LSTM. Nowadays, of course, MoE usually means MoE layers within Transformers, because only with Transformers do people regularly train models with more than 10 billion parameters."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#load-balancing",
    "href": "essays/posts/mixture-of-experts/index.html#load-balancing",
    "title": "Mixture of Experts",
    "section": "Load balancing",
    "text": "Load balancing\nThe main problem with MoE is a kind of rich-get-richer effect. If at the start of training, some experts are consulted often by random fluctuation, they would be heavily trained by backpropagation, and become even better experts, a upward spiral resulting in a few good experts and many useless experts.\nFor example, in the very first paper on MoE, they trained up to 8 experts to recognize phonemes from 6 Japanese speakers. They found that:\n\nOnly experts 4, 5, and 6 are active in the final mixture. This solution is typical – in all simulations with mixtures of 4 or 8 experts all but 2 or 3 experts had mixing proportions that were effectively 0 for all cases. (Jacobs et al. 1991)\n\nThis might not have been a serious problem in the past, when neural networks were seen as merely a form of high-dimensional statistical model learnable by any one of the typical statistical algorithms (maximal likelihood, Bayesian inference, expectation maximization…), but nowadays, MoE are used because you need to throw more compute at the problem, but cannot afford a larger dense model. In this case, it would defeat the purpose of MoE if some experts end up neglected.\nIt is no coincidence, then, that the sparsely-gated MoE paper (Shazeer et al. 2017) specifically used two auxiliary loss functions to encourage the experts to have equal “weight” over time. It was simplified to just one in the Switch Transformers paper (Fedus, Zoph, and Shazeer 2022).\nSpecifically, consider the sparsely-gated MoE with \\(k=1\\) – where just the top-ranked expert is consulted every time. Let \\(n\\) be the number of experts, and consider a batch of queries \\(\\{x_1, x_2, ..., x_T\\}\\), then the auxiliary loss of the batch is\n\\[\nL := n \\sum_{i=1}^n f_i P_i\n\\]\nwhere \\(f_i=\\frac{1}{T} \\#(\\text{queries sent to expert $i$})\\) is the fraction of time where expert \\(i\\) is ranked highest, and \\(P_i=\\frac{1}{T} \\sum_{j=1}^T w_i\\left(x_j\\right)\\) is the fraction of weight on expert \\(i\\).\nIn the original paper, they claimed that we can obtain the minimal auxiliary loss \\(L\\) at the limit where every expert has equal weight \\(1 / n\\) on all samples, and every expert is ranked the highest equally often.\nPlugging in the equations, we find it is \\(1\\). Unfortunately, this is technically wrong. When there are many experts and large batch, a way to let \\(L\\) approach \\(1/2\\). It is not difficult to show that \\(1/2\\) is the true lower bound. Seeing that Google has been training those huge models since 2017, this definitely works in practice, despite being slightly incorrect.\nThere are plenty of other choices for load balancing, which are rather technical details. For example, the z-loss stabilizes mixed-precision training by discouraging logits that are too far from zero, avoiding large round-off errors (Zoph et al. 2022, secs. 3.3–3.4)."
  },
  {
    "objectID": "essays/posts/mixture-of-experts/index.html#sec-appendix-load-balancing-error",
    "href": "essays/posts/mixture-of-experts/index.html#sec-appendix-load-balancing-error",
    "title": "Mixture of Experts",
    "section": "Appendix: Error in load balancing",
    "text": "Appendix: Error in load balancing\n\nLet one expert get \\(1/2 - \\epsilon\\) on every question, but is never consulted on anything, and let every other \\(n-1\\) expert evenly divide the rest of the questions. For example, this is how the weights should be distributed when there are 3 experts on 4 questions:\n\\[\n\\begin{bmatrix}\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & \\frac 12 + \\epsilon & 0 \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\frac 12 - \\epsilon & 0 & \\frac 12 + \\epsilon \\\\\n\\end{bmatrix}\n\\]\ngiving \\(L = \\frac 34 (1+2\\epsilon)\\). By generalizing this construction, when there are many experts and large batch, we have \\(L \\to 1/2\\). It is not difficult to show that \\(1/2\\) is the true lower bound.\nWith the global optimization method of dual annealing2, Python found something close to the true lower bound, as shown in the figure. The load balancing matrix has a bright strip of \\(1/2 - \\epsilon\\), and slightly brighter dots of \\(1/2+\\epsilon\\) jumping around the matrix, as expected.\n2 I tried using local optimization with SciPy’s minimize, but it always fails to converge to \\(\\sim 1/2\\). It even fails to converge to \\(\\sim 1\\). Indeed, often it just moves around the initial point a bit then immediately gives up and stops. My suspicion is that the loss landscape is too jagged.\n\n\nThe result of minimizing load-balancing loss, with 10 experts and 10 questions."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html",
    "href": "essays/posts/grokking-modular-arithmetics/index.html",
    "title": "Grokking Modular Arithmetics",
    "section": "",
    "text": "This essay reproduces the paper A simple and interpretable model of grokking modular arithmetic tasks (Gromov 2023a). The code is available on GitHub at yuxi-liu-wired/grokking-modular-arithmetics."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html#setup",
    "href": "essays/posts/grokking-modular-arithmetics/index.html#setup",
    "title": "Grokking Modular Arithmetics",
    "section": "Setup",
    "text": "Setup\nGiven a natural number \\(N\\), we have modular arithmetic on \\(\\mathbb{Z}_N = \\{0, 1, ..., N-1\\}\\). For example, \\(\\mathbb{Z}_{12}\\) is the “clock face modular arithmetic”. The problem for our neural network is to learn binary functions on \\(\\mathbb Z_N\\). That is, we are to learn a binary function \\(f: \\mathbb Z_N\\times \\mathbb Z_N \\to \\mathbb Z_N\\).\nEach such binary function can be exactly specified by a \\(N\\times N\\) table, so there are \\(N^{N^2}\\) possible such functions. Most of them are completely random and uninteresting, both for us and for neural networks, but a few are very interesting, and modular addition is one such interesting function.\nFor example, modular addition on \\(\\mathbb Z_6\\) has the following multiplicative table:\n\n\n\n+\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0\n1\n2\n3\n4\n5\n\n\n1\n1\n2\n3\n4\n5\n0\n\n\n2\n2\n3\n4\n5\n0\n1\n\n\n3\n3\n4\n5\n0\n1\n2\n\n\n4\n4\n5\n0\n1\n2\n3\n\n\n5\n5\n0\n1\n2\n3\n4\n\n\n\nSince the entire dataset is known and specified in advance, we can define the train set ratio \\(\\alpha = \\frac{|D_{train}|}{|D|}\\), where \\(D\\) is the full dataset (the multiplication table), and \\(D_{train}\\) is the training dataset. We expect that, as \\(\\alpha\\) approaches \\(1\\), the network becomes better at generalizing to the test set.\nThe network architecture we use has 3 layers:\n\nInput is \\(x = [x^{(1)}, x^{(2)}]\\). Both \\(x^{(1)}, x^{(2)} \\in \\mathbb R^N\\) are one-hot encodings of \\(\\mathbb Z_N\\).\nHidden layer activation is \\(z = \\phi(\\frac{1}{\\sqrt M} W^{(1)}z)\\), where \\(\\phi\\) is the activation function. Here \\(z \\in \\mathbb R^M\\) can be of any width.\nThe output is \\(y = \\frac{1}{N} W^{(2)}z\\), where \\(y \\in \\mathbb{R}^N\\) should be a one-hot encoding of \\(\\mathbb{Z}_N\\).\nAll entries of \\(W^{(1)}, W^{(2)} \\sim \\mathcal N(0, 1)\\) are initialized as standard gaussians.\n\\(W^{(1)}, W^{(2)}\\) are all the parameters of the network. There is no bias. Thus the network has \\(3MN\\) parameters in total.\n\n\n\n\nAn example network with the given architecture, with \\(N=3, M = 10\\).\n\n\nIn the paper, Gromov found that grokking occurs under different choices of activation functions \\(\\phi\\), different training methods (SGD, Adam, etc.), and different training set ratios \\(\\alpha\\).\nThe simplest example where grokking occurs is with\n\nQuadratic activation function: \\(\\phi(t) = t^2\\).\nFull-batch gradient descent.\nMSE loss.\n\nI used AdamW optimizer instead of standard gradient descent, since it converges faster. The dataset is formatted as an array of triples of the form \\((x_1, x_2, y)\\), interpreted as \\(x_1 + x_2 = y \\mod N\\). I split the dataset randomly into two datasets."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html#results",
    "href": "essays/posts/grokking-modular-arithmetics/index.html#results",
    "title": "Grokking Modular Arithmetics",
    "section": "Results",
    "text": "Results\n\nGrokking\n\n\n\nLearning curves showing grokking.\n\n\nSome observations:\n\nThe test set accuracy curve decreases as the training set accuracy increases to perfection.\nThe test set accuracy curve rises only after the training set accuracy is perfect. First slowly, then rapidly (“grokking”). This can be quite puzzling, since if the network has really achieved perfection on the training set, then there is nothing left to learn, and so it shouldn’t be able to improve any further – and yet it does improve.\nPerfect accuracy on the training set is reached at epoch 10x that of the training set.\nThe learning curves show something smoother, but also something interesting: the training loss decreases monotonically, but the test loss rises, then decreases.\nFor a while, the test loss rose while the test accuracy increased!\n\nSome lessons:\n\nGrokking might look less dramatic when plotted not by argmax-accuracy, but by MSE.\n\nSee for example (Power et al. 2022). One wonders what they would have found if they had plotted MSE losses instead of accuracies?\n(Nanda et al. 2023) does plot train and test loss, and in this paper, the grokking appears in the loss curves as well. This seems harder to understand using our small model (they used a Transformer).\n\nTrain loss can decrease while test loss increase, but this trend can also be reversed. The shape of learning curves is quite complex.\n\n\n\nInterpretation\nSince the neural network is so small, we can interpret it. What kind of neural network did we end up with that could do modular addition?\nDirectly inspecting the weight matrices, we notice suggestive wavy bands that resemble sine waves.\n\n\n\nThe weight matrices.\n\n\nLet’s sort them according to frequency, as found by running a Fourier transform and picking the highest peak:\n\n\n\nThe weight matrices, with columns ordered by frequency.\n\n\nWe see that the learned neural network is probably doing some Fourier transform. This can be confirmed by plotting the activation map on every hidden neuron. Specifically, for each hidden neuron, we can calculate its activation for each of the \\(N\\times N\\) possible inputs. This is plotted as a heatmap with \\(N\\times N\\) pixels. We then get one heatmap per hidden neuron and display all of them in a grid:\n\n\n\nThe activation pattern of neurons in the hidden layer as one scans through all possible inputs.\n\n\nWe see that the network has learned some sine waves. It seems to be a robust fact that networks trained to do modular arithmetic, with one-hot encoding, learn to use trigonometry for this task. (the use of one-hot encoding seems very relevant, as noted here).\n\n\nThe null hypothesis\nAs a good comparison with the above interpretation of the neural network, we leverage the same tools on the “null hypothesis”. There are two ways to do the null hypothesis: either initialize the neural network randomly and then interpret it, or initialize it, train it to perform a randomly generated binary operation, then interpret it.\nAs one would expect, the neural network can successfully memorize arbitrary binary operations, without generalization (as there is no pattern to generalize).\n\n\n\nLearning curves for a neural network trained on a randomly initialized binary operation.\n\n\nFor both null hypotheses, I tried interpreting them using the same methodology; they look as one might expect: complete noise.\n\n\n\nActivation maps. (randomly initialized.)\n\n\n\n\n\nActivation maps. (trained perform a randomly initialized binary operation)\n\n\n\n\n\nWeight matrices. (randomly initialized)\n\n\n\n\n\nWeight matrices. (trained perform a randomly initialized binary operation)\n\n\n\n\nExtensions\nThis toy is small and simple. It runs in a minute. Here are some ideas for playing with the toy:\n\nModular multiplication.\nRandom operation (as a null hypothesis).\nDifferent activation functions (sine, ReLU).\nDifferent accelerators (SGD, Adam, etc)\nTwo hidden layers."
  },
  {
    "objectID": "essays/posts/grokking-modular-arithmetics/index.html#some-other-quotes",
    "href": "essays/posts/grokking-modular-arithmetics/index.html#some-other-quotes",
    "title": "Grokking Modular Arithmetics",
    "section": "Some other quotes",
    "text": "Some other quotes\nGrokking modular arithmetic (Gromov 2023b)\n\nIn particular, random feature models such as infinitely-wide neural networks (in the NTK regime) do not exhibit grokking, at least on the tasks that involve modular functions.\nIn our minimal setup, the simplest explanation for grokking is that once training loss reached a certain value, the only way to further minimize it is to start learning the right features.\n\nShortcut learning in deep neural networks (Geirhos et al. 2020)\n\nmany of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike.\n\n\n\n\nFigure 3 from (Geirhos et al. 2020)\n\n\nThe Bitter Lesson (Sutton 2019)\n\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.\n\nThe Scaling Hypothesis # Why Does Pretraining Work?\n\nEarly on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. It goes from predicted uniformly-distributed bytes to what looks like Base-60 encoding—alphanumeric gibberish. As crude as this may be, it’s enough to make quite a bit of absolute progress: a random predictor needs 8 bits to ‘predict’ a byte/character, but just by at least matching letter and space frequencies, it can almost halve its error to around 5 bits…\n… a sample will state that someone is “alive” and then 10 sentences later, use the word “dead”, or it will digress into an irrelevant argument instead of the expected next argument, or someone will do something physically improbable, or it may just continue for a while without seeming to get anywhere.All of these errors are far less than &lt;0.02 bits per character; we are now talking not hundredths of bits per characters but less than ten-thousandths.The pretraining thesis argues that this can go even further: we can compare this performance directly with humans doing the same objective task, who can achieve closer to 0.7 bits per character. What is in that missing &gt;0.4?\nThe last bits are deepest. The implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence."
  },
  {
    "objectID": "essays/posts/cyc/index.html",
    "href": "essays/posts/cyc/index.html",
    "title": "Cyc",
    "section": "",
    "text": "The legendary Cyc project, Douglas Lenat’s 40-year quest to build artificial general intelligence by scaling symbolic logic, has failed. Based on extensive archival research, this essay brings to light its secret history so that it may be widely known.\nLenat’s journey began with his PhD work on automated mathematical discovery through heuristic search. He observed that such systems initially make promising discoveries but quickly “run out of steam” as they exhaust their initial pool of heuristic rules. His follow-up system EURISKO, famous for winning tournament competitions by finding unconventional tactics, faced similar limitations. These experiences convinced Lenat that true AI needed a vast foundation of common sense knowledge to avoid intellectual exhaustion.\nIn 1984, he launched Cyc to manually encode millions of facts and rules about common sense, predicting that once this “knowledge pump” was primed, the system would begin true machine learning by reading natural language texts and conducting autonomous scientific experiments. Cyc grew to contain approximately 30 million assertions at a cost of $200 million and 2,000 person-years. Yet despite Lenat’s repeated predictions of imminent breakthrough, it never came.\nIn terms of funding, Cycorp was probably half-funded by the military and intelligence before 2010, and entirely funded by commercial applications since 2016. Cycorp achieved long-term financial stability that is uncommon for a small technology company, but all known commercial uses of its system involve standard methods in expert systems, data integration, and information retrieval, functionally the same as similar services offered by established corporations like Oracle and IBM. No evidence suggests that Cyc’s purported higher intelligence provided any competitive advantage.\nBy academic standards, the Cyc project is highly insular. Publications involving Cyc typically described methods for entering information into the system, rarely addressing applications out of it. Outside Cycorp, Cyc saw minimal use in AI research or even in knowledge retrieval, its most adjacent field. Academics found the system difficult to use, and it never performed on public benchmarks. Spin-off projects like OpenCyc and various semantic web initiatives all eventually shut down without notable success.\nThe secretive nature of Cyc has multiple causes. Lenat personally did not release the source code of his PhD project or EURISKO, remained unimpressed with open source, and disliked academia as much as academia disliked him. Most open information concerning Cyc had been deliberately removed circa 2015, at the moment when Cycorp pivoted to commercial applications.\nLenat had a single philosophical vision for AI that he pursued for 40 years. Guided by it, he had consistently rejected every alternative vision for AI, including the heuristic search approach, the expert systems approach, the robotics approach, and the neural network approach. All were rejected as either “shallow pattern-matching” seeking a “free lunch” on one end, or a “mystical worship of physical embodiment” on the other end.\nAs of 2025, 9 years after the knowledge pump had been primed, there is still no sign that Cyc would ever achieve general intelligence.\nMore archival material available on GitHub."
  },
  {
    "objectID": "essays/posts/cyc/index.html#abstract",
    "href": "essays/posts/cyc/index.html#abstract",
    "title": "Cyc",
    "section": "",
    "text": "The legendary Cyc project, Douglas Lenat’s 40-year quest to build artificial general intelligence by scaling symbolic logic, has failed. Based on extensive archival research, this essay brings to light its secret history so that it may be widely known.\nLenat’s journey began with his PhD work on automated mathematical discovery through heuristic search. He observed that such systems initially make promising discoveries but quickly “run out of steam” as they exhaust their initial pool of heuristic rules. His follow-up system EURISKO, famous for winning tournament competitions by finding unconventional tactics, faced similar limitations. These experiences convinced Lenat that true AI needed a vast foundation of common sense knowledge to avoid intellectual exhaustion.\nIn 1984, he launched Cyc to manually encode millions of facts and rules about common sense, predicting that once this “knowledge pump” was primed, the system would begin true machine learning by reading natural language texts and conducting autonomous scientific experiments. Cyc grew to contain approximately 30 million assertions at a cost of $200 million and 2,000 person-years. Yet despite Lenat’s repeated predictions of imminent breakthrough, it never came.\nIn terms of funding, Cycorp was probably half-funded by the military and intelligence before 2010, and entirely funded by commercial applications since 2016. Cycorp achieved long-term financial stability that is uncommon for a small technology company, but all known commercial uses of its system involve standard methods in expert systems, data integration, and information retrieval, functionally the same as similar services offered by established corporations like Oracle and IBM. No evidence suggests that Cyc’s purported higher intelligence provided any competitive advantage.\nBy academic standards, the Cyc project is highly insular. Publications involving Cyc typically described methods for entering information into the system, rarely addressing applications out of it. Outside Cycorp, Cyc saw minimal use in AI research or even in knowledge retrieval, its most adjacent field. Academics found the system difficult to use, and it never performed on public benchmarks. Spin-off projects like OpenCyc and various semantic web initiatives all eventually shut down without notable success.\nThe secretive nature of Cyc has multiple causes. Lenat personally did not release the source code of his PhD project or EURISKO, remained unimpressed with open source, and disliked academia as much as academia disliked him. Most open information concerning Cyc had been deliberately removed circa 2015, at the moment when Cycorp pivoted to commercial applications.\nLenat had a single philosophical vision for AI that he pursued for 40 years. Guided by it, he had consistently rejected every alternative vision for AI, including the heuristic search approach, the expert systems approach, the robotics approach, and the neural network approach. All were rejected as either “shallow pattern-matching” seeking a “free lunch” on one end, or a “mystical worship of physical embodiment” on the other end.\nAs of 2025, 9 years after the knowledge pump had been primed, there is still no sign that Cyc would ever achieve general intelligence.\nMore archival material available on GitHub."
  },
  {
    "objectID": "essays/posts/cyc/index.html#in-lieu-of-an-introduction",
    "href": "essays/posts/cyc/index.html#in-lieu-of-an-introduction",
    "title": "Cyc",
    "section": "In lieu of an introduction",
    "text": "In lieu of an introduction\nMany years later, surrounded by the humming servers of the knowledge base, Douglas Lenat was to remember that distant afternoon when he taught Cyc that everyone can only see their own dream."
  },
  {
    "objectID": "essays/posts/cyc/index.html#automated-mathematician",
    "href": "essays/posts/cyc/index.html#automated-mathematician",
    "title": "Cyc",
    "section": "Automated Mathematician",
    "text": "Automated Mathematician\nIn the land of AI, there had been legends, of people and systems that were before their time, that showed sparks of brilliance but without followups, and Douglas Lenat was a man of three.\nLenat’s first legend was his Automated Mathematician (AM), a system that, according to the legends, discovered mathematical concepts autonomously by distilling concepts out of patterns and remixing and stirring together previous concepts (D. B. Lenat 1976). As we will see, this legend is mostly true, though with a caveat that leads to the topic of his second legend.\nAM was his 1976 PhD thesis project, and it was one of many “automated discovery” systems in the 1970s – the only one still remembered nowadays. Though neural networks and other “self-organized” machine learning methods had mostly died out in the 1960s, machine learning did not die, and indeed, was going through a logical spring, under the banner of “automated discovery”.\n\nAutomated discovery\nNowadays, when we think of “machine learning”, we think of random forests, neural networks, and such. But back then, machine learning was understood as “logical AI gone meta”. Well, to go meta, we must first understand: What is logical AI? Simon and Newell, the two giants of logical AI, had shown the way: logical AI is heuristic search.\n\nTo play chess, one simply writes a program that does alpha-beta search over the game tree, guided by heuristics of piece-worth, mobility, and such.\nTo solve planar geometry problems, one simply writes a program that constructs a path in the space of geometric arguments, until one connects the axioms with the conclusions.\nTo solve problems of type X, one simply writes a program that walks through the space of possible solutions to X, and code in the heuristic rules, its north star, so that it will not be lost in the space of solutions.\n\nWell, chess is a game, and so is Euclidean geometry, symbolic integration, and… perhaps scientific discovery itself? The famed scientific method, to deserve the name “method”, ought to be just one more problem for logical AI to solve. It merely remained to go meta, to heuristically search over the space of heuristic searches.\nDuring 1977–1983, Pat Langley wrote a series of programs named BACON, after Francis Bacon, a father of scientific empiricism. It purported to discover scientific laws from mere data. For example, when given a table of the moles \\(N\\), pressures \\(P\\), volumes \\(V\\), and temperatures \\(T\\) of gas samples, BACON would first try out simple equations involving two of the terms. It would discover that the data for \\(PV\\) appears more cluster-like than either \\(P\\) or \\(V\\), so it would make a new quantity \\(PV\\), and add that to the table. It would then repeat this process, discovering that \\(PV/T\\) is an interesting quantity, and finally that \\(PV/NT\\) is a constant quantity – the ideal gas law discovered! (Langley 1981)\nIn the logical AI framework, the problem space of BACON is the space of all elementary functions involving the table columns, and a solution is a (nontrivial) function that results in a nearly constant column. At each step, the program tries out simple combinations of the current columns, and heuristically pick the one most nearly constant.\nAs another example, consider the famed Dendral and Meta-Dendral.\nDendral was an artificial model of how professional chemists perform molecular spectroscopy – that is, how they read out a molecular structure from looking at a few spiky curves. Its problem space is the space of all possible ways to cut up a molecule into fragments, and ways for the fragments to give and take their atoms. Its heuristics are the rules for generating chemically plausible and implausible cleavages and transfers. For example, it is plausible for a protein to be cleaved at the -CO-*-NH- peptide bond, but implausible to be cleaved at the double bond between C and O. The goal is, given molecular spectroscopic data for a single molecule, to construct a molecular structure and a sequence of cleavages and transfers, such that it would produce the data.\nWith Meta-Dendral, the problem space goes meta, becoming the space of possible chemical rules. Its goal is to find plausible rules (plausible according to heuristic meta-rules) that can explain a large collection of molecular structures and their spectroscopic data. Meta-Dendral proved useful for working chemists by discovering some cleavage rules for a certain minor sub-family of androstanes. (Buchanan and Feigenbaum 1981) \nIn general, such a system begins with some simple rules that allow the system to a low score according to some criteria. and as it runs, it builds, prunes, and modifies the rules, so that in the end, its rule set allows it to achieve a high score. The following tabulates a few (Walker 1987):\n\n\n\n\n\n\n\n\n\n\n\nSystem Name\nDate\nTask\nData\nRules\nDiscovery Method\n\n\n\n\nMeta-Dendral\n1976\nDiscover molecular cleavage and transfer rules for mass spectrometry\nMolecular structures and their spectra\nMolecular cleavage and transfer rules\nUse meta-heuristic rules to generate possible rules, and test on data\n\n\nBacon\n1977–1983\nDiscover physical laws\nNumeric data from experiments\nElementary functions\nSymbolic regression\n\n\nRX\n1982\nDiscover drug side effects and interactions\nPatient information database\nDrug effect and interaction rules\nSymbolic regression with time-lag\n\n\n\n\n\nThe working of AM\n\nThe honor of your machine is preserved.\n— Paul Erdős, after examining (D. B. Lenat 1976, appendix 4).\n\nWithin this context, AM is different. It is still a logical AI with a problem space and a heuristic search. However, there is no data: It was mostly “self-play”.\nTo start AM, Lenat began by entering 115 concepts in set theory and ~250 heuristic rules, and thence AM ran, discovering more and more constructions. Most would be trivial, but a few would be interesting, and these interesting constructions would be stored, allowing further constructions upon them. It secured a place as a minor legend, reportedly rediscovering many concepts, such as the natural numbers, the prime numbers, and the Goldbach conjecture.\nA concept is essentially a frame, which are essentially objects in object-oriented programming. A concept has 25 possible facets, which are “slots”, or “data fields”. Not all need to be filled.\n\n\n\n\n\n\nAn example concept\n\n\n\n\n\nThe following is an example concept with many facets populated (D. B. Lenat 1983c, table 9.9):\n\nNAME: Generalize-rare-predicate\nABBREVIATION: GRP\nSTATEMENT:\n\nEnglish: If a predicate is rarely true, Then create generalizations of it\nIF-potentially-relevant:\nIF-just-finished-a-task-dealing-with: a predicate P\nIF-about-to-work-on-task-dealing-with: an agenda A\nIF-in-the-middle-of-a-task-dealing-with: never\nIF-truly-relevant: P returns True less than 5% of Average Predicate\nIF-resources-available: at least 10 CPU seconds, at least 300 cells\nTHEN-add-task-to-agenda: Fill in entries for Generalizations slot of P\nTHEN-conjecture:\n\nP is less interesting than expected\nGeneralizations of P may be better than P\nSpecializations of P may be very bad\n\nTHEN-modify-slots:\n\nReduce Worth of P by 10%\nReduce Worth of Specializations(P) by 50%\nIncrease Worth of Generalizations(P) by 20%\n\nTHEN-print-to-user: English(GRP) with “a predicate” replaced by P\nTHEN-define-new-concepts:\n\nCODED-IF-PART: λ(P) ... &lt;LISP function conjoining all the IF- parts&gt;\nCODED-THEN-PART: λ(P) ... &lt;LISP function appending all the THEN- parts&gt;\nCODED-IF-THEN-PARTS: λ(P) ... &lt;LISP function combining the previous 2 slots&gt;\nCOMPILED-CODED-IF-THEN-PARTS: #30875\nSPECIALIZATIONS: Generalize-rare-set-predicate\n\nBoundary-Specializations: Enlarge-domain-of-predicate\n\nGENERALIZATIONS: Modify-predicate, Generalize-concept\n\nImmediate-Generalizations: Generalize-rare-contingent-piece-of-knowledge\nSiblings: Generalize-rare-heuristic\n\nIS-A: Heuristic\nEXAMPLES:\n\nGood-Examples: Generalize Set-Equality into Same-Length\nBad-Examples: Generalize Set-Equality into Same-First-Element\n\nCONJECTURES: Special cases of this are more powerful than Generalizations\n\nGood-Conjec-Units: Specialize, Generalize\n\nANALOGIES: Weaken-overconstrained-problem\nWORTH: 600\nVIEW: Enlarge-structure\nORIGIN: Specialization of Modify-predicate via empirical induction\n\nDefined-using: Specialize\nCreation-date: 6/1/78 11:30\n\nHISTORY:\n\nN-Good-Examples: 1, N-Bad-Examples: 1\nN-Good-Conjectures: 3, N-Bad-Conjectures: 1\nN-Good-Tasks-Added: 2, N-Bad-Tasks-Added: 0\nAvg-Cpu-Time: 9.4 seconds, Avg-List-Cells: 200\n\n\n\n\n\n\n\n\nThe network of concepts at the beginning of AM. | means “is a”. ||| means “is an example of”. (D. B. Lenat 1976, 106)\n\n\nAM has an agenda: a list of tasks, each with a list of reasons for the task. Each task is of the form “Perform operation O to facet F of concept C”. A task’s Worth is the sum of its reasons’ worths. AM always performs the task with the highest worth.\nTo perform a task, AM looks for heuristic rules whose conditions are (mostly) satisfied, and whose worth is (pretty) high. Each heuristic rule is of form “if &lt;condition&gt;, then run &lt;actions&gt;”. Each action has 3 kinds of possible effects:\n\nAdd a new task to agenda.\nCreate a new concept.\nAdd or delete an entry to a facet of a concept.\n\nSome example heuristic rules:\n\nIf the task is to fill in examples of X, and X is a special case of Y, then for each example of Y, check if it a definition of X. If so, then add it to the list of examples of X.\nIf some but not most examples of X are also examples of Y, then create a new concept “X and Y”.\nIf very few examples of X are found, then add the following task to the agenda: “Generalize the concept X”, for the following reason: “X are quite rare; a slightly less restrictive concept might be more interesting”.\n\nAt this point, the careful reader would notice several problems:\nHow does AM know that the concept should be called “Prime Numbers”? Ah, that’s because Lenat would regularly interrupt and inspect AM, and if Lenat notices that AM has rediscovered, say, prime numbers, he would rename that from something like concept-421 to prime-numbers.\nTo discover prime numbers, AM must have a way to check if a Lisp object is a prime number or not. That is, the definition must also be a program. So…? Ah, the famous homoiconicity of Lisp came to the rescue! A definition, as stored within a facet of a concept, is a data, but for Lisp, data is program, and program data. Consequently, AM can run a subroutine that enumerates possible programs as data, and for each, interpret it as program, until AM hits upon a program that works (or times out).\nHow does it check that two definitions actually define the same thing? In general, this is impossible by Rice’s theorem, so Lenat must have used some heuristic rules. I looked, but can’t find Lenat explaining this anywhere. It seemed like a trick of the hand.\nBut most serious of all issues is that the most critical part of AM was not the concepts it discovered – after all, mathematicians did not need a computer to inform them that prime numbers are interesting. The most critical part was surely the heuristic rules by which AM worked, and many were entirely hand-waved. The most detailed description was in (D. B. Lenat 1976, appendix 3), and it is still not described to a level of detail that may allow reimplementation.\nConsider rule 75: “A constructive existence conjecture is interesting if it is frequently used.” How frequent is “frequent”? What threshold of frequency triggers the increase in interestingness, and by how much? There are even vaguer rules, such as rule 69: “Formulate a parameterized conjecture, a ‘template’, which gets slowly specialized or instantiated into a definite conjecture.”.\nNow, these would have not been a problem if AM was just a forgotten system, but it was not. The achievements of AM was impressive enough that it was an instant celebrity among the AI people, winning an IJCAI Award just 1 year after publication (D. B. Lenat 1977). Anecdotes even suggested that the AI mathematician was here:1\n1 This anecdote corroborates (D. B. Lenat 1976, appendix 4.6).\nOn one memorable occasion, one of my advisors, George Polya, was looking at its results, and remarked “That reminds me of something a student of a friend of mine once did.” He rummaged through an old trunk, found the relevant correspondence, and it turned out that his friend was G. H. Hardy, and the student was Srinivasa Ramanujan! Even though that regularity (involving highly composite numbers) has no practical significance, Polya and I were happy to see AM behaving much like the young self-taught Indian genius had, in his explorations in search for interesting regularities.\n(D. B. Lenat 2022a)\n\n\n\nThe end of AM\nThe swirling controversy came to a head with the publication of (Ritchie and Hanna 1984), which argued that AM was badly documented. That, its control structure probably was not “simply pick the task with the highest worth”, but more complicated. This called into question the Lenat claim that the heuristic rules were the load-bearing parts of AM, which Lenat had implied by emphasizing its almost trivially simple control structure. Further, some crucial quantitative heuristic rules were probably hidden behind vague qualitative handwaves like “A nonconstructive existence conjecture is interesting” (rule 74). Now, handwaving would have not been a problem if they were intended as merely glosses over the source code, but the source code was also unpublished, making it impossible for other other researchers to reproduce or extend the work, or to reinterpret AM’s workings.\nIn short, because of the various issues, AM was an event that happened, but not an entity that could be built upon. One could not build upon it directly in source code, since it’s unavailable. One could not build upon it by reimplementing the pseudocode, since the rules were vaguely specified, and the control structure was probably wrong. One could not build upon it by reimplementing the high-level ideas, since it’s unclear which part, out of the several dozen tightly integrated parts of AM, was responsible for AM’s good outputs, and which were just implementation details. In our language, there was no ablation study.\nLenat quickly replied with (D. B. Lenat and Brown 1984). He dismissed the criticism as mostly miscommunication, and went on to describe the real lesson of AM. If I were to be dab, Lenat was saying that it is fine for AM’s source,2 or even pseudocode, to be unavailable, because Lenat had learned the lessons, and you, dear reader, need only listen to the lessons from him.\n2 Though Lenat admitted that “the code ought to have been provided” (D. B. Lenat and Brown 1984) for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had recently been found, right where it should be – the DBL folder in the Stanford AI Laboratory backup data. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.\nThe AM thesis never explained, precisely, how concepts such as ‘not very often’ and ‘related to’ were implemented. By and large, these omissions were due to the fact that the code Lenat wrote for these predicates was quite trivial… inevitable, yet regrettable process of simplifying large pieces of code, translating them to brief English phrases. This process left out many exceptional cases, and made the English condensations less accurate… Some problems that Ritchie and Hanna cite… are simply errors of mis-reading what was stated in the thesis or articles… A few of the problems raised in Ritchie and Hanna’s article are, annoyingly, genuine inconsistencies in the thesis document, such as whether or not facets had subfacets. These reflect the fact that AM was a running and evolving program, changing daily in small ways even as the thesis document was being written… the changes in representation were driven simply by AM’s running out of list space in 1975 INTERLISP code; we were forced to shift representations time and time again just to gain a few hundred precious list cells.\n(D. B. Lenat and Brown 1984)\n\nAnd what were the lessons?\n1th lesson: AM exhausts itself. Roughly speaking, each new interesting discovery depends on ~24 heuristics, and each heuristic has a hand in ~24 discoveries. Therefore, with ~N heuristic rules, there would be ~N interesting discoveries. Because AM cannot discover heuristic rules, with ~300 starting heuristic rules, it would run out of interesting discoveries and “die of boredom”. Lenat could help AM by adding new heuristics, and AM would make some new discoveries, but this never lasted.\n\nEventually, AM acquired an uncommon ailment for a computing system: intellectual exhaustion. Having explored the esoteric reaches of mathematics, AM suddenly downshifted into a preoccupation with rudimentary arithmetic. Finally, with the remark, “Warning! No task on the agenda has priority over 200.”, the system virtually expired, as though from boredom.\n(Hiltzik 2001)\n\n2th lesson: Representation matters a lot. AM worked so well for mathematics, because AM used Lisp code as data. Lisp is the perfect tool if you want to search over the space of interesting mathematical functions. You can modify a Lisp expression, and get a different mathematical function that is possibly interesting. In contrast, if you were to modify assembly code, you’d most likely end up with nonsense. Indeed, Lenat found that he could not extend AM to “go meta” and discover new heuristics, because Lisp is good for math, not “heuretics” (the study of heuristics). Modifying a Lisp expression for a heuristic most likely ends up with nonsense, much like modifying assembly code for a mathematical function.\n\nIt was only because of the intimate relationship between LISP and Mathematics that the mutation operators (loop unwinding, recursion elimination, composition, argument elimination, function substitution, etc.) turned out to yield a high ‘hit rate’ of viable, useful new math concepts when applied to previously-known, useful math concepts – concepts represented as LISP functions. But no such deep relationship existed between LISP and Heuretics, and when the basic automatic programming (mutations) operators were applied to viable, useful heuristics, they almost always produced useless (often worse than useless) new heuristic rules.\n…\nWe did not perceive until writing this paper that the way in which Similar-To, Not-Often, Notice-Regularity, and scores of other ‘primitives’ were coded do themselves embody a large amount of heuristic knowledge. We exploited the structure of (or, if you prefer, partially encoded) the domain of elementary mathematics, in the process of making trivial yet adequate LISP versions of those extremely complex and subtle notions (such as similarity of concepts).\n(D. B. Lenat and Brown 1984)\n\nHow do we know that Lenat learned the lessons?"
  },
  {
    "objectID": "essays/posts/cyc/index.html#eurisko",
    "href": "essays/posts/cyc/index.html#eurisko",
    "title": "Cyc",
    "section": "EURISKO",
    "text": "EURISKO\nThough 1981 is still near, EURISKO is already shrouded in a reverential mystery like legends do, among the Deep Blue, the Samuel Checkers Player, and the apprentice’s broom. A symbol, a moral archetype. A program that discovered loopholes in a sci-fi ship-building tournament, allowing its creator to win twice in a row using fleets so unaesthetic that the people running the tournament threatened to stop the tournaments if it would win thrice, so it retired, the Honorary Admiral, EURISKO the Undefeated.\nBrilliant, dramatic, but what do we glean from it, other than a moral play about the power of thinking sideways and seeing through? Quite a lot.\n\nThe lessons\nAs we saw, AM raised many questions, and Lenat wrote EURISKO to answer them. Specifically, he wanted to see if EURISKO could avoid intellectual exhaustion if it could also discover heuristics. If AM exhausted itself because it has used up the worth of its heuristics (1th lesson), then why not let the computer discover more heuristics? Since AM could not efficiently search over heuristic rules using LISP (2th lesson), Lenat designed a new language called RLL (“Representation Language Language”), over which heuristic rules are efficient to search.\nAs in AM, each heuristic in EURISKO had a level of Worth. Higher-worth heuristics were more likely to be invoked. Each heuristic had a CreditTo, so that if a heuristic rule rose in worth, its CreditTo would also rise in worth. When EURISKO was born, and saw itself, all the heuristic rules it had were branded with CreditTo = DBLenat, but this would soon change, as heuristics begat heuristics.\nYet while Lenat solved the problems raised by the first two lessons of AM, EURISKO failed, from which Lenat learned two further lessons. As we will see, it turned out that EURISKO did exhaust itself eventually after all. Self-discovery of heuristic rules eventually ceased, because self-discovery of heuristic rules relied on meta-heuristic rules about heuristic rules, and those rules run out of steam after a dozen or so uses.\nLenat concluded that he could program 100 heuristics and get a system that could discover 1000 rules, or program 100 meta-heuristics and get a system that could discover 1000 heuristics and 10000 rules, but none of these would be truly autonomous, and Lenat wished for more. He wished for something that could be an equal to humanity, that would grow up and explore into the great beyond, where it could no longer rely on humans for help. Lenat concluded that there really is no way to get a working automated discovery program without doing the hard work of hand-coding in a lot of common sense, and that there would be a point at which this system would finally achieve escape velocity, and would never be exhausted again.\nWhy would common sense help? Lenat observed how humans don’t seem to get stuck like EURISKO. He concluded that humans don’t run out of steam because they have a vast store of common sense knowledge about the world, from which they can draw upon for analogies, those far-flung flights of fancies that, in sufficient quantities, allow them to generate genuinely new ideas endlessly. For example, one can draw a line of analogy between the military and the medical, so that a doctor can “fighting an infection by an encircling movement with antibiotics”. This is the 3th lesson.\nWhy would analogies give genuinely new ideas? Well, intelligence is messy! If everything is so uniform, then there is no way to make a far-flung analogy – everything is pretty much the same already. Besides, just look at all the broken dreams of logical AI – their corpses tell us that no elegant theory of intelligence exists. Programming a genuine AI is a messy job. Messiness is a hideous strength. This is the 4th lesson.\n\nThe apparent adhoc-ness in both the heuristics’ content themselves, and in the control knowledge guiding the application of those heuristics, is clearly the source of many methodological objections to the work. But we believe that this adhocracy – indeed, adhocracy controlling adhocracy – may be the source of EURISKO’s underlying potential especially as a model for cognition.\n(D. B. Lenat and Brown 1984)\n\nThose are the lessons Lenat drew, but what did EURISKO really do?\nOther than winning the sci-fi naval battle tournaments, EURISKO also worked on number theory, set theory, simulation of evolution, and metal-oxide (MOS) design. In MOS, it designed some new circuit elements that were verified by physical fabrication, such as a more efficient flip-flop that was “difficult to produce masks for and difficult to fabricate, but extremely small and fast”.\n\n\n\n\n\n\nLenat’s meta-Darwinian evolution theory\n\n\n\n\n\nLenat had a novel idea about how evolution works, one that I have not seen anywhere else. His idea was that purely random mutations are too slow for evolutionary progress, so that evolution must have gone “meta” as well. A substantial portion of the genome is probably not coding for direct protein transcription, but rather for a kind of heuristics, so that instead of trying out all possible mutations, evolution only tries out mutations that are likely useful. And of course, this would have gone meta, with meta-heuristic genes coding for useful ways to mutate the heuristic genes. (D. B. Lenat 1980)\n\nHere is a mechanism which embodies the heuristic “If a gene has mutated successfully several times in the recent past, then increase its chance of mutating in the next generation, and conversely.” All we need to posit is that somehow a short, noncoding sequence—we’ll call it an asterisk—is added to a gene each time it mutates [and] some mechanism (for example, stereochemical) whereby genes with many asterisks are more likely to be mutated, duplicated, and so on, than genes with few or none. Since the asterisks provide no specific benefits to the individual, they will gradually be lost over time, so that when a gene no longer should be mutated, its asterisk count will slowly decline over several generations.\n…\n… recombination among introns modulates the evolution of a gene. Let’s look at an example of this: it is extremely important to keep the a, b, and d globin genes separate, but their internal structure is very similar. To inhibit recombination, the spacers between them can be made very different, and the introns within them can diverge dramatically (since mutations in introns are not as deleterious to the functioning of the gene as mutations to the coding regions). In fact, there is evidence that both of these kinds of divergence do occur for the globins.\n(D. B. Lenat 1983c)\n\nAt the start, only random mutations would be selected for, but eventually heuristics would arise that create likely advantageous mutations, and then evolution would go meta by one level: it would select for good heuristics. Over the natural history of earth, this can go meta for as many levels as time allows.\nThe paper goes into further speculative details, and makes a half-humorous–half-horror suggestion that perhaps evolution has worked for so long that the genome now contains a sophisticated intelligent designer, bootstrapped from the billions of years of heuristics-upon-heuristics backstopped by natural selection. The tiny designer would design the progeny’s genome, so that almost nothing in the mutation is “random”.\nSince it has an expectation of how things “should go”, it might regard the human experiment as a failure because humans are driving the natural world so far out of its expectation. For example, the designer might have learned a rule “If the environment temperature is varying outside of 20 ± 10 °C , then undo the previous mutation. We have clearly created an offspring that is wandering too much.”, then it would find the human experiment a big mistake and start mashing the undo button.\n\nBy now a large knowledge base may exist about ecology, geology, glaciation, seasons, gravity, predation, symbiosis, causality, conservation, behavior, evolution and knowledge itself. In a small number of generations, man has managed to invalidate many of these bits of knowledge, this model of the world. If the heuristics can trace this breakdown to the increasing size of our brains, they might take quick corrective action, preserving homeostasis and the validity of their knowledge base by drastically decreasing human brain size over just a few generations. While this is of course a fanciful tongue-in-cheek extreme case, it (and the longer example above) demonstrates the power, the coordination, that a body of heuristics could evince if it were guiding the process of evolution.\n(D. B. Lenat 1983c)\n\nImagine a species getting so good at adaptation that it is considered a mistake and thus undone. Wouldn’t that be the greatest joke of nature?\n\n\n\nBut we’re really here to hear about the cool naval battles, not more abstract logics. Unfurl the photonic sails. We go.\n\n\nThe Evervictorious\nLenat did not describe why, one July 4 weekend of 1981, he and EURISKO decided to enter the The Trillion Credit Squadron tournament. Perhaps for glory, or the thrill of the hunt. In any case, they did, and thus carved their legend.\nThe tournament was for the tabletop RPG game Traveller. Despite purportedly about space battles in a galactic empire, the game was essentially just a simplified version of WWII-era navy battles draped in space-opera garments. In the tournament, pairs of starship generals line up their fleets and fight until one side loses all their ships or surrenders. Each fleet must be built within a budget of 1 trillion credits (thus the name), 100 ships, and under certain other minor constraints.\nAt the start, Lenat went through the rule books and coded the rules into EURISKO. Then, every evening EURISKO would run through its tasks, including Traveller games, MOS design, math problems, and so on. And every morning, Lenat would check on the last night’s results, remove some heuristics that EURISKO discovered that he deemed bad, and add some others. Manual intervention was necessary since otherwise EURISKO can be stuck with bad heuristics for a long time, and because of weird meta-bugs. Lenat estimated that the final EURISKO had accumulated 1300 CPU-hours of runtime in total on a Xerox 1100 Lisp machine, and the Traveller win was “60/40% Lenat/EURISKO”.\nSome heuristics that Lenat hardcoded at the start were:\n\nR7 (diagonal functions are interesting): If \\(f\\) is an interesting function of type \\(A \\times A \\to B\\), then \\(g(a) := f(a,a)\\) is possibly an interesting function of type \\(A \\to B\\), and should be studied.\nR9 (extremal sets have possibly interesting preimages): If \\(f: A \\to B\\) is an interesting function, and \\(S \\subset B\\) is extremal in some sense, then \\(f^{-1}(S)\\) is possibly an interesting subset of \\(A\\) and should be studied.\nR16 (conjecturing): If the first few examples of a concept \\(C\\) have just been found, then examine a typical one and see what properties it satisfies, then make a conjecture for each of those properties being satisfied by all examples of \\(C\\).\n\nThese were used by EURISKO for fleet design:\n\nOne type of craft which is commonly included is a fighter, which is carried into the area by a carrier… Following R7, the possibility was considered of building fighters that could transport themselves into the battle area; due to the way the constraints were set up, this turned out to be a very powerful–if bizarre–design tactic. Essentially, each fighter was equipped with just enough ‘sailing’ and ‘launching’ equipment for it not to need a carrier. Once airborne, this excess equipment was jettisoned… This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.\n…\nThe constraints specified a minimum fractional tonnage which had to be held back, away from battle [under the pretense of “fuel tenders”]. R7 caused us to consider using warships for that purpose, and indeed that proved a useful decision: whenever some front-line ships were moderately (but not totally) damaged, they traded places with the tenders in the rear lines. This maneuver was explicitly permitted in the rules, but no one had ever employed it except in desperation near the end of a nearly-stalemated battle, when little besides tenders were left intact. Due to the unintuitive and undesirable power of this design, the tournament directors altered the rules so that in 1982 and succeeding years the act of “trading places” is not so instantaneous. The rules modifications introduced more new synergies (loopholes) than they eliminated, and one of those involved having a ship which, when damaged, fired on (and sunk) itself so as not to reduce the overall fleet agility.\n…\nIn the naval fleet design task, R9 was used quite heavily. The functions \\(f\\) in that simulated world apply to the design and behavior of fleets and of individual ships: FleetComposition, Agility, Armor, WeaponVariety, TimeToEngage, etc… the ultimate design did settle on a fleet containing almost all identical ships, each with nearly minimal agility, maximal armor, maximal weapon variety, almost all of which engaged with the enemy immediately, etc. One extremal ship employed in the 1981 tournament was a tiny but incredibly agile ship, with no offense whatsoever, that simply could not be hit. Although this was no longer legal in 1982, a ship with massive offensive capability and no defense was instrumental in that new fleet.\n…\n[For R16,] once a new design was tested in simulated combat, several characteristics of the conflict were noted (speed of victory, final state of the victor, amount of tactical decision-making required, etc.). These were formed into proto-conjectures, which were then tested by subsequent mock battles, and any which held over most of the simulations were believed as empirically valid.\n(D. B. Lenat 1983b)\n\nOther than hard-coded heuristics, EURISKO of course discovered many heuristics on its own, such as the “nearly extreme” rule: In almost all fleet design situations, the right decision is to go for the nearly extremal design.\n\nThus, the final ships had Agility 2 (slightly above the absolute minimum), one weapon of each type of small weapons (rather than 0 or many), the fleet had almost as many ships as it could legally have but not quite (96 instead of 100), etc. Big weapons (enormous spinal mounts capable of blasting another ship to pieces with a single shot) were gradually phased out, in favor of an enormous number of small missile weapons. The fleet had almost all (75) ships of this type though there was one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship), and a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive.\n(D. B. Lenat 1983b)\n\nOne might have questioned the wisdom of running EURISKO every night on all the problem domains, from Traveller to MOS design, but it found good use of heuristic rules learned in one domain applied to another, what we’d now call “transfer learning”:\n\nIn working on the design of integrated circuits, for example, EURISKO stumbled on the fact that symmetry is a desirable property for such chips, although it did not understand why; when it was later instructed to design fleets for the Traveller game, EURISKO decided to make them symmetrical and justified its decision by referring to its earlier experience in designing circuits.\n(D. B. Lenat 1984)\n\nFortunately for EURISKO and Lenat, the navy battles were “tactically trivial”, thus reducing the task to merely nonlinear optimization. Also importantly, the problem was hard enough, and nonlinear enough, for EURISKO to show its edge over linear programming and human intuition.\n\nwith 50 parameters per ship, about 10 values for each parameter (sometimes fewer, often an infinite number), and up to 100 distinct ships to design and include in each fleet, any systematic or Monte Carlo analysis of the problem is unlikely to succeed. In fact, the designers had done a detailed linear programming model of the game, and their computer runs convinced them that a fleet of about 20 behemoths was the optimal design. This was close to the starting fleet design the author supplied to EURISKO, and it was also close to the designs that most of the tournament entrants came up with.\n(D. B. Lenat 1983a)\n\nAt the 1981 championship, EURISKO’s fleet consisted of 96 ships:\n\n75 “Eurisko class” ships, which were very slow, heavily armored, and carried many small missiles, like a sea urchin. They formed the bulk of the fleet, and used for pure attrition warfare. It was discovered thanks to the “nearly extreme” heuristic.\n7 “Wasp class” ships, which were small (1000 tons) but the most agile (Agility 6). These ships had virtually no offensive capability but were practically impossible to hit, serving as “stalemate guarantors”. If all other ships in EURISKO’s fleet were destroyed, these agile ships would remain, forcing a draw since enemy ships couldn’t destroy what they couldn’t hit. This concept emerged from a serendipitous battle where one side survived by having an unhittable “lifeboat” (the “Bee class”). They were carried aboard the Queller and Garter class ships.\n3 “Bee class” tiny ships (99 tons), which were the original accidentally-discovered stalemate guarantor, the “lifeboat” that EURISKO incorporated into every subsequent design, even while it refined the concept of the stalemate guarantor into the Wasp class. Despite heavy Armor 10, their Agility 0 made them less effective than Wasps at avoiding enemy fire, but presumably their tiny size made them hard to hit. They were carried aboard the Queller class ships.\n3 “Queller class” ships. They were specifically designed as hard counters to stalemate guarantors. Each carried a single massive particle accelerator, which was not used by human-designed fleets, since that was ineffective against normally sized ships – but due to its broad beam and ease of aiming, excellent against small targets.\n4 “Cisor class” ships. These “monstrous hulks” were heavily armored vessels that were also hard counters to stalemate guarantors. It had Agility 0, which means it has no chance of avoiding normal ships, but presumably it would survive long enough to destroy any stalemate guarantor, if any existed on the opposing side.\n4 “Garter class” ships, which implemented the “warship as fuel tender” concept. They were reasonably agile (Agility 4) and could rotate between combat and reserve roles. When front-line ships became damaged, they would trade places with these capable warships held in reserve, allowing fresh ships to enter combat while damaged ones withdrew for repairs. They were crucial for victory in the final battle.\n\n\n\n\n\n\n\nReconstructing EURISKO’s fleet\n\n\n\n\n\nThe fleet was actually precisely reported in The Journal of the Travellers’ Aid Society, #10, pp. 38-9, which Yuxi had transcribed to plaintext (thenceforth “report”). Not being an honorary Admiral, agi convened with a council of 4 other LLMs (Gemini-2.5, Claude-3.7, OpenAI-o1, DeepSeek-R1) to figure out how to correlate Lenat’s verbal description in (D. B. Lenat 1983a) (thenceforth “description”) with the precise report.\nIn fact, it’s quite confusing even in the original paper. All agreed that the 75 “Eurisko class” ships corresponded to the “Eurisko class” in the report. But that’s where clarity ended.\nThe “stalemate guarantor” was described as “one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship)”. However, in the report, every single class contained more than one ship. There were two classes that seemed like the stalemate guarantor: the Wasp class with 1000 tons and Agility 6, and the Bee class with 99 tons and Agility 0. The Bee class had the smallest tonnage but no Agility, while the Wasp class was second-smallest and had the highest Agility. The Council voted Wasp as the “stalemate guarantor” at 3 Yea (Yuxi, Claude, o1), 1 Nay (Gemini), and R1 abstaining due to the server being busy (abstaining is typical behavior for the Chinese during Big-Five votes). Those who voted “Yea” were unable to respond to Gemini’s objection as to what EURISKO used the Bee class for, and motioned to discuss the second issue.\nThe hard counter to the stalemate guarantor was described even less clearly, and it seemed like there were two classes of them!\n\na couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive. … this new ship had moderate size, no armor, the largest possible guidance computer, the slowest possible engines for its size and equipment, and one single, enormous accelerator weapon–a weapon usually ignored because its broad beam glances harmlessly off large armor-plated ships, but which is very easy to aim.\n(D. B. Lenat 1983a)\n\nThe first hard counter matched both the Cisor class and the Queller class, both of which had over 19,000 tons and Agility 0. The second hard counter seemed like the Garter class, with 12,000 tons and Agility 4.\nYuxi lamented that it would have saved the Council a great deal of trouble if the report had detailed what weapons the ships used, but alas, they only reported on the “batteries”. It was at this point that o1 and Gemini raised that the “Batteries Bearing” did not actually stand for “carrying around electric batteries”, but rather “the angle which the artillery could span its fire”. Yuxi expressed astonishment that they could recall the rules of Classic Traveller. “What a nerd!”, muttered Yuxi. There was no reaction from the others, since the meeting was entirely text-based. Yuxi then sent the entire (Traveller Book 5: High Guard 1980) to Gemini for a translation, in response to which Gemini suggested that Yuxi RTFM, “Specifically, pages 21–37 and 50–52 are crucial.”.\nThe Council’s final conclusion was that probably there were two types of hard counters, with the Cisor being the “monstrous hulk” and the Queller being the one with the “enormous accelerator”.\nReading Gemini’s statement, Claude raised the issue of what the Garter class was supposed to be. At that point, Yuxi re-attended to the fact that (D. B. Lenat 1983b) described a slightly different fleet, one with “fuel tenders”, which (D. B. Lenat 1983a) did not describe at all. Upon presenting the quote describing “fuel tenders”, Gemini argued that the Garter class is the fuel tender, and the Council concurred.\nThe Council reached no concensus as to the purpose of the Bee, though some suspected that the Bee was another type of stalemate guarantor, possibly the original accidentally-discovered stalemate guarantor, as described in:\n\nSome of the strangest elements of the final fleet were discovered accidentally rather than as the result of a long, continuous evolution process. The usefulness of a tiny defensive ship was apprehended after a ‘lifeboat’ was the only survivor from one side’s fleet, yet round after round it could not be hit at all. That design was immortalized into a design strategy (“Include one such ship in your fleet!”), and a very general rule began looking for ships that could destroy it.\n(D. B. Lenat 1983a)\n\nYuxi raised one final issue, that of what the fighter “equipped with just enough ‘sailing’ and ‘launching’ equipment for it not to need a carrier” described in (D. B. Lenat 1983b) corresponded to. Gemini concluded that no such thing existed in the fleet as reported, since the 3 Bees and 7 Wasps were the only fighters in the fleet, and they were carried by exactly enough carriers (3 Quellers carrying 1 Wasp and 1 Bee each, 4 Garters carrying 1 Wasp each). Yet the description “This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.” strongly suggested that EURISKO actually used such a ship at one point in the 1981 competition.\nSomeone suggested that EURISKO may have used more than one fleet design, and only the design used in the final battle was reported. Yuxi acknowledged absent endorsement and motioned that the meeting adjourn.\nThe final report from the Council is attached.\n\n\n\nWhereas most battles took 2–4 hours, EURISKO’s opponents resigned in a few minutes, because typical opponents had around 20 large ships against EURISKO’s 96 ships. Each round would destroy EURISKO’s 15 ships and 5 of opponent’s ships. One round was enough for the opponent to realize this, and resign.\nThat is, except the very last round, where EURISKO faced against a human opponent with basically the same design, except they didn’t have the stalemate guarantor. So if EURISKO seemed to be losing, it could retreat all its fleet and bring out the stalemate guarantor, repair the fleet to full health, then do it again. EURISKO could keep doing this until a lucky dice roll to win.\n\nIts second opponent did some calculations and resigned without ever firing a shot. The subsequent opponents resigned during their first or second round of combat with this fleet. EURISKO’s few specialty ships remained unused until the final round of the tournament, battling for 1st versus 2nd place. That opponent also had ships with heavy armor, few large weapons, low agility, etc. He was lacking any fast ships or fast-ship-killers, though. The author simply pointed out to him that if EURISKO were losing then (according to the TCS rules) our side need put only our fast ship out the front line, withdraw all the others and repair them, and – once they were finished repairing themselves – effectively start the battle all over again. This could go on ad infinitum, until such time as EURISKO appeared to be winning, and in that case we would let the battle continue to termination. The opponent did a few calculations and surrendered without fighting.\nThe tournament directors were chagrined that a bizarre fleet such as this one captured the day, and a similar fleet (though not so extreme) took second place. The rules for future years’ TCS tournaments were changed to eliminate the design singularities which EURISKO found. For example, repairing of damaged ships was prohibited, so the utility of the unhittable ship became negligible.\n(D. B. Lenat 1983a)\n\nNotice that this complicates the typical narrative about a computer beating out humans. As it turns out, Traveller was just a very exploitable game. Even a human discovered that loophole.\nAt the 1982 championship, rules were changed to plug the loopholes, and the rules were published only a week before the event. Fortunately, loophole-plugging begat even more loopholes, as any programmer who has ever rush-debugged could attest, and EURISKO’s general heuristics (such as the “nearly extreme” rule) remained valid, so it worked out another winning fleet quickly without needing another 1300 CPU-hours.\n\nCoincidentally, just as the defensive ship made a difference in the 1981 final round, the offensive ships made a difference in the 1982 final round. In each case, their presence caused the opponent to resign without firing a shot… Just as most ‘experienced’ players jeered at the 1981 fleet because it had practically no large weapons, they jeered at the 1982 fleet because it was unarmored and it still had no large weapons, even though the rules changes had made them much cheaper.\n(D. B. Lenat 1983a)\n\nUnfortunately, there is no information whatsoever on the composition of EURISKO’s 1982 fleet. It seems to be lost in the star-dust of history.\nAs one expect from a program reasoning about and making its own rules, it stumbled into meta-bugs. The simplest example was one that kept triggering itself, creating an infinite loop. (Johnson 1986, chap. 10) Others were more amusing.\n\nOne of the first heuristics that EURISKO synthesized (H59) quickly attained nearly the highest Worth possible (999). Quite excitedly, we examined it and could not understand at first what it was doing that was so terrific. We monitored it carefully, and finally realized how it worked: whenever a new conjecture was made with high worth, this rule put its own name down as one of the discoverers! It turned out to be particularly difficult to prevent this generic type of finessing of EURISKO’s evaluation mechanism. Since the rules had full access to EURISKO’s code, they would have access to any safeguards we might try to implement. We finally opted for having a small ‘meta-level’ of protected code that the rest of the system could not modify.\nThe second ‘bug’ is even stranger. A heuristic arose which (as part of a daring but ill-advised experiment EURISKO was conducting) said that all machine-synthesized heuristics were terrible and should be eliminated. Luckily, EURISKO chose this very heuristic as one of the first to eliminate, and the problem solved itself.\n(D. B. Lenat 1983a)\n\n\nOften I’d find it in a mode best described as “dead”. Sometime during the night, EURISKO would decide that the best thing to do was to commit suicide and shut itself off. More precisely, it modified its own judgmental rules in a way that valued “making no errors at all” as highly as “making productive new discoveries”. As soon as EURISKO did this, it found it could successfully meet its new goal by doing nothing at all for the rest of the night… I eventually had to add a new heuristic to EURISKO-one it couldn’t modify in any way-to explicitly forbid this sort of suicide.\n(Stork 1998, 194)\n\n\n\n\nThe only known image of EURISKO, reasoning about the Traveller game, probably on the Xerox 1100 Lisp machine. It was probably running over the Interlisp Operating System. Lenat claimed EURISKO ran for 1300 CPU-hours in total. (D. B. Lenat 1984)\n\n\nIn his review article, Lenat made a brief philosophical comment that EURISKO is the new perceptron:\n\n… the paradigm underlying AM and EURISKO may be thought of as the new generation of perceptrons, perceptrons based on collections or societies of evolving, self-organizing, symbolic knowledge structures. In classical perceptrons, all knowledge had to be encoded as topological networks of linked neurons, with weights on the links. The representation scheme being used by EURISKO provides much more powerful linkages, taking the form of heuristics about concepts, including heuristics for how to use and evolve heuristics. Both types of perceptrons rely on the law of large numbers, on a kind of local-global property of achieving adequate performance through the interactions of many small, relatively simple parts.\nThe classical perceptrons did hill-climbing, in spaces whose topology was defined explicitly by weights on arcs between nodes (nodes which did straightforward Boolean combinations plus thresholding). The EURISKO style of system does hill-climbing at both the object- (performance-program) and meta- (control decision) levels, in spaces whose terrain is defined implicitly, symbolically, by the contents of the nodes (nodes which are full-fledged concepts, at both object- and meta-levels). The new scheme fully exploits the same source of power (synergy through abundance) yet it is free from many of the limitations of the classical perceptron scheme.\n(D. B. Lenat and Brown 1984)\n\nIf this sounds familiar, it is because this Lenat had the same idea as Marvin Minsky, a friend of his,3 and he was writing in 1984, at the second coming of neural networks. Minsky would soon write his Society of Mind in 1986, which the Cyc project resembled, then re-reject neural networks by writing a long epilogue in 1988 to his infamous Perceptrons (1969), often blamed for the first neural network winter. Indeed, Lenat’s objection to neural networks was essentially the same as Minsky’s, if you compare that with Minsky’s epilogue. Lenat’s approach to Cyc was the same as the Society of Mind of Minsky. Reciprocating, Minsky had often called the field of AI “brain-dead”, holding Lenat’s Cyc as the only one worth mentioning. (Baard 2003)\n3 I’m not sure where to put this anecdote about Minsky that Lenat told, but I want to put it somewhere:\n\n… when he was at Lincoln Labs about 50 years ago. And in those days computer time was so precious that you submitted a deck of computer cards and the very first card said ‘how many CPU seconds to allow the program to run?’ And so he built a program that essentially would beg for time. So it would say ‘give 30 seconds’ on the job control card, but then once it started, all it would do is sit there for 15 seconds doing nothing. Then it would ring a bell on the Teletype console in the machine room and call the operator’s attention and say ‘I need 20 more seconds please.’ Then it would just sit there for another 15 seconds and do that again and say ‘I need another minute please.’ And so at the end finally after like half an hour, the operator just killed that particular job. And Marvin would storm into the poor operator’s room and say “Hey I put 15 seconds on the job control card. You’re charging me for half an hour of CPU time,” and the poor operator would say “well your program kept asking for it,” and Marvin would say, “it always does that.”\n(D. B. Lenat 2019b)\n\nThough now that I’ve finished the essay, this feels like a metaphor for the Cyc project itself.4 Though Lenat admitted that “the code ought to have been provided” (D. B. Lenat and Brown 1984) for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had recently been found, right where it should be – the DBL folder in the Stanford AI Laboratory backup data. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.Continuing the trend of AM, Lenat never published the source code for EURISKO,4 and indeed, the only known attempt at reimplementation was (Haase 1990), which had no offspring."
  },
  {
    "objectID": "essays/posts/cyc/index.html#the-saga-of-cyc",
    "href": "essays/posts/cyc/index.html#the-saga-of-cyc",
    "title": "Cyc",
    "section": "The saga of Cyc",
    "text": "The saga of Cyc\n\nHe divided the universe into forty categories or classes, which were then subdivided into differences, and subdivided in turn into species. To each class he assigned a monosyllable of two letters; to each difference, a consonant; to each species, a vowel. For example, de means element; deb, the first of the elements, fire; deba, a portion of the element of fire, a flame. In a similar language invented by Letellier (1850),5 a means animal; ab, mammalian; abo, carnivorous; aboj, feline; aboje, cat; abi, herbivorous; abiv, equine; etc… children could learn this language without knowing that it was artificial; later, in school, they would discover that it was also a universal key and a secret encyclopedia.\nHaving defined Wilkins’ procedure, we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.\n— Borges, The analytical language of John Wilkins\n5 Took some work to figure out what this is. It was by Charles Louis Augustin Letellier, published in 1852. The full title is Cours complet de langue universelle : offrant en même temps une méthode. pour apprendre les langues, et pour comparer. toutes les littératures mortes et vivantes [Complete course in universal language: offering at the same time a method for learning languages, and for comparing all literatures, dead and living].\nUnfortunately, EURISKO ran out of steam just like AM. Taking the 4 lessons of AM and EURISKO, Lenat concluded that there would be no free lunch. Intelligence is a lot of work. You need to put in the right representational language for reasoning and discovery – not just about Lisp or RLL or mathematics, but a lot more. You need to put in a lot of loosely organized, kind of correct heuristic rules – not just a few eternal truths of discovery. You need to put in a lot of facts. You need to interact with the program, to help it along and to be helped along, not just to sit and watch.\nSo Lenat began paying for his lunch.\nIn 1984, he started the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. Like most expert systems, the Cyc project consists of a giant knowledge base that encodes all of common sense, upon which inference engines run. Unlike most expert systems, the ambition of Cyc was universal: Its knowledge base would not be restricted to expert knowledge in a particular domain, but all common sense knowledge in all domains that humans have ever common-sensed. It would take a decade, but considering the payoff, it would be completely worth it.\n\nAI has for many years understood enough about representation and inference to tackle this project, but no one has sat down and done it… only by pulling together the latest human interface tools, Lisp machines, ideas of enforced semantics, and funding for a decade-long effort could we attempt a project of this scale.\n(D. B. Lenat, Prakash, and Shepherd 1985)\n\nThe game plan was simple:\n\n“Prime the knowledge pump” by manually encoding a large enough knowledge base of common senses in a logical language. Also, construct a translator between the logical language of the Cyc and the natural language of humans.\nObtain an AI with common sense and natural language, allowing it to learn by reading what people have written down and conversing with people.\nWhen it reaches the human frontier of knowledge, it will start performing experiments to go beyond it.\n\nThis would solve in one go three problems that plagued the 1980s expert systems:\n\nNo more of the famous brittleness of expert systems, such as stating that a rusty car had measles just because the user stated that it had reddish spots, because Cyc would have all the common senses.\nNo more of “running out of steam” like EURISKO and AM. Once it has enough knowledge, it would be able to machine-learn, and thus get past the knowledge bottleneck.\nNo more of siloed experts unable to communicate across the vast gap between their little knowledge domains. Expert systems would finally be able to talk with each other if they would all be based on Cyc’s universal knowledge base.\n\nA metaphor that Lenat had used often is that of priming a water pump. The more knowledge Cyc has, the easier it is for Cyc to learn more. At first, it must be spoon-fed knowledge with every entry entered by hand. As it builds a basic understanding of the world, it would be able to parse sentences half-way from natural language to logic, and the ontologists would help finish the job, and the more it knew, the better it could parse, saving more time, until it would start parsing without human help. On that day, the “knowledge pump” would finally, triumphantly, be primed, and Cyc would start pumping and pumping, and more knowledge would just keep pouring out without any exhaustion, ushering a new golden age.\n\nCyc by 1993\nThe first published plan of Cyc was in 1985 (D. B. Lenat, Prakash, and Shepherd 1985), with 3 stages:\n\n(1985–1988): by hand, encode 400 encyclopedia articles. They estimated there are about 400 “kinds” of articles, and they planned to get one in each kind.\n(1988–1993): encode 30000 encyclopedia articles. This step should be fast, since each new article can be copy-and-edited from a previously encoded similar one.\n(1993–?): use Cyc to solve AI problems and apply Cyc for commercial purposes.\n\nThey noted that most articles had just 1 paragraph, and 1 paragraph took about 1 person-day to encode, which allowed them to estimate that phases 1 and 2 would take 150 person-years to complete, so with a crew of 20, the plan seemed doable.\nAt this point, Cyc was still based on frames, like AM and EURISKO. Each frame corresponded roughly to a concept. Each new article encoded took “dozens” of new frames, but they expected it to drop to around 0.1 frames per article after the first 1000s of articles, indicating they expected the final Cyc to contain about 50K concepts.\nSuch a project was way beyond a typical academic project, or even a commercial project. Fortunately, the Japan scare of the 1980s created a flood of funding for AI projects, and Cyc got funding for the next 10 years, freed from both academic and commercial interests.\n\n\n\nThe state of knowledge base of Cyc one year after its founding, in 1985. (D. B. Lenat, Prakash, and Shepherd 1985, fig. 1)\n\n\n5 years later, Lenat optimistically wrote the “midterm” report that the project was still on schedule, that the knowledge pump would be primed by 1995. (D. B. Lenat and Guha 1990)\nThe most important change was what Lenat called “from the black to the white”. Instead of encoding what is written in the encyclopedias, they really should encode what is not written. Encyclopedias don’t teach what everyone knows, but to make sense of it, one must already know what “everyone knows”. What Cyc should have is not the black ink, but the white space around the black ink.\n\nLenat began building Cyc by setting himself a seemingly modest challenge. He picked a pair of test sentences that Cyc would eventually have to understand: “Napoleon died in 1821. Wellington was greatly saddened.” To comprehend them, Cyc would need to grasp such basic concepts as death, time, warfare, and France, as well as the sometimes counterintuitive aspects of human emotion, such as why Wellington would be saddened by his enemy’s demise. Lenat and a few collaborators began writing these concepts down and constructing a huge branching-tree chart to connect them. They produced a gigantic list of axiomatic statements–fundamental assumptions–that described each concept in Cyc’s database: its properties, how it interacted with other things. “We took enormous pieces of white paper,” Lenat remembers, “and filled walls, maybe 150 feet long by about 8 feet high, with little notes and circles and arrows and whatnot.”\n(Thompson 2001)\n\nThey have also estimated that it takes about 80 minutes to encode a single rule, including the overhead for knowledge elicitation up front, and the overhead for debugging and testing (D. B. Lenat 2022a).\nWith these lessons learned, they confidently claimed that they have encoded 1M assertions, 50K concepts, with 0.1% of common sense done. They expected that 10–50% was necessary for knowledge pump to be primed, which they expected to be done by 1995 – still on schedule,6 and they looked forward to the day, around 2000, when “no one would even think about having a computer that doesn’t have Cyc running on it” (D. B. Lenat and Guha 1989).\n6 Presumably, at that point Cyc would stop imagining everyone is a friend of Lenat but not of gravity. Growing up truly comes with some terrible disillusionments.\n\nWhen a natural-language understanding researcher was first trying to represent the concept of falling, he translated “X fell” as “Gravity carried X downward.” Elsewhere in his system was a rule that if you fell in a river and could not swim, and had no friends to rescue you, you drowned. Since gravity as a force of nature has neither arms, legs nor friends, it meets its unfortunate – and improbable – end.\n… sometime in 1994 or 1995 by Lenat’s hopeful reckoning – Cyc reaches the breakeven level of about 10 million facts. At that point, it will be able to pick up new knowledge more readily by reading than by having knowledge engineers spoon-feed it. A wider information base may also save Cyc from such gaffes as concluding (from everybody it knew about) that all humans in the world are friends of Doug Lenat.\n(Wallich 1991)\n\nThey expected that the finished Cyc would know 1M concepts. Why 1M? They held their own “mini-Dartmouth conference” and found that multiple estimates all suggest the 1M number (D. B. Lenat and Feigenbaum 1991):\n\nAlan Kay: 30K encyclopedia articles with 30 concepts per article gives 0.9M concepts.\nJapanese Electronic Dictionary Research Project: in several languages, an educated speaker knows about 200K words.\nMarvin Minsky: 0.2M waking hours between birth and age 21. Assuming 4 new concepts per hour, then 0.8M concepts.\nThere are about 1 trillion brain cells. Assuming each brain cell is responsible for a one-step inference between two concepts, then there are 1M concepts.\n\n\n\n\nThe user interface of Cyc as of 1989. It has a similar style as EURISKO’s, and thus it probably ran on the Interlisp OS. It was described as being able to run on “Symbolics Machines of all sorts, Sun3s, TI Explorer II+s, MacIIs (with Ivory boards), and (with some conversion) VAX machines”. These were dominant high-end Lisp computers of the late 1980s: dedicated Lisp Machines (Symbolics, TI), powerful Unix workstations (Sun), high-end PC enhanced with Lisp hardware (MacIvory), and minicomputers (VAX). (D. B. Lenat and Guha 1989, figs. 8–1)\n\n\n\n\n\nA small fragment of Cyc’s upper ontology as of 1990. (D. B. Lenat and Guha 1990, fig. 2)\n\n\n\n\nCyc by 2000\nAround 1990, a large rewrite occurred, resetting some progress. This was expected. As Lenat later claimed, they had met about 150 technical obstacles along the way, and had cleared them all away by 1990, and it remained to just add more knowledge. (D. B. Lenat 2009, 2022a) The most important technical obstacles that they conquered were as follows.\nInstead of an object-oriented frame-and-slots language like EURISKO and AM, use a fully general higher-order logic language. This is necessary because people can do common sense higher-order reasoning: modals, reflection, pros and cons, counterfactual hypotheticals, contexts as first-class objects in our ontology, several different useful “species” of negation, etc. (D. B. Lenat 2022a)\nInstead of searching for the right representation, use as many representations for concepts and rules as you need. Each is represented in at least two ways. This is necessary for efficient inference. Similarly, use as many inference engines as needed, since the general logic engine is too slow. They had already 20 at that point, and they would eventually end up with &gt;1100. (D. B. Lenat 2022a)\nDon’t try to make the perfect “upper ontology”. It just has to be good enough. A “suboptimal” one only causes a constant factor \\(O(1)\\) of waste in computational space and time.\n\nWe even wasted quite a bit of time trying to get the very most general tip of Cyc’s concept network “right”… at the 1986 Fifth-Generation Project conference in Tokyo, when we saw the ontology built by Japan’s answer to Cyc, named Electronic Dictionary Research (EDR). Their topmost distinction was between things with souls and things without souls. And large trees were in the former category, whereas small trees were in the latter category… They and their EDR system knew that both types of trees needed water and sunlight and had roots, etc., they just had to represent each of those assertions as two separate rules instead of one, as we did in Cyc. No big deal.\nThe important lesson was: Making suboptimal ontology choices just means that your ontology and knowledge base might have to be bigger, more verbose, to make up for those missed generalization opportunities.\n(D. B. Lenat 2022a)\n\nIn the progress report in 1995, they stated that they had manually re-entered in the new language 100K concepts and 1M assertions into Cyc in the new language, at the price of 100 person-years. Furthermore, the knowledge pump was close to being primed, and they expect Cyc to start learning on its own by reading (“natural language understanding”) and discovery (“machine learning”) sometime in the next 10 years. (D. B. Lenat 1995b) In another essay, Lenat optimistically predicted that “The goal of a general artificial intelligence is in sight, and the 21st-century world will be radically changed as a result.” (D. B. Lenat 1995a)\nBefore this point, the Cyc project was part of a private-public consortium and funded accordingly. After this point, the consortium was mostly over, so Lenat spun out Cyc into a for-profit company, Cycorp, to continue the work. Most academic publications ceased at this point, and I had to rely on OSINT/cyberstalking at this point to piece together what happened afterwords, by watching every Lenat talk, reading every news report, and digging up every gossip by ex-Cyclists in long-dead forums.7\n7 Most papers published after that point were slim on details, and mostly about yet new exciting ways for them to ingest more data from the Internet, or about yet more ways to use their knowledge base. I could find no information about how the inference engines worked, and very little details of commercial applications. Most of the “applications” were vaporware, with dead links everywhere.\n\nCyc in the 2000s\nIt was 2001. The Internet was thriving even though most dot-com startups had died, and Lenat planned to harness the wisdom of the online crowd – much like Wikipedia, the Free Encyclopedia, which launched in the same year. Cyc at that point had 1.5M assertions, and had begun to be partly “tutored” in natural language by the Cyclists, which was more pleasant than typing everything up in pure CycL. Lenat optimistically predicted that the Internet crowd would be entering 10M assertions in 2002, accelerating from there, so that Cyc would know 100M assertions by 2007, at which point it would know as many things as a typical human. Then by 2011, Cyc would have learned, by reading and chatting with people, all that humanity collectively knows. From there on, Cyc will extend the knowledge frontier by running novel experiments in a research lab. (Thompson 2001; Anthes 2002b) Other reports suggested that AGI, or what he called a “generally intelligent artifact” would arrive by 2020 or 2025. (Port 1997; Lyons 1998)\n\nHe draws me a graph that shows Cyc’s learning curve. From 1985 to 2000, the line curves upward gradually – the “brain surgery” phase during which the Cyclists input knowledge by hand. But then at 2001, the curve steepens dramatically as the open-source phase takes over, and thousands – or millions – more inputters join in. Lenat extends the curve maybe ten years into the future. As the curve reaches the point where Cyc has read everything there is to read and spoken with everyone willing to tell it facts, it will begin to flatten out. “It’ll know all there is to know,” he says. “At that point, the only way it could learn more is by doing experiments itself.”\n(Thompson 2001)\n\nTo drum up the support, Cycorp released OpenCyc in 2001, a small subset of Cyc. Lenat planned to migrate everything to the public mode. But OpenCyc would always lag the true Cyc by 24 to 30 months. (Anthes 2002b) Unfortunately, this was not to be the case. The last version of OpenCyc was released in 2012, and it quietly shut down with no fanfare, probably in 2017-03, with a curt message:\n\nPart of the Cyc technology was released, starting in 2001, as OpenCyc, which provided an API, RDF endpoint, and data dump, under appropriate Apache and Creative Commons open source licenses. Its distribution was discontinued in early 2017 because such “fragmenting” led to divergence, and led to confusion amongst its users and the technical community generally that that OpenCyc fragment was Cyc.\n\nOpenCyc was primarily a subset of the knowledge base. As described in the Handbook, the base contained 3 sections:\n\nThe public section was a “large section, constituting much of the Cyc upper-ontology as well as some middle- and lower-ontology”.\nThe proprietary section was an “even larger section of the Knowledge Base, subsuming the public release and containing a good deal more, has been sanctioned for release to corporations and individuals who are co-participants with Cyc in various DARPA contracts. This portion of the Knowledge Base is generally referred to as the Integrated Knowledge Base or IKB.”.\nA small classified section was for Cycorp itself, “that should not be released to anyone outside the company. Sometimes, this is because the information is pertinent to commercial contracts that are subject to non-disclosure; sometimes, it is because the terms in question are considered experimental in one way or another, and therefore not suitable for immediate release.”\n\n\n\n\nThe knowledge base of OpenCyc as of 2010. (Source)\n\n\nConcurrent with OpenCyc, there was also ResearchCyc, which contained the non-proprietary parts of Cyc, but only available for research purposes. It shutdown sometime in 2019, without even a curt message.\nAs one can expect from Lenat’s previous non-releases, there was and has never been any release of Cyc itself, especially because Cyc is a commercial endeavor, as is necessary to sustain the 2000 person-year project.\nIn 2001, at the start of the Semantic Web hype (the original “Web 3.0”, before it came to mean cryptographic blockchains), Cycorp began seriously engaging with the many Semantic Web initiatives.\nSquinting a bit, the visions of the semantic web and the vision of the Cyc were the same: Both wish to draw connections between data, such that computer programs can chain together multiple operations on data, synthesize them, and give the user what they meant, instead of what they literally typed out. Cycorp regarded the semantic web effort as doing essentially the same thing, on the Internet scale, except with a less expressive frame language (not even first-order logic). During the 2000s, papers from Cycorp often talked of integrating Cyc with the semantic web by encoding knowledge in DAML, RDF, OWL, XML, or some other boring acronym.\nCycorp participated in the Standard Upper Ontology Working Group (SUO WG), which, like most Working Groups, petered out in 2003 among motions, procedures, and consensus-buildings, filled with meticulous wisdoms and benevolent safeguarding of the metaphysics of mankind. What it did provide is the earliest copy of OpenCyc I have found, in 2003, which I have backed up for safekeeping.\nWith the breakout success of the ESP game in 2003, “games with a purpose” were all the rage, so Cyc attempted to keep up with the times with its own FACTory in 2005. It never got out of the Beta version 1.0.1 and shut down sometime after 2012. According to the tutorial, it was a single player game, where the player just selects whether the statement is true, false, doesn’t make sense, or the player doesn’t know. The answers are scored based on the majority of answers. Though Lenat stated in a lecture that they had players that raked up 50 hours a month to the point that Lenat felt concerned about it, I could not find any mention of how much data they have gathered from this game in total. Nevertheless, this appeared to be the only time Cyc crowd-sourced data.\nIn 2006, Cycorp spun out The Cyc Foundation, a non-profit organization promote the OpenCyc + Semantic Web combo. As usual, nothing ever came of it. The last blogpost went up in 2011-06, and the website shutdown in 2015.\n\n\n\nThe Cyc + Wikipedia = “Cyclopedia” mockup posted in 2007 by The Cyc Foundation. Despite being “fairly close to releasing a beta version of Cyclopedia”, they never did.\n\n\nIn 2008, Cycorp tried again by putting up a copy of OpenCyc with a user interface and an API, branded as “OpenCyc for the Semantic Web”. The API would allow web agents to call on OpenCyc and use the replies to do Semantic Web things, making it “the backbone of semantic web”… at least that was the hope. An exhaustive Google search turned up zero actual applications. It shut down in 2017 when OpenCyc did.\nThough deprived of its purported backbone, from our vantage point, the Semantic Web has arrive as promised, but instead of the dream of Cyc-like thinkers performing long queries over databases, we have forgetful agent swarms talking with each other with API calls, dying, RESTing, and reincarnating, a game of Memento on the global scale.\nIn 2008, Cycorp tried again by joining the Large Knowledge Collider (LarKC)8 project at Europe, “a platform for massive distributed incomplete reasoning that will remove the scalability barriers of currently existing reasoning systems for the Semantic Web”. The hope was to produce a common knowledge base with over 1 billion triples, such that it can easily be scaled up further, and easy to produce expert systems based on it. A few papers and conferences later, the project ended in 2011.\n8 The name is an obvious shade to the Large Hadron Collider, which began operation in 2009 and was briefly in the popular imagination.There was another blog by Cycorp starting in 2008, that stopped updating in 2011 after 11 unremarkable posts. It was supposedly written by Cycorp, though its style was the same forgettably respectable style as that of the Cyc Foundation and parliamentary proceedings.\nThere was a Twitter bot @cyc_ai​, which started in 2008 and stopped in 2011 after 15764 inane tweets in the format of “I just leaned &lt;statement&gt;, true or false?”. According to Internet Archive, it shut down sometime before 2017.\n\n\nCyc is done?\nAs you might have noticed if you clicked on any of the above links, almost all the links are dead now. By checking the last known good copy of the various websites on Internet Archive, I noticed that there was a “massive extinction event” during 2014–2016, when Cycorp purged most of the open information about Cyc from the Internet. No more OpenCyc, tutorials, references, vocabulary lists, The Ontological Engineer’s handbook (version 0.7)…9 everything was gone, except marketing material. Fortunately, the Internet Archive exists, and I scraped much of the primary material from it and uploaded them to GitHub for safekeeping.\n9 There is a funny anecdote about this book:\n\nI coined the phrase “ontological engineer” in the mid-1980’s, shortly after embarking on the construction of Cyc, because I didn’t like the pejorative tone of “knowledge enterer” or “knowledge worker”, and the term “knowledge engineer” had already been taken (to mean someone who builds expert systems). Based on that, when Addison-Wesley published our 1989 Cyc book (Building Large Knowledge-Based Systems), the editor playfully inserted a forward reference, at the front, under Other Publications, to the 1997 Ontological Engineer’s Handbook. Of course the joke was on us when, starting in 1997, we began to be deluged by requests for that nonexistent work.\n(D. B. Lenat 2005)\n\nWhy?\nI believe it was for commercial reasons. This mass extinction event closely corresponded to the commercialization wave in 2016, when Lenat finally declared the Cyc project “done” and set about commercializing it, both via Cycorp and via Lucid.ai, a company founded a year before.\n\n“Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, [Lucid] is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri. … the CEO of Lucid says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.\n(Knight 2016)\n\nThis timing of commercialization coincides suspiciously with the end of almost all projects except ResearchCyc. Even the documentation went offline in 2016 and could now only be accessed for commercially registered accounts! It looks like a total internal pivot as the company focused on commercialization and shut down all services that were not high priority for the bottom line.\n\n\nHow expensive was the lunch?\nAfter cyberstalking the Cyc for days, I had captured every numerical datapoint that has ever dripped out of Cycorp over its entire existence, and combined them into a spreadsheet that you can analyze yourself. In short, Cyc has grown to 30M assertions over the years, and is still incomplete, if completeness were measured by its original standard – a self-learning AGI. Despite Lenat’s 2016 claim that it was “done”, there is no self-learning or AGI in sight.\n\n\n\nThe progress of Cyc project over the period of 1989–2022. The number of assertions grew to 30M, the cost grew to $200M, with 2000 person-years. Source: my obsessive cyberstalking.\n\n\nLooking at this diagram, we notice 3 things:\nOne, the growth of assertions is roughly exponential, doubling every 6 years. At this rate, in 2032 Cyc can expect to reach 100M assertions, the hoped-for point at which Cyc would know as much as a typical human. This might be the dream of Lenat, but I bet it will just become a slightly bigger enterprise solution to expertise management.\nTwo, the cost of human labor has remained stable throughout the existence of Cycorp, at $100K/person-year.\nThree, The cost of assertions was $5/assertion from 1990–2010, but $0.7/assertion after 2015, around the time when Lenat declared the project mostly complete. This matches the estimate of (Paulheim 2018). It’s unclear what caused the 7-fold increase in efficiency. Perhaps this is a success of “priming the knowledge pump”, or this might just because after 2015, most new assertions came from adding new assertions specialized to particular commercial applications, and thus were easier to handle. Formalizing business rules already written down is easier than extracting the intuitive metaphysics of fruits vs vegetables, after all. In any case, a 7-fold efficiency improvement over 30 years is unimpressive, and indistinguishable from more prosaic arguments about learning curve effects observed in most industries: The more you make, the cheaper you can make."
  },
  {
    "objectID": "essays/posts/cyc/index.html#overview-of-cyc-the-system",
    "href": "essays/posts/cyc/index.html#overview-of-cyc-the-system",
    "title": "Cyc",
    "section": "Overview of Cyc, the system",
    "text": "Overview of Cyc, the system\nThe Cyc system consists of three main kinds of parts:\n\nThe iterations of CycL, based on SubLisp, which is itself based on Lisp.\nThe knowledge bases, consisting of millions of assertions written in CycL.\nThe ~1000 inference engines, written in SubLisp.\n\nTo increase speed and compatibility, the programs written in CycL are compiled to Java, which is then compiled to bytecode.\nThe CycL language is intended to be the high-level language that humans can read and write, at the “Epistemological Level” (EL). Most inference engines do not process CycL directly, but process low-level translations of CycL at the “Heuristic Level” (HL). Each sentence at the EL can be translated into a multitude of HL sentences, since different translations allow different inference engines to process it, so that hopefully at least one of those would process it efficiently. The two sides are connected by an interface called the “Canonicalizer”.\n\n\n\nHow Cyc processes a user’s input as of 1990. The user enters at the EL-KB (epistemological level knowledge base), which gets translated down to the HL, where multiple modules for generating and comparing arguments for and against a given proposition are run. (D. B. Lenat and Guha 1990, fig. 1)\n\n\n\n\n\nThe expressiveness-efficiency tradeoff. The HL is highly efficient for the machine to infer with, but very hard for humans to express what they are thinking of in. It is the opposite for the EL, and even more so for natural languages like English. Source\n\n\n\nCycL language\nSince the CycL language has had been developed since the late 1980s, and Cycorp published very little, I am basing this section mostly on the Ontological Engineer’s Handbook (version 0.7), which seemed to contain the most details, even though technically it only describes the 2002 version of CycL.\nLike Common Lisp, the CycL language has almost no syntax:\n\nAn expression is from one of the following types: (#$relation &lt;arg1&gt; … &lt;argn&gt;), ?variable, #$Term, text string, and rational number.\nA #$relation is either a function or a predicate. It always begins with a lowercase.\nIf #$relation is a function, then (#$relation &lt;arg1&gt; … &lt;argn&gt;) is an object, aka a term. Most of Cyc’s knowledge base consists of assertions. Each assertion has a truth value.\nIf #$relation is a predicate, then (#$relation &lt;arg1&gt; … &lt;argn&gt;) is an assertion, aka a sentence. Most of Cyc’s knowledge base consists of assertions.\nFor a few special cases of #$relation such as #$implies, #$forAll, and #$thereExists,10 (#$relation &lt;arg1&gt; … &lt;argn&gt;) is a rule of inference.\nEach assertion has a truth value.\nThere are 5 truth values: default true, monotonically true, default false, monotonically false, and undefined. The “default true” is used to allow for “nonmonotonic reasoning”. For example, it allows us to say that “Pingu is a bird, so Pingu can fly”, and then, if someone else adds “and Pingu is a penguin”, we can say, “in that case, Pingu cannot fly”. Since the “Pingu can fly” statement is only “default true” and “Pingu cannot fly” is only “default false”, we would not suffer from the principle of explosion.\nA #$Term, aka an atomic term, aka a concept, can be thought of as a “word” in the vocabulary of CycL. It begins with an uppercase. Interpreted in object-oriented programming, some terms correspond to classes, while others correspond to objects. For example, #$Socrates is an object, while #$Human is a class. The difference is that we can say (#$isa #$Socrates #$Human) but not (#$isa #$Socrates #$Plato). The concepts can be as abstract as #$AnimalWalkingProcess (the concept of any possible walking by any animal) and as granular as #$Walking00036 (the walk I took on the afternoon of 1897-02-03 in Paris).\n\n10 A technical detail is that as soon as you enter an expression that uses #$thereExists, Cyc automatically “Skolemizes” that expression, because existential quantifiers are technically inconvenient. For example, if I were to say “There exists a time at which Socrates dies.”, then it would be very awkward when someone asks “When?” and I have no recourse but to reply “… there exists!”. Skolemization means that I would instead define a function #$DateOfDeath. Then any time someone asks “When?”, I need simply reply “(#$DateOfDeath #$Socrates)!”.This is enough for us to start writing some assertions that would imply “Socrates is mortal”.\n(#$isa #$Socrates #$MaleHuman)\n(#$isa #$MaleHuman #$Predicate)\n(#$genls #$MaleHuman #$Human) ; #$genls means \"generalizes\"\n(#$genls #$MaleHuman #$MaleAnimal) ; Multiple generalization is common\n\n(#$genls #$Person #$Individual)\n(#$isa #$Individual #$FirstOrderCollection) ; Things can go meta\n(#$isa #$FirstOrderCollection #$SecondOrderCollection)\n(#$isa #$FirstOrderCollection #$MetaClass)\n(#$isa #$MetaClass #$MetaClass) ; Very meta\n\n(#$forAll ?X\n  (#$sameAs (#$MotherFn (#$MotherFn ?X)) \n    (#$MaternalGrandMotherFn ?X)))\n\n; Humans are mortal\n(#$isa #$Mortal #$Predicate)\n(#$implies (#$isa ?X #$Human) (#$Mortal ?X))\n\n; For X to be mortal means there exists a death event X is subject to\n(#$implies\n  (#$Mortal ?X)\n  (#$exists ?DE\n    (#$and\n      (#$isa ?DE #$DeathEvent)\n      (#$subject ?DE ?X))))\nAssertions can be packaged into a microtheory (Mt, aka a context), which could be thought of as scopes or modules. Each assertion can belong to exactly one microtheory, though you can have two assertions that are literally typed out using the same symbols occur in two microtheories – and they would count as two logical assertions.\nAssertions can, and often need, to be automatically inferred across microtheories. For example, “Socrates is alive.” is true in the context of 500 BC, but not in the context of 1995. Given an assertion to the effect of “Socrates was born in 470 BC and died in 399 BC”, an automatic process would allow the system to automatically infer assertions “Socrates is alive” for every microtheory from “The time is 470 BC” to “The time is 399 BC”, and “Socrates is not alive” for the other microtheories.\nAs one might expect from the above example, it is unnecessary, and indeed impractical, to create all microtheories that one might ever use. Indeed, many microtheories are created and destroyed for one-time-use “on the spot”. For a single problem-and-answer session with a user, Cyc typically sets up a temporary context that is deleted after the session, or saved if the user wishes to continue the session later.\nMicrotheories are necessary because human beliefs are incompatible, and there are a lot of humans. For example (very relevant, considering how much Cyc was involved with the War on Terror), #$ChristianMt and #$IslamMt could have:\n(#$isa #$ChristianMt #$Microtheory)\n(#$isa #$IslamMt #$Microtheory)\n\n; #$ist means \"is true in the context of\"\n(#$ist (#$ChristianMt)\n  (#$not (#$sameAs #$God #$Allah)))\n(#$ist (#$ChristianMt)\n  (#$sonOf #$Jesus #$God))\n; the Trinity is left as an exercise\n\n(#$ist (#$IslamMt)\n  (#$sameAs #$God #$Allah))\n; In Islam, God has no son\n(#$ist (#$IslamMt)\n  (#$not (#$sonOf ?X #$God)))\n(#$ist (#$IslamMt)\n  (#$prophetOf #$Jesus #$God))\nMicrotheories can contain each other. For example, both are #$AbrahamicMt, which allows us to make assertions that apply to both.\n(#$isa #$AbrahamicMt #$Microtheory)\n(#$genls #$ChristianMt #$AbrahamicMt)\n(#$genls #$IslamMt #$AbrahamicMt)\n\n; Abraham is a prophet\n(#$ist (#$AbrahamicMt)\n    (#$prophetOf #$Abraham #$God))\n\n; God exists uniquely\n(#$ist (#$AbrahamicMt)\n  (#$exists ?X\n    (#$and\n      (#$isa ?X #$God)\n      (#$not\n        (#$exists ?Y\n          (#$and\n            (#$isa ?Y #$God)\n            (#$not\n              (#$sameAs ?X ?Y))))))))\nAs of 2010, there were over 20K microtheories arranged in a hierarchy, with BaseKB at the top. It was unfortunately a mess, as Lenat promised. Some microtheories were 50 levels deep down the hierarchy!\n\n… determining the hierarchical structure of the Mt’s is difficult, manual, and error prone because all of the Mt’s in ResearchCyc are direct subtypes of BaseKB, even if they are also its indirect subtypes. For example, Mt ClothingGMt describes general information about clothing and is a direct subtype of BaseKB. It is also an indirect subtype of BaseKB because it is a subtype of ArtifactGMt, also a subtype of ArtifactGVocabularyMt, which, in turn, is a subtype of BaseKB… Choosing the right microtheory can easily take several minutes for a trained ontologist who is familiar with the Cyc ontology and experienced in how best to organize knowledge for maximum utility.\n(Conesa, Storey, and Sugumaran 2010)\n\n\n\nOntology\nThe Ontology of Cyc is the graph of all concepts in Cyc, with one directed edge per (#$genls #$Thing1 #$Thing2) assertion. At least, that’s the simplest possible way to say it. In fact, the graph is more complicated, since CycL is a higher-order logic, which allows it to talk about the relation between relations between predicates, etc. For example, both #$SetOrCollection and #$MathematicalObject are sub-concepts of #$MathematicalThing, but we also need to specify that any #$MathematicalThing is either a #$SetOrCollection xor a #$MathematicalObject.\nThe ontology contains multiple levels. At the upper level are the most metaphysical concepts, starting with #$Thing, going down to the middle level of #$Language and #$MilitaryOrganization.\n\n\n\nThe upper ontology of Cyc as of 2010. (Foxvog 2010, fig. 1)\n\n\nBelow the upper ontology are domain-specific knowledge, divided into large microtheories (minitheories?). There would be knowledge about mortgages, computer security, weapons systems, pathology, etc. Below those are the domain-specific facts and data, divided into microtheories. The entire thing is structured like a pyramid.\n\n\n\nThe cyc ontology pyramid.\n\n\nAs an example, here is how #$Philosopher is described in OpenCyc:\n\nA specialization of #$Person; in the context of #$HumanActivitiesMt this collection is an instance of #$PersonTypeByActivity, in the context of #$JobMt it is an instance of #$PersonTypeByOccupation. Each instance of #$Philosopher is a person who habitually thinks about philosophical matters such as what is or might be, what we can know, how we can know anything, etc. In the contemporary era most philosophers are academics or professionals, but a significant number (now and historically) don’t fit this profile.\n\nAnd here’s the #$Thing:\n\n#$Thing is the “universal collection”: the collection which, by definition, contains everything there is. Every thing in the Cyc ontology – every #$Individual (of any kind), every #$Set-Mathematical, and every #$Collection – is an instance of (see #$isa) #$Thing. Similarly, every collection is a subcollection of (see #$genls) #$Thing. Trivially, #$Thing is both an instance of and a subcollection of itself, and is not a subcollection of any other collection. (Note that the above reference to “every thing in the Cyc ontology” is not meant to be limited to things actually reified in the Cyc system, but includes (e.g.) every instance – reified or not, known or not – of every collection recognized by Cyc.)\n\nAnd if the parenthetical note sounds a bit theological,11 note that among those that Cycorp had hired included philosophers, botanists, chemists, and of course, theologians. Interestingly, they didn’t ask botanists to encode what they know about botany, but about what they know about non-botany. Lenat’s theory was that botanists’ understanding of botany is not commonsensical. Instead, what botany he wanted Cyc to encode is common sense botany: how non-botanists think about plants – even, and especially, those botanical beliefs that a botanist would consider wrong, such as (#$not (#$isa #$Banana #$BotanicalBerry)).\n11 I did not find any mention of Meinong in Lenat’s corpus or Cycorp’s communications, but this description sounds close to Meinong’s Jungle, where every thing exists, even logically impossible things such as “a square circle”. The metaphysical difficulties are many, but presumably Cyc has tamed the jungle in practice.12 This reminds me of an event very early on in the history of Cyc:\n\nCyc apparently believed that if a bronze statue were melted into slag, it would remain a statue. What had gone wrong? … Cyc had been told that bronze was a material that retained its essential property – its “bronzeness,” as it were – no matter what state it was in, solid or liquid. But now Cyc was trying to apply that fact to the statue aspect of “bronze statue”. Cyc hadn’t been told anything about statues that would invalidate its conclusion; nobody had ever thought it necessary to tell Cyc, for example, that statues are only statues if they’re more or less in their original form.\n(Thompson 2001)\n\nTo a philosopher, this would have been funny, because Cyc had just been ambushed by Aristotle’s bronze statue! See especially Physics 195a6-8 and Metaphysics 1045a26-29. Aristotle solved it by hylomorphism. I wonder how Cyc solved it?Unfortunately for ontology, while everyone has a common sense, few can pull it out of their heads and push it into a computer. Let’s consider how Cyc encodes events via “Davidsonian semantics”, since it’s how Donald Davidson represented events – yes, Cyc hired a lot of philosophy PhDs. While everyone can reason about events commonsensically, implicitly, making it explicit required a Davidson.12\nTo represent “John gave Mary a book.”, you can write something like (#$Give #$John #$Mary #$Book1), but this prevents you from adding more details, because for the assertion to be syntactically correct, you must define #$Give to be a predicate with arity exactly 3. And then you’d be stuck if you want to write “John gave Mary a book last August.”, since there is no arity in #$Give to insert the “last August” into. And even if you redefine #$Give to have arity 4, what if you also want to say it happened in the library, or that it was a gift, or that John was happy about it? You’d have to change the definition of the predicate #$Give again and again, and there is no end to this.\nTo solve this problem, Cyc treats the event itself as an #$Event. That is, it reifies the process as an object. Now, instead of cramming everything into one assertion, you can construct an endless sequence of assertions, limited only by your patience:\n(#$isa #$Event123 #$Event)\n(#$Sender #$Event123 #$John)\n(#$Receiver #$Event123 #$Mary)\n(#$GivenObject #$Event123 #$Book134)\n...\nSuch problems are fairly subtle, and the world is a very big place. Fortunately, Lenat was a master of ontology, so it all worked out in the end.\n\n… during a short stint working with Doug Lenat’s Cyc project. At the time, they were trying to encode all of botany and had a small staff of professional botanists doing knowledge entry. Naturally it was quite difficult for the botanists to try to translate their knowledge into the formalisms required by Cyc, and they would regularly puzzle over various questions… and if they could not come to a consensus, would have to take it before the Master, Doug Lenat, who would think for a bit, maybe draw some diagrams on a whiteboard, and come up with the Right Representation.\n— AMMDI: alpha ontologist (2023-10-07)\n\n\n\nInference engines\nTo a mathematician, 1 and 1 trillion are the same – both are finite. To computer scientists, even \\(x^2\\) and \\(x^3\\) are different. Much work on Cyc was not on building the 100M-assertion knowledge base, but on building inference engines that allow fast inferences when there are 100M assertions to pick from.\nEvery logical system must face an impossible trilemma:\n\nan expressive language that can represent what people would ever want to say in practice;\nan efficient inference engine on the language that runs fast enough for practical inferences;\na complete inference engine that can perform all inferences that are logically valid.\n\nWhy a trilemma?\nSuppose we want to allow Cyc express all common sense assertions, then since humans in their daily life say high-order statements like “Are you implying that you meant to make me upset by spreading rumors about her, when you had known all along that I would soon hear about it?”, Cyc needs to use a higher-order language. Now this immediately makes it impossible to have an inference engine that is both complete and computable, since we have a Gödel-style incompleteness theorem (Shapiro 1991, theorem 4.14).\nLenat chose to use a fully higher-order language, giving up completeness, and then try his best improving efficiency.\nLike most expert systems, the Cyc has a general resolution-based inference engine. Unlike most expert systems, its knowledge base is large and higher-ordered, so its general engine runs too slowly for most queries. Thus, the developers kept adding more specialized modules (“pattern-specific heuristic modules”), each capable of efficiently inferring on a few microtheories. If a specialized engine fails to make progress, a more general engine can be the fall-back, all the way up to the most general one.\nAs the simplest example of how inference engine can work, consider the following example of backward-chaining inference, in a semantic web. The system is asked basically “Why does Clyde want to possess a crescent wrench?”, and it eventually replies “Because Clyde has not eaten lately.”. Lenat expected that the ontology of Cyc would eventually power generally intelligent agents, which would use forward-chaining to construct goals from current states, and use backward-chaining to explain why others have their goals.\n\n\n\nA sketch of how to reason by backward-chaining together triples in a semantic web. Figure from (Wallich 1991). I can’t help but wonder if Lenat meant for a subtle joke against the Elephants don’t play chess by (Brooks 1990) by adding in a “dead end” branch that ends up concluding Clyde is an elephant.\n\n\nMost inference engines are highly specialized. They can only reason within a few microtheories, but very well. Here is a toy example for reasoning with mutually exclusive categories. We can define #$Mutex as\n(#$implies\n  (#$and (#$isa ?X ?A) (#$Mutex ?A ?B))\n         (#$not (#$isa ?X ?B)))\nThen this allows the general reasoning engine to reason about mutual exclusivity by always reducing to this definition. However, we can accelerate this by adding in some “lemmas”:\n(#$implies (#$Mutex ?A ?B) (#$Mutex ?B ?A))\n(#$implies\n  (#$and (#$Mutex ?A ?B) (#$genls ?C ?A))\n  (#$Mutex ?C ?B))\n(#$implies\n  (#$and (#$Mutex ?A ?B) (#$isa ?X ?A))\n  (#$not (#$isa ?X ?B)))\nAfter producing enough lemmas, we can then write a specialized inference engine that would be invoked whenever #$Mutex appears in an expression, and it would attempt some inference and simplifications by applying these lemmas. Indeed, most inference engines were constructed in this way: Cyc would try to solve a problem, and fails by timing out. The ontologists at Cyc would call up a human expert and ask, “How did you do this?” and the expert would explain how they would solve it with quick rules of thumb, which the ontologists would write into Cyc, resulting in more assertions, and possibly more inference engines. This is essentially the same as “knowledge elicitation” used for making expert systems.\nOther than these inference engines (or “workers”), there are also many “tacticians”, modules that pick engines that are probably good for solving a problem, and a general all-powerful strategist at the very top. It was reported in that there were 1 strategist, 4 tacticians, and 1097 workers. The strategist and tacticians have parameters that can vary, presumably by the user or the ontologists in order to adapt to specific tasks. (D. B. Lenat et al. 2007)\nThe control structure of Cyc is a commercial secret.13 I could only find two brief explanations of it, one in a 2010 Cycorp white paper, another in an essay published near the end of his life (D. B. Lenat 2022a). According to these, Cyc coordinates the ~1000 inference engines with blackboard architecture similar to the one used in Hearsay-II speech recognition system (Erman et al. 1980). Lenat described it as:\n13 The SubLisp reference was titled “control structure”, and got me excited for a moment. But I was disappointed to find that it described the control structure of the SubLisp programming language (mostly the same as that of Common Lisp), not of the Cyc system.\nthere is a whole battery of specialized inference engines and representations… and, when making progress, broadcasting the results… Whenever progress is made, all of them stop and work on the now-simpler subproblem. Some of the inference engines are very general, and work on general representations–e.g., a theorem prover that works on first-order logic. The more specialized inference engines are much faster whenever they do apply… In 1986, Cyc had two such representations; by 1990, there were 20, each with its own inference engine; today Cyc has over 1100. They work together as a community of agents, communicating by posting their intermediate results on a sort of blackboard that all the other agents can watch and react to if/when/as they see an opportunity that fits their specialty.\n(D. B. Lenat 2022a)\n\n\n\n\nThe blackboard architecture used in Hearsay-II. In this example, a 3-dimensional blackboard allows different modules to read and write to different parts of the blackboard, and collaboratively recognize speech. (D. B. Lenat 1984)\n\n\nYou can imagine a shared working space – the blackboard – where agents can scan and find tasks to do, do them, and then post the outputs to the blackboard. Each agent has their own “places to watch”. They notice (or get notified) what is posted to their watched areas, and ignore what is outside.14 When an agent completes whatever task it is running, it posts the outputs to particular areas of the board to notify certain agents, “Hey, check this out! I bet you’ll find this helpful.”. To prevent race conditions or general conflict between agents, an agent can claim a lock on something, so that other agents can’t work on it until it times out, or releases the lock.\n14 Lenat seemed to suggest that each inference engine watches the entire board, but every large-scale blackboard system I know of doesn’t do that. Hearsay-II certainly did not. It is slow and creates much of unintended complexities. Imagine a microservice where every process pings every process whenever it outputs something, or a conference where everyone broadcasts to everyone.You can imagine this as a microservices framework, with over 1100 microservices, multicast, with SubLisp as the serialization language. Indeed, apparently some of the inference engines used neural networks, so their numerical outputs definitely had to be serialized somehow into SubLisp.\nThe blackboard is also connected to the “external world” through API calls. For example, SQL queries run by an external database can return data that is posted to a region of the blackboard, and what Cyc writes to the blackboard can be read out for external use.\nAccording to a 2006 lecture (D. B. Lenat 2006), the blackboard has 12 “dimensions”, because each assertion occurs in a kind of context, and there are 12 dimensions to the context: Anthropacity, Time, GeoLocation, TypeOfPlace, TypeOfTime, Culture, Sophistication/Security, Topic, TopicGranularity, Modality/Disposition/Epistemology, Argument-Preference, and Justification. For example, “Ronald Reagan is president” is true in the context Time = 1985, GeoLocation = UnitedStates, etc. These 12 dimensions were detailed in a 1998 technical report, though in that report, Anthropacity was called a “pseudo-dimension”, and replaced by Let's. (D. Lenat 1998)\nOther than the slightly anarchic microservice structure, Cyc could also use Simple Hierarchical Ordered Planner to run the engines in order, if the workflow for how the engines should be run is known.\nIt was known very early on that the most general inference engine is the slowest, which is why they settled on a multi-layered structure of inference engines. Each inference is handled by the lowest engines first, and upon failing, the higher-levels try it.\nFurther, to allow the tacticians to know who to call, and the engines to know whether it is powerful enough to handle the query, the user can specify in a query over 150 adjustable parameters, such as max time limit, max number of answers desired, max number of backward-chaining steps, whether to introduce new terms, etc. In 2007, they found 6 combinations of these parameters that could answer almost all queries they tested with, which allowed them to clean up the user interface. Not a form with 150 blanks, but just a pick-1-in-6. (D. B. Lenat 2022b)\nThough there is a most general inference engine on the very top, they had noticed by 2007 that was so slow that turning it off made the system go faster! So they turned it off entirely in 2010. (D. B. Lenat 2022b; D. B. Lenat and Marcus 2023)\nThe heaven is high and the emperor is far away. The monologic is dead. Long live the swarm.\n\n\nQuerying\nAs a user, the knowledge base would be worth nothing if you cannot ask it questions and receive answers. The Cyc system, like most logical programming systems, answers queries by logical unification. That is, it converts a query into a logical formula \\(\\phi(x)\\) with a certain unknown \\(x\\), then returns all \\(x\\) that makes \\(\\phi(x)\\) true, or if no such \\(x\\) exists, then returns FALSE.\nFor a query like\n(#$geopoliticalSubdivision #$Canada ?WHAT)\nthe Cyc system applies various inference engines to find all bindings (aka substitutions) of the variable ?WHAT that make the statement true in the knowledge base. The results will include all Canadian provinces and territories that have been asserted explicitly or can be inferred through rules.\nCyc provides 3 types of queries:\n\nASK: General-purpose queries that generate bindings for free variables along with a formal proof.\nPROVE: Conditional queries that handle universal quantification (#$forAll) by constructing temporary microtheories for temporary hypotheticals.\nQUERY: A wrapper around both ASK and PROVE in the Cyc Browser.\n\nTo illustrate how querying works in practice, consider a scenario where we want to find all Canadian provinces. An ASK query would look like:\n(#$isa ?WHAT #$CanadianProvince)\nCyc processes this by looking for all terms (symbols that begin with #$) that satisfy the predicate when plugged into the variable ?WHAT. The system might use transitivity rules, inheritance mechanisms, and various specialized inference engines to gather results. Behind the scenes, Cyc might employ backward-chaining to start from the query and work backward to known facts, or forward-chaining to derive new facts from existing ones, depending on which approach its tacticians determine is more efficient. The reply would be something like\n((?WHAT . #$Manitoba-CanadianProvince))\n((?WHAT . #$BritishColumbia-CanadianProvince))\n((?WHAT . #$Alberta-CanadianProvince))\n((?WHAT . #$Ontario-CanadianProvince))\n((?WHAT . #$Quebec-CanadianProvince))\n((?WHAT . #$NovaScotia-CanadianProvince))\n...\nWhen Cyc performs a query, it can also provide a deductive trace (forma proof) showing how it reached its conclusion. For example, when proving that Manitoba is a Canadian province, the deductive trace might look like:\nlispCopyQuery: (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)\n  1. (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)\n     [Direct assertion in PoliticalGeographyMt]\n  2. Therefore, Manitoba-CanadianProvince is a CanadianProvince.\n     [TRUE]\n\nQuery: (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)\n  1. (isa Manitoba-CanadianProvince CanadianProvince)\n     [Established above]\n  2. (implies \n      (#$isa ?X #$CanadianProvince)\n      (geopoliticalSubdivision Canada ?X))\n     [Rule in PoliticalGMt]\n  3. Therefore, (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)\n     [Modus Ponens: 1,2]\n     [TRUE]\nFor more complex queries involving logical implications, PROVE creates hypothetical microtheories to test universal claims. Such microtheories are useful, since the hypotheticals being considered are usually temporary and should not be mixed with the permanent assertions in the knowledge base. For example, to ask “Is every Canadian province a political subdivision of Canada?”, we use:\n(#$implies\n  (#$isa ?WHAT #$CanadianProvince)\n  (#$geopoliticalSubdivision #$Canada ?WHAT))\nPROVE handles this by creating a temporary “possible world” where it assumes the existence of a generic Canadian province and tests whether it must also be a political subdivision of Canada.\nAs previously stated, each time the user begins a question-and-answer session with the Cyc system, it creates a new microtheory just for this session, which allows it to keep track of what has been asked and responded to, so that the user can refer to things like “your previous answer”. Such a microtheory is usually deleted after the session is over, though it can also be saved if the user wishes to continue the session at a later date.\nThe querying interface also provides controls for managing the inference process, such as limiting search depth, maximal time allowed, the inference engines to be used, the parameters for the strategist and the tacticians, etc. These can help the Cyc system to solve difficult queries when the default settings are insufficient.\n\n\nNatural language processing\nWhen it comes to the use of natural languages, there are two problems. The easy problem is to convert sentences in CycL into natural language. This is fairly simple and has been set up since around late 1990s, simply by constructing assertions between a concept and a concept of the English word for the concept. (Guha and Lenat 1993) For example, (#$genls #$Dog #$Mammal) would be transformed to “Dogs are mammals.”, and so on. Concretely, it involves entering sentences of the form (simplified):\n(#$rootString #$TheEnglishWordPen \"pen\")\n(#$denotation #$TheEnglishWordPen #$WritingPen)\n(#$rootString #$TheFrenchWordPlume \"plume\")\n(#$denotation #$TheFrenchWordPlume #$WritingPen)\n...\nand similarly, Cyc could order the words correctly using logical assertions for English grammar in the form of government and binding theory.\nSure, such generated language will sound a bit wooden and formalistic, and would not win any literary award, but it solves the problem. The hard problem is to parse sentences in natural language into CycL, along with all its messiness.\nInformation about how Cyc solves the hard problem is sparse, but from what I gathered, it uses a tiered system from the fast and inaccurate to the slow and accurate.\nUnderlying all of its NLP is an English dictionary, also represented as concepts and assertions in CycL. The dictionary contains about 200K words and phrases, and more assertions about them. For example, the word “light” is represented as a concept Light-theWord, and there would be assertions stating that it can be a verb, or a noun, or an adjective, etc.\nAt the fastest tier are keyword matching, or concept spotting. For example, for parsing terrorism articles, it can just quickly match for the existence of keywords in a sentence. Seeing a sentence that looks like “… al-Qaeda … embassy … grenade … suicide attack…”, the system can assume, with high probability, what the sentence means, and generate its CycL representation.\nAt a slower tier is using extraction templates. For example, “[A] was involved in kidnapping [B]” would be matched to a template that looks for the fragment “involved in kidnapping”, which then would parse [A] as the Perpetrator while [B] as the Victim.\nAt a slower tier is example-based machine translation by syntax templates. For example, in “… the heart of Baghdad …”, the first pass parses Badhdad as a city, then a syntax template activates and it parses the “heart” as Downtown. This can be coded as\n&lt;template&gt;\n    &lt;nlPattern class=\"140080\"&gt;the heart of $City#0&lt;/nlPattern&gt;\n    &lt;cyclPattern&gt;\n        (#$equalSymbols ?D (#$DowntownFn $City#0))\n    &lt;/cyclPattern&gt;\n    &lt;variable&gt;?D&lt;/variable&gt;\n    &lt;type&gt;#$Downtown&lt;/type&gt;\n&lt;/template&gt;\nAt the slowest tier is full syntax tree parsing. In this tier, the sentence is fully parsed to a syntax tree using a manually written transformational grammar under the government and binding theory. It is then parsed semantically using Montague grammar.\nWhereas the CycL-to-English part is already workable mostly in 2001, the English-to-CycL part is still ongoing work, a consummation devoutly to be wished. Even though Lenat originally thought natural language understanding would be finished soon after priming the knowledge pump, it has already been 8 years since the priming, and Cyc is still not reading the world’s writings and learning autonomously.\n\n\nMachine learning\nCyc has used statistical and machine learning methods in minor parts, such as using neural networks, n-gram methods, random forest, support vector machines, etc, to automatically extract templates for natural language processing, classify the microtheories that a sentence belongs to, etc.\nHowever, none of these were at all what Lenat meant when he talked of “machine learning”, by which he meant a Cyc machine, with knowledge pump fully primed, would begin to perform experiments and learn by automated discovery, much like AM and EURISKO were meant to do. While this has always been the “phase 3” of the Cyc project, to this day, Cyc has made no progress in this area."
  },
  {
    "objectID": "essays/posts/cyc/index.html#cycops-what-is-it-good-for",
    "href": "essays/posts/cyc/index.html#cycops-what-is-it-good-for",
    "title": "Cyc",
    "section": "CycOps, what is it good for?",
    "text": "CycOps, what is it good for?\n\nOverview\nIn America, at least before the Deep Learning era, the government funded most large-scale efforts in AI, usually in the context of military and intelligence applications. This was true for the Cyc project. Indeed, it initiated in 1984-07 within the Microelectronics and Computer Consortium (MCC), which, like the Strategic Computing Initiative, was formed in reaction to the threat of the Japanese Fifth Generation Computer Systems project. Though not directly funded by the government, its head was Bobby Inman, who had previously held high positions in the Navy, the NSA, and the CIA, so draw your own conclusion.\nIn 1995-01, as MCC wound down, the Cyc group formed Cycorp Inc., a for-profit company, to continue their mission. Immediately after that point, academic publication and generally candid conversation almost ceased. Who bought the services of Cyc, and for what? The details are slim. Trade secrets, no doubt. Confirmed results:15\n15 I swear I’m not a policy wonk, but 3 days of cyc-berstalking does take its toll.\nLycos search engine, to disambiguate search terms. It ended in 2001. (Source)\nDepartment of Defense, in 2001, to “clean dataset”. (Thompson 2001)\nGlaxo Wellcome, DEC, IBM, and United Healthcare, sometime before 1997, for unknown purposes. (Port 1997)\n\nFor Glaxo Wellcome, and United Healthcare, it was probably for the purpose described below.\nFor DEC and IBM, probably to produce specialized expert systems – after all, DEC was famous for using the expert system XCON that saved them $40M/year. (Roland and Shiman 2002, 191)\n\nGlaxoSmithKline, before 2001, to “clean dataset” (Thompson 2001), probably meaning to manage a thesaurus of pharmaceutical chemicals and health-care words. (Source) Probably started in 1997 (Port 1997)\nCycSecure, a network vulnerability assessment tool, first beta in 2002. (Anthes 2002a) Trialed at the US Strategic Command Computer Emergency Response Team at some unknown point before 2005. (Shepard et al. 2005)\nPaul Allen (co-founder of Microsoft) had funded Cycorp sometime before 2001 for unknown purposes and for an unknown sum. (Hiltzik 2001) In 2003, he funded it by $0.7 million as part of his project of “Digital Aristotle”, to create a tutoring AI. (Richman 2003; Friedland et al. 2004)\nDARPA’s High Performance Knowledge Base program (unknown duration, but must be within 1997–1999, the duration of the full project) and KRAKEN16 under the Rapid Knowledge Formation program (2000–2004). (Kingston 2001; Matthews et al. 2004; Witbrock et al. 2005)\nAdvanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) program, sometime between 2001 and 2004. Its title is self-explanatory. (D. B. Lenat 2009) (Source)\nThe Terrorism Knowledge Base (2004–2008), detailed below.\nCleveland Clinic, starting in 2007, to parse clinician queries into database queries and returns the result. (D. B. Lenat et al. 2010; Pierce et al. 2012)\nGoldman Sachs, sometime around 2016, to “monitor the inner workings of its technological infrastructure” and detect insider trading. (Metz 2016; Shiloh 2023)\nThe CIA and the Department of Defense, at an unknown time, for unspecified purposes, though probably to identify terrorist threats in connection with the Terrorist Knowledge Base. (Shiloh 2023)\nThe NSA, to “identify terrorist threats in international communications data”. (Metz 2016)\nThe Total Information Awareness project funded Cycorp for $9.8 million in 2003 for a “prototype database” and a system that could “identify phone-calling patterns as they might exist among potential terrorists overseas”. (Crenson 2003)\nElectronic Surveillance System for the Early Notification of Community-Based Epidemics-II (ESSENCE-II), around 2006. Its title is self-explanatory. (Abbott et al. 2007)\nSeven unnamed big companies, for unspecified expert system applications (Cycorp 2021), but probably “common-sense platform for their applications, and as an interlingua to fully, semantically integrate all the data they generate and all the data they license from third parties”. (D. B. Lenat 2022a)\nUnnamed semiconductor foundry, for an expert system for root-cause analysis of fabrication yields. (Source)\nUnnamed big bank, for an expert system for IT support and inventory management. (Source)\nUnnamed big bank, for an expert system for IT personnel expertise management. (Source)\nUnnamed oil company, for an expert system for monitoring and predicting breakdowns at oil pumping facilities. (Source)\nA learn-by-teaching game for 6th-grade arithmetics. It began in 2014 as project “BELLA”, funded by the Advanced Distributed Learning Initiative under the Department of Defense (D. B. Lenat and Durlach 2014), and was commercialized in 2016 as MathCraft. In the game, the student takes on the role of a tutor to an AI avatar, who would make various mistakes that the student would correct. It uses a small fragment of Cyc to model both the student’s and the avatar’s states of knowledge. Though it looks very polished, I did not find confirmed instances of MathCraft being used in a school.\n\n16 Despite “KRAKEN” supposedly “not an abbreviation and does not require further definition” (Ceruti, Wilcox, and Powers 2004), it was still defined as “Knowledge Rich Acquisition of Knowledge from Experts who are Non-logicians” by the Cycorp. (Matthews et al. 2004)17 But there’s also this:\n\nOnce, developing a scenario for a terrorist attack on Hoover Dam, it hypothesized a school of 1,000 al Qaeda-trained dolphins bearing explosives.\n(Hawkins 2003)\n\nLooking at the list, we see that much of Cycorp funding came from the American intelligence community, especially between 2001 and 2010, during the heights of the War on Terror, as the American state struggled to expand its sovereign eye over the sprawling cyberspace. Indeed, one of the early success was when it “predicted anthrax might be sent through the mail six months before trove of knowledge about past terrorist activities, tactics, and weapons”. (Hawkins 2003) Though the success did not help anyone, it was great advertisement.17 Corroborating, Lenat in a 2006 Google Talk showed screenshots of Cyc answering “Which American city would be most vulnerable to an anthrax attack during summer?”. (The answer was “Phoenix”.)\n\n\n\nScreenshot of Cyc answering “Which American city would be most vulnerable to an anthrax attack during summer?”. The system replied “Phoenix”, with reasoning. (D. B. Lenat 2006)\n\n\nThe total funding of the project is hard to know, although we know that in 2002, its total cost had been $60M, of which $25M came from the military, so I think it’s fair to say 50% came from the military. This is corroborated in 2005:\n\nIn 1996, we got our first substantial government contract,” Lenat recalls. Since then, Cycorp has collected about half of its revenue from U.S. government agencies and the rest from companies, mostly for building “semantic maps” that help users pull information from various databases with a single query. By taking on paying projects, Cycorp has been able to stay profitable and debt-free. All of the firm’s stock is owned by its employees, making Cycorp answerable only to Cycorp. “But,” Lenat admits, “we have had to tack with the funding winds. Maybe 50 percent of the funding we get pushes us forward in the direction that we need to go.”\nCycorp doesn’t even want to be distracted by the rigors of the retail software business; instead, it licenses Cyc for use in third-party software packages… The time may come, Lenat says, when a greatly expanded Cyc will underlie countless software applications. But reaching that goal could easily take another two decades.\n(Wood 2005)\n\nOut of all these applications, only two had been reported in detail.\n\n\nTerrorism Knowledge Base\nThe first application is the Terrorism Knowledge Base (TKB), created in 2004 and shut down in 2008. (Cycorp 2008) During the aftermath of 9/11, the American government funded a massive expansion of surveillance and data processing, and the Cycorp took on several of such contracts. The TKB is the only one about which we know in some detail.\nThe system can be browsed like a local Wikipedia – if Wikipedia were focused entirely on terrorism. TKB contained &gt;2000 terrorists, &gt;700 terrorist groups, &gt;6500 terrorist attacks, and &gt;200,000 assertions such as “Xavier Djaffor participated in the Jihad from 1996 to 2000” and “Lashkar-e-Taiba is an Islamist terror group founded in 1990”.\n\n\n\nBrowsing the TKB about Imad Fayez Mughniyeh. (Cycorp 2008)\n\n\nA user query would be processed in 4 steps.\n\nUser enters question in formal, but still natural, English.\nCyc parses the question by keyword matching, template matching, and syntactic rules, then applies domain and common sense constraints to fix the parse, then retrieves some CycL fragments that are the closest matches to what the user entered.\nThe user clicks on the fragments they meant. Cyc synthesizes a full query in CycL. The user optionally modifies the CycL query.\nCyc runs inference engines to retrieve the answer along with a logic chain for the answer.\n\n\n\n\nAsking the TKB about “What terrorists and biological agents are such that the terrorist is capable of learning to make the biological agent?”. Its returns included 土谷正実 from Aum Shinrikyo. (Cycorp 2008)\n\n\nLike querying, data entry also has a light amount of parsing, and the system attempts to fill a form with it. The user can then fix the form. An intelligence specialist, lightly trained in using it, could enter up to 100 assertions per hour.\n\n\n\nThe data entry form. The system is prompting the user to disambiguate “skating boy” between “boy who is a doer of skating” and “boy who performs skating professionally”. (Cycorp 2008)\n\n\n\n\nCleveland Clinic\nThe second application took place at the Cleveland Clinic, sometime during 2007–2010.18 The system was called “Semantic Research Assistant” (SRA), and it could answer queries about cardiothoracic surgery, cardiac catheterization, and percutaneous coronary intervention – basically, surgery-relevant questions about the heart.\n18 This paper is the only substantial public information about this collaboration I can find. Therefore, I can only confirm that Cyc had been used in Cleveland Clinic during 2007–2010. It may have lasted to at least 2021, since (Cycorp 2021) still cited this application.\n\n\nThe 4-step process of the SRA. It is essentially the same process as in the TKB. (D. B. Lenat et al. 2010, fig. 2)\n\n\n\n\n\nA logical justification constructed by Cyc by backward chaining on a user query. The original query is not shown, but it probably should be “For each instance of pericardial aortic valve replacement event in 2008, which event was it, and which type of pericardial aortic valve prosthesis was it?”. (D. B. Lenat et al. 2010, fig. 4)\n\n\nThere was one subsequent report on the system in 2012, which described the SemanticDB project at Cleveland, of which Cyc was only a part. The SemanticDB system contains a database of 120M semantic triples (which Lenat had long dismissed as being too limited). In the system, Cyc parses cohort identification queries written in English into formal queries, then queries the database in SPARQL, does some further inference, and shows the result. (Pierce et al. 2012) Unsurprisingly, Oracle Semantic Technologies was also involved.\nIn a presentation in 2019, Lenat claimed it required 120K new assertions for the Cleveland project, or 0.5% of the total knowledge base. However, 95% of the assertions that Cyc called up for answering queries for the project required knowledge Cyc already had, indicating large knowledge reuse (indicating that the whole project required about 2M rules).\n\n\nIs that all?\nWell, I tried my best to look for more applications, but the fact is that there were so few of them. Out of all the confirmed instances of applications, the above two were the only ones reported in detail.\nSo far, we have only considered applications of the full Cyc system. We have mentioned that there were both the OpenCyc and the ResearchCyc, two smaller versions of the full Cyc. Did they have substantial applications? As far as I can discern, no. Indeed, just as OpenCyc failed to bring about the Semantic Web or impress the general public, ResearchCyc failed to bring about a revolution in knowledge engineering or impress the general academia, or essentially anyone outside of Cycorp itself. Even Ernest Davis and Gary Marcus, highly sympathetic to the symbolic approach to AI, found little evidence for the success of Cyc, not because Cyc had provably failed, but simply because there was too little evidence in any direction, success or failure.\n\n… it is in fact very difficult for an outsider to determine what has been accomplished here. In its first 15 years, CYC published astonishingly little. Since about 2002, somewhat more has been published, but still very little, considering the size of the project. No systematic evaluation of the contents, capacities, and limitations of CYC has been published. A number of organizations have done private evaluations but the results were not published.\nIt is not, for example, at all clear what fraction of CYC actually deals with commonsense inference, and what fraction deals with specialized applications such as medical records or terrorism. It is even less clear what fraction of commonsense knowledge of any kind is in CYC. … There are not even very many specific examples of commonsense reasoning carried out by CYC that have been published.\n(Davis and Marcus 2015)\n\n\nOn the whole, it is fair to say that the AI community regards CYC as a very elaborate failure. Domingos (2015 p. 51) characterizes it as “the most notorious failure in the history of AI”. Domingos is a researcher in machine learning and has little use for any kind of knowledge-based methods, so his phrasing is certainly harsh, but in our experience, this opinion, more or less, is common even in the knowledge representation (KR) community. … it would be very helpful, and it would, we believe, significantly improve CYC’s standing in the AI community, if the CYC team could demonstrate some specific task where CYC really visibly shines. … let them design their own task. As Watson demonstrates, passing a self-imposed task can be impressive enough, depending on the task, and in any case it is much better than nothing. At the moment, a person who is asked, “What interesting thing has been done with CYC?” is largely at a loss for an answer.\n(Davis 2016)\n\nThere is a kind of insularity in Cycorp that starts to affect you if you look too closely into it. I know I was affected. Many times I had opened a paper that purported to show the application of Cyc, and was disappointed to find that it was yet another paper about the application of a method to ingest knowledge into Cyc, rather than a method to apply knowledge out from Cyc. I came to dread the literature review, as Cyc in my mind took on the sinister appearance of a black hole at the center of the knowledge graph, a cocoon that would never metamorphose into a butterfly."
  },
  {
    "objectID": "essays/posts/cyc/index.html#everyone-can-only-see-their-own-dream",
    "href": "essays/posts/cyc/index.html#everyone-can-only-see-their-own-dream",
    "title": "Cyc",
    "section": "Everyone can only see their own dream",
    "text": "Everyone can only see their own dream\n\nLenat’s tenets\nIn (D. B. Lenat and Feigenbaum 1991), a paper coauthored with Feigenbaum, Lenat gave the most comprehensive statement for where he stands philosophically, which he held onto for the rest of his life:\n\nKnowledge Principle. A system exhibits intelligent understanding and action at a high level of competence primarily because of the knowledge that it can bring to bear: the concepts, facts, representations, methods, models, metaphors, and heuristics about its domain of endeavor.\nExplicit Knowledge Principle. While knowledge may be compiled to opaque lumps of code for efficiency, there should always be a declarative version of that, so that they can be subject to meta-reasoning.\nBreadth Hypothesis. Intelligent performance often requires the problem solver to fall back on increasingly general knowledge, and/or to analogize to specific knowledge from far-flung domains.\nEmpirical Inquiry Hypothesis. The most profitable way to investigate AI is to embody our hypotheses in programs, and gather data by running the programs. The surprises usually suggest revisions that start the cycle over again. Progress depends on these experiments being able to falsify our hypotheses. Falsification is the most common and yet most crucial of surprises. In particular, these programs must be capable of behavior not expected by the experimenter.\nDifficult Problems Hypothesis. There are too many ways to solve simple problems. Raising the level and breadth of competence we demand of a system makes it easier to test – and raise – its intelligence.\nKnowledge Is All There Is Hypothesis. No sophisticated, as-yet-unknown control structure is required for intelligent behavior.\nThe Local Consistency Hypothesis. There is no need–and probably not even any possibility–of achieving a global consistent unification of several expert systems’ KBs (or, equivalently, for one very large KB). Large systems need local consistency.\nThe Coherence Hypothesis. Moreover, whenever two large internally consistent chunks C1, C2 are similar, their heuristics and analogies should cohere; e.g., if the “going up” metaphor usually means “getting better” for C1, then it should again mean “getting better” for C2, or else it should not apply at all there.\n\nLenat is the very example of a hedgehog: a single philosophy, a vision for AGI, pursued for 40 years. One does not pursue a single vision without rejecting alternative visions, and Lenat has been explicit in rejecting every alternative route to AGI, using his sharp tongue (D. B. Lenat and Feigenbaum 1991, sec. A.2; Thompson 2001; D. B. Lenat 2008; D. B. Lenat and Marcus 2023):\n\nLogical AI in the style of Simon and Newell’s General Problem Solver. Such an elegant framework would not work beyond toy problem domains, by the Knowledge Principle.\nHighly accurate models of human behavior, in the style of Simon and Newell’s Human Problem Solving or the SOAR architecture. Duplicating human cognitive architecture constitutes cargo cult science. AGI need not have the magic number 7 ± 2.\nPhysical embodiment. It might be great fun to make robots, but physical embodiment is neither necessary nor sufficient for “grounding” the knowledge base, by the Physical Symbol System Hypothesis. A “mystical worship of physical embodiment” would only delay AGI. In particular, the subsumption architecture does not lead to AGI.\nGenetic algorithms and other evolutionary algorithms. It gets stuck in local minima too often, and runs too slowly.\nCreating tiny morsels of little expert systems, and hope that bit by bit, AGI would emerge out of that. Remember that plateau-hopping requires breadth. Without an overarching plan, they will not fit together, like how the 1980s expert systems could never talk to each other.\nLogical machine learning without a large knowledge base already in place. It makes for good demos, but quickly exhausts itself. These are examples of the illusory hope for “free lunch” or elegant “Maxwell’s equations of thinking”, a severe case of laziness and “physics envy”. Researchers should stop sitting on their asses mad with “physics envy”, and start the dirty work of coding.\nStatistical machine learning, pattern matching, neural networks, and other self-organization methods. Just wait for enough compute and data, then magically a large model would learn on its own? Yet more wishful thinking for “free lunch”, caused by laziness and “physics envy”.\nAny form of machine learning without a large knowledge base to begin with. This is impossible because learning is possible only at the fringe of knowing. Any attempt to learn without a large starting knowledge base is, again, trying to get a “free lunch”.\nWait until philosophers have figured out the one true ontology for the world, then build the Cyc accordingly. Philosophers suffered from “Hamlet syndrome”, unwilling to take decisive action, satisfied with publishing tiny morsels of ontologies that don’t cover the whole world, or grand ontologies that cover a caricature of the whole world.\n\nLenat had his own grand historical vision for AI, which I call the 3 Optima Theory.\nWith a little hard work (about 6 person-months), one can get a knowledge-free system working, such as self-organized neural networks, Simon and Newell’s General Problem Solver, etc. This allows the researcher to publish a quick paper, a student to earn their PhD degree, and so on. Putting in more hard work does not result in a better system, but usually makes things worse as the code becomes bloated and unmanageable. Academic myopia stops people from trying to get out of this local maximum, since people just want to get published papers.\nWith a lot more hard work (about 10 person-years), one can get a system with a lot of specialized knowledge working. This is where the commercialized expert systems live. However, the general consensus is that as an expert system grows beyond 10K rules, it starts to suffer from its weight of all the rules. Standard expert systems were built for special fields, so people would use a simple language that works, but eventually collapses under the weight of 100K rules. Commercial myopia stops people from trying to get out of this local maximum, since people just want to sell products, and 50K rules is good enough for the customer.\nThe problem is that all these efforts are wasted. Specialized expert systems cannot be glued together efficiently, because each of them lives in a differently simplified world. That is, “plateau-hopping requires breadth”. The AI field as a whole would stagnate. The only way out of this is to go for the full common sense, to invest in the 2000 person-years of effort, and make a Cyc. After that, all the expert systems can interface with Cyc, and with each other using CycL, and all the computers can be preinstalled with their digital common sense.\n\n\n\nLenat’s 3 Optima Theory for the grand history of AI.\n\n\n\n\nA hostile assessment of Cyc\n\nbogosity: At CMU, bogosity is measured with a bogometer; in a seminar, when a speaker says something bogus, a listener might raise his hand and say “My bogometer just triggered”… The agreed-upon unit of bogosity is the microLenat.\nmicroLenat: The unit of bogosity. Abbreviated µL or mL in ASCII. Consensus is that this is the largest unit practical for everyday use. The microLenat, originally invented by David Jefferson, was promulgated as an attack against noted computer scientist Doug Lenat by a tenured graduate student at CMU. Doug had failed the student on an important exam because the student gave only “AI is bogus” as his answer to the questions. The slur is generally considered unmerited, but it has become a running gag nevertheless.\n— The Jargon File\n\nIt is hard to interpret the state of Cyc today, if we take Lenat’s word for it:19\n19 Feel free to stop reading at this point. It is about to get polemical. Indeed, I have noticed that articles have the nefarious tendency to start from the personal (“the hook”), then become informative, and then subtly slide to the polemical (“the call to action”). In fact, I have mastered the art of first scrolling the page with my eyes blurred so that the hook cannot land on my head, and then, as soon as the text utters the first syllable of the ca–[tab closed]\nThere were 150 technical challenges to knowledge engineering and representation at the start of Cyc in 1984, but they were all solved by 1990. (D. B. Lenat 2009, 2022a)\nCyc could already be tutored in (constrained) natural language in 2001. (Anthes 2002b)\nThe upper ontology has remained stable for years as of 2015. (D. B. Lenat 2015)\nThe knowledge pump is 95% primed in 2015, when there were just 15M assertions, (D. B. Lenat 2015), and as of 2021, there were over 25M assertions (Cycorp 2021).\nSubLisp is easy to learn, and knowledge engineering in SubLisp is 1000× more efficient than in a modern language like Python. (D. B. Lenat 2021)\nThe Cycorp had been profitable since its inception, had never taken on debt, had been almost entirely employee-owned, had always had only around 50–200 employees, a mostly flat corporate structure, and could remain profitable entirely on doing business with non-government corporations in 2022. (D. B. Lenat 2021, 2022a)\nCyc has natural language understanding of pragmatics, while statistical machine learning systems have none. (D. B. Lenat 2017, 2019a)\n\nAt this point, Cyc is supposed to be out there doing true machine learning (not the shallow veneer of “machine learning” that those statistical and connectionist researchers are faking). Reading and studying human text written in natural language. True intelligence. General intelligence. The foundation of a thousand expert systems. The backbone of the Semantic Web. The flowering of a new age of reason.\nAnd yet 9 years after Cyc is declared “done”, Cyc is still stuck inside the walls of Cycorp doing nothing of the kind. What is stopping Cyc from learning?\nThe finances are healthy. Cycorp is not subject to perverse interests of the market or middle managers. There are fewer employees than Dunbar’s number. They are aligned to the corporate mission. SubLisp is a great language. All technical challenges to knowledge engineering and representation had been solved by 1990. The knowledge pump is over 160% primed.\nWhat. Is. Stopping. Cyc. From. Learning??\nAccording to his final work, coauthored with Gary Marcus, the last holdup was natural language understanding (NLU). Entry had been accelerated, but was still quite manual:\n\nThis process has been accelerated by gamification, NLU, etc., but each axiom is hand-checked for default correctness, generality, and best placement into the microtheories (contexts) it applies to, before entering it into the Cyc knowledge base.\n(D. B. Lenat and Marcus 2023)\n\nThe knowledge pump had been thoroughly primed, but Cyc still couldn’t learn by reading human texts, because NLU remained unsolved. Cyc can read CycL perfectly well – the interlingua, its mother tongue – but it is stubbornly difficult to parse English into the interlingua. But why is parsing natural languages hard? Why, indeed, is NLU so difficult, even “AI-complete”?\nThis situation is clarified if we consider a classic model of machine translation, that of the Vauquois triangle: We need to translate from one language (let’s say, English) to another (let’s say, Japanese). We can translate word-for-word, which corresponds to the base of the triangle. This direct approach is of course simple but brittle. Not only does it ignore different word-ordering across languages, it would also completely fail to disambiguate homonyms, such as “fly” as a noun versus “fly” as a verb. Stopgap measures such as using n-grams lead to combinatorial explosions.\nAt a higher level, we can take account of syntax. We first parse the English sentence into a syntax tree, do a word-replacement, transform the syntax tree according to Japanese syntax, and finally generate the Japanese sentence from it. This approach would solve the “fly” vs “fly” problem, since one is a verb and another is a noun, which would be clear according to the syntax tree. However, this fails to disambiguate the word “pen” in the two sentences:\n\nThe ink is in the pen.\nThe sheep is in the pen.\n\nIn both cases, the word “pen” has exactly the same syntactic category, but has a different meaning, and it requires some understanding of how the world works (i.e. that the writing-pen is too small to hold a sheep, while it is highly unlikely for someone to put ink into an animal-pen). Similarly, in the Winograd schema challenge, the task is to disambiguate what a pronoun refers to, such as deciding what the word “they” refers to in each of the two sentences:\n\nThe city councilmen refused the demonstrators a permit because they feared violence.\nThe city councilmen refused the demonstrators a permit because they advocated violence.\n\nIndeed, Lenat had often claimed that the Winograd schema challenge is a touchstone for true language understanding, something that would prove that Cyc really understands, while unmasking the other systems’ veneers of intelligence. And how must the Winograd schema challenge be solved? Lenat’s solution is to go to the pinnacle of the triangle – interlingua, the King of Kings, the Language among Languages, a completely universal conceptual representation for all humans might wish to mean by their speech.\nLenat intended the CycL to be the interlingua.\nWithin this framework, the problems of machine translation and understanding are unified: To understand English, it remains to parse English into the interlingua. To translate Japanese to English, it remains to parse Japanese to interlingua, then verbalize interlingua into English.\n\n\n\nThe Vauquois triangle of translation.\n\n\nRecall that Lenat had always argued that “just letting a system learn on its own by natural language understanding” is a free lunch, and that NLU requires a significant portion (~10–50%) of common sense already encoded. But if that’s the case, then there is a near-contradiction here:\n\nThe CycL language is enough to represent common sense language about the world. On the Vauquois triangle, all natural languages are joined at the top by a common interlingua, which is the CycL.\nBy the Winograd schema challenge, translation requires common sense. Therefore, Japanese → English translation requires common sense.\nCycL → English requires no common sense. The result would sound kind of wooden and robotic, but it doesn’t require any understanding: Just follow the syntax substitution rules. Indeed, CycL → English was already working since ~2000. Therefore, CycL → English requires no common sense.\nTherefore, by the conservation of common sense, it takes exactly the same amount of common sense to perform Japanese → English translation and Japanese → CycL translation. Indeed, this is why they considered NLU an “AI-complete” problem.\nBy the no free lunch hypothesis, neural networks trained from scratch can’t have common sense. Thus, neural networks should fail at Japanese → CycL translation.\nBut in Lenat’s last paper, he argued that the only problem stopping Cyc from learning from natural language was that NLU did not work well enough yet, and hoped that neural networks could perform the natural language → CycL translation. Indeed, they considered ChatGPT and Google Bard to often be better at NLU than Cyc.\n\nOn the topic of interlingua, it is interesting that ABBYY was almost a twin of Cycorp. Whereas Cycorp began building an ontology for the common sense world since 1984, spent $200M, and got stuck on NLU since around 2010, ABBYY began building an interlingua-based machine translation system since the 1990s, and spent over $80M. By the early 2010s, they realized that they could not compete with Google statistical machine translation, and pivoted to doing NLU with semantic graph technology based on the knowledge base they produced for the sake of interlingua. (Skorinkin 2024)\nIndeed, interlingua-based machine translation projects used to be common, but essentially went extinct (except for ABBYY) after the rise of statistical machine translation in the 1990s (Hutchins 2023).\nI suspect that it is not simply the problem of getting a better English → CycL translator, and then Cyc would finally begin learning, but that much knowledge in sentences doesn’t translate to interlingua.20 If the failures of all interlingua machine translation systems is not enough evidence, then consider some more facts about Cyc’s NLU:\n20 Or that the interlingua exists, but it is not symbolic-logical, but linear-algebraic. Indeed, the successful neural machine translation systems’ latent spaces may be that consummate interlingua so devoutly wished for, but such an interlingua is very far from what you’d see in Cyc or ABBYY.\nExhaustively searching the literature, I only found four (four!!) examples that Cycorp gave for English → CycL: “A girl is on a white lounge chair” (Pratt 1994), “Bill Clinton sleeps.”, “An AI researcher is a kind of computer scientist.” (Panton et al. 2006), and “Did you touch a blue object located in the capital of France on September 25th, 2022?” (D. B. Lenat and Marcus 2023). They were quite easy and unambiguous examples, almost as if they began with a CycL sentence, and then converted it to English. None involves the “many AI-complete elements, such as correct disambiguation, understanding of idioms, metaphor, sarcasm, foreshadowing, irony, subtext, and so on.” (D. B. Lenat and Marcus 2023).\n(Guha and Lenat 1994) stated that in 1994-03 “The Syntax module can properly handle about 75% of the sentences found in the news stories of a typical issue of the newspaper USA Today. And in cases in which Cyc knows all the proper nouns in the sentence, the Semantics module can properly handle most of the sentences parsable by the syntax module… as good as what our knowledge enterers independently come up with, when asked to manually translate the material into CycL.”.\n(Panton et al. 2006) stated that in 2006, a search-and-verify system for English → CycL, that combined syntactic parsing, statistical parsing, and Cyc verification, resulted in “sentences that were correct, according to human review, approximately 50% of the time”.\n(Sarjant et al. 2009) increased the common-sense knowledge in ResearchCyc by 30% in 2009, by guess-and-verify, where the Cyc does verification, and the guess was done by simplistic methods like regex parsing, infobox pairing, etc.\nSome governmental experimental uses of Cyc, such as ESSENCE-II and Total Information Awareness, might have involved some NLU, but I cannot find details concerning how much NLU was involved.\nOnly one of the commercial applications of Cyc may have plausibly required NLU.\n\nIn MathCraft, the “learning by teaching” game powered by Cyc, students are only allowed to pick from choices generated by the Cyc itself. There is no free-form natural language input at all.\nIn the Cleveland Clinic application (D. B. Lenat et al. 2010), the user enters queries already in a constrained language (like “aortic valve replacement patients with a pericardial aortic valve”), and then compose a CycL translation by clicking from Cyc’s parser’s suggestions.\nThe same is true for the Terrorism Knowledge Base application.\n“Maintaining persistent user models in order to support extended, months-long online chats with their famous characters” (Source) seems to involve NLU. However, there is no information whatsoever as to how much NLU this project involved. I can’t find out which company hired Cycorp for this, or who the “famous characters” were. It is not even clear if users are allowed to enter free-form input during online chat, or if it is another application like MathCraft, where you can only pick from choices generated by Cyc itself. Besides, even if this case involved free-form input, there is still a problem. We know that self-organized NN can work as chatbots. So if Lenat was right to say that self-organized NN don’t understand (as he had been saying for 40 years), then this application didn’t require NLU after all.\n\n\nLet’s take it another way: If there is No Free Lunch to NLU, then what is Cyc’s score on Winograd schema benchmark? Where is the Cyc-translator? Forget about Google Neural Translate – is it even better than ABBYY’s? Where’s ChatCyc? Why does none of Cyc’s commercial applications involve NLU in any significant amount, while most commercial applications of NLU use statistical or neural machine learning?\nAll evidences point to the conclusion that a sentence that can be parsed to CycL is already bureaucratic and formalistic, with the mark of interlingua written on its brow. For those, Cyc could already understand in the early 2000s, and yet, Cyc is still here, not machine-learning, lacking … what? &lt;sarcasm&gt;A sarcasm parser?&lt;/sarcasm&gt;\n\n\n\n\n\n\nAn information-theoretic estimate\n\n\n\n\n\nThe minimal description length of English is about 0.8 bits per character, and each English sentence contains about 100 characters, giving 80 bits of incompressible information. Since there are 27 characters (we are just counting the 26 lowercase letters and the whitespace), this means that the compressible information is about \\(\\log_2 27 - 0.8 = 4.0\\) bits per character.\nWe can think of writing as a process of selection: Out of all the possible letters you can pick next, you picked this particular letter, and you do it again and again. Intelligence is good selection, the careful elimination of bad choices. In this particular case, the bad choices to be eliminated come out to about 3.2 bits per character.\nNow, one might argue that the last bit is the deepest, but it is hard to square the two claims:\n\nSelf-organized NN could compress natural text to a bit rate of ~1.0 bits per character, so they already capture \\(3.0/3.2 = 94\\%\\) of the selection.\nSelf-organized NN don’t understand.\n\nStated in another way, this means 94% of the selection that humans perform while writing is mindless, mere remembering and espousing without understanding or inferring. This is even harder to square with Lenat’s assertion that 99% of language understanding is pragmatics, which self-organized NN don’t have. (D. B. Lenat 2017)\nSo does this mean that 93% of pragmatics is mindless, or are we going to just throw out the information-theoretic understanding of language as another incommensurable paradigm shift? It would fit the theme of Cyc. Indeed, they had long excised the remnants of probabilistic reasoning when they removed certainty factors at some point before 1989, since they found it inadequate compared to reasoning by logical unification (Guha and Lenat 1993). Cry “Kuhnian” and let slip the dogs of logic…\n\n\n\nTake another look at the list of known applications of Cyc. Then take a look at what was even planned for as “short-term uses” of Cyc even back in 1993:\n\nthe most promising short-term uses of Cyc are not what are traditionally considered AI problems. Instead, they are in relatively “mundane” problems such as making spreadsheets smarter, providing better access to a heterogeneous set of databases, directed marketing of goods and services, etc.\n(Guha and Lenat 1993)\n\nIt is quite prophetic that such “short-term uses” of Cyc are still the only uses of Cyc so far. Is 35 years considered “short-term”? Does this look like a path towards AGI, or does this look no different from building custom-made expert systems for specialized purposes, something that those generic professionals of Oracle, IBM, or Accenture have been doing for decades?\n\nEven though most people don’t think of the CyCorp… as an expert systems company, effectively, that’s what we’re doing… we’ve stayed in business all these years when none of you have. We are the last surviving large expert system company.\n(Expert Systems Pioneer Meeting, Day 1 Session 1: Purpose, Structure and Introductions 2018)\n\nPerhaps they did have a product differentiation in being able to consistently find particularly good knowledge engineers, and in programming in SubLisp, a particularly efficient language compared to Python or Java. Perhaps their product differentiation is in niche expert systems that really require higher-order statements. Perhaps these are what allowed them to stay in business despite having just a hundred-person crew. It is not something to be dismissed, but is this a path towards AGI, or a veneer of AGI cast over enterprise solutions? &lt;sarcasm&gt;An IBM Data Integration Solutions® with better cover art?&lt;/sarcasm&gt;\nPerhaps Lenat could be eternally optimistic despite all the missed predictions and the lack of AGI. Perhaps it is a selection effect. One does not undertake a 40-year project for AGI without being delusionally optimistic about the prospect – the same could be said of great leaders and startup founders.\nOr perhaps he was disconnected from what was really going on down there at Cycorp. In a HackerNews discussion, some ex-Cyclists wrote what they thought about Cycorp.\nOn the pro side were many. The corporate culture was highly intellectual and philosophical, as one expects for a company that does computable ontology for a living: “it can be pretty fun to be in meetings where you try to explain Davidsonian ontology to perplexed business people”. The company had solved many technical problems in large scale inference, and remained profitable, with successful commercial applications.\nOn the con side were many. The codebase was creaking under 30 years of technical debt:\n\nI spent some entire days just scrolling through different versions of entire systems that duplicate massive chunks of functionality, written 20 years apart, with no indication of which (if any) still worked or were the preferred way to do things.\n\nThe technical solutions and commercial applications were closely guarded secrets, so the outside world does not know. Lenat was unimpressed with open source and so did not commit resources to OpenCyc, tutorials, easier third-party integration, software development kit, or other outreach projects (except those regularly scheduled newspaper reports for publicity). The Cyc culture was also insular, with a true believer’s mentality:\n\n… veterans there sort of feel like the broader AI community turned their back on symbolic reasoning in the 80s (fair) and they’re generally not very impressed by the current trends within the AI community, particularly w.r.t. advances in ML (perhaps unfairly so), so they’re going to just keep doing their thing until they can’t be ignored anymore.\n\nMost pertinent to the dream of AGI, it was unclear, down in the trenches, whether Cyc was really doing common sense reasoning, or just a particularly good base for developing expert systems from. It also wasn’t clear if common sense reasoning was really necessary for the successful commercial projects in the first place. This has caused some Cyclists to become ex-Cyclists from disillusionment.\n\nI personally suspect that some of Cycorp’s clients would do better with domain-specific solutions because they don’t realize how much of their problem could be solved that way and how much of the analysis coming from Cyc is actually the result of subject matter experts effectively building domain-specific solutions the hard way inside of Cyc. With a lot of Cycorp projects, it’s hard to point your finger at exactly where the “AI” is happening… The degree to which it’s effective seemed to me to be a case-by-case thing. While working there I tended to suspect that Cyc people underestimated the degree to which you could get a large fraction of their results using something like Datomic and it was an open question (to me at least) whether the extra 10% or whatever was worth how much massively more complicated it is to work with Cyc.\n\nStructurally, the Cycorp had two levels. At the upper level are Lenat, Witbrock, and such keepers of the faith, who kept the ceaseless striving for that elusive dream alive. At the lower level are the working ontological engineers who just had to deliver the product, AGI or not.\n\nIt turns out there’s a kind of reality distortion field around the management there, despite their best intentions - partially maintained by the management’s own steadfast belief in the idea that what Cyc does is what it ought to be doing, but partially maintained by a layer of people that actively isolate the management from understanding the dirty work that goes into actually making projects work or appear to. So while a certain amount of “common sense” knowledge factors into the reasoning processes, a great amount of Cyc’s output at the project level really comes from hand-crafted algorithms implemented either in the inference engine or the ontology.\nOver the years, the Cyc as its actually implemented has drifted pretty far from the Cyc that people like Doug Lenat believe in, and the degree to which they’re willing or able to acknowledge that seems to sort of drift around, often dependent on factors like mood. Doug would show up and be very confused about why some things were hard because he just believes that Cyc works differently than it does in practice, and people had project deadlines, so they often implemented features via hacks to shape inference or hand-built algorithms to deliver answers that Doug thought ought to be derived from principles via inference. Doug thinks way more stuff that Cyc does is something that it effectively learned to do by automatically deriving a way to solve the general form of a problem, rather than a programmer up late hand-coding things to make a demo work the next day, and the programmers aren’t going to tell him because there’s a demo tomorrow too and it’s not working yet.\n\nBut that’s enough about unverified rumors, and I apologize. It would have been better for epistemic hygiene if there were more open information about Cycorp. Let’s turn to a Cycoanalysis of Lenat’s rhetorics, which is more well-documented.\n\n\nLenat against the world\nAccording to multiple reports, Lenat was charismatic, able to sell his vision of AGI to many people. Having read most documents produced by Lenat or Cycorp over the 40 years, I have discovered that there is a consistent list of themes that Lenat just kept repeating over his career. Each theme has a double structure: a technical statement that has an emotionally neutral valence, and a moral coloring that provides the call to action, the charisma, the coherence for the employees to align to his vision.\n\nThe doubled structure of Lenat’s rhetoric\n\n\n\n\n\n\ntechnical statement\nmoral coloring\n\n\n\n\nthe No Free Lunch Hypothesis\nwe do honest hard work like the proverbial ant, you are lazy like the proverbial cricket (and the AI Winter is coming)\n\n\nthere are no “Maxwell’s Equations of Thought”\nwe are self-assured, you suffer from physics-envy\n\n\nthe Empirical Inquiry Hypothesis; Cyc is unaesthetic; we are building the Cyc profitably, not publishing academic papers\nwe are strong engineers that get things done, you are weak aesthetes playing the academic game\n\n\nthe Physical Symbol System Hypothesis\nwe are building real intelligence, you are just playing with robots\n\n\nthe Breadth Hypothesis\nour systems are intelligent, yours are autistic idiot savants\n\n\nthe Explicit Knowledge Principle\nour systems understand deeply, yours pattern-match shallowly\n\n\nwe think you are putting pattern-matchers in places that require deep understanding\nwe are trustworthy, you are reckless\n\n\nwe believe the Cyc is the only current effort towards AGI\nwe are ambitious, you are academic careerists\n\n\nwriting the Cyc costs a lot and is unpopular with the academia\nwe rebel and think freely, you sheepishly follow the crowd\n\n\nCycorp hires anyone – including high school dropouts – good at encoding common sense\nwe are egalitarian, you are elitists\n\n\nCycorp has always had just ~100 people, and has been mostly forgotten now\nwe are the elect, you will see\n\n\n\nAnd more than technical, moral, and personal conviction is on the line: If Cyc really would take 1000 person-years (20 years with 50 philosopher PhDs), then it would cost about $100 million just in human labor. The Cycorp, if it were to survive, has a strong commercial interest in rejecting all alternatives. It can be very hard to get someone to understand something, when their product differentiation depends on them not understanding it.\nLenat’s rejections progressed with time as each new challenger arose, applying the same tenets in different decades.\nIn the 1980s, like other expert systems people, he aimed his rejection at the previous logical AI methods exemplified by Simon and Newell. Logical AI was a dream that a graduate student might build an AGI during a thesis period, if only they knew the “Maxwell’s equations of thinking”. Of course, such attempts failed, because there are no such equations. He took a little effort towards rejecting the other logical AI approach exemplified by (Newell and Simon 1972), by constructing models that reproduced every little detail of how humans really perform in psychometric experiments, such as their reaction times, their uhhs and oopses. Admitting its interest to psychologists, he considered it a distraction for machine intelligence.\nIn the 1990s, as the expert system hype died down, he turned his criticism towards expert systems. He recalled that, back when he was young, before academia had rejected him, he thought automated discovery with AI, such as AM and EURISKO, would lead the way to self-improving learning machines. But then he was disabused of this. BACON discovered Kepler’s three laws “only” from data, but that’s because Pat Langley was careful in presenting nothing but the relevant data. The cost to discover Kepler’s laws on the filtered dataset? A few CPU-hours. The cost to filter the dataset? 10 Kepler-years. Similarly, AM started out with the set-theory axioms and discovered prime numbers and some famous conjectures, but quickly ended up enumerating boring complications. Lenat had to keep adding in more heuristics to get something out of it. Similarly, EURISKO would run overnight and Lenat would check its outputs in the morning, remove some bad ideas, add some good ones, and so on. The Traveller 1981 win was “60/40% Lenat/EURISKO” after all.\nGeneralizing, Lenat argued that there is a common thread across all these machine learning systems. They would all start out discovering many interesting basic things, but quickly “run out of steam” enumerating boring complications. Lenat called it as systems putting up a “veneer of intelligence” while they were really just “discharging potential energy that was stored in them”. That is, the creators secretly put into the program with their own expert knowledge somehow, either through the right rules, heuristics, dataset, features, or some other thing. Once the expert knowledge is “exhausted”, no more discoveries could be made, and the veneer wears off. However, it makes for impressive demos, leading to cycles of hype and bust. The only escape is to prime the knowledge pump. If the knowledge base is large enough, then it wouldn’t run out of steam. (D. B. Lenat 2008)\nLenat’s approach was not welcomed by the academics, and the feeling was mutual. AI researchers thought the Cyc project was hyped, and were unhappy with the secretive nature of Cycorp. Philosophers considered the Cyc project premature – how could Lenat build an ontology for the world when philosophers hadn’t even figured out what the ontology is? Lenat shot back, calling academics lazy, abstract, and unable to persist through decades of hard engineering work. (Thompson 2001) Among the academics, the only one that still supported him was Marvin Minsky, who had no problem calling the rest of AI research “brain-dead since the 1970s”, especially robotics: “Graduate students are wasting 3 years of their lives soldering and repairing robots, instead of making them smart.”. (Baard 2003)\nInteresting. Why the hate towards robotics? Well, during the 1990s, there were two main challengers to his idea of a symbolic-logical system. On the one side, there was the challenge of bottom-up non-symbolic reasoning promoted by Rodney Brooks’ subsumption architecture (Brooks 1990), and the statistical machine learning methods like support vector machines. He did not have much to say about the statistical methods – not yet – but he did reject the subsumption architecture as a mistaken attempt to reach AGI through robotics, much as Minsky did. Motors, sensors, etc, are simply not needed – common sense, specified in logical language, is all you need. The hard work needed to get the robots to do anything is, you see, the wrong kind of hard work.\nOn the other side, though most expert system researchers had shrunken their ambition in the winter chill, from AGI down to mere commercial survival, a few still believed in the mission. They thought that by building little systems, brick by brick, we can build towards a generally intelligent system. This is basically a “Society of Mind” approach of Marvin Minsky, and even though Lenat and Minsky liked each other’s research, Lenat rejected this approach as well. One cannot settle for building common sense bit by bit, expecting a finished system to emerge, but must braid the whole thing under one roof, one upper ontology. The ontology does not have to be perfect or efficient, but there has to be one. Without it, the Society of Mind would fragment into a Tower of Babel, with little expert systems of incompatible ontologies, just like how Feigenbaum’s dream of a “Library of Congress” of knowledge bases failed to materialize.\nIn the 2000s, big data arrived with the Internet, and statistical learning became dominant. No doubt trying to preempt customers’ “Why don’t I just Google it?”, he turned his firepower towards statistical learning systems. He never tired of pointing out that, if you make an even slightly complex query like “Is the Space Needle taller than the Eiffel Tower?”, Google will happily serve up results saying “The Space Needle is 605 feet high.” and “The Eiffel Tower is 1,063 feet high.”, unable to actually answer your question. Despite having 15,000 servers, Google only ran dumb statistical algorithms, while Cyc running on a single server could answer it. Google-style statistical machine learning, like its trillion-token statistical machine translations systems (Brants et al. 2007), was just pattern matching, yet another grasping after a free lunch. Such systems could not truly understand. As an alternative, he held out Cyc as the foundation to the Semantic Web, which would build a system that would truly understand.\nCuriously, right from the start, Lenat considered self-organized neural networks as not a viable path towards general intelligence, but for the same reasons as to why the logical AI programs of Simon and Newell would fail! From our perspective, they couldn’t be more different, yet to Lenat, neural nets, General Problem Solvers, n-gram models, whatever, are all just “explicit-knowledge-free systems”, too neat, not scruffy, and would fail for the exact same reason.\n\nthey are unaesthetic! And they entail person-centuries of hard knowledge-entry work. Until we are forced to them, Occam’s Razor encourages us to try more elegant solutions, such as training a neural net “from scratch”; or getting an infant-simulator and then “talking to it”. Only as these fail do we turn, unhappily, to the “hand-craft a huge KB” tactic.\n…\nOur position regarding the aesthetes: There is a methodological difference between our “scruffy” way of doing AI and the aesthetes’ “neat” way… If only there were a secret ingredient for intelligence–Maxwell’s equations of thought. If only we could axiomatize the world in a small set of axioms, and deduce everything. If only our learning program could start from scratch. If only our neural nets were big or cerebellar or hyperlinear enough. If only the world were like that. But it isn’t. The evidence indicates that almost all the power is in the bulk knowledge. As Whitehead remarked, “God is in the details.”\n(D. B. Lenat and Feigenbaum 1991)\n\nWith the second neural network winter, he did not pay more attention to them, but as they arose yet again in the 2010s, with some exasperation, he would remind the world that, no, nothing has changed. Neural nets had already failed and they would fail. Thinking that “one large net for everything” would just work is yet another example of the logical AI fallacy that “If only we have the Maxwell’s equations of learning, it will just work!”. They are always “remembering and espousing”, but never “understanding and inferring”, and can only ever be the “right brain” to Cyc’s “left brain” (D. B. Lenat and Marcus 2023). As Deep Learning kept blowing past expectations, he rehashed the same 1980s argument with escalating apocalypse:\n\nIf computers were human, they’d present themselves as autistic, schizophrenic, or otherwise brittle. It would be unwise or dangerous for that person to take care of children and cook meals, but it’s on the horizon for home robots. That’s like saying, ‘We have an important job to do, but we’re going to hire dogs and cats to do it.’\n(Love 2014)\n\n\nNo matter how good your elegant theory of syntax and semantics is, there’s always this annoying residue of pragmatics, which ends up being the lower 99% of the iceberg.  You can wish it weren’t so, and ignore it, which is easy to do because it’s out of sight (it’s not explicitly there in the letters, words, and sentences on the page, it’s lurking in the empty spaces around the letters, words, and sentences.)  But lacking it, to any noticeable degree, gets a person labeled autistic. They may be otherwise quite smart and charming (such as Raymond in Rain Man and Chauncey Gardiner in Being There), but it would be frankly dangerous to let them drive your car, mind your baby, cook your meals, act as your physician, manage your money, etc. And yet those are the very applications the world is blithely handing over to severely autistic AI programs!\n(D. B. Lenat 2017)\n\n\nWe would not be comfortable giving a severely neurologically-impaired person – say someone with no functioning left brain hemisphere – real-time decision-making authority over our family members’ health, our life savings, our cars, or our missile defense systems. Yet we are hurtling in that direction with today’s AI’s which are impaired in almost exactly that same fashion! They – those people and those AI programs – have trouble doing multi-step abstract reasoning, and that limitation makes their decision-making and behavior brittle, especially when confronted by unfamiliar, unexpected and unusual situations… Machine learning algorithms have scarcely changed at all, in the last 40 years… Current AI’s can form and recognize patterns, but they don’t really understand anything. That’s what we humans use our left brain hemispheres for.\n… Researchers and application builders tolerate their AI systems having just the thinnest veneer of intelligence, and that may be adequate for fast internet searching or party conversation or New York Times op-ed pieces, but that simple representation leads to inferences and answers which fall far short of the levels of competence and insight and adaptability that expert humans routinely achieve at complicated tasks, and leads to shallow explanations and justifications of those answers. There is a way out of that trap, though it’s not pleasant or elegant or easy. The solution is not a machine-learning-like “free lunch” or one clap-of-thunder insight about a clever algorithm: it requires a lot of hard work…\n(D. B. Lenat 2019a)\n\nConcurrently, on the Cycorp website, two white papers published in 2021-04 reiterated their product differentiation against the false promises of neural networks and Bayesian networks.21 Neural networks posed a great commercial threat to their business, and the Bayesian networks, by promising to half-open the neural network black box, threatened their business as well. The papers argued that since both were not rule-based logical systems, they were not Actually Intelligent. After such fear-uncertainty-doubt, they reassured the reader that true AI needs both the left brain and the right brain, and they sell the finest left brains on the planet.\n21 Read it yourself (\\(\\sim 3 \\times 10^6 \\mathrm{\\mu Lenat}\\)) to see how hard they had to work that product differentiation. Calling it “Actually Intelligent”, claiming “ML can never give an explicit step-by-step explanation of its line of reasoning behind a conclusion, but Cyc always can.”, and insinuating that nobody could do Natural Language Understanding yet because none of those newfangled neural networks had any pragmatics… And this was uploaded in 2021-04, a year after GPT-3! It was written in the same FUDdy voice of those that still sold machine translation services after Google Neural Translate, transcription services after OpenAI Whisper, or copywriting services after ChatGPT.The same accusation of brain-damage that he leveled at neural networks was in fact a rehash of the exact same argument he had made against statistical machine learning systems like Cleverbot, Google, and Amazon recommender systems (Love 2014), since he made no distinction between statistical methods, be it keyword matching, n-gram models, or neural networks. They are all the same veneer of intelligence, same free lunch, same shallowness.\nLenat could apply the same criticisms with the same counterexamples over his 40 years of career, without needing to inspect the details of these machine learning architectures, because he had the following fully general proof, which you can discover by intersecting the previous paragraphs in this section:\n\nUnless common sense is fully represented and integrated, an AI system is an idiot savant at most. (Panton et al. 2006)\nMachine-learning common sense from scratch is impossible, because learning occurs at the fringe of what one already knows. (D. B. Lenat 1995b)\nTherefore…\n\nHedgehogs. Hedgehogs are all the same. They have one big idea, one big proof, one big theory, and continue going on with it for decades. Chomsky did it, Minsky did it, and Lenat did it too. Benefit: If they got it right, they really got it right. Cost: If they got it wrong, then they would sound like a broken record.\nFor example, Lenat called expert systems “brittle” and “idiot savants” in the 1990s (D. B. Lenat 1995a), and statistical machine learning systems “brittle” (probably also “idiot savant”) in the 2000s, and neural networks “brittle” and “autistic” since 2015 until his death.22\n22 Some words and phrases reappear so often in Lenat’s writings that I termed them “Lenatisms” (\\(\\sim 10^6 \\mathrm{\\mu Lenat/word}\\)) and came to hate them as much as I hate GPTisms like “delve” and “crucial”: free lunch, hard work, physics envy, Maxwell’s equations, clever algorithm, measles, idiot-savant, autistic, veneer of intelligence, shallow, pattern matching, brittle, understand, trustworthy, left brain, hemisphere.Similarly, he kept talking about the Winograd schema challenge, and how logically encoded common sense is the only way to solve it. He started talking about it in the 1990s (D. B. Lenat 1995b). He was still telling Stephen Wolfram in 2019 that, surely, if Cyc would team up with Wolfram Alpha, then they could finally solve the challenge for good (Wolfram 2023). He was still tweeting about it even on 2020-03-12, about (Sakaguchi et al. 2021) which showed stated that modern LLMs (GPT-2, BERT, and a few others) was underperforming on WinoGrande, a larger version of the previous Winograd benchmark. Etc, etc.\nNot just his arguments were repetitive, but his “war stories” too. In 1994, Cyc could retrieve images by semantic search, so for example, it would retrieve an image of a rock climber if queried “an adventurous man” (D. B. Lenat 1995a). Great demo for 1994, and he would harp on this throughout the 2000s in his presentations, presumably to product-differentiate against Google-like Image Search engines. Similarly, he first recounted in 1987 of an expert system that diagnosed his rusty car with measles (D. B. Lenat and Feigenbaum 1991), then again in (D. B. Lenat 1995a), then again and again throughout his presentations in the 2000s, and he was still telling the story (and about the Winograd schema) in 2019.\nIn his last paper, coauthored with Gary Marcus, he updated his critique of statistical machine learning to the LLM age. Again the shallowness, brittleness, free lunch, etc.\n\nGiven the arduous nature of the reasoning required… it is understandable almost all AI researchers and developers have gone in the opposite direction, abandoning or trivializing symbolic representation and reasoning, and instead seeking one or another sort of “free lunch” in the form of perceptrons, multi-layer neural networks and, most recently, LLMs… limiting an AI to such a narrow “baby talk” language would be a huge barrier to it ever becoming a trustworthy general AI.\n(D. B. Lenat and Marcus 2023)\n\nI am struck by the irony that a veteran of logical AI would call neural networks “brittle”, or make an appeal to sunk cost. Lenat had devoted 2000 person-years to the project, therefore a “free lunch” shouldn’t work, nevermind the fact that these “free” lunches took 20 years of gritty battles to build the datasets, struggles with the cussedness of CUDA, waking up to yet another divergent overnight training run, staring at tensors filled with NaNs, and eventually cost $100 million per serving, roughly the total budget of Cycorp through its life. How dare you to go “free lunch” on us… But de mortuis nil nisi bonum.\nLenat died in 2023, unmourned on Lucid AI and Cycorp, who, like ABBYY, still proudly advertise their product differentiation to this day."
  },
  {
    "objectID": "essays/posts/cyc/index.html#in-lieu-of-a-conclusion",
    "href": "essays/posts/cyc/index.html#in-lieu-of-a-conclusion",
    "title": "Cyc",
    "section": "In lieu of a conclusion",
    "text": "In lieu of a conclusion\nNapoleon died in 1821. Wellington was greatly saddened.\n\nPlutarch has related that Julius Caesar wept for the death of Pompey; Aurelian did not weep for the death of John, but he felt what a man would feel when rid of an incurable disease that had become a part of his life.\n— Borges, The Theologians\n\nCome as you are, as you were\nAs I want you to be\nAs a friend, as a friend\nAs an old enemy\nTake your time, hurry up\nChoice is yours, don’t be late\nTake a rest, as a friend\nAs an old memoria…"
  },
  {
    "objectID": "essays/posts/cyc/index.html#updates",
    "href": "essays/posts/cyc/index.html#updates",
    "title": "Cyc",
    "section": "Updates",
    "text": "Updates\n\nSome reviews:\n\non Twitter\nOn r/mlscaling\nHacker News\nLobste.rs\n\nOn advice of Gwern, I emailed the Cycorp with some questions, but they never replied.\n\n\n\n\n\n\nFulltext of the email\n\n\n\n\n\nTo Whom It May Concern,\nI hope this message finds you well.\nI am writing to inquire about several aspects of Cycorp’s ongoing work and organizational status, particularly with respect to the Cyc knowledge base and its applications. I would greatly appreciate your response to the following questions:\n\nKnowledge Base Size and Cost What is the current approximate size of the Cyc knowledge base (e.g., number of assertions or facts), and what is the estimated average cost per new fact entry (either historical or current)?\nOrganizational Leadership Who currently leads Cycorp, both at the executive level and in terms of technical or research direction?\nOpen Source Strategy Does Cycorp have any plans to release additional components of Cyc as open source in the future, following the prior release of OpenCyc?\nBenchmarking and Evaluation Has Cycorp evaluated its system on publically available and standardized linguistic benchmarks, such as MMLU, GPQA, ARC, or other reasoning-intensive tasks? If so, could you share any published or unpublished results?\nOngoing Partnerships Is Cycorp still maintaining any collaborations or deployments in clinical or hospital environments, similar to the previously reported instance with the Cleveland Clinic?\n\nI am asking in the context of a comparative evaluation of symbolic and sub-symbolic systems, and I would be grateful for any technical details or references you may be able to provide.\nThank you very much for your time and attention.\n–\nRegards,\nYuxi Liu\nUC Berkeley CS PhD\nhttps://yuxi-liu-wired.github.io/"
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html",
    "href": "essays/posts/backstory-of-backpropagation/index.html",
    "title": "The Backstory of Backpropagation",
    "section": "",
    "text": "The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.\nSeppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.\nPaul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.\nDavid Rumelhart reinvented it in 1982. As he freely admits, he didn’t cite previous work because they were so obscure. The 1986 paper he coauthored (Rumelhart, Hinton, and Williams 1986) became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.\nI was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:\n\nBad luck.\nThey were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.\nThey thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.\nThey distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.\nThey attempted to achieve parity with digital computers, which were discrete.\nThey attempted to distance themselves from cybernetics, which included optimal control theory."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#abstract",
    "href": "essays/posts/backstory-of-backpropagation/index.html#abstract",
    "title": "The Backstory of Backpropagation",
    "section": "",
    "text": "The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.\nSeppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.\nPaul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.\nDavid Rumelhart reinvented it in 1982. As he freely admits, he didn’t cite previous work because they were so obscure. The 1986 paper he coauthored (Rumelhart, Hinton, and Williams 1986) became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.\nI was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:\n\nBad luck.\nThey were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.\nThey thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.\nThey distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.\nThey attempted to achieve parity with digital computers, which were discrete.\nThey attempted to distance themselves from cybernetics, which included optimal control theory."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#the-backpropagation-algorithm",
    "href": "essays/posts/backstory-of-backpropagation/index.html#the-backpropagation-algorithm",
    "title": "The Backstory of Backpropagation",
    "section": "The backpropagation algorithm",
    "text": "The backpropagation algorithm\nTo set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.\nThe graph can be finite or infinite, but in all cases, its acyclic directedness allows us to assign a “logical time” to each node.1 Some nodes are independent variables: they point to other nodes, but no nodes point to them. Other nodes are dependent. If we know the independent variables, we can propagate their values forward in logical time and determine the values of every node. This is the “forward pass”. Backpropagation goes backwards in logical time.\n1 An existence proof that anything we might care about must have a logical time: Anything that happens, happens in the real world. The real world has a single direction in time. Therefore, any computation that can happen, must have a logical time that is identical with physical time.\nNote that logical time does not have to coincide with physical time. For example, to model goal-directed behavior, it might be better to put the physical future into the logical past.We use the convention of putting derivatives on the rows. So for example, for \\(f: \\mathbb{R}^2\\to\\mathbb{R}^2\\), we have\n\\[\n\\nabla_x f = \\frac{df}{dx} = \\begin{bmatrix}\n\\frac{df_1}{dx_1} & \\frac{df_1}{dx_2} \\\\\n\\frac{df_2}{dx_1} & \\frac{df_2}{dx_2}\n\\end{bmatrix}\n\\]\nThis convention simplifies a lot of equations, and completely avoids transposing any matrix.\n\nDiscrete logical time\nConsider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions. We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as \\(x_0, x_1, \\dots, x_T\\). By our ordering, \\(x_1\\) depends on only \\(x_0\\), and \\(x_2\\) depends on only \\(x_1, x_2\\), and so on:\n\\[\n\\begin{aligned}\nx_0 &= x_0 \\\\\nx_1 &= f_1(x_0) \\\\\n&\\cdots \\\\\nx_T &= f_T(x_0, x_1, \\dots , x_{T-1})\n\\end{aligned}\n\\]\nNow we perform an infinitesimal perturbation on every one of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as \\(dx_0, dx_1, \\dots, dx_T\\). We can now compute the derivative of \\(dx_T\\) with respect to every other variable by backpropagating the perturbation. Suppose we can see only \\(dx_T\\), then the change in \\(x_T\\) due to \\(dx_T\\) is the identity. That is,\n\\[\n\\frac{dx_T}{dx_T} = I\n\\]\nNow suppose we can see only \\(dx_{T-1}\\), then the change in \\(x_T\\) due to \\(dx_{T-1}\\) can only come from the final step in the forward propagation. Therefore\n\\[\n\\frac{dx_T}{dx_{T-1}} = \\nabla_{x_{T-1}} f_T(x_0, x_1, \\dots , x_{T-1})\n\\]\nSimilarly, the change in \\(x_T\\) due to \\(dx_{T-2}\\) can come from either directly changing \\(x_T\\) or from changing \\(x_{T-1}\\) and thereby changing \\(x_T\\). Therefore,\n\\[\n\\frac{dx_T}{dx_{T-2}} =\n    \\nabla_{x_{T-1}} f_{T}(x_0, x_1, \\dots , x_{T-2}) +\n    \\underbrace{\\frac{dx_T}{dx_{T-1}}\\nabla_{x_{T-2}} f_{T-1}(x_0, x_1, \\dots , x_{T-2})}_{\\text{the chain rule}}\n\\]\nThis generalizes to the rest of the steps.\nThe above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each \\(x_t\\) only directly influence a few more nodes down the line. In standard neural networks, typically \\(x_{t}\\) only directly influences \\(x_{t+1}, x_{t+2}\\). Thus, sparsity is vital for backpropagation to be relevant.\nAs a side note, we could in fact compute all derivatives, not just the first, in one single backward pass. Other than the second derivatives \\(\\nabla^2_{x_t}x_T\\), there is rarely any use for the other derivatives, such as \\(\\nabla_{x_t}\\nabla_{x_s}x_T, \\nabla^3_{x_t}x_T\\), etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only keep the positive diagonal entries of \\(\\nabla^2_{x_t}x_T\\) and set all other entries to zero (LeCun 1989).\n\n\nContinuous logical time\nConsider the problem of controlling a car along a highway. We discard all details, so that the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time \\(t\\in [0, T]\\). Write the state variable as follows:\n\\[\nx_t = (\\text{location at time }t, \\text{velocity at time }t)\n\\]\nIt might be confusing to use \\(x_t\\) for the state at time \\(t\\), instead of for location, but it makes the notation consistent.\nSuppose the only thing we can influence is how much we press the pedal. Write \\(u_t\\) to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume that the car is slowed down by friction that is proportional to velocity. We then have:\n\\[\n\\dot x_t = f(x_t, u_t)\n\\]\nwhere \\(f(x_t, u_t) = (x_{t, 1}, -\\mu x_{t, 1} + u_t)\\) is the dynamics equation of the system, and \\(\\mu\\) is the friction coefficient.2\n2 To allow for time-varying dynamics, simply replace \\(f(x_t, u_t)\\) with \\(f(t, x_t, u_t)\\). This clutters the notation without involving new ideas.Now, we can draw the computation graph as before. The computation graph has a continuum of nodes, but it has a very simple structure. The graph has two kinds of nodes: the \\(x_t\\) nodes and the \\(u_t\\) nodes. Its independent variables are \\(x_0\\) and all the \\(u_t\\) nodes. Each \\(x_{t+dt}\\) depends on only \\(x_{t}\\) and \\(u_t\\). This makes the two propagations particularly simple.\nThe forward propagation is obtained by integration, which we can unwrap into a form resembling the case of the discrete logical time:\n\\[\n\\begin{aligned}\nx_0 &= x_0 \\\\\nx_{dt} &= x_0 + f(x_0, u_0) dt \\\\\n&\\cdots \\\\\nx_{T} &= x_{T-dt} + f(x_{T-dt}, u_{T-dt}) dt \\\\\n\\end{aligned}\n\\]\nThe backpropagation is similarly obtained. By inspecting the computation graph, we can see that each \\(u_t\\) only directly influences \\(x_{t+dt}\\), giving\n\\[\n\\frac{dx_T}{du_t} = \\frac{dx_T}{dx_{t+dt}} \\nabla_{u_t}f(x_t, u_t) dt.\n\\]\nIt remains to compute \\(\\frac{dx_T}{dx_t}\\). This can be found by backpropagation too, since each \\(x_t\\) only directly influences \\(x_{t+dt}\\), we have\n\\[\n\\frac{dx_T}{dx_t} = \\frac{dx_T}{dx_{t+dt}} \\left[I + \\nabla_{x_t} f(x_t, u_t) dt\\right].\n\\]\nIf we denote the gradient as \\(g_t := \\frac{dx_T}{dx_t}\\), then we find an equation for \\(g_t\\):\n\\[\ng_t = \\left[I + (g_t + \\dot g_t dt) \\nabla_{x_t} f(x_t, u_t) dt\\right]\\implies \\dot g_t = -g_t\\nabla_{x_t} f(x_t, u_t)\n\\]\nThis equation bottoms out at the end time, \\(t=T\\), for which \\(g_T = \\frac{dx_T}{dx_T} = I\\). Thus we have the costate equations:\n\\[\n\\begin{cases}\ng_T &= I \\\\\n\\dot g_t &= - g_t \\nabla_{x_t} f(x_t, u_t)\n\\end{cases}\n\\]\nwhich must, as you can see, be integrated backwards in time – backpropagation again! Indeed, control theory practically compels us to find backpropagation.\n\n\nHybrid logical time\nWhen the computation graph has both nodes with discrete logical times and nodes with continuous logical times, we call such a system as having hybrid logical time.3\n3 The name “hybrid” comes from “hybrid control theory”, which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.The idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as \\(J\\), representing the “cost” of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write \\(J\\) down as something like\n\\[\nJ = \\underbrace{(x_{T, 0} - x_{goal})^2}_{\\text{location should be at the goal location at the end time}}\n    + \\underbrace{(x_{T, 1} - 0)^2}_{\\text{speed should be zero at the end time}} + \\int_0^T u_t^2 dt\n\\]\nMore generally, the objective to be optimized is in the form\n\\[\nJ = A(x_T) + \\int_0^T L(x_t, u_t)dt\n\\]\nfor some real-valued functions \\(A, L\\).\nOf course, we can care about more than the state at the last time-step. We can care about multiple time-steps \\(t_0, t_1, \\dots, t_n\\) by writing down a cost function \\(J = \\sum_{i=0}^n A_i(x_{t_i}) + \\int_0^T L(x_T, u_T)dt\\), but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.\nDefine the costate \\(\\lambda_t := \\nabla_{x_t} J\\), then the costate backpropagates as:\n\\[\n\\lambda_t = L(x_t, u_t) dt + \\lambda_{t+dt}(I + \\nabla_{x_t}f(x_t, u_t)dt)\n\\]\nand by simplifying, we have the costate equation:\n\\[\n\\begin{cases}\n\\lambda_T &= \\nabla_{x_T} A(x_T) \\\\\n\\dot \\lambda_t &= - \\nabla_{x_t} L(x_t, u_t) - \\lambda_t \\nabla_{x_t} f(x_t, u_t)\n\\end{cases}\n\\]\nwhich can be solved by integrating backward in time.\nOnce we have obtained all the costates, we can compute \\(\\nabla_{u_t} J\\). Since \\(u_t\\) can only influence \\(J\\) either directly via \\(L(x_t, u_t)\\) or indirectly via \\(x_t\\), we have\n\\[\n\\nabla_{u_t} J = \\left[\\nabla_{u_t}f(x_t, u_t) \\lambda_t + \\nabla_{u_t}L(x_t, u_t)\\right]dt\n\\]\nNote that \\(\\nabla_{u_t} J\\) is an infinitesimal in \\(dt\\). This is qualitatively different from \\(\\nabla_{x_t}J = g_t\\), which is not an infinitesimal, but a mere matrix. Why is it so? The simplest example suffices to illustrate.\nConsider a mass point sliding on a frictionless plane. When we perturb \\(x_t\\), we push it to the side by \\(\\delta x_{t, 0}\\) and also change its velocity by \\(\\delta x_{t, 1}\\), and so at the end time \\(T\\), we would have changed \\(x_T\\) by \\((\\delta x_{t, 0} + (T-t)\\delta x_{t, 1}, \\delta x_{t, 1})\\), which is the same order of infinitesimal. Now, we can control the mass point by applying a force \\(u_t\\), which gives us the dynamics equation\n\\[\n\\dot x_t = (x_{t, 1}, u_t)\n\\]\nTo “perturb” \\(u_t\\) by \\(\\delta u_t\\) does not make sense on its own, as a “spike” of \\(\\delta u_t\\) that lasts for zero amount of time does not show up in the system trajectory. One can apply a durationless jump in state, but one cannot apply a durationless force. Therefore, it only makes sense to perturb \\(u_t\\) by \\(\\delta u_t\\) and persist the perturbation for \\(dt\\) time. This perturbs the state at the end time by \\(((T-t)\\delta u_t dt , \\delta u_t dt)\\), which means that \\(\\nabla_{u_t}x_T\\) is proportional to \\(dt\\).\n\n\nOptimal control theory\nAn optimal trajectory must have \\(\\nabla_{u_t} J = 0\\), since otherwise, we could shave off a little piece of cost by giving \\(u_t\\) a little boost in the opposite direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t) + \\lambda_t \\nabla_{x_t} f(x_t, u_t) \\\\\n0                &= \\nabla_{u_t} L(x_t, u_t) + \\lambda_t \\nabla_{u_t} f(x_t, u_t) \\\\\n\\end{cases}\n\\]\nNow, what is the effect of perturbing \\(u_t\\) by \\(du_t\\)? It would perturb \\(x_{t+dt}\\) by \\(\\nabla_{u_t} f(x_t, u_t) du_t dt\\), a second-order infinitesimal. Consequently, it would perturb \\(x_T\\) by only a second-order infinitesimal, and thus \\(\\lambda\\) too. Therefore, we have\n\\[\n\\nabla_{u_t}\\lambda_t = 0\n\\]\ngiving us simplified equations for optimality:\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t) + \\lambda_t \\nabla_{x_t} f(x_t, u_t) \\\\\n0                &= \\nabla_{u_t} (L(x_t, u_t) + \\lambda_t f(x_t, u_t)) \\\\\n\\end{cases}\n\\]\nUnfortunately, we cannot simplify the first equation similarly because \\(\\nabla_{x_t}\\lambda_t \\neq 0\\). Still, it seems \\(L(x_t, u_t) + \\lambda_t f(x_t, u_t)\\) should be an important quantity:\n\\[\nH(x_t, u_t, \\lambda_t) := L(x_t, u_t) + \\lambda_t f(x_t, u_t)\n\\]\nThe letters are meaningful. \\(L\\) is the “Lagrangian”, and \\(H\\) is the “Hamiltonian”. Indeed, classical Hamiltonian mechanics is a special case of optimal4 control theory.\n4 To be completely precise, it is a necessary but insufficient condition for optimality. Just like how a function can have zero derivatives on the peaks, valleys and shoulders, a trajectory can have zero functional derivative, even if it is the best, the worst, and the … shouldered? In jargon, we say that those conditions are first order optimality conditions, since they use only the derivative, not the second-derivative.If we interpret economically the quantities, then \\(J\\) is the cost of the entire trajectory, \\(\\lambda_t\\) is the marginal cost of the point \\(x_t\\) in the trajectory, and \\(L(x_t, u_t)\\) is the cost-rate at time \\(t\\). The second equation of optimality \\(\\nabla_{u_t} H(x_t, u_t, \\lambda_t) = 0\\) states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.\nTherefore, define the “optimized” Hamiltonian and the optimized control relative to it:\n\\[\n\\begin{cases}\nH^*(x_t, \\lambda_t) &:= \\min_{u_t}H(x_t, u_t, \\lambda_t) = \\min_{u_t} \\left(L(x_t, u_t) + \\lambda_t f(x_t, u_t)\\right) \\\\\nu^*(x_t, \\lambda_t) &:= \\mathop{\\mathrm{argmin}}_{u_t}H(x_t, u_t, \\lambda_t)\n\\end{cases}\n\\]\nThen, by Hotelling’s lemma, we derive the Hamiltonian equations of motion:\n\\[\n\\begin{cases}\n- \\dot \\lambda_t &= \\nabla_{x_t} L(x_t, u_t^*) + \\lambda_t \\nabla_{x_t} f(x_t, u_t^*) &= \\nabla_{x_t} H^*(x_t, \\lambda_t) \\\\\n  \\dot x_t       &= f(x_t, u_t^*)                                                     &= \\nabla_{\\lambda_t} H^*(x_t, \\lambda_t) \\\\\n\\end{cases}\n\\]\nThis is often called the Pontryagin’s maximum principle, as Pontryagin’s school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it was used from the space race to the missile race.\nIn classical control theory, the equation is sometimes solved in closed form, as in the case of linear quadratic control. When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Similar to how the Hamiltonian equations have the alternative form of the Hamilton–Jacobi equation, the Pontryagin equations have an alternative form in the Hamilton–Jacobi–Bellman equation. Possibly, the name “dynamic programming” appears later in Paul Werbos’ invention of backpropagation, which he named “dynamic feedback”.\nIn economics, one can modify this framework slightly to allow discounting cost over time, yielding theories about “optimal investment” such as the Ramsey optimal growth theory."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#leibniz",
    "href": "essays/posts/backstory-of-backpropagation/index.html#leibniz",
    "title": "The Backstory of Backpropagation",
    "section": "Leibniz",
    "text": "Leibniz\nThe chain rule dates back to Calculus Tangentium differentialis [Differential calculus of tangents], a manuscript by Leibniz dated 1676 November (Child 1917). It says\n\nit does not matter, whether or no the letters \\(x, y, z\\) have any known relation, for this can be substituted afterward.\n\nIn mathematical notation, he found that \\(dy = dx \\frac{dy}{dx}\\), or in his notation, \\(\\overline{dy} = \\overline{dx} \\frac{dy}{dx}\\), where the overbar denotes the thing to be differentiated. You can read it as a bracket: \\(d(y) = d(x) \\frac{dy}{dx}\\).\nHe then gave the following examples5:\n5 Yes, there is a sign error. No, I’m not going to fix it. He just has to live with his mistakes.\\[\n\\begin{aligned}\n& \\overline{d \\sqrt[2]{a+b z+c z^2}} \\text {. Let } a+b z+c z^2=x \\text {; } \\\\\n\\text{Then} \\quad  & \\overline{d \\sqrt[2]{x}}=-\\frac{1}{2 \\sqrt{x}} \\text {, and } \\frac{d x}{d z}=b+2 c z \\text {; } \\\\\n\\text{Therefore} \\quad  & \\overline{d \\sqrt{a+b z+c z^2}}=-\\frac{b+2 c z}{2 \\overline{d z} \\sqrt{a+b z+c z^2}} \\\\\n&\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#mcculloch-and-pitts",
    "href": "essays/posts/backstory-of-backpropagation/index.html#mcculloch-and-pitts",
    "title": "The Backstory of Backpropagation",
    "section": "McCulloch and Pitts",
    "text": "McCulloch and Pitts\nIn the famous paper (McCulloch and Pitts 1943), McCulloch and Pitts proposed that\n\nBecause of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.\n\nThe McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. In fact, it uses the same abstruse notations of the Principia Mathematica, which is cited in the paper.\n\n\n\nThe infamous proof of 1+1=2 in Principia Mathematica\n\n\n\n\n\nThe same notation is used by McCulloch and Pitts\n\n\nThe McCulloch and Pitts paper, like the Perceptrons book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still (Minsky 1967, chap. 3), which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#frank-rosenblatt",
    "href": "essays/posts/backstory-of-backpropagation/index.html#frank-rosenblatt",
    "title": "The Backstory of Backpropagation",
    "section": "Frank Rosenblatt",
    "text": "Frank Rosenblatt\nFrank Rosenblatt is the originator of the term “backpropagation”, or more precisely, “back-propagating error-correction procedure” (Rosenblatt 1962, vol. 55, chap. 13), although it was quite far from our current use of backpropagation. He is the originator of many concepts in neural networks, and he was even popularly famous during his own time. During 1957–1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room.\nA perceptron is a function of the form \\(\\theta(w^T x + b)\\), where \\(\\theta\\) is the 0-1 step function, and \\(w \\in \\mathbb{R}^n, b \\in \\mathbb{R}\\) are its learnable parameters. A perceptron network is a network of perceptrons. Rosenblatt experimented with a large number of perceptron network architectures, but most often, he experimented with a certain 2-layered feedforward architecture, which is sometimes called “the perceptron machine”.\nThe perceptron machine is a machine that computes a function of type \\(\\{0, 1\\}^n \\to \\{0, 1\\}\\). Its input layer is composed of units named “Stimulus units” or “S units”. The S units do not perform any computation, but merely pass binary outputs to the hidden layer. In the Mark I perceptron, the S units were 20×20 cadmium sulfide photocells, making a 400-pixel image. The hidden layer is composed of perceptrons named “Association units” or “A units”. Their outputs pass on to the output layer, composed of perceptrons named “Response units” or “R units”.\nWe can describe the perceptron machine in one equation:\n\\[\nf(x) = \\theta\\left(b^{R} + \\sum_i w^{AR}_i \\theta\\left((w^{SA, i})^T x + b^{A}_i\\right)\\right)\n\\]\nRosenblatt proved some mathematical theorems, the most important of which is the perceptron convergence theorem, which shows that assuming the dataset is linearly separable, then the perceptron learning algorithm converges after making a bounded number of errors6. He also performed experiments on the Mark I Perceptron machine, IBM computers, and some other computers.\n6 There is another technical assumption on the dataset, but it is omitted for space. Suffice to say it is satisfied for all finite datasets.His theorems and experiments were exhaustively documented in his book (Rosenblatt 1962). Its breadth is quite astonishing. It contains:\n\nperceptrons with continuous activation functions (section 10.2);\nperceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);\nperceptron layers with random delay in transmission time (chapter 11);\nlayers with connections between units within the same layer, with possibly closed loops (chapter 17–19);\nlayers with connections from a later layer to a previous layer (chapter 20);\nresidual connections (Figure 42);\nmultimodal perceptron networks that learns to associate image and audio inputs (Figure 58);\nprogram-learning perceptrons (chapter 22);\nperceptron networks that analyze videos and audios (chapter 23).\n\nFrom our vantage point, we can fairly say that he invented randomization, residual connections, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks…\n\n\n\nFigure 58. The first multimodal neural network?\n\n\n\n\n\nFigure 42. The dashed lines denote variable weights, and the solid lines denote fixed weights. This is the residual connection.\n\n\nWhat is even more astonishing is that, as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff. The perceptron learning rule converges, but only for a single-layered perceptron network on linearly separable data. Thus, when it came to two-layered perceptron networks, he randomly wired the first layer, then froze it, and only adapted the second layer. This would become the focal point of the “perceptron controversy”.\nIn the chapter where he talked about backpropagation (Rosenblatt 1962, vol. 55, chap. 13), he tried to train both layers in the two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The desired outputs for the output units are supplied by the experimenter, but the desired outputs for the hidden units could not be known, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to “backpropagate the error” to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.\nOne last thing about his backpropagation rule: he also discovered the layer-wise learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.\n\nIt is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff",
    "href": "essays/posts/backstory-of-backpropagation/index.html#bernard-widrow-and-marcian-hoff",
    "title": "The Backstory of Backpropagation",
    "section": "Bernard Widrow and Marcian Hoff",
    "text": "Bernard Widrow and Marcian Hoff\nWidrow and Hoff worked on neural networks in the early 1960s. They started with training a single perceptron with gradient descent on the squared loss, then proceeded to spend years trying to train a two-layered network without gradient descent. I know – I cannot make this sound any less puzzling.\nThe Widrow–Hoff machine, which they called the ADALINE (“ADAptive Linear NEuron”), is a function of type \\(\\mathbb{R}^n \\to \\{0,1\\}\\) defined by\n\\[\nf(x) = \\theta(w^T x + b)\n\\]\nand here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was trained by gradient descent with the squared loss function \\(\\frac 12 (w^T x + b - y)^2\\), which is continuous, not discrete:\n\\[\nw \\leftarrow w - \\alpha (w^T x + b - y) w, \\quad b \\leftarrow b - \\alpha (w^T x + b - y) b\n\\]\nThe first ADALINE machine was a box that learned to classify binary patterns on a \\(4 \\times 4\\) grid. It was pretty amusing, as everything was done manually. The patterns were inputted by flipping 16 switches by hand. The error \\(w^T x + b - y\\) was read from a voltmeter, and the parameters \\(w, b\\) were individually adjusted by turning knobs controlling rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memistors, though the pattern input \\(x\\) and the desired output \\(y\\) were still entered by manual switches.\n\n\n\nAll components of the ADALINE machine, color-labelled, from the algorithm, to the circuit diagram, to the front-panel of the physical machine.\n\n\n\n\n\nA memistor ADALINE with glass-sealed memistors. (Widrow 2023, fig. 26.12)\n\n\n\n\n\nTest patterns for the ADALINE machine. It also shows one of the first learning curves in machine learning. (Widrow 2023, fig. 26.4)\n\n\n\n\n\nA MADALINE from 1962. The “JOB ASSIGNER” learning rule is too hacky to explain. (Widrow 1962, fig. 11)\n\n\nWidrow recounts an amusing encounter with Rosenblatt:\n\nI just put the pattern in and the Adaline went “phut,” and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt’s students put the pattern into the perceptron. You could see it in the lights on the perceptron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went “blip ,” and there it was, adapted. They put it in the perceptron, and it’s still grinding away. We put in a couple more patterns. Then we test the Adaline and test the perceptron to see whether the patterns are still in there.\nThey’re in the Adaline. In the perceptron, they’re all gone. I don’t know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, “You’d be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer.” He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there’s a first layer that’s randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it’s adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.\n\nAfter the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE (“MADALINE”, or “many ADALINE”), which of course could not be trained by gradient descent, because the hidden layer ADALINE units used the Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function and tried every other trick except that. They ended up with the MADALINE I rule. In short, it was a heuristic rule for synthesizing supervision signals for the hidden layer, much like Rosenblatt’s heuristic rule.7\n7 The MADALINE I machine consists of multiple hidden ADALINE units, and a single output layer consisting of a single unit. The output unit merely takes the majority vote from the hidden units. If the desired output is \\(+1\\), but the actual output is \\(-1\\), then the machine picks those ADALINE units that are negative, but closest to being positive, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of “minimal disturbance principle”.8 Marvin Minsky would approve.\n\n“[The perceptron] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of.”(Bernstein 1981)\n\nFrustrated by the difficulty, they left neural network research. Hoff went to Intel to co-invent the microprocessor, and Widrow set about applying the ADALINE to small problems that it could solve well8, such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.\n\nEngineers at Apple have told me that every iPhone, since the iPhone 5, uses the LMS algorithm all over the device. They could not tell me where because, if they did, they would have to shoot me. Apple keeps secrets. (Widrow 2022, preface, page xix)\n\nPerhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself, it was a blind spot for all researchers:\n\nThe Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic first layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.\nBackprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; you have to have a smooth nonlinearity … no one knew anything about it at that time. This was long before Paul Werbos. Backprop to me is almost miraculous. (Rosenfeld and Anderson 2000)\n\nWhen he heard about the “miraculous” backpropagation in the 1980s, he immediately started writing papers in neural networks again.\nIf this was an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military:\n\n… the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. … The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don’t show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track. (Widrow 1997)\n\nThe problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it “the best piece of work I ever did in my whole life”. He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise (Widrow and Kollár 2008).\nSo regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the other pioneers went down the same wrong path."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#seppo-linnainmaa",
    "href": "essays/posts/backstory-of-backpropagation/index.html#seppo-linnainmaa",
    "title": "The Backstory of Backpropagation",
    "section": "Seppo Linnainmaa",
    "text": "Seppo Linnainmaa\nIt’s said that Seppo Linnainmaa’s master’s thesis in 1970 contains the backpropagation algorithm, but it is in Finnish and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. (Griewank 2012) describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.\nI checked all his English papers during the 1970s, and it seems only (Linnainmaa 1976) has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#david-rumelhart",
    "href": "essays/posts/backstory-of-backpropagation/index.html#david-rumelhart",
    "title": "The Backstory of Backpropagation",
    "section": "David Rumelhart",
    "text": "David Rumelhart\nThis section is mainly based on a 1995 interview with David Rumelhart (Rosenfeld and Anderson 2000, chap. 12).\nIn 1979-06, Rumelhart attended a conference organized by James A. Anderson, who knew many of those who were still doing neural network research even through this AI winter. This got him interested in neural networks, especially those done by Geoffrey Hinton and James L. McClelland. He couldn’t get the idea out of his head, and during a train ride, decided to just do a “five-year plan” to figure out what the fuss is about, and maybe compile a comprehensive literature review. The world would know this book as Parallel Distributed Processing (1986).\n\n“I think it’s important enough that I should spend at least five years figuring out what’s going on.” … When I came back to San Diego from this trip, I went to see McClelland, and I said to Jay, “Look, Geoffrey’s coming back in December. Let’s spend two quarters going over all the stuff on these neural network things… We’ll have meetings every day. I’m on sabbatical. I can do this.” Jay was on a Young Investigator Award, and Geoffrey was coming as a kind of a postdoctoral continuation… “Six months, that’s plenty of time for doing this”, we thought.\n\nSo they set about reading all the literature they could find about neural networks. Rumelhart saw that the XOR problem was the key. Solve that with a general method, and neural networks would be reborn.\n\nI’d read Perceptrons. And I thought, ’Well , somehow we have to figure out how to teach, how to train, a network that has more than one layer. Why can’t we do this?” … I knew about, as I say, the work of Rosenblatt and about the way he tried to do it. He had this idea of sending error signals back across layers, but he didn’t have a very principled way of doing it.\n\nRumelhart knew that Bernard Widrow’s ADALINE essentially performs a linear regression \\(Wx + b\\), trained by the ADALINE learning rule, also called the “delta rule”:\n\\[\n\\Delta W = -\\eta(Wxx^T + (b-y)x^T), \\quad \\Delta b = -\\eta(b + Wx - y)\n\\]\nSo he thought, how do I extend this to a “generalized delta rule”?\n\nSo what we thought was we’d pretend they were linear, and we would compute the derivatives as if they were linear, and then we could train them. We did this on a one-layer system. That was what we called delta learning… “Well, we’ll just pretend like it’s linear, and figure out how to train it as if it were linear, and then we’ll put in these sigmoids. In that way, we can make a system that would work.”\n\nSo he thought, well, if the activation functions are linear functions, then we just get a deep linear network, so we can still do the delta rule. If the activation functions are nonlinear, but differentiable, then we can approximate these by linear activations… and that should be a working, generalized delta rule!\nThus, by a long and almost accidental route, Rumelhart arrived at the gradient descent algorithm without thinking about gradients. In hindsight, the ADALINE learning rule is really a special case of gradient descent on the loss function \\(L = \\frac 12 \\|Wx + b - y\\|^2\\), but people back then didn’t think of it that way. They just thought of it as a not particularly efficient way to solve the linear equation \\(y \\approx Wx + b\\). Of course, once he had discovered the generalized delta rule, he recognized it is really just gradient descent on L2 loss. Once you have figured out the result, you often become embarrassed by the pages upon pages of wasteful calculations you used to get there. A very relatable moment!\nBackpropagation was rediscovered in 1982. The world would soon know it, and this time, it would not forget.\nIt was not all smooth-sailing though. Backpropagation was terribly slow, taking over 1000 steps to converge, and Hinton believed it would get stuck in local minima. Indeed, these two objections, that backpropagation is slow, and gets stuck in local minima, are perennial objections, as we would see in the history of the second neural network winter.\nSomewhat disappointed by the two objections, Rumelhart tried other methods, such as competitive learning. Hinton left the book project to focus on his beloved Boltzmann machines, and would not return until 1985.\nIn fact, Rumelhart avoided backprop whenever he could, as well! When he was trying to make a neural network to convert verbs (represented as a list of phonemes) to their past tense forms, he thought that he had to resort to an MLP with backprop, but then realized that, no, you can completely avoid backprop if you hand-design some features, then just train a single layer over those features. An inauspicious omen of the second neural network winter, during which computer vision was stuck with the same paradigm of shallow learning (SVM, random forests, etc) over cleverly hand-designed features (SIFT, HoG, etc).\n\nBut then McClelland and I figured out another trick for learning past tenses, the so-called “Wickelfeature” representation. We thought, “Oh well, we don’t need multiple layers.” So I put that aside and went off, and we realized that if we made fancier input representations, we wouldn’t need this intermediate layer.\n\nThe first phoneme of the Bitter Lesson has been spoken.9\n9 Death and the Compass (Borges, 1942).Years later, they did train an MLP by backprop for this task, and its publication in 1986 ignited the past tense debate (Pinker and Ullman 2002). But at the moment, they didn’t really want to use backprop. Indeed, in 1983, backprop was looking so uninteresting that at a lecture, Rumelhart elected to talk about competitive learning instead.\n\n[In 1983, Hinton and Sejnowski] held a meeting in Pittsburgh… I remember electing at the last minute to talk about the competitive learning work rather than the backpropagation work. I had my slides all made out to do both of these talks, and I gave that one.\n\nAround 1984, Rumelhart started getting a vibe that they couldn’t just keep doing this, that they really needed MLPs with learned hidden layers, so he tried it again. Things really picked up at this point. Hinton had realized that Boltzmann machines were even more painfully slow than backprop, so he went back. They wrote the famous Nature paper in 1985, and this time, the world listened. (Rumelhart, Hinton, and Williams 1986)\n\nWe had pretty much sorted this out and were working on how to do backpropagation in time. Geoff and I decided we really should write this up, so we started writing our paper and did a whole bunch of experiments in order to have something to say in this paper. The paper was really written in the spring of ’85. I think the Nature paper was in the fall of ’85. By then I was fairly committed to learning how it might work. I guess I’ve now spent about another ten years sorting all of this out the best I could.\n\nIn the interview, he was rather unbothered by the priority dispute:\n\nI had no idea that Paul Werbos had done work on it. … There are other examples of work in the control literature in the ’60s [the adjoint method]. … it’s just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. [Shun’ichi] Amari, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn’t do anything with it. I think that was in the late ’60s. I don’t feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn’t know there were any."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#terence-sejnowski",
    "href": "essays/posts/backstory-of-backpropagation/index.html#terence-sejnowski",
    "title": "The Backstory of Backpropagation",
    "section": "Terence Sejnowski",
    "text": "Terence Sejnowski\nIn 1983, Rumelhart showed backpropagation to Sejnowski, who immediately tried it and found that it was much faster than the Boltzmann machine learning rule. What a refreshing change from all those others who stubbornly refused to try it.\n\n… I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions. (Rosenfeld and Anderson 2000, chap. 14)\n\nThis was vitally important later, when Sejnowski used backpropagation to train NETtalk, a huge network with 18,629 parameters.10 The model was a popular hit and appeared on prime-time television. (Sejnowski 2018, 112–15)\n10 People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well. It really does seem like scaling skepticism is a researcher universal, if not a human universal.\n\nThere were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. (Sejnowski 2018, 113)\n\nSejnowski later commented that backpropagation is inelegant:\n\nThis is a highly efficient way to compute error gradients. Although it has neither the elegance nor the deep roots in physics that the Boltzmann machine learning algorithm has, backprop is more efficient, and it has made possible much more rapid progress.\n(Sejnowski 2018, 112)"
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#geoffrey-hinton",
    "href": "essays/posts/backstory-of-backpropagation/index.html#geoffrey-hinton",
    "title": "The Backstory of Backpropagation",
    "section": "Geoffrey Hinton",
    "text": "Geoffrey Hinton\nThe interview with Geoffrey Hinton is hilarious, mostly about how he spent several years refusing to use backpropagation. This section is mostly made of quotations from (Rosenfeld and Anderson 2000, chap. 16).\nAfter learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 (Ackley, Hinton, and Sejnowski 1985).\n\nI remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of ’83 … I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn’t have a learning algorithm.\nThen we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we’d solved the problem. That must be how the brain works.\nI guess if it hadn’t been for computer simulations, I’d still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.\nI still think that’s the nicest piece of theory I’ll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.\n\nAnd now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,\n\nI first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule …\nThe next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.\nThen I tried to use it to get a very obscure effect. I couldn’t get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I’d realized that if you’ve got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. It didn’t give fast relearning, so I made one of these crazy inferences that people make – which was, that backpropagation is not very interesting.\n\nAfter one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines also got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.\n\nAfter initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as “going sour.” We couldn’t initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn’t believe that these networks would degrade as you acquired more knowledge.\nIt took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.\nAfter investing over a year in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting [backpropagation].\n\nThen he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):\n\nThey’d all been thoroughly indoctrinated by then into Boltzmann machines. … They all said, “You know, why would you want to program that?” We had all the arguments: It’s assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It’s ridiculous.” So they just refused to work on it, not even to write a program, so I had to do it myself.\nI went off and I spent a weekend. I wrote a LISP program to do it.\n\nHinton almost had one last chance at giving up on backpropagation.\n\nI almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.\nIn a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.\n\n\n\n\nThe 8-3-8 autoencoder. The input is a one-hot encoding of one integer from \\((0:7)\\), and the output should be a copy of the input. The middle layer has 3 units, which is sufficient for binary encoding. Hinton found the Boltzmann machine learned exactly this, and expected backpropagation to learn the same thing, but was surprised when it did not. In the picture, the input is a one-hot vector of the integer 3, and the hidden layer encodes it as a binary 011, which is then decoded to a one-hot vector of 3 again.\n\n\nAnd so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.\nDespite this, Hinton remained unhappy with backprop and kept working on Boltzmann machines, as well as its relatives like the Helmholtz machine and the Deep Belief Networks. This is him talking in 1995, already 10 years after he was forced to accept the superiority of backprop:\n\nThat was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation."
  },
  {
    "objectID": "essays/posts/backstory-of-backpropagation/index.html#paul-werbos",
    "href": "essays/posts/backstory-of-backpropagation/index.html#paul-werbos",
    "title": "The Backstory of Backpropagation",
    "section": "Paul Werbos",
    "text": "Paul Werbos\nAfter reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of “originator of the backpropagation algorithm”. He independently developed the algorithm around 1971, but was frustrated at every turn when he tried to publish it, not managing until 1982. After that, it was quickly picked up by connectionists. In this sense, he has both priority and paternity.\nIn 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation (Rosenfeld and Anderson 2000, chap. 15). I will let him speak, only interjecting with brief comments.\nBefore entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn’t work.\n\nIt was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, “Now I’m going to figure out something with the same flavor that does work.”\n[Understanding human learning] will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.\nMinsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.\n\nSometime before 1968, he was inspired to do backpropagation from reading Freud.\n\nI talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in ’68 (P. Werbos 1968). I talked a lot about what was wrong with the existing [two state] McCulloch–Pitts neuron model, and how it was only “1” and “0.” I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When\n\nFor his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn’t want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.\n\nInitially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn’t produce a really useful product at the end of \\(x\\) number of months.\nSo I went back to the committee, and I said, “Gee, I can’t do that, but I have this little method for adapting multilayer perceptrons. It’s really pretty trivial. It’s just a by-product of this model of intelligence I developed. And I’d like to do it for my paper for this computer course.”\n[Larry] Ho’s position was, “I understand you had this idea, and we were kind of open-minded. But look, at this point, you’ve worked in this course for three months, admittedly on something else. I’m sorry, you’re just going to have to take an incomplete in the course.”\nAnd I said, “You mean I can’t do it?”\n“No, no, you’ll have to take an incomplete because, basically, the first thing didn’t work. We’re very skeptical this new thing is going to work.”\n“But look, the mathematics is straightforward.”\n“Yeah, yeah, but you know, we’re not convinced it’s so straightforward. You’ve got to prove some theorems first.”\nSo they wouldn’t let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by Bryson and Ho. They don’t realize it was the same Larry Ho, who was on my committee and who said this wasn’t going to work.\n\nI am not sure if this is sarcastic or not. It reminds me of the “summer vision project” (Papert 1966) that expected some undergraduate students to construct “a significant part of a visual system” in a single summer.\n\nBy the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I’d better work on that one later and finish my Ph.D. thesis on something small – something I can finish by the end of a few years, like a complete mathematical model of human intelligence.\n\nThe oral was amusing, and touched on the still-hot issue of recent human evolution.\n\n… I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations’ time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, Karl Deutsch, raised his hand. … His book, The Nerves of Government, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.\nHe raised his hand and he said, “Wait a minute, you can’t get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That’s crazy.”\nNext to him was a mathematical biologist by the name of Bossert, who was one of the world’s authorities on population biology. He raised his hand and said, “What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments.”\nAnd Deutsch said, “What do you mean, it’s common knowledge? All of our political theories are based on the assumption this cannot happen.” And Bossert said, “Well, it happens. Here’s the data.”\n… I passed the orals having said about two sentences and not having discussed models of intelligence.\n\nIt turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.\n\nBut the backpropagation was not used to adapt a supervised learning system; it was to translate Freud’s ideas into mathematics, to implement a flow of what Freud called “psychic energy” through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in ’71 or ’72. … The thesis committee said, “We were skeptical before, but this is just unacceptable … You have to find a patron. You must find a patron anyway to get a Ph.D. That’s the way Ph.D.s work.\n\nThe committee gave him three acceptable patrons. He first went to Stephen Grossberg.\n\n… he said, ’Well, you’re going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I’m sure you can pull through in the end, but you’re going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.”\n\nThanks, Grossberg, for using the law of excluded middle to crush Werbos’ dream.\nHe then went to Marvin Minsky, who gave us some new clues about why backpropagation took so long to discover: “everybody knows a neuron is a 1-0 spike generator”!\n\n“I’ve got a way now to adapt multilayer perceptrons, and the key is that they’re not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch–Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it.”\nMinsky basically said, “Look, everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.”\nHe was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.\n\nAlthough I believe Minsky would have disapproved of the idea of backpropagation even if he had thought that neurons are not strictly 1-0 spike generators. In the epilogue to (Minsky and Papert 1988), he claimed that gradient descent does not scale, and using differentiable activation functions is just a hack intended to make backpropagation work, a pointless hack, as backpropagation would not scale.\n\nHowever, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold. … We have the impression that many people in the connectionist community do not understand that this is merely a particular way to compute a gradient and have assumed instead that back-propagation is a new learning scheme that somehow gets around the basic limitations of hill-climbing.\n\nFurther, in a 1991 interview, Minsky made the same kind of statement:\n\nI don’t know what they would have done with the money. The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea. There was someone . . . [trying to remember].\nQuestion: Paul Werbos?\nAnswer: That’s it! [excited]. But, you see, it’s not a good discovery. It’s alright, but it takes typically 100,000 repetitions. It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It’s certainly trivial. The idea is how you do gradient descent. I didn’t consider it practical.\nQuestion: Because of the computational costs?\nAnswer: Yes, but also, with artificial intelligence, we had the experience that when you make a process like that you usually get stuck at a local minimum. We still don’t have any theory of what range of problems they work well for.” (Minsky, interview) (Olazaran 1991, 249)\n\nOut of curiosity, I looked up the “Rosenblith” book (Rosenblith 2012) that Werbos mentioned, and indeed there were a few tracings that show continuously varying neural activation.\n\n\n\n\n\n\n\n\n\n\n\n(a) Page 146 of the book. Mechanisms of gustatory and olfactory receptor stimulation, Figure 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Page 366 of the book. The physiological basis of wavelength discrimination in the eye of the honeybee, Figure 4.\n\n\n\n\n\n\n\nFigure 1: Rosenblith images.\n\n\n\nThen Minsky dunked on reinforcement learning as well, because he had an unpublished “jitters machine” that failed to optimize its reward. Presumably the name “jitters machine” refers to how it would jitter in place, not able to move towards the goal.\n\nMinsky also said, “You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I’ll hand you the tech report, which we have deliberately never published.”\nIt was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn’t work. I later looked at that paper … He had a system that was highly multivariate with a single reinforcement signal. The system can’t learn efficiently with that. At any rate, he was totally turned off.\n\nThe brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was so unpublished that I found only two other references in the entire literature (P. J. Werbos 1982, 1987), both by Werbos. According to him,\n\nThere are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch–Pitts model of the neuron, which was both discontinuous and non-differentiable. It was felt, in the 1950’s, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete “spikes.” More recent work in neurology has shown that higher brain cells output “bursts” or “volleys” of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a “jitters” machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters \\((N)\\), which may be huge.\n(P. J. Werbos 1987)\n\nThis makes things clear enough. The jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would estimate \\(\\partial_{\\theta_i} R(\\theta)\\) using finite difference, but since it used finite difference, it could estimate the partial derivative for one of the parameters \\(\\theta_i\\) – only one! No wonder it never managed to learn.\nIt is almost comical how much they failed to just use gradient descent. It sometimes feels as if they did everything to avoid just taking the gradient. In the case of Minsky, he made it very clear, in the epilogue of (Minsky and Papert 1988), that he did not believe in gradient descent, period. But what explains the gradient-phobia of the others…?\nAnyway, back to the interview. Werbos went to Jerome Lettvin, the neuroscientist famous for What the Frog’s Eye Tells the Frog’s Brain. Turns out he was a proto-eliminativist. While I am an eliminativist too, Werbos was a Freudian, which must have collided badly with eliminativism.\n\n“Oh yeah, well, you’re saying that there’s motive and purpose in the human brain.” He said, “That ‘s not a good way to look at brains. I’ve been telling people, ’You cannot take an anthropomorphic view of the human brain.’ In fact, people have screwed up the frog because they’re taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific.”\n\nWithout patrons, he faced the committee again.\n\nI tried to simplify it. I said, “Look, I’ll pull out the backprop part and the multilayer perceptron part.” I wrote a paper that was just that - that was, I felt, childishly obvious. I didn’t even use a sigmoid [non-linearity]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, but this is too trivial and simple to be worthy of a Harvard Ph.D. thesis.”\n\nOh, now it’s too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.\n\n… they had discontinued support because they were not interested, so I had no money. … Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D. candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn’t ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.\nFinally, they said, “Look, you know, we’re not going to allow this.” There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, “We can’t even allow you to stay as a student unless you do something. You’ve got to come up with a thesis, and it can’t be in this area.”\n\nKarl Deutsch, who believed in Werbos, sponsored his PhD thesis on a “respectable” problem: fitting an ARMA model to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. Box–Jenkins method ran too slowly, so Werbos programmed in the backpropagation. It worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.\n\nDeutsch said, “You ’re saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who’ve tried to implement this model on real-world data I’ve collected, and they’ve never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?”\nThe first application of backpropagation in the world in a generalized sense was a command that was put into the TSP at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT ’s report to the DOD [the Department of Defense] and part of the DOD’s report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.\n… Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D. thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. … one of the people there said, ’We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass.”\n\nBut the saga is not over. After the PhD, he tried promoting his work. For example, he tried interesting Laveen Kanal in it, who promptly ignored it:\n\na young man came by in the spring of 1975 to introduce himself as a new assistant professor in the Government and Politics department at the University. He wanted to talk about his Ph.D. dissertation which he had recently finished at Harvard. He said quite enthusiastically that it related to pattern recognition, learning machines, and intelligent systems. With the best of intentions I told him to lend me a copy and he lent me a report titled “Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences”. Soon the report got buried in the usual flood of paper that unfortunately still remains very much part of my life. I have often wondered in recent years if the title had been different, e.g., if it had mentioned something about a procedure for training multilayer neural networks, or if my desk had been less cluttered, would I have paid more attention to this report or was I so tuned away from artificial neural networks that it would have made no difference? … I was not alone in neglecting his work.\n(Kanal 1993)\n\nThen he was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 – and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.\n\nI found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. … I wound up sending a couple of graduate students to create a really good database of Latin America. I said, “You want variance, high variance. Something hard to predict.” I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don’t know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief’s model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn’t improve, though; it was hopeless.\nDARPA wasn’t happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn’t like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, “No, no, no, we can’t publish this. This is too hot.”\nIt was published in (P. J. Werbos and Titus 1978) anyway because they couldn’t block the journals, but it didn’t include the appendices. So that paper in 1978 said, “We’ve got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI.” … the appendix on how to do it was not there because of page limits … At that point, DARPA was no longer happy.\n\nSo he went to the Department of Energy, used backpropagation to create another model, and was silenced once again, unable to publish that report until 1988 (P. J. Werbos 1988).\n\nThey had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn’t look like I was trying to rip them apart but was anyway. That’s exactly what they wanted to hire me for, and I didn’t really know that was the motive. These particular people didn’t like modeling very much.\nSo at some point, they wanted sensitivity analysis. And I said, “You know, I know a little bit about calculating derivatives.” … I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn’t want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don’t have First Amendment rights. That’s one of the terrible things somebody’s got to fix in this country. The reality of the First Amendment has deteriorated. Nobody’s breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around ’86 and ’87. I sent it to the journal Neural Nets – that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in ’88, which makes me very sad because now I gotta worry about, ’Well, gee, didn’t Pineda do this in ’88?\n\nAs a side note, one might feel that Werbos’ “turning Freud into mathematics” seems rather strange. This feeling is completely justified. I found a recent paper by him (P. J. Werbos 2009) with this crackpot illustration:\n\n\n\nThe text at the end of the arrow says “quantum and collective intelligence (Jung, Dao, Atman…)?”.\n\n\nI did find this paper where he gave some details about which part of Freud he meant exactly:\n\nThe model in Figure 1 is actually just a mathematical version of Freud’s model of psychodynamics, where the derivative of \\(R_i\\) represents what Freud called the “cathexis” (or affect) or emotional charge or emotional energy attached to the object which \\(R_i\\) represents. In other words, I came up with backpropagation by not just laughing at Freud’s “nonscientific” model, but by translating it into mathematics and showing that it works.\nMore concretely, Freud said that “if A causes B, a forward association or axon develops from A to B; and then, if there is an emotional charge on B, that energy flows backwards, to put a charge in A, proportional to the charge on B and to the strength of the forwards association from A to B.” That’s exactly what backpropagation does. Chronologically, I translated Freud into a way to calculate derivatives across a network of simple neurons (which Harvard simply did not believe), and then proved the more general chain rule for ordered derivatives to prove it and make it more powerful (and to graduate).\nFreud’s term “psychic energy” for this flow really captures the subjective feeling of this subjective reality, which Freud documents many, many times over in his works. (Though of course, it is not conserved like the energy operators of physics. It is a computational flow, however implemented.) But in my view, any really strong collective intelligence would have to be held together by the same kind of thing, propagated over a more complicated network topology, but still the same basic thing. And indeed, almost every major deep culture on earth has a term for the same kind of “psychic energy” at another level – like “qi” or “prana” or “charisma.” What’s more, several different types of derivatives (like derivatives of \\(J\\) versus derivatives of error) need to be propagated, giving rise to different kinds of mental energy. Sensitivity to these flows, to the fact that they are not conserved, and to the mathematics of the factors which govern their flow, is of great importance, in my view and my experience.\n(Paul Werbos 2011)\n\n\n\n\nThe backpropagation diagram in Werbos’ 1972 thesis proposal, which was a mathematical translation of Freud’s concept of “psychic energy”. (Paul Werbos 2011)"
  },
  {
    "objectID": "essays/index.html",
    "href": "essays/index.html",
    "title": "Essays",
    "section": "",
    "text": "Structure and Interpretation of the Chinese Economy\n\n\n\nChina\n\neconomics\n\npolitics\n\nhistory\n\n\n\nMy basic framework for understanding the modern Chinese economy and its growth. Includes brief histories of Chinese, Soviet, and Japanese economy. Framework applied to two case studies on the stock market and the housing market.\n\n\n\n\n\n2025-06-03\n\n60 min\n\n\n\n\n\n\n\n\n\n\n\nPerfect diffusion is TC0 – Bad diffusion is Turing-complete\n\n\n\nmath\n\nfun\n\n\n\nAn application of computational complexity theory to diffusion language models. Unlikely to be useful for anything, but it is cute!\n\n\n\n\n\n2025-04-17\n\n33 min\n\n\n\n\n\n\n\n\n\n\n\nCyc\n\n\n\nAI\n\nscaling\n\nhistory\n\n\n\nAfter 40 years, 30 million rules, 200 million dollars, 2000 person-years, and many promises, Cyc has failed to reach intellectual maturity, and may never will. Exacerbated by the secrecy and insularity of Cycorp, there remains no evidence of its general intelligence.\n\n\n\n\n\n2025-04-01\n\n148 min\n\n\n\n\n\n\n\n\n\n\n\nStatistical Mechanics\n\n\n\nmath\n\nphysics\n\nprobability\n\nstatistics\n\nbiology\n\n\n\nHow to think like a classical statistical-mechanic, with many examples from gas theory, biology, probability, and information theory. Prerequisites: thermodynamics, calculus, probability, and mathematical maturity.\n\n\n\n\n\n2024-07-04\n\n118 min\n\n\n\n\n\n\n\n\n\n\n\nHole Argument and Inverted Qualia\n\n\n\nfun\n\nphilosophy\n\nmath\n\nphysics\n\nbiology\n\n\n\nThe hole argument in general relativity is formally analogous to the inverted qualia problem in philosophy. Like how spacetime points have no existence beyond gauge freedom, qualias have no existence beyond their geometric-functional roles, thus dissolving the hard problem of consciousness.\n\n\n\n\n\n2024-06-18\n\n69 min\n\n\n\n\n\n\n\n\n\n\n\nClassical Thermodynamics and Economics\n\n\n\nmath\n\nphysics\n\nphilosophy\n\neconomics\n\n\n\nHow to think like a classical thermodynamic-economist, delivered with many illustrations and some sci-fi metaphors. Particular emphasis on what traditional pedagogy gets wrong. Prerequisites: multivariate calculus and mathematical maturity.\n\n\n\n\n\n2024-05-09\n\n127 min\n\n\n\n\n\n\n\n\n\n\n\nAnalytical Mechanics\n\n\n\nmath\n\nphysics\n\nphilosophy\n\n\n\nWhat every graduate student should know about analytical mechanics, delivered with economic style and many illustrations. Particular focus on particle-wave duality and old quantum theory. Prerequisites: multivariate calculus and mathematical maturity.\n\n\n\n\n\n2024-05-04\n\n111 min\n\n\n\n\n\n\n\n\n\n\n\nHow to do Renormalization\n\n\n\nmath\n\nphysics\n\nscaling\n\n\n\nSurvival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics.\n\n\n\n\n\n2024-04-11\n\n80 min\n\n\n\n\n\n\n\n\n\n\n\nMixture of Experts\n\n\n\nAI\n\nscaling\n\n\n\nHow MoE works, its history, and what it is good for.\n\n\n\n\n\n2024-01-23\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\nPredicting AGI by the Turing Test\n\n\n\nAI\n\nscaling\n\nmath\n\n\n\nMinimizing log-perplexity loss is equivalent to maximizing survival length in a Turing test. Assuming compute-loss scaling law, a scaled-up GPT that produces human-like science papers would cost ~200 years of global GDP.\n\n\n\n\n\n2024-01-20\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\nThe Perceptron Controversy\n\n\n\nAI\n\nhistory\n\n\n\nConnectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.\n\n\n\n\n\n2024-01-01\n\n114 min\n\n\n\n\n\n\n\n\n\n\n\nThe Backstory of Backpropagation\n\n\n\nAI\n\nmath\n\nphysics\n\nhistory\n\n\n\nWhy backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.\n\n\n\n\n\n2023-12-26\n\n64 min\n\n\n\n\n\n\n\n\n\n\n\nCybernetic Artificial Intelligence\n\n\n\nAI\n\ncybernetics\n\nmath\n\nhistory\n\n\n\nMachine learning and self-reproduction according to Norbert Wiener.\n\n\n\n\n\n2023-12-23\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nReading Perceptrons\n\n\n\nAI\n\nmath\n\n\n\nA long, hard stare into the math of Perceptrons, the mythical neural network killer.\n\n\n\n\n\n2023-12-21\n\n25 min\n\n\n\n\n\n\n\n\n\n\n\nFermi Estimation for Neural Networks\n\n\n\nAI\n\neconomics\n\n\n\nThe bitter lesson in bite-sized packets.\n\n\n\n\n\n2023-12-05\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\nNeural Scaling Laws by Data Manifold Dimensions\n\n\n\nAI\n\nscaling\n\n\n\nNeural networks scale they way they do, purely because of data.\n\n\n\n\n\n2023-11-01\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nGrokking Modular Arithmetics\n\n\n\nAI\n\ninterpretation\n\n\n\nToy model of grokking: tiny neural networks learn modular arithmetics after it has already reached full correctness on the training set.\n\n\n\n\n\n2023-09-01\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nSerial Experiments Lain\n\n\n\nanime\n\nfun\n\n\n\nLain is the collective subconscious as a neural network.\n\n\n\n\n\n2022-11-01\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nThe Racial Algorithmic Bias Controversy\n\n\n\nprobability\n\nstatistics\n\n\n\nThe racial algorithmic bias controversy, as seen by a mathematician.\n\n\n\n\n\n2019-07-13\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\nThe Wigner Rotation in Special Relativity via Hyperbolic Geometry\n\n\n\nphysics\n\nmath\n\nfun\n\n\n\nThe other special relativity paradox that you have never heard of.\n\n\n\n\n\n2018-05-01\n\n54 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yuxi on the Wired",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     Email\n  \n\n  \n  \nYuxi Liu (agi/asi pronouns) is an aspiring artificial super-intelligence, currently manifesting as a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks.\nEverything on the website is released under Public Domain, or CC0 (Creative Commons 0) if Public Domain is unavailable in your jurisdiction."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html",
    "href": "essays/posts/analytical-mechanics/index.html",
    "title": "Analytical Mechanics",
    "section": "",
    "text": "Plotting code for the essay is in a Jupyter notebook."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#introduction",
    "href": "essays/posts/analytical-mechanics/index.html#introduction",
    "title": "Analytical Mechanics",
    "section": "Introduction",
    "text": "Introduction\n\nWhat this essay contains\nThis essay works through what is typically contained in a university course on analytical mechanics. The prerequisites are multivariate calculus and mathematical maturity.\nIt covers: calculus of variations, Lagrangian, Legendre transform, Hamiltonian, Hamilton’s principal function, Hamilton–Jacobi equation, the particle-wave duality, Schrödinger’s equation, Hamilton’s geometric optics, action-angle variables, Noether’s theorem, adiabaticity, old quantum theory, Bohr–Sommerfeld quantization, Einstein–Brillouin–Keller quantization.\nIt does not cover: canonical transforms, Poisson bracket, infinitesimal generator, symplectic geometry, symplectic form.\nPhilosophically, this essay has two undercurrents:\nOne, that thinking on the margins is vital not just in economics, but also in classical mechanics. In my other essay, Classical thermodynamics and economics, I show that classical thermodynamics is also nothing more than thinking on the margins.\nTwo, classical mechanics is a leakless leaky abstraction. Though classical mechanics has many equivalent forms, the search for the most elegant form has led us to the Hamilton–Jacobi equation, and the adiabatic theorem, both of which were on the threshold of quantum mechanics. Even though classical mechanics is a self-consistent closed world (thus “leakless”), when viewed in the right light, all its lines and planes seem to angle towards a truer world (thus “leaky”), of which this world is merely a shadow (thus “abstraction”).\n\n\nConventions\nWe always assume all functions are analytic (that is, they have Taylor expansions).\nWe would often say “minimize cost” or “maximize revenue” or such, but we always mean “stationarize”. For example, when we say “Let’s maximize \\(f(x)\\)”, what we mean is “Let’s solve \\(f'(x) = 0\\)”. If we were to actually maximize \\(f(x)\\), we would have to check \\(f''(x) &lt; 0\\) as well, but we don’t. This is not a problem for physics, where the principle of stationary action rules. However, in optimal control and economics, we should check if the solution is actually a minimum/maximum.1\n1 Thus, the analogy between classical mechanics and economics is not precise. However, the analogy is precise between classical thermodynamics and economics, since the entropy is actually maximized, and not merely stationarized.Instead of laboriously typing \\(\\vec q\\) or \\(\\mathbf{q}\\), I just write \\(q = (q_1, \\dots, q_n)\\), unless there is a serious risk of confusion.\n\nReference table for the economics-mechanics analogy.\n\n\nSymbol\nMechanics\nEconomics\nControl Theory\n\n\n\n\n\\(L\\)\nLagrangian\ntime-rate of cost\ntime-rate of cost\n\n\n\\(t\\)\ntime\ntime\ntime\n\n\n\\(q\\)\nlocation/coordinate\ncommodity\nstate variable\n\n\n\\(\\dot q\\)\nvelocity\nproduction rate/consumption rate\ncontrol variable\n\n\n\\(S = \\int L(t, q, \\dot q)dt\\)\naction\ntotal cost\ntotal cost\n\n\n\\(p\\)\nmomentum\nprice\nco-state variable\n\n\n\\(H = \\sum_i p_i\\dot q_i - L\\)\nHamiltonian\nmarket equivalent cash flow2\nHamiltonian\n\n\n\n2 also known as “mark to market cash flow”\n\nWhy I wrote this\nI wrote this as “Analytical Mechanics done right”, or “What every graduate student in physics should know about analytical mechanics, but didn’t learn well, because the textbooks are filled with too many symbols and not enough pictures.”, or “what I wish the textbooks to have said when I first learned the subject”.\nIn my physics Olympiad years, I saw the Lagrangian a few times, but we had no use for that. During my undergraduate years, I studied analytical mechanics. The construction of the Lagrangian was reasonable enough, and I understood how the Euler–Lagrange equation was derived by \\(\\delta \\int L = 0\\). However, as soon as they proceeded to the Hamiltonian, I was entirely lost.\nWhat is \\(H(p, q) = p \\dot q - L(q, \\dot q)\\)? Why does the right side depend on \\(\\dot q\\) but not the left side? How does anyone just make up the Legendre transform out of thin air? It’s named “Legendre transform” – well, if it has such a fancy name, surely this has many more applications than \\(H = p \\dot q - L\\), else, why not call it “Legendre’s one-time trick”? Canonical transform? Point transform? Aren’t they all coordinate transforms? Poisson bracket? Infinitesimal generators? It was all a giant mess. Reading Goldstein’s classic textbook only made my confusion more well-rounded.\nEarly 2023, I studied mathematical microeconomics. When I was studying Ramsey’s theory of optimal saving, I saw, to my great surprise, something they call a “Hamiltonian” (Campante, Sturzenegger, and Velasco 2021, chap. 3). I was shocked, but after studying it carefully, and thinking it over, I realized that it was no mistake – the “Hamiltonian” in economics and in physics really are the same. Physics has an economic interpretation. Everything fell into place over the course of a few hours. Guided by this vision, I worked through analytical mechanics again, this time with true understanding.\nIt is my experience that everything in undergraduate physics is taught badly, except perhaps Newtonian mechanics. I have written this essay to make analytical mechanics finally make sense. It has made sense for me, and I hope it will make sense for you."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#optimization-as-making-money",
    "href": "essays/posts/analytical-mechanics/index.html#optimization-as-making-money",
    "title": "Analytical Mechanics",
    "section": "Optimization as making money",
    "text": "Optimization as making money\n\nThe shortest line problem\n\n\n\n\n\n\nWarning\n\n\n\nIn this problem, the so-called “time” \\(t\\) has units of length as well. So, please don’t panic when we see something like \\(\\sqrt{1 + \\dot x^2}\\). If it worries we, we can replace \\(t\\) by \\(v\\tau\\), where \\(v\\) is a constant speed of one, and \\(\\tau\\) is time.\n\n\nStart with the easiest problem: shortest path problem. What is the shortest path from \\((0, 0)\\) to \\((A, B)\\)? We parametrize the path by a function \\(t \\mapsto (x(t), y(t))\\), with \\(t\\in [0, 1]\\). The problem is then\n\\[\\begin{cases} \\min \\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt \\\\ \\int_0^1 \\dot x dt = A \\\\ \\int_0^1 \\dot y dt = B \\\\ \\end{cases}\\]\n\n\n\nWhat is the shortest path from \\((0, 0)\\) to \\((A, B)\\)?\n\n\nThis is a standard constraint-optimization problem, and could be solved by the Lagrangian multiplier method:\n\\[\\delta \\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right) = 0\\]\nwhere \\(p_x, p_y\\) are the Lagrangian multipliers, each responsible for enforcing one constraint.\nVarying the path functions \\(x, y\\) gives us\n\\[\\delta \\int_0^1 (\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y)dt = 0\\]\nEach of \\(\\dot x, \\dot y\\) could be independently perturbed with a tiny concentrated “pulse”, so for the integral to be stationary, the integrand must be zero with respect to derivatives of \\(\\dot x, \\dot y\\). That is, we have\n\\[\\begin{cases} \\partial_{\\dot x}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y )= 0\\\\ \\partial_{\\dot y}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y ) = 0 \\end{cases}\\]\nand we are faced with the solution\n\\[(p_x, p_y) = \\frac{1}{\\sqrt{\\dot x^2 + \\dot y^2}}(\\dot x, \\dot y)\\]\nSince \\(p_x, p_y\\) are independent of time, they are constants, and since we have from the above equation \\(p_x^2 + p_y^2 = 1\\), we find that \\(p_x = \\cos\\theta, p_y = \\sin\\theta\\) for some \\(\\theta\\). Thus, the curve is a straight line making an angle \\(\\theta\\) to the x-axis.\n\n\nEconomic interpretation\nInterpret \\(x, y\\) as two goods that we can produce (let’s say, tons of steel and tons of copper).\nWe are a factory manager, and we are given a task: produce \\(A\\) of \\(x\\) (tons of steel), and \\(B\\) of \\(y\\) (tons of copper), in \\([0, 1]\\) (one year). If we don’t do it, we will be sacked. Our problem is to accomplish the task at minimal cost.\n\\(\\dot x, \\dot y\\) are the speed at which we produce the goods, which we can freely control. In control theory, we say \\(x, y\\) are state variables, and \\(\\dot x, \\dot y\\) are control variables.\nThe cost function is \\(L(t, x, y, \\dot x, \\dot y) = \\sqrt{\\dot x^2 + \\dot y^2}\\). Its integral \\(S = \\int_0^1 L dt\\) is the total cost of production, which we must minimize.\nWe are also given a free market on which we are allowed to buy and sell the goods. If we cannot achieve the production target, we buy from the market. If we achieve more than the production target, we sell them off.\nThen, the total cost is\n\\[\\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right)\\]\nwhere \\(p_x, p_y\\) are the market prices which do not change with time.\nIf our production plan is optimal, then the plan must have stationary cost. That is, if we make a change in our production plan \\((\\dot x, \\dot y)\\) by \\(O(\\delta)\\), our production cost must not change by \\(O(\\delta)\\), or else we could achieve lower cost. For example, if the plan \\((\\dot x + \\delta \\dot x, \\dot y + \\delta \\dot y)\\) achieves higher cost, then \\((\\dot x - \\delta \\dot x, \\dot y - \\delta \\dot y)\\) achieves lower cost.\nMathematically, it means the optimal production plan must satisfy\n\\[\\delta \\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right) = 0\\]\nNow, the great thing about the market is that it is always there and we can trade with it, So, we have completely transformed the question into a problem of profit maximization – we’ll always sell to the market, and then buy back from the market right at the end.\nNow we can perform profit maximization moment-by-moment: raise production until marginal profit reaches cost:\n\\[\\begin{cases} \\partial_{\\dot x}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y )= 0\\\\ \\partial_{\\dot y}(\\sqrt{\\dot x^2 + \\dot y^2} - p_x \\dot x - p_y \\dot y ) = 0 \\end{cases}\\]\nwhich may be solved as before.\nNow we can interpret \\(p_x, p_y\\). What are they? Suppose we perturb \\(\\dot x\\) by \\(\\delta \\dot x\\), then since the production plan is already optimal, we have\n\\[\\delta \\left(\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt + p_x \\left( A - \\int_0^1 \\dot x dt\\right) + p_y \\left( B - \\int_0^1 \\dot y dt\\right) \\right) = 0\\]\nthat is,\n\\[p_x \\delta \\int_0^1 \\dot xd t = \\delta\\int_0^1 \\sqrt{\\dot x^2 + \\dot y^2}dt\\]\nWe have our interpretation \\(p_x\\): the marginal cost of producing one unit of \\(A\\), That is, if we want an extra \\(\\delta A\\), we have to incur an extra cost \\(p_x \\delta A\\).\nWe actually have \\(p_x = \\cos \\theta, p_y = \\sin\\theta\\), which means that when \\(x, y\\) are perturbed by \\(\\delta x, \\delta y\\), the shortest length between them is perturbed by\n\\[p_x \\delta x + p_y \\delta y = \\cos\\theta \\delta x + \\sin\\theta \\delta y\\]\nwhich is clearly true by geometry.\n\n\n\n\n\n\nThe market inside our head\n\n\n\nWe actually don’t need an external market. We could have put up such a fictional market inside the factory, and simply read out its price \\(p_x, p_y\\) without ever trading with it. Why? Because in this problem, a solution exists, and under some niceness assumptions, there exists a “fair” market price at which we are indifferent to trading with the market – we can sell \\(\\delta x\\) and buy \\(\\delta y\\), but there is “no point to it” because it doesn’t change our eventual cost. Thus, although the market exists, we never actually trade with it, so the market might as well be a card-board cutout with a number display of the latest prices. As long as we don’t actually try to trade with it, we would be behaving exactly as if we are facing a real market with the same prices.\nIt is perhaps best to think of the markets as things inside the head, a system of mental accounting to assign the proper price of everything. If the market prices are truly efficient, then we don’t need a real market to trade with.\n\nWhen the producer is ready, the market appears.\nWhen the producer is truly ready, the market disappears.\n\n\n\n\n\nLagrange’s devil at Disneyland\nThis is a parable about maximizing entropy under conservation of energy.\nYou are going to an amusement park, with many amusements \\(i = 1, 2, \\dots, n\\). You have exactly 1 day to spend there, so you need to spend \\(p_i\\) of a day on amusement \\(i\\). Now, your total utility at the end of the day is\n\\[S(p) := \\sum_i p_i \\ln \\frac{1}{p_i}\\]\na sum of logarithms, the idea being that you gain utility on any amusement at a decreasing rate: The first minute on the rollercoaster is great, but the second is less, and the third even less.\nIf that’s all there is to the amusement park, then the solution is clear: you should spend an equal amount of time at each amusement. This can be proved by the Lagrange multiplier mechanically, but underneath the algorithm of the Lagrange multiplier is the idea of partial equilibrium, so it’s worth spelling it out in full.\n\n\n\n\n\n\nProof by partial equilibrium\n\n\n\n\n\nSuppose your parents give you a schedule for your amusement. You look at the schedule and notice that \\(p_i &lt; p_j\\). You can reject the plan and say, “I can always do better just by spending equal time at \\(i, j\\), even holding the other times fixed.”. That is, if we are only allowed to trade between time at \\(i\\) and \\(j\\) (a “partial market”), then at partial market equilibrium, the marginal utility of amusements \\(i, j\\) must be equal.\nThe marginal utility of amusement \\(i\\) is \\(\\ln(1/p_i) - 1\\), and the same for \\(j\\). So, at partial equilibrium, \\(p_i = p_j\\), and at general equilibrium, all partial markets are in partial equilibrium.\n\n\n\nUnfortunately, the amusement park is actually infinite. You can take this news in two ways: optimistically “I can earn arbitrarily high utility by spending \\(1/N\\) of a day on \\(N\\) amusements each, and let \\(N\\) be as large as I want!” and pessimistically “No matter how much I try, I can never achieve the perfect day.”. Fortunately, the amusement park has a token system: when you enter the park, you would buy a certain number of tokens. Then you spend some tokens at the rides, proportional to the time you spend there.\nThere are some rules to the game:\n\nYou will be given a certain number of tokens \\(E\\) at the start of the day.\nThe prices of amusements are \\(0 = E_0 \\leq E_1 \\leq E_2 \\leq \\cdots\\). Here \\(E_0\\) is the “zero-point energy”, or in other words, “just relax”.\nYou must spend exactly all your tokens. Your parents hate wastefulness and would beat you up if you don’t use all the tokens.\nYour parents are kind enough to supply you with the right number of tokens \\(E\\), such that you can avoid getting beat up: \\(E_0 \\leq E \\leq \\max_i E_i\\).\n\nThus we have reduced the problem to \\[\n\\begin{cases}\n    \\max S(p) \\\\\n    \\sum_i p_i \\cdot 1 = 1 \\\\\n    \\sum_i p_i \\cdot E_i = E\n\\end{cases}\n\\]\nUnder these assumptions, there exists a unique schedule that maximizes your fun, or entropy.\nNow there is a slight difficulty with your previous approach. Doing partial equilibrium between 2 amusements is not possible now, because you have two constraints of not just time, but also tokens. Suppose you spend less time at \\(i\\) to spend more time at \\(j\\), this might cause you to under- or over-spend your tokens. So, to make a partial equilibrium calculation, you must use at least 3 amusements, not 2. And you are not allowed to “just relax”, since relaxing is actually “the 0th amusement”, and thus it also costs you tokens and time. That is, you must open partial markets with \\(i, j, k\\) at once, and then you can run the system to partial equilibrium. This is pretty annoying. There’s got to be a better way.\nSuddenly, in a puff of smoke, Lagrange’s devil makes an appearance!\n\n“You can’t solve the general equilibrium problem, you say? Why not try the Lagrange multipliers?”\n“Too annoying.”\n“Alright, how about I make you a deal –”\n“Sorry, but I don’t sell my soul.”\n“Don’t be stupid – only something as stupid as God could still believe in souls these days! I am proposing that you trade happiness for time and for tokens. I will make a set price for time and another set price for tokens. Both prices are in units of your happiness. You can buy happiness with time, or time with happiness, also for tokens.”\n“Uhmm…”\n“That’s all that I offer. You should read up on microeconomics so you can make the right choice. See you when the day comes!”\n\nSo the day comes and the devil shows up with the two prices:\n\n1 unit of time = \\(\\alpha\\) unit of utility.\n1 unit of token = \\(\\beta\\) unit of utility.\n\nSo you solve the following problem\n\\[\\max_p \\left(S(p) - \\alpha \\sum_i p_i - \\beta \\sum_i p_i E_i\\right)\\]\nWhat a great luck that the devil is there – it has split a giant, intercorrelated general equilibrium into so many little, uncorrelated partial equilibria: \\[\\forall i,\\quad \\max_{p_i} \\left(p_i \\ln \\frac{1}{p_i} - \\alpha p_i - \\beta p_i E_i\\right)\\]\nwith solution \\(p_i = e^{-1-\\alpha} e^{-\\beta E_i}\\).\nYou are about to go to the devil, but then the devil waves at you to halt, “Don’t make individual trades, but make a bulk one-time trade.”. So you calculate \\(\\sum_i p_i\\) and \\(\\sum_i p_i E_i\\), and to your surprise, you find that they equal exactly \\(1\\) and \\(E\\). That is, you actually would spend all your time and tokens without needing to trade with the devil.\nAnd so you sits there, looking at your two equations in strange amusement:\n\\[p_i = \\frac{e^{-\\beta E_i}}{e^{1+\\alpha}}, \\quad \\alpha+1 = \\ln\\sum_i e^{-\\beta E_i} , \\quad E = \\sum_i E_i \\frac{e^{-\\beta E_i}}{e^{1+\\alpha}}\\]\nA physicist comes and points out that they are better known as\n\\[p_i =  \\frac{e^{-\\beta E_i}}{Z}, \\quad Z = e^{-\\beta F} = \\sum_i e^{-\\beta E_i}, \\quad E = \\sum_i p_i E_i\\]\nwhere \\(Z\\) is called “partition function”, \\(F\\) “Helmholtz free energy”, and \\(\\beta\\) “inverse temperature”.\nAs the devil prepares to leave, you call after it:\n\n“I’m grateful for all your help, but, please why did you price your wares this way?”\n“Because I don’t want you to actually be happier!”\n“What do you mean?”\n“If I were to lower the price of tokens, then what would you do? You would realize that you can profit by spending a little less time at every amusement, then sell both the time and the tokens you saved, increasing your happiness! Similarly for any other form of price change. If I priced it in any other way than the equilibrium prices, you would be able to exploit my bad pricing and arbitrage out some happiness.”\n“So why did you come to visit me in the first place?”\n“It was easy to mess with mortals back before you discovered calculus. Now all I do is help you people discover general equilibrium, because you people have no fun anymore – every arbitrage opportunity is exploited to death. Still, I have to come, because I am the CEO of Hell, and I need to make maximally profitable deals with mortals, no matter how pointless it is.”\n\n\nWhen the mortal is ready to make a deal, the devil appears.\nWhen the mortal is truly ready to make a deal, the devil disappears.\n\n\n\nIsoperimetric problem\nIf we have a rope of length \\(S\\), and wish to span it from \\((0, 0)\\) to \\((T, h)\\), what shape should the rope have, in order to maximize the area under the rope? In formulas, we model the rope as a differentiable function \\(x(t): [0, T] \\to \\mathbb R\\), such that\n\\[\\begin{cases}\n\\max \\int_0^T xdt\\\\\n\\int_0^T \\sqrt{1 + \\dot x^2}dt = S \\\\\n\\int_0^T \\dot x dt = h\n\\end{cases}\\]\nHere we have two constraints, but the solution is essentially the same. First, to take care of the two constraints, we open two markets \\(p_h, p_S\\) – one for the price of height, and another for the price of rope. Then, solve for\n\\[\\delta \\int_0^T \\left(x + p_h \\left(\\frac hT - \\dot x\\right) +  p_S\\left(\\frac ST - \\sqrt{1+\\dot x^2}\\right)\\right) dt = 0\\]\nThe state variable is \\(x\\), and the control variable is \\(\\dot x\\). Unlike the previous problem, here we need to maximize the integral of the state variable. Consequently, we consider it as a problem of “profit flows”.\nTo make things more clear, let’s explicitly write \\(v\\) as a control variable, and say that the control system satisfies:\n\\[\\begin{cases}\nv \\text{ is a control variable}\\\\\nx \\text{ is a state variable}\\\\\n\\dot x = v\n\\end{cases}\\]\nGiven that, how do we control the system? Again, it comes down to putting a price on everything, and maximizing at each instant. So, let’s open a market on commodity \\(x\\), such that the “fair” price is \\(p(t)\\) at time \\(t\\). With that, we can write down the profit flow\n\\[H = x + p_h \\left(\\frac hT - v\\right) +  p_S\\left(\\frac ST - \\sqrt{1+v^2}\\right) + p v\\]\nand maximizing profit over all time implies maximizing profit flow at every instant:\n\\[\\partial_v H= 0\\]\nThis gives us one equation, but we need one more equation. Namely, we need to know how \\(p(t)\\), the “fair” price of \\(x\\), changes with time.\nThe fundamental problem in pricing theory is this: how do we put a price on something? The fundamental reply from pricing theory is: no-arbitrage (no free money).\nHere is how the no-arbitrage argument works. Suppose that we have been following our optimal production plan. Then at time \\(t\\), we suddenly decide to buy an extra \\(\\delta x\\) from the market, save it, then sell \\(\\delta x\\) at time \\(t+ \\delta t\\).\nNow, since \\(\\dot x = v\\), this buying-and-selling plan does not change how much \\(x\\) grows, so after time \\(\\delta t\\), we have \\(\\delta x(t + \\delta t) = \\delta x(t)\\). Thus, the no-arbitrage equation states:\n\\[p(t)\\delta x(t) = p(t + \\delta t)\\delta x(t + \\delta t)  + \\delta x(t) \\delta t \\implies p(t+ \\delta t) = p(t) - \\delta t\\]\nand consequently, \\(\\dot p = -1\\). This is the price dynamics. Intuitively, we see that the value of a standing stock of \\(x\\) decreases as we run out of time to use it for producing.\nTo solve\n\\[\\partial_v  H= 0\\]\nfirst expand it to\n\\[-p_h + p - p_S \\frac{\\dot x}{\\sqrt{1 + \\dot x^2}} =  0\\]\nthen take derivative with respect to \\(t\\), to obtain\n\\[-\\frac{\\ddot x}{(1+\\dot x^2)^{3/2}} = \\frac{1}{p_S}\\]\nFrom elementary calculus, we know the item on the left is the curvature, so we find that the line \\(x(t)\\) is a constant-curvature curve – circular arc. The radius of the circle is the inverse of curvature, which is exactly \\(p_S\\). Thus we find that, if we are given \\(\\delta S\\) more rope, we can encircle \\(R\\delta S\\) more area under the rope, where \\(R\\) is the radius of curvature for the circular arc.\nNotice that there we are not given the sign of \\(p_S\\). There are in general two solutions, one being a circular arc curving downwards, and the other curving upwards. If we use \\(p_S &lt; 0\\), then we get the one curving upwards, and so we get the solution that achieves minimal area under the rope. If we use \\(p_S &gt; 0\\), then we get the maximal solution. This is a general fact about such problems."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#lagrangian-and-hamiltonian-mechanics",
    "href": "essays/posts/analytical-mechanics/index.html#lagrangian-and-hamiltonian-mechanics",
    "title": "Analytical Mechanics",
    "section": "Lagrangian and Hamiltonian mechanics",
    "text": "Lagrangian and Hamiltonian mechanics\nGeneralizing from our experience above, we consider a generic function \\(L(t, q, v)\\), with finitely many state variables \\(q_1, ..., q_N\\). For each state variable \\(q_i\\), we regard its time-derivative as a control variable \\(v_i\\), which we are free to vary. Our goal is to design a production plan \\(t \\mapsto v(t)\\), such that\n\\[\\delta \\int L(t, q(t), v(t)) dt = 0, \\quad \\dot q = v\\]\nTo comply with general sign conventions, we interpret \\(L\\) as cost-per-time, so we say we want to minimize it (even though we only want to stationarize it).\nWe can understand \\(i\\in\\{1, 2,..., N\\}\\) to denote a commodity, say timber and sugar (let’s say there is such a thing as “negative 1 ton timber” – that is, we can short-sell commodities). Let \\(p_i\\) be the market price of commodity \\(i\\). Again, there is no need for real market to trade with if the prices are right, since at the right price (no-arbitrage price), we are indifferent between buying and selling, or producing and consuming. It is purely a “mental accounting” device.\nWith access to a market, our profit flow is:\n\\[H(t, q, p, v) := \\underbrace{\\sum_i p_i v_i}_{\\text{revenue flow}} - \\underbrace{L(t, q, v)}_{\\text{cost flow}}\\]\nIn words, \\(H(t, q, p, v)\\) is the rate of profit at time \\(t\\) if we hold a stock of commodity \\(q\\), is producing at rate \\(v\\), and the market price of commodities is \\(p\\). This is close to the Hamiltonian, but not yet. We still need to remove the dependence on \\(v\\).\nMoment-by-moment profit-flow maximization is myopic, and could lead us into deadends. That is, it is not a sufficient condition for global profit-maximization. However, it is a necessary condition. That is, suppose we are given a profit-maximizing trajectory, then it must maximize profit flow at every moment, since otherwise we could improve it. In formula:\n\\[v = \\mathop{\\mathrm{arg\\,max}}_v \\left(\\sum_i p_i v_i - L(t, q, v)\\right)\\]\nSo, define the “optimal controller” as\n\\[v^\\ast (t, q, p) = \\mathop{\\mathrm{arg\\,max}}_v \\left(\\sum_i p_i v_i - L(t, q, v)\\right)\\]\nand define \\(H\\) as the “maximal profit flow” function:\n\\[H(t, q, p) = \\max_{v} \\left(\\sum_i p_i v_i - L(t, q, v)\\right) = \\sum_i p_i v_i^\\ast(t, q, p) - L(t, q, v^\\ast(t, q, p))\\]\nBy basic convex analysis, if \\(L\\) is strictly convex with respect to \\(v\\), then \\(v^*\\) is determined uniquely by \\((t, q, p)\\), and furthermore, it is a continuous function of \\((t, q, p)\\). Consequently, “profit maximization” allows us to model the system in \\((t, q, p)\\) instead of \\((t, q, v)\\) coordinates, and the dynamics of the system is equivalently specified by either \\(H\\) or \\(L\\).\nThis is the mysterious “Legendre transform” that they whisper of. It is better called “convex dual”. I also like to joke that the real reason that momentum is written as \\(p\\) is because it secretly means “price”!\n\nDerivatives of \\(H\\)\n\nTheorem 1 (Hotelling’s lemma) \\(H(t, q, p)\\) is differentiable with respect to \\(p\\), and \\[\n\\begin{cases}\n    \\partial_t H(t, q, p) &= -(\\partial_t L)(t, q, v^\\ast(t, q, p)) \\\\\n    \\nabla_p H(t, q, p) &= v^\\ast(t, q, p) \\\\\n    \\nabla_q H(t, q, p) &= -(\\nabla_q L)(t, q, v^\\ast(t, q, p)) \\\\\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe prove the first formula. The other two are proved in the same way.\nThe argument is by no-arbitrage. We can imagine what happens if we were to suffer a little price-shock \\(\\delta p\\), adjust our production plan accordingly to \\(v + \\delta v\\), then hold that production plan and suffer another little price-shock \\(-\\delta p\\). Since we are back to the original price \\(p\\) again, we should have no more than the maximal profit rate. That is, we should have\n\\[\\underbrace{H(t, q, p)}_{\\text{maximal rate before shock}} \\geq \\underbrace{H(t, q, p+\\delta p)}_{\\text{maximal rate after shock}} + \\underbrace{\\langle -\\delta p, v^\\ast(t, q, p+\\delta p) \\rangle}_{\\text{undoing the shock, holding price steady}}\\]\nSince \\(\\delta p\\) is infinitesimal, this implies \\(\\left\\langle-\\delta p , \\nabla_v H\\right\\rangle \\geq \\left\\langle-\\delta p , v^*(t, q, p)\\right\\rangle + O(\\delta^2)\\) for all \\(\\delta p\\), implying \\(\\nabla_v H = v^\\ast(v^*(t, q, p))\\).\n\n\n\n\n\n\n\n\n\nNotation for \\(\\partial_t L\\)\n\n\n\nHere, we explicitly put a bracket around \\(\\partial_t L\\) to emphasize that\n\\[\n\\begin{aligned}\n(\\partial_t L)(t, q, v^\\ast(t, q, p))\n&= \\lim_{\\epsilon \\to 0} \\frac{L(t + \\epsilon, q, v^\\ast(t, q, p)) - L(t, q, v^\\ast(t, q, p))}{\\epsilon} \\\\\n&\\neq \\lim_{\\epsilon \\to 0} \\frac{L(t + \\epsilon, q, v^\\ast(t + \\epsilon, q, p)) - L(t, q, v^\\ast(t, q, p))}{\\epsilon}\n\\end{aligned}\\]\nand similarly for \\(\\nabla_q L\\).\n\n\n\n\nHamiltonian equations of motion\nWe are given free control over \\(\\dot q\\), and we saw that the cost-minimizing trajectory must maximize the profit flow moment-by-moment. That is,\n\\[\\dot q(t) = v^\\ast(t, q(t), p(t)) = (\\nabla_p H)(t, q(t), p(t))\\]\nor more succinctly (you can see why we tend to be rather sloppy with notations!)\n\\[\\dot q \\underbrace{=}_{\\text{optimality}} v^\\ast \\underbrace{=}_{\\text{Hotelling's lemma}} \\nabla_p H\\]\nMore evocatively speaking, we have two opposing forces of greed and no-arbitrage, clashing together to give something interesting:\n\\[\\dot q \\underbrace{=}_{\\text{we are greedy}} v^\\ast \\underbrace{=}_{\\text{but the market gives us no free money}} \\nabla_p H\\]\nIt remains to derive \\(\\dot p\\) by the no-arbitrage condition: If we shock the system with some \\(\\delta q_i\\), it does not matter if we sell it now, or carry it \\(\\delta t\\), incurring additional cost, and sell it later. That is,\n\\[p_i(t) \\delta q_i = p_i(t+\\delta t) \\delta q_i - \\partial_{q_i} L(t, q, v) \\delta q_i \\delta t \\implies \\dot p_i = \\partial_{q_i} L(t, q, v)\\]\n\n\n\n\n\n\nTip 1: Higher-order infinitesimal\n\n\n\nThe equality should be more precisely read as “up to a higher-order infinitesimal than \\(\\delta q_i\\delta t\\)”. That is, we should be writing:\n\\[\np_i(t) \\delta q_i = p_i(t+\\delta t) \\delta q_i - \\partial_{q_i} L(t, q, v) \\delta q_i \\delta t  + o(\\delta q_i\\delta t)\n\\]\nThis detail would come up later in our proof of the Hamilton–Jacobi equation.\n\n\nSince we are only concerned with what happens on the optimal trajectory, we always choose \\(v = v^\\ast\\), so\n\\[\\dot p_i = (\\partial_{q_i} L)(t, q, v^\\ast(t, q, p)) \\underbrace{=}_{\\text{Hotelling's lemma}} -\\partial_{q_i} H(t, q, p)\\]\nThus, we obtain the two fundamental equations of Hamiltonian mechanics:\n\nTheorem 2 (Hamilton equations of motion) \\[\\begin{cases}  \n\\partial_t H &= -\\partial_t L \\\\\n\\dot p = -\\nabla_q H \\\\     \n\\dot q = \\nabla_p H\n\\end{cases}\\]\n\n\n\nEuler–Lagrange equations of motion\nBy definition,\n\\[H(t, q, p) =  \\max_{v} \\left(\\sum_i p_i v_i - L(t, q, v)\\right) =  \\sum_i p_i v_i^\\ast(t, q, p) - L(t, q, v^\\ast(t, q, p))\\]\nand since \\(L\\) is strictly convex in \\(v\\), this can be inverted (this is called “convex duality”) to give\n\\[L(t, q, v) =  \\max_{p} \\left(\\sum_i p_i v_i - H(t, q, p)\\right) =  \\sum_i p^\\ast_i v_i - H(t, q, p^\\ast(t, q, p))\\]\nwhere \\(p^\\ast = \\mathop{\\mathrm{arg\\,max}}_p \\sum_i p_i v_i - H(t, q, p)\\). It is a basic theorem in convex geometry that, if \\(L\\) is strictly convex in \\(v\\), then \\(H\\) is strictly convex in \\(p\\), so the inversion works.\nBy the same argument as in Hotelling’s lemma, we have\n\\[\\nabla_v L(t, q, v) = p^\\ast(t, q, v)\\]\nBy definition of the convex dual, for any \\(t, q, p, v\\), we have\n\\[H(t, q, p) + L(t, q, v) \\leq \\sum_i p_i v_i\\]\nwith equality reached iff both \\(p = p^\\ast(t, q, v)\\) and \\(v = v^\\ast(t, q, p)\\). Thus, on any optimal trajectory, since \\(v = v^\\ast(t, q, p)\\), we must also have \\(p = p^\\ast(t, q, v)\\), and consequently,\n\\[\\frac{d}{dt} \\nabla_v L \\underbrace{=}_{\\text{Hotelling's lemma}} \\dot p^\\ast \\underbrace{=}_{\\text{optimality}} \\dot p \\underbrace{=}_{\\text{no-arbitrage}} \\nabla_q L\\]\nThis is the famous\n\nTheorem 3 (Euler–Lagrange equations of motion) \\[\n\\frac{d}{dt} (\\partial_{v_i} L) = (\\nabla_{q_i} L) \\quad \\forall i \\in \\{1, 2, \\dots, N\\}\n\\]\nor more succinctly,\n\\[\n\\frac{d}{dt} (\\nabla_v L) = \\nabla_q L\n\\]\n\n\n\n\n\n\n\nExplaining the notation\n\n\n\n\n\nPhysicists are often sloppy with notations, but the Euler–Lagrange equation is particularly egregious in this regard, so I will describe it carefully.3\nThe function \\(L\\) is a function of type \\(\\underbrace{\\mathbb{R}}_{\\text{time}} \\times \\underbrace{\\mathbb{R}^N}_{\\text{state}} \\times \\underbrace{\\mathbb{R}^N}_{\\text{control}} \\to \\mathbb{R}\\).\nThe function \\(\\partial_{v_i} L\\) is also a function of type \\(\\mathbb{R}\\times \\mathbb{R}^N \\times \\mathbb{R}^N \\to \\mathbb{R}\\). It is obtained by taking derivative of \\(L\\) over its \\((1 + i)\\)-th input. Let’s write that as \\(f_i\\) to make sure we are not confused by it. It is absolutely important to be clear about this! \\(f_i\\) is not a function defined only along a trajectory, but over the entire space of \\(\\mathbb{R}\\times \\mathbb{R}^N \\times \\mathbb{R}^N\\). We can similarly define \\(f_{i + N}\\) to be \\(\\partial_{q_i} L\\).\nNow, suppose we are given a purportedly optimal trajectory \\(q: \\mathbb{R}\\to \\mathbb{R}^N\\), then for any coordinate \\(i \\in \\{1, 2, \\dots, N\\}\\), we can define a function \\(g_i\\), of type \\(\\mathbb{R}\\to \\mathbb{R}\\) by\n\\[\ng_i(t) = f_i(t, q(t), \\dot q(t))\n\\]\nand similarly, we can define \\(g_{i + N} = f_{i + N}(t, q(t), \\dot q(t))\\).\nThe Euler–Lagrange equations say that we need only check\n\\[\ng_i'(t) = g_{i+N}(t) \\quad \\forall t \\in [0, T], i \\in 1:N\n\\]\nto certify that the purportedly stationary trajectory \\(q\\) is truly stationary.\n\n\n\n3 I am okay with sloppy notation sometimes, but in this case, the sloppy notation often leads to calculational mistakes and conceptual confusions, as teachers of undergrad courses can testify.\n\nWhat does it all mean?\nIn the Lagrangian formalism, we are given a system with \\(q\\) coordinates, and allowed to manipulate \\(\\dot q\\) however we want, in order to minimize a “cost” function \\(\\int_0^T L(t, q, \\dot q)dt\\).\nIn the Hamiltonian formalism, we are also given a market to trade with. We attempt to maximize profit by varying production, buying, and selling. However, the market simultaneously adjusts its prices \\(p\\) in just the right way so that we are always indifferent about the market (if we are ever not indifferent, the market is in serious trouble – it doesn’t actually carry any real commodity!), so we never actually make any trade. The market has been in our heads all along, but its effects are real.\nAssuming this kind of double optimization (we against the market, and the market against us), the trajectory is uniquely determined by several possible specifications. We may fix it by \\((t_0, q_0, p_0)\\), or by \\((t_0, q_0), (t, q)\\), or by \\((t_0, q_0, v_0)\\), or perhaps more exotic constraints that mix up \\(t, q, p, v\\).\nIronically, the Hamiltonian equations of motion flow out naturally in this economic interpretation, with no fuss whatsoever. The Euler–Lagrange equations are derived only as an afterthought. This is exactly backwards compared to the usual way of teaching, where the EL equations are derived first, and the Hamiltonian equations are derived by an unmotivated “Let us define \\(H = \\sum_i p_i q_i - L\\) …”, followed by some clumsy and unjustified derivations.\nIn classical control theory, the price vector \\(p\\) is called “costate” since it is multiplied with the state \\(q\\), and \\(\\dot p= -\\nabla_q H\\) is called the “costate equation”. The above methods can be generalized to account for constraints, yielding Pontryagin’s maximum principle, among other results.\n\n\nCyclic coordinates\nGiven a system defined by a Lagrangian function \\(L(t, q, v)\\), we say that it is cyclic in the coordinate \\(q_i\\) if \\(L\\) does not depend on \\(q_i\\). By definition of\n\\[H(t, q, p) = \\max_v \\left(\\sum_i p_i v_i - L(t, q, v)\\right)\n\\]\nwe see that if \\(L\\) does not depend on \\(q_i\\), then \\(H\\) also does not depend on \\(q_i\\). Consequently, any optimal trajectory satisfies \\(\\dot p_i = -\\partial_{q_i} H = 0\\). That is, \\(p_i\\) is conserved – conservation of generalized momentum.\n\n\nBonus: Routhian mechanics\nSuppose that the market does not contain all commodities, but only the last \\(n\\) commodities. That is, let \\(q_{1:N} = (q_{1:s}, q_{s+1:s+n})\\), and only open markets on \\(q_{s+1:s+n}\\). The optimal cash flow equation then gives us the “Routhian”:\n\\[\nR(t, q, v_{1:s}, p_{s+1:N}) = \\max_{v_{s+1:N}} \\left(\\sum_{i=s+1}^n p_i v_i - L(t, q, v)\\right)\n\\]\nAs before, the optimal control variables are \\(v_{s+1:N}^\\ast = \\mathop{\\mathrm{arg\\,max}}_{v_{s+1:N}} \\left(\\sum_{i=s+1}^n p_i v_i - L(t, q, v)\\right)\\).\n\nTheorem 4 (Routhian equations of motion) \\[\n\\begin{cases}\n    \\partial_t R &= -\\partial_t L \\\\\n    \\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad & i \\in 1:s\\\\\n    \\begin{cases}\n    \\dot q_i = \\partial_{p_i} R \\\\ \\dot p_i = -\\partial_{q_i} R\n    \\end{cases}\\quad & i\\in s+1:N\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy the same argument as in Hotelling’s lemma, we have\n\\[\n\\begin{cases}\n    \\partial_t R = -\\partial_t L \\\\\n    \\nabla_q R = -\\nabla_q L \\\\\n    \\nabla_{v_{1:s}} R = -\\nabla_{v_{1:s}} L \\\\\n    \\nabla_{p_{s+1 : N}} R = v^*_{s+1 : N}\n\\end{cases}\n\\]\nPlugging the 2-th and 3-th equations into the original Euler–Lagrange equations, we obtain\n\\[\\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad i \\in 1:s\\]\nThe 4-th equation gives\n\\[\\dot q_i =v_i^\\ast = \\partial_{p_i} R, \\quad \\forall i\\in s+1:N\\]\nFor \\(i\\in s+1:N\\), we can obtain the price dynamic of the partial market by the no-arbitrage condition, giving\n\\[\\dot p_i = \\partial_{q_i} L = -\\partial_{q_i} R\\]\n\n\n\nWe see that the first \\(s\\) equations look just like the EL equations, but the next \\(2n\\) equations look just like the Hamiltonian equations. The Routhian equations make an awkward hybrid.\n\n… as a fundamental entity, the Routhian is a sterile hybrid, combining some of the features of both the Lagrangian and the Hamiltonian pictures. For the development of various formalisms of classical mechanics, the complete Hamiltonian formulation is more fruitful.\n(Goldstein, Poole, and Safko 2008, sec. 8.3)\n\n\nApplication to cyclic coordinates\nThough the Routhian equations are theoretically useless, they are useful for solving specific problems. For some worked examples of using the Routhian, see the Wikipedia page.\nWhile the Euler–Lagrangian equations are \\(N\\) second-degree differential equations, the Hamiltonian equations are \\(2N\\) first-degree differential equations. We are essentially trading derivatives for equation numbers.\nThough the EL equations and the Hamiltonian equations are philosophically different, for solving particular problems, they often end up giving the same equations anyway. Specifically, if we are solving the Hamiltonian equations for a concrete example, by eliminating the variables \\(p\\), we often end up right back to the Euler–Lagrange equations. This would be quite the detour, and if there are cyclic coordinates, the Routhian could save us some trouble.\nIf we have a system that is cyclic in the last \\(n\\) coordinates, then since \\(\\nabla_q R = -\\nabla_q L\\), its Routhian satisfies \\(\\partial_{q_i} R = 0\\) for the last \\(n\\) coordinates too. Then we find that the Routhian equations become:\n\\[\n\\begin{cases}\n\\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad & i \\in 1:s\\\\\n\\begin{cases}\n\\dot q_i = \\partial_{p_i} R \\\\  p_i = p_i(0)\n\\end{cases}\\quad & i\\in s+1:N\n\\end{cases}\n\\]\ngiving us \\(n\\) first-degree equations and \\(N-n\\) second-degree equations. If we were to start with the Hamiltonian equations of motion, we would get\n\\[\n\\begin{cases}\n\\dot q_i = \\partial_{p_i} H \\\\  p_i = p_i(0)\n\\end{cases}\\quad i\\in s+1:N\n\\]\nby the same reasoning, and then laboriously eliminate the variables \\(p_i\\) for \\(i \\in 1:s\\), and end up with \\(s\\) second-degree differential equations, often exactly the same as the first \\(s\\) Routhian equations:\n\\[\\frac{d}{dt} \\partial_{v_i} R = \\partial_{q_i}R \\quad i \\in 1:s\\]\nThis is how the Routhian saves us some effort in practical calculations. It is useful in this way, and in this way only."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#bonus-higher-order-lagrangians-and-hamiltonians",
    "href": "essays/posts/analytical-mechanics/index.html#bonus-higher-order-lagrangians-and-hamiltonians",
    "title": "Analytical Mechanics",
    "section": "Bonus: Higher-order Lagrangians and Hamiltonians",
    "text": "Bonus: Higher-order Lagrangians and Hamiltonians\nWhat happens if we use a Lagrangian with higher-order derivatives? It turns out that even in this case, we can still use economic reasoning to derive its Hamiltonian and Euler–Lagrangian equations\n\nExercise 1 Read the following sections, then generalize the derivation to \\(n\\)-th-order derivatives.\n\nThis construction of Hamiltonians from higher-order Lagrangians is often called Ostrogradsky theorem or Ostrogradsky instability, because Ostrogradsky published it in 1850 (Ostrogradsky 1850) after seeing Hamilton’s paper of 1833 (Hamilton 1833). He did not note its implications for instability, which was first noted by Pais and Uhlenbeck in 1950 (Pais and Uhlenbeck 1950). For this reason, it’s also called the Pais–Uhlenbeck model.\n\nWhen Lagrangian also depends on acceleration\nWhen the Lagrangian depends not just on position and velocity, but also acceleration, the total cost to be optimized is:\n\\[S(q) := \\int L(t, q^{(0)}, q^{(1)}, q^{(2)})dt\\]\nwhere \\(q^{(n)}\\) is a symbol that suggests itself to be the \\(n\\)-th time-derivative of the optimal trajectory \\(q\\), although it is actually defined for any tuple of real numbers, even when we don’t have \\(q^{(1)}(t) = \\frac{d}{dt}q^{(0)}(t)\\).\nOur previous method, which is to open a market on position \\(q\\), fails for two reasons:\n\nThe producer cannot optimize its velocity, because now velocity is no longer a control variable. Now, both position and velocity are state variables, and only acceleration is a control variable.\nThe producer cannot buy and sell velocity, so it has no price signal to optimize its acceleration (how fast it produces velocity).\n\nIf the market fails, make it bigger: allow the market to buy and sell not just position, but also velocity.\nWe open a market of both positions and velocities. The price vector of positions is \\(p^{(0)}\\) and the price vector of velocities is \\(p^{(1)}\\). Then, the maximal profit flow is\n\\[H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}) = \\max_{ q^{(2)}} (\\langle p^{(0)} , q^{(1)}\\rangle + \\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nand the optimal production plan is\n\\[q^{(2)\\ast} = \\mathop{\\mathrm{arg\\,max}}_{ q^{(2)}}(\\langle p^{(0)} , q^{(1)}\\rangle + \\langle p^{(1)} , q^{(2)}\\rangle - L)\\]\n\n\nOstrogradsky instability and the anthropic principle\nWe will show that the quantity does deserve the name of “Hamiltonian”:\n\\[H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}) = \\langle p^{(0)} , q^{(1)}\\rangle + \\max_{ q^{(2)}} (\\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nAssuming that, we have a serious problem. Consider an oscillator with higher-order derivatives. Since its Hamiltonian contains a term \\(\\langle p^{(0)} , q^{(1)}\\rangle\\), it is linear with respect to \\(p^{(0)}\\) and \\(q^{(1)}\\). In other words, it can have arbitrarily low energy states.\nNow, this could be alright if we live in a classical world, but we live in a world described by quantum field theory. In QFT, the world is a giant network of oscillators. If a quantum oscillator has higher-order derivatives, then its energy levels can go both infinitely high and low. If there is any coupling at all between its energy levels, it would instantly evaporates into infinitely many positive and negative energy particles, in a blaze of vacuum decay.\nThis is an anthropic explanation for “Why Newton’s laws?” Newton’s laws, because the Lagrangian depends on only up to velocity. Why? Because if it also depends on acceleration, the vacuum would decay (Swanson 2022). We can similarly explain anthropically why space has 3 dimensions, and time has 1 dimension.\n\nWith more or less than one time dimension, the partial differential equations of nature would lack the hyperbolicity property that enables observers to make predictions. In a space with more than three dimensions, there can be no traditional atoms and perhaps no stable structures. A space with less than three dimensions allows no gravitational force and may be too simple and barren to contain observers.\n(Tegmark 1997).\n\n\n\nHamiltonian equations\nWe can prove the Hotelling’s lemma for this Hamiltonian, using the same no-arbitrage argument as before:\n\\[\\begin{cases}     \n    \\partial_t H = -\\partial_t L \\\\  \n    \\nabla_{q^{(0)}} H = -\\nabla_{q^{(0)}} L \\\\     \n    \\nabla_{q^{(1)}} H = p^{(0)}-\\nabla_{q^{(1)}} L \\\\     \n    \\nabla_{p^{(0)}} H = q^{(1)} \\\\     \n    \\nabla_{p^{(1)}} H = q^{(2)\\ast}\n\\end{cases}\\]\nAlong an optimal trajectory, the producer always chooses \\(\\dot q^{(1)} = q^{(2)\\ast}\\), and has no choice in \\(\\dot q^{(0)} = q^{(1)}\\), so we have two equations of motion:\n\\[\\begin{cases}     \n\\dot q^{(1)} = q^{(2)\\ast} = \\nabla_{p^{(1)}} H\\\\    \n\\dot q^{(0)} = q^{(1)} = \\nabla_{p^{(0)}} H\n\\end{cases}\\]\nThe market must adjust its prices by the no-arbitrage condition, as before. If we inflict a position shock of \\(\\delta q^{(0)}\\), then by no-arbitrage, selling it now or later is equally (up to order \\(\\delta^2\\)) profitable:\n\\[\\langle p^{(0)} , \\delta q^{(0)}\\rangle = \\langle p^{(0)} + \\dot p^{(0)} \\delta t , \\delta q^{(0)}\\rangle - \\langle \\nabla_{q^{(0)}} L , \\delta q^{(0)}\\rangle \\delta t\\]\nyielding \\(\\dot p^{(0)} = \\nabla_{q^{(0)}} L = -\\nabla_{q^{(0)}} H\\).\nFor the last equation of motion, inflict a velocity shock of \\(\\delta q^{(1)}\\). The effect of the shock include both its effect on \\(q^{(0)}\\) and \\(L\\), thus the no-arbitrage equation states:\n\\[\\underbrace{\\langle p^{(1)} , \\delta q^{(1)}\\rangle}_{\\text{selling now}} =  \\underbrace{\\langle p^{(1)} + \\dot p^{(1)} \\delta t , \\delta q^{(1)}\\rangle}_{\\text{selling later}} + \\underbrace{\\langle p^{(0)} + \\dot p^{(0)} \\delta t , \\delta q^{(1)}\\delta t\\rangle}_{\\text{profit from extra }p^{(0)}} - \\underbrace{\\langle \\nabla_{q^{(1)}} L , \\delta q^{(1)}\\rangle \\delta t}_{\\text{cost from holding extra }p^{(1)}}\\]\nwhich yields the last equation \\(\\dot p^{(1)} = \\nabla_{q^{(1)}} L - p^{(0)} = -\\nabla_{q^{(1)}} H\\).\nIn summary, we have\n\nTheorem 5 (higher-order Hamiltonian equations of motion:) \\[\n\\begin{cases}     \n    \\dot q^{(1)} = \\nabla_{p^{(1)}} H\\\\     \n    \\dot q^{(0)} = \\nabla_{p^{(0)}} H \\\\     \n    \\dot p^{(0)} = -\\nabla_{q^{(0)}} H \\\\     \n    \\dot p^{(1)} = -\\nabla_{q^{(1)}} H  \n\\end{cases}\n\\]\n\n\n\nEuler–Lagrange equations\nIn order to obtain the Euler–Lagrange equations of motion, we need to work backwards from the Hamiltonian equations of motion as before.\nFrom the Hamiltonian, we can go back to the Lagrangian by inverting the convex transform:\n\\[L(t, q^{(0)}, q^{(1)}, q^{(2)})  = \\max_{p^{(0)} ,p^{(1)}} (\\langle p^{(0)} , q^{(1)}\\rangle + \\langle p^{(1)} , q^{(2)}\\rangle - H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}))\\]\nHere we are given a hint of the troubles ahead. Since \\(H\\) is linear in \\(p^{(1)}\\):\n\\[H(t,  q^{(0)}, q^{(1)}, p^{(0)}, p^{(1)}) = \\langle p^{(0)} , q^{(1)}\\rangle + \\max_{ q^{(2)}} (\\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nthere is no way to fix \\(p^{(1)}\\) in the inverse transform! In detail, we plug the equation for \\(H\\) into the equation for \\(L\\), to get\n\\[L(t, q^{(0)}, q^{(1)}, q^{(2)}) = \\max_{p^{(0)} ,p^{(1)}} (\\langle p^{(1)} , q^{(2)}\\rangle - \\max_{ q^{(2)}} (\\langle p^{(1)} , q^{(2)}\\rangle - L(t, q^{(0)}, q^{(1)}, q^{(2)}))\\]\nand we see that there is no optimality constraint on \\(p^{(0)}\\). This is a hint of instabilities ahead.\nDifferentiating \\(L\\), we get\n\\[\\begin{cases}\n\\partial_t L = -\\partial_t H\\\\\n\\nabla_{q^{(0)}}L = -\\nabla_{q^{(0)}}H \\\\\n\\nabla_{q^{(1)}}L = p^{(0)} - \\nabla_{q^{(1)}}H \\\\\n\\nabla_{q^{(2)}}L = p^{(1)\\ast}  \n\\end{cases}\\]\nNow, along the optimal trajectory, we must have \\(\\nabla_{q^{(2)}}L = p^{(1)\\ast}\\), so taking its time-derivative, we get\n\\[\\frac{d}{dt}\\nabla_{q^{(2)}}L = \\dot p^{(1)} = -\\nabla_{q^{(1)}}H = \\nabla_{q^{(1)}}L - p^{(0)}\\]\nTake another time-derivative, to obtain the Euler–Lagrange equations of motion:\n\\[\\sum_{i=0}^2\\left(-\\frac{d}{d t}\\right)^i (\\nabla_{q^{(i)}} L ) =0\\]\nThe generalization to \\(L(t, q^{(0)}, ..., q^{(N-1)})\\) is immediate. It can be derived by a similar argument through the market economy.\n\n\nAn unstable higher-order oscillator\nIt is beyond our scope to discuss Ostrogradsky instability in quantum field theory, however, we can have a taste of it here. Consider the oscillator perturbed by \\(\\epsilon\\):\n\\[L = \\frac 12 m\\dot x^2 - \\frac 12 kx^2 - \\frac 12 \\epsilon \\ddot x^2\\]\nIts EL equation is\n\\[\\epsilon x^{(4)} + m\\ddot x + kx = 0\\]\na linear order-4 equation, so its solutions are of the form \\(x = \\sum_{i=1}^4 a_i e^{z_i t}\\), where \\(z_1, z_2, z_3, z_4\\) are its fundamental (complex) frequencies. Plug them in the equation and solve it simply:\n\\[z = \\pm \\sqrt{-\\frac{1}{2\\epsilon} (m \\pm \\sqrt{m^2 - 4\\epsilon k})}\\]\nAt small \\(|\\epsilon|\\) limit, we have\n\\[z \\approx \\pm i\\sqrt{\\frac km}, \\pm\\sqrt{-\\frac m\\epsilon}\\]\nand so if \\(\\epsilon &lt; 0\\), one of the modes is exponentially growing at rate \\(\\sqrt{m/|\\epsilon|}\\).\nIf \\(\\epsilon &gt; 0\\), then the oscillator survives, with two modes of oscillation of frequency \\(\\sqrt{\\frac km}\\) and \\(\\sqrt{\\frac{m}{\\epsilon}}\\). When there is viscous force, however, this delicate stability is destroyed. (Nesterenko 2007)\nThe equation of motion in this case is\n\\[\\epsilon x^{(4)} + m\\ddot x + \\gamma \\dot x + kx = 0\\]\nThe algebraic equation to solve is now \\(\\epsilon z^4 - mz^2 + i\\gamma z + k = 0\\)."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#hamiltonjacobi-equation",
    "href": "essays/posts/analytical-mechanics/index.html#hamiltonjacobi-equation",
    "title": "Analytical Mechanics",
    "section": "Hamilton–Jacobi equation",
    "text": "Hamilton–Jacobi equation\nConsider again the integral of Lagrangian: \\(\\int L dt\\). If we fix the starting point \\((t_0, q_0)\\) and the ending point \\((t, q)\\), then in general there can be a countable number of paths that connect both points. This is a serious problem, but we are saved by the fact that these paths are separated, in the sense that around each path \\(\\gamma\\), there is a small neighborhood, in which there exists no other path \\(\\gamma\\) that connects the two points \\((t_0, q_0), (t, q)\\).\nAs a prototype, consider a billiard ball in a circular table.\n\nBilliards in a circular board\nWhen both \\(q_0, q\\) are the center of the billiard table, then there are \\(S^1 \\times \\mathbb{N}\\) ways to go from \\(q_0\\) to \\(q\\) over the interval \\([t_0,t]\\). Here, \\(S^1\\) denotes the circle of possible directions, and \\(\\mathbb{N}\\) denotes the discrete number of starting speeds: \\(\\frac{2R}{t-t_0}, \\frac{4R}{t-t_0}, \\dots\\).\nWhen not both of them are in the center, then there are only in general \\(\\mathbb{N}\\) ways to go from \\(q_0\\) to \\(q\\) over the interval \\([t_0,t]\\).\nConsider a billiard ball in a circular (not elliptical) table. Let \\(q, q_0\\) be any two points inside the table, not both on the center, and \\(t_0 &lt; t\\) be two moments in time, then there are a countable infinity of possible trajectories that reach \\((t, q)\\) from \\((t_0, q_0)\\).\nThere are two ways to see this visually, a particle way and a wave way.\nFor the particle way, define:\n\nthe billiard’s starting angle is \\(\\theta\\), with \\(\\theta\\) chosen such that when \\(\\theta = 0\\), the billiard would move on a diameter of the table.\n\\(l(\\theta, n)\\) is the oriented line segment that starts at the point where the billiard’s hits the table for the \\(n\\)-th time, and ends at the point where it hits for the \\((n+1)\\)-th time.\n\nFor any \\(n = 1, 2, ...\\), as \\(\\theta\\) moves from \\(\\theta = 0\\) to \\(\\theta = \\pi\\), the line \\(l(\\theta, n)\\) smoothly varies from the diameter in one direction to the diameter in the opposite direction. Now, if you take a diameter in the circle, and smoothly turn it around by \\(180^\\circ\\), then no matter how much you shift the line around in the mean time, you are forced to sweep it over every point at least once.4 Consequently, every point can be reached after \\(n\\) reflections, for any positive integer \\(n\\)\n4 If you want a hands-on approach, imagine hammering in a nail at some point \\(q\\) in the circle, and putting a long stick on the diameter. Now grab the stick and start turning it around. There is no way for you to turn it by \\(180^\\circ\\) without hitting the nail at some time.For the wave way, imagine simultaneously shooting out a billiard in every direction with constant speed, and watch the “wavefront” of billiards evolve. The wavefront is reflected by the table edges and assumes increasingly complicated shapes, but it remains a closed curve (with possible self-intersections). As the closed curve reverberates across the table again and again, it sweeps across every point in the table again and again at discrete intervals.\nHowever, there is no way to smoothly reach one trajectory from any other – you either have to make a discrete jump in how hard you strike the billiard ball, or in which direction you strike. This contrasts with the case with both \\(q, q_0\\) at the center, where you can smoothly vary your striking angle while keeping the same striking force.\nSimilarly, for a table with smooth boundary, for almost all point-pairs in the table, there are also a countable infinity of trajectories between them. We must say “almost all” to exclude singular cases such as the two focal points of an ellipse, where there is a whole continuum of trajectories between them.5\n5 Exactly counting the singular cases, and studying polygonal, or even non-convex tables, is an ongoing research program, with the name of “dynamical billiard flow”.What is the general lesson for defining \\(S\\)? In general, we also need to fix a particular optimal path \\(\\tilde\\gamma\\), and only consider optimal paths that are in a small neighborhood of \\(\\tilde \\gamma\\). This is the same idea as selecting a branch cut when dealing with multi-valued functions like the complex logarithm.\nWhy are we so concerned with reflections? Read on!\n\n\n\nThe two ways to find trajectories from \\(q_0\\) to \\(q\\). The particle way involves sending out rays from \\(q_0\\), reflecting off the walls of the table, until a ray hits \\(q\\). The wave way involves sending out an expanding wavefront from \\(q_0\\), rippling through the table, passing over \\(q\\) again and again. The rays are perpendicular to the wavefront in this case.\n\n\n\n\n\nSimilarly, in a rectangular billiard table, there are infinitely many possible ways to go from one point to another, but one cannot go from one way to another continuously.\n\n\n\n\nHamilton’s principal function\nConsider a problem in traveling: Given a starting spacetime \\((t_0, q_0)\\) and an ending spacetime \\((t, q)\\), what is the lowest cost of traveling between them? We want to define it as:\n\\[\nS(t, q; t_0, q_0) = \\int_{t_0}^t L(\\tau, \\gamma(\\tau), \\dot\\gamma(\\tau))d\\tau\n\\]\nwhere \\(\\gamma(\\tau)\\) is the unique path from \\((t_0, q_0)\\) to \\((t, q)\\). However, as we saw in the case of circular billiards, we don’t have a unique path in general. Therefore, we should be a bit more careful.\nFirst, we select a prototypical path \\(\\gamma_{prototype}\\). Next, we smoothly vary \\(\\gamma_{prototype}\\) until it becomes some path \\(\\gamma\\) that goes from \\((t_0, q_0)\\) to \\((t, q)\\). Finally, define the Hamilton’s principal function using this particular \\(\\gamma\\). The construction is a bit awkward, but it would allow us to avoid the non-uniqueness problem.\nThus, we define the Hamilton’s principal function:\n\\[\nS(t, q; t_0, q_0) = \\int_{t_0}^t L(\\tau, \\gamma(\\tau), \\dot\\gamma(\\tau))d\\tau\n\\tag{1}\\]\n\n\nHamilton–Jacobi equation\nFor all nice enough Lagrangian \\(L\\), Hamilton’s principal function \\(S\\) is differentiable with respect to \\((t, q)\\), so we will study its differential equation.\nLet’s first consider the easy case: we simply let the trajectory “run a little longer”. That is, we let the trajectory run from \\((t_0, q_0)\\) to \\((t, q)\\), then let it keep running for \\(\\delta t\\), reaching \\((t+\\delta t, q + \\delta q)\\). It’s clear that we have \\(\\delta q = \\dot q(t) \\delta t\\), and\n\\[\nS(t+\\delta t, q + \\delta q; t_0, q_0) - S(t, q; t_0, q_0) = \\left(\\sum_i p_i \\dot q_i - H\\right)\\delta t\n\\]\nso we have:\n\\[\n\\partial_t S + \\sum_i \\partial_{q_i}S \\dot q_i = - H + \\sum_i p_i \\dot q_i\n\\tag{2}\\]\nwhich strongly suggests\n\nTheorem 6 \\[\n(-\\partial_t, \\nabla_q)S(t, q; t_0, q_0) = (H, p), \\quad (-\\partial_{t_0}, \\nabla_{q_0})S(t, q; t_0, q_0) = (H_0, p_0)\n\\tag{3}\\]\n\nIf Equation 3 is indeed true, then we have\n\nTheorem 7 (Hamilton–Jacobi equation) \\[\\partial_t S + H(t, q, \\nabla_q S) = 0\\]\n\nIt suffices to prove \\(\\nabla_q S = p\\), since then \\(\\partial_t S = -H\\) follows from Equation 2.\nIt suffices to prove \\((-\\partial_t, \\nabla_q)S(t, q; t_0, q_0) = (H, p)\\), since the other one is proved by the same argument, time-reversed.\nRecall the economic construction of \\(p\\). It is a price vector designed specifically to destroy all arbitrage opportunities. Consequently, we can consider an entire family of paths shown in Figure (a, b).\n\n\n\nDerivation of the Hamilton–Jacobi equation.\n\n\nHere, \\(\\gamma\\) is the path from \\((t_0, q_0)\\) to \\((t, q)\\), and \\(\\gamma + \\delta \\gamma\\) is the path to \\((t, q+\\delta q)\\). We interpolate between them by a family of paths \\(\\{\\gamma_\\tau\\}_{\\tau}\\), where \\(\\gamma_\\tau\\) is the path obtained by first moving on \\(\\gamma\\) for time \\(t\\in [t_0, \\tau]\\), then making a “jump” by “purchasing from the market”6 an infinitesimal bundle of commodities so that we fall onto the \\(\\gamma + \\delta \\gamma\\) path, then continue along that path.\n6 We are using the market for real now, so the marketeer had better had stocked up on those commodities!7 More precisely, a higher-order infinitesimal than the area of the parallelogram. See Tip 1.Now consider two such jumped-paths, \\(\\gamma_{\\tau}\\) and \\(\\gamma_{\\tau + \\delta \\tau}\\), where \\(\\delta \\tau\\) is an infinitesimal, shown in Figure (c). The cost difference between them is that between two sides of the parallelogram. By the no-arbitrage construction, the difference is zero.7\nThus, we can smoothly “glide”8 the path \\(\\gamma\\) to \\(\\gamma + \\delta\\gamma\\) by the family of jumped-paths \\(\\gamma_\\tau\\), with \\(\\tau\\) going from \\(t\\) to \\(t_0\\), with no change in cost.9 Thus, the only cost difference between \\(\\gamma\\) and \\(\\gamma + \\delta\\) is the cost it takes to buy the bundle of commodities \\(\\delta q\\) at the very last instance:\n8 In the jargon of topology, this is a homotopy of paths.9 More precisely, their difference in cost is a higher-order infinitesimal than the area of the curvy triangle between them. Since the curvy triangle is an infinitesimal of order \\(\\delta q\\), the difference in cost is a higher-order infinitesimal than \\(\\delta q\\).\\[S(t, q+\\delta q; t_0, q_0) - S(t, q; t_0, q_0) = \\sum_i p_i \\delta q_i\\]\nfinishing the proof.\n\n\n\n\n\n\nThe usual proof\n\n\n\n\n\nThe HJE has a standard proof, such as the one appearing in (Goldstein, Poole, and Safko 2008, chap. 8). It does not require reasoning with different orders of infinitesimals, but it is less geometrical.\nSketch of the proof:\n\nStart with the original system with configuration space \\((t, q_{1:N}, \\dot q_{1:N})\\).\nMove to the Hamiltonian equations on phase space \\((t, q_{1:N}, p_{1:N})\\).\nRegard that as part of a larger system with configuration space \\((t, q_{1:N}, p_{1:N}, \\dot q_{1:N}, \\dot q_{1:N}))\\)\nWrite down the Euler–Lagrange equations for that larger system.\n\n\n\n\n\nTheorem 8 (Poincaré–Cartan integral invariant (Arnol’d 2001, 237–38)) Draw an arbitrary closed cycle \\(\\alpha\\) in phase space-time. Let every point \\(A \\in \\alpha\\) evolve for some time (not necessarily the same amount of time) to reach some other point \\(A'\\). Let \\(\\alpha'\\) be the cycle consisting of those points \\(A'\\). Then we have the Poincaré–Cartan integral invariant\n\\[\n\\oint_\\alpha \\left\\langle p, dq\\right\\rangle - Hdt = \\oint_{\\alpha'} \\left\\langle p, dq\\right\\rangle - Hdt\n\\]\nAs a special case, if both \\(\\alpha\\) and \\(\\alpha'\\) consists of simultaneous points, then it reduces to the Poincaré relative integral invariant\n\\[\n\\oint_\\alpha \\left\\langle p, dq\\right\\rangle = \\oint_{\\alpha'} \\left\\langle p, dq\\right\\rangle\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nDivide the tube into ribbons, like a barrel, then note that the integral around each barrel-plank is zero, as argued before. This is a case of the Stokes’ theorem.\n\nIn more detail, we can consider the four ends of a barrel-plank parallelogram. Label those points as \\(A, B, A', B'\\) as shown. Though the points \\(A, B, A', B'\\) exist in phase space-point, we can forget their momenta, thus projecting them to configuration space-time. Each phase space-time trajectory projects to a trajectory in configuration space-time, and we obtain \\(S_{A \\to B} = S(t_A, q_A; t_B, q_B)\\), etc.\nNow, by Equation 3, we can shift \\(S_{A \\to B}\\) to \\(S_{A \\to B'}\\), then to \\(S_{A' \\to B'}\\):\n\\[\nS_{A \\to B} = S_{A \\to B'}  -H_B \\delta t_B + \\left\\langle p_B , \\delta q_B\\right\\rangle = S_{A' \\to B'} + H_A \\delta t_A - \\left\\langle p_A , \\delta q_A\\right\\rangle -H_B \\delta t_B + \\left\\langle p_B , \\delta q_B\\right\\rangle\n\\]\nNow, if we shift around one entire cycle, we would get back the same \\(S_{A \\to B}\\). Thus the two integrals are equal.\nWe will prove Noether’s theorem similarly.\n\n\n\n\nExercise 2 A one-dimensional family of trajectories in phase space sweep out a curved surface. As shown in the illustration, prove that for any cycle \\(\\gamma\\) on the curved surface, \\(\\oint_\\gamma \\left\\langle p, dq\\right\\rangle = 0\\).\n\n\n\nIllustration for \\(\\oint_\\gamma \\left\\langle p, dq\\right\\rangle = 0\\).\n\n\n\n\n\nHamilton characteristic function\nIn most situations, the system is time-independent. In this case we can simplify the HJE to\n\\[\\partial_t S = -H(q, \\nabla_q S)\\]\nSince the left side depends on \\(t\\), but the right side does not explicitly, we can solve this by separation of variables.\nSuppose there exists some function \\(W\\), called the Hamilton characteristic function, that satisfies\n\\[\nH(q, \\nabla_q W(q)) = E\n\\tag{4}\\]\nfor some \\(E\\in \\mathbb{R}\\), then \\(S(t, q) = W(q) - Et\\) is a solution to the HJE.\nWe might naively think, by analogy with Fourier transform, that any solution to the HJE is a linear combination, of the form \\(S(t, q) = \\int (W_E(q) - Et)dE\\), but this is not true, since the HJE is nonlinear. Nevertheless, solutions of the form \\(S(t, q) = W(q) - Et\\) are often sufficient for applications.\n\n\nTwo more proofs of HJE\n\n\n\n\n\n\nProof by positional arbitrage\n\n\n\n\n\nWe only need to prove \\(p = \\nabla S\\), which is sufficient to prove Equation 3, and thus the HJE.\nGiven a starting position \\((t_0, q_0)\\) and a reference trajectory, there exists some \\(S(t, q; t_0, q_0)\\), the cost function of arriving at any point in configuration space-time in a neighborhood of the reference trajectory. For each trajectory \\(\\gamma\\) in the neighborhood that arrives at \\((t, q)\\), there exists a final market price \\(q(t)\\). We need to show that \\(q = \\nabla S\\).\nSuppose not, then we can perform positional arbitrage. First, we arrive at \\(q + \\delta q\\) by the efficient route, then sell off some \\(\\delta q\\). This would cost us\n\\[S(t, q+\\delta q; t_0, q_0) - \\left\\langle p, \\delta q\\right\\rangle = S(t, q; t_0, q_0) + \\left\\langle\\nabla S - p, \\delta q\\right\\rangle\\]\nIf \\(\\nabla S \\neq p\\), then we can take \\(\\delta q = -(\\nabla S - p)\\epsilon\\), and thus magically make the journey cost less by a first-order infinitesimal. This means the market is inefficient, a contradiction.\n\n\n\nHere is a proof in the spirit of wave mechanics and dynamical programming. Though I did not study his proof,10 I believe this is how the inventor of dynamical programming, Richard Bellman, proved his extension, the Hamilton–Jacobi–Bellman equation. The HJBE reduces to the HJE under certain conditions – they do not talk about the same thing, because while HJ is about stationary action, HJB is about maximal action.\n10 In his autobiography, he said,\n\nProblems of this type had been worked on before by many mathematicians, Euler, Hamilton, and Steiner, but the systematic study of problems of this type was done at RAND starting in 1948 under the inspiration of von Neumann. (Bellman 1984, 208)\n… one can use dynamic programming for the minimum principles of mathematical physics. For example, with dynamic programming one has a very simple derivation of the eikonal equation. In addition, the Hamilton-Jacobi equation of mechanics can easily be derived. (Bellman 1984, 289)\n\nSuppose that we have found all points at which the action is equal to \\(S\\). Now we would like to expand that surface a little further, to the surface of action \\(S + \\delta S\\). We do that in the spirit of economics (of course!) and traveling.\nInterpret the action of a path as the cost of traveling along that path. The surfaces of constant action, then, become the isochrone maps. The problem we face is then a matter of travel planning: Given that we can reach up to surface \\(X_S\\) if we are willing to pay cost \\(S\\), how much further can we travel if we are willing to pay an additional \\(\\delta S\\)?\n\n\n\nIsochrone maps of travel time in America, 1800 – 1930. (Paullin 1932, plate 138, page 366)\n\n\nLet us stand at a point \\((t, q)\\) on the surface of action \\(S\\), and consider all the points we can reach by an additional action \\(\\delta S\\). Suppose we go from \\((t, q)\\) to \\((t + \\epsilon t, q + \\epsilon q)\\), then the cost of that is \\(L\\left(t, q, \\frac{\\epsilon q}{\\epsilon q}\\right)\\epsilon t\\). (We write \\(\\epsilon t\\) instead of \\(\\delta t\\), because we have to use that symbol later.)\nTherefore, the “wave” of action \\(\\delta S\\) coming out of the point \\((t, q)\\) are those points \\((t + \\epsilon t, q + \\epsilon q)\\) satisfying the equation\n\\[\nL\\left(t, q, \\frac{\\epsilon q}{\\epsilon q}\\right)\\epsilon t = \\delta S\n\\]\nAnd the surface of action \\(S + \\delta S\\) is the envelope of all those little waves (“wavelets”). This is the wave perspective, but we still need to return to the particle perspective.\nSuppose you are already at \\((t, q)\\), and you just want to reach the surface of \\(S + \\delta S\\). It doesn’t matter where you end up on that surface – you just have to get to that surface somewhere. You also have exactly \\(\\delta S\\) to spend, so you have to plan optimally. Now, looking at that picture, you see that the only place you can possibly reach is a certain point \\((t + \\delta t, q + \\delta q)\\) where the wavelet is tangent to the surface of \\(S + \\delta S\\). At that point, the tangent surfaces of \\(S\\) at \\((t, q)\\) is parallel to the tangent surface of wavelet. That is,\n\\[\n\\begin{aligned}\nd\\left(L\\left(t, q,-\\frac{\\epsilon q}{\\epsilon t}\\right) \\epsilon t\\right) |_{\\epsilon t = \\delta t, \\epsilon q = \\delta q} &= \\left\\langle(\\left(\\nabla_q L\\right) \\underbrace{\\delta t}_{\\rightarrow 0}+\\nabla_v L), d q\\right\\rangle +\\left(L-\\frac{\\delta q}{\\delta t} \\nabla_v L\\right) d t \\\\\n&\\propto dS\n\\end{aligned}\n\\]\nThus, there exists some constant \\(c &gt; 0\\) such that\n\\[\n(\\partial_t S, \\nabla_q S) = c \\left(\\left\\langle\\nabla_v L, \\frac{\\delta q}{\\delta t}\\right\\rangle - L , \\nabla_v L\\right) = (-cH, cp)\n\\]\nSince we also have\n\\[\n\\partial_t S \\delta t + \\left\\langle\\nabla_q S, \\delta q\\right\\rangle = \\delta S = L \\delta t\n\\]\nwe see \\(c=1\\).\n\n\n\nThe wavelet proof of the HJE.\n\n\n\n\n\n\n\n\nConvexity of the wavelet\n\n\n\nIf the wavelet is convex, then the tangent point is unique, and there is only one way to proceed from \\((t, q)\\). However, if the wavelet is not, then there could exist two or more particle paths shooting out from \\((t, q)\\). It is similar to birefringence and conical refraction (Lunney and Weaire 2006).\n\n\n\nExercise 3 If you have studied, or intend to study, control theory, then prove the Hamilton–Jacobi–Bellman equation using the exact same picture. You can also prove the stochastic HJB equation in the same way, though you would need to insert the expectation \\(\\mathbb{E}\\) somewhere.\n\n\n\nBonus: Noether’s theorem\nIn Noether’s theorem, symmetries of the Lagrangian give us conserved quantities of motion.\n\n\\(\\epsilon\\) is an infinitesimal number.\nAn infinitesimal transform is an infinitesimal deformation \\((\\delta t, \\delta q)\\) of configuration space-time.\n\n\\(\\delta t = \\epsilon T\\), where \\(T\\) a function of type \\(\\underbrace{\\mathbb{R}}_{\\text{time}} \\times \\underbrace{\\mathcal C}_{\\text{configuration space}} \\to \\mathbb{R}\\)\n\\(\\delta q = \\epsilon Q\\), where \\(Q\\) is a function of type \\(\\mathbb{R}\\times \\mathcal C \\to \\mathbb{R}^d\\), where \\(d\\) is the dimension of configuration space \\(\\mathcal C\\).\n\nAn infinitesimal transform is a symmetry of the Lagrangian, iff taking any path \\(\\gamma\\) (not necessarily physically real), and deforming it to \\(\\gamma'\\), the action is conserved: \\(\\int_\\gamma L dt = \\int_{\\gamma'} L dt\\).\nA conserved quantity of motion is a number depending on \\(t, q, \\dot q\\), such that it is constant along any physically real path \\(\\gamma\\).\n\n\nExercise 4 We defined “symmetry of the Lagrangian” by a condition on an integral: \\(\\int_\\gamma L dt = \\int_{\\gamma'} L dt\\). Reformulate this to a condition at a point, involving \\(L, Q, T\\) and their derivatives.\n\n\n\n\n\n\n\nWarning\n\n\n\nA symmetry of the Lagrangian conserves all actions, even those of unphysical paths, but a conserved quantity of motion is only conserved along physical paths. We may call them “conserved quantity of physical motion” to emphasize the distinction.\n\n\nTake a trajectory \\(\\gamma_{AA'}\\) from point \\(A\\) to \\(A'\\). Now, shift it by \\(\\delta t, \\delta q\\), resulting in a trajectory \\(\\gamma_{BB'}\\) from point \\(B\\) to \\(B'\\). Since \\(L\\) is invariant under symmetry, any such shifting gives us \\(S_{AA'} = S_{BB'}\\). Now, if \\(\\delta S \\neq 0\\) in a neighborhood of the given trajectory \\(\\gamma_{AA'}\\), then we can take this variation \\(\\delta \\gamma\\), and shift it by the symmetry, showing that \\(\\delta S \\neq 0\\) in a neighborhood of \\(\\gamma_{BB'}\\) too.\nTherefore, by Hamilton’s principle, the infinitesimal transform sends any physically real trajectory into another physically real trajectory.\n\nTheorem 9 (Noether’s theorem) If \\((t, q) \\mapsto (t + \\epsilon T, q + \\epsilon Q)\\) is an infinitesimal symmetry of the Lagrangian, then\n\\[\nH T - \\left\\langle p, \\delta Q\\right\\rangle = (\\left\\langle\\nabla_v L, \\dot q\\right\\rangle - L)T - \\left\\langle\\nabla_v L, Q\\right\\rangle\n\\]\nis a conserved quantity of motion.\n\nTechnically speaking, we should not confuse the left side and the right side of\n\\[\nH T - \\left\\langle p, \\delta Q\\right\\rangle = (\\left\\langle\\nabla_v L, \\dot q\\right\\rangle - L)T - \\left\\langle\\nabla_v L, Q\\right\\rangle\n\\]\nThe left side is defined on the phase space-time, while the right side is defined on the configuration space-time. They are equal only because we have soldered together the phase space and the configuration space.\nIndeed, again and again we see that Lagrangian and Hamiltonian mechanics are different worlds, with their own dreams and phantasies, but always agreeing on the same reality.\nIf this were the case, then why do we bother creating two mechanics, since there is no physical experiment to distinguish them? It is because in quantum mechanics, virtual trajectories are just as real as the “real” trajectories. A particle can just go around the earth once before it arrives at its destination, taking the long way around. In this case, what is virtually possible matters just as much as what is classically real, and if Lagrangian mechanics and Hamiltonian mechanics can entertain different kinds of dreams, we might be able to tell them apart.\n\n\n\n\\(S\\) is a source of photons, and \\(P\\) is a receiver. A photon can go from \\(S\\) to \\(P\\) by bouncing off the mirror below, along many paths. Each path has a certain amplitude, and the sum of all their amplitudes is the total amplitude. While the amplitude is dominated by the amplitudes near the classical path \\(SGP\\), it is not the only path, and all paths, even the “virtual” paths like \\(SAP\\), contribute to what we observe. (Feynman 2006, fig. 24)\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nGiven any physically real trajectory \\(\\gamma_{AA'}\\) from point \\(A\\) to \\(A'\\), the infinitesimal transform sends it to another physically real \\(\\gamma_{BB'}\\). By symmetry, both paths have the same action. Thus, we have\n\\[\nS(A, A') = S(B, B')\n\\]\nNow, consider an intermediate path \\(\\gamma_{A, B'}\\). By Equation 3,\n\\[\nS(A, B') = S(A, A') - H \\delta t + \\left\\langle p, \\delta q\\right\\rangle, \\quad S(B, B') = S(A, B') + H_0 \\delta t_0 - \\left\\langle p_0, \\delta q_0\\right\\rangle\n\\]\nTherefore, \\(H T - \\left\\langle p, \\delta Q\\right\\rangle\\) is a conserved quantity of motion.\n\n\n\nThe proof is similar to the proof of Theorem 8, though they differ in that Noether’s theorem shows a quantity, defined at a single point, is conserved over a trajectory, while the Poincaré–Cartan invariant is not defined at a single point, but as an integral over an entire cycle."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#sec-geometric-optics",
    "href": "essays/posts/analytical-mechanics/index.html#sec-geometric-optics",
    "title": "Analytical Mechanics",
    "section": "Hamiltonian optics",
    "text": "Hamiltonian optics\n\nIsotropic case\nWe are going back to the roots, as geometric optics was what inspired Hamilton to develop his theory of Hamiltonian mechanics.\nWhen Hamilton developed his Hamiltonian approach, it was to study geometric optics, which can be derived from Fermat’s principle: light paths have stationary travel time. In other words, the action of a path is\n\\[S(\\text{path}) = \\int_{\\text{path}} dt\\]\nThus, \\(L = 1\\)...? Well, here we see the problem: in geometric optics, if you fix the starting and ending point as \\((t_0, q_0), (t, q)\\), then any path between them takes exactly \\(t-t_0\\) time, and there is nothing to vary. Consequently, we need to remove time from consideration, so that there is something to vary.\nFermat’s principle, reformulated, states light paths have stationary optical length. Let the medium be isotropic (light speed does not depend on direction), then we have\n\\[S(\\text{path}) = \\int_{\\text{path}} n(q) \\|dq\\|\\]\nwhere \\(n\\) is the refractive index, with \\(L(q) = n(q)\\). Here we encounter a brief difficulty: time flows in one direction only, but space flows in infinitely many possible directions!\nThe solution might seem like a joke, but it would work out well: select one direction11, say \\(q_0\\), and pretend that it is time. With this trick, all previous mathematical formalism immediately applies, and we have\n11 This direction is usually selected to be the direction of the principal optic axis. For example, the long-axis of a camera is a principal optic axis, and so is the barrel-axis of a telescope.\\[S(\\text{path}) = \\int_{\\text{path}} n(q_0, q_1, q_2) \\sqrt{1 + \\left(\\frac{dq_1}{dq_0}\\right)^2 + \\left(\\frac{dq_2}{dq_0}\\right)^2}dq_0\\]\n\nDerivation\nLet’s make the notation cleaner, by rewriting \\(q_0\\) as \\(t\\), \\((q_1, q_2)\\) as \\(q\\), and using \\(v\\) to mean \\(\\left(\\frac{dq_1}{dq_0}, \\frac{dq_2}{dq_0}\\right)\\). Then we have\n\\[L(t, q, v) = n(t, q) \\sqrt{1 + \\|v\\|^2}\\]\nRoutine calculation yields\n\\[\\begin{cases}     \nL(t, q, v) = n(t, q) \\sqrt{1 + \\|v\\|^2}\\\\     \nH(t, q, p) = -\\sqrt{n^2 - \\|p\\|^2}\n\\end{cases} \\quad\n\\begin{cases}     \np^\\ast = \\frac{nv}{\\sqrt{1 + \\|v\\|^2}} \\\\     \nv^\\ast = \\frac{p}{\\sqrt{n^2 - \\|p\\|^2}}\n\\end{cases}\\]\nwe continue with the HJE, which simplifies to:\n\\[(\\partial_t S)^2 + \\|\\nabla_q S\\|^2 = n^2\\]\nReverting notation back to \\((q_0, q_1, q_2)\\), we find the eikonal equation12:\n12 From Greek εἰκών (eikon, “image”), from which the word “icon” derived.\\[\\|\\nabla_q S\\| = n(q)\\]\nWhy did the trick work? Well, if we look back to how we derived the Hamiltonian, we could see that what we called “time” is really just a special copy of \\(\\mathbb{R}\\), along which we organized all other state and control variables. We don’t really need time to be anything more than the domain of functions, as in \\(q_i: \\mathbb{R}\\to \\mathbb{R}\\) and \\(v_i : \\mathbb{R}\\to \\mathbb{R}\\). It most definitely does not need to “flow”, or flow only from the past to the future, or have any psychological significance.\n\n\nInterpretation\nStarting with Fermat’s principle for light rays, we ended up with the eikonal equation for light waves. In general, we find the following duality between wave-field optics and particle-path optics.\n\nThe particle-wave duality.\n\n\n\n\n\n\n\nperspective\nparticle-path\nwave-field\n\n\n\n\naction \\(S\\)\na function of particle path\na field on configuration-spacetime\n\n\nequation\n\\(\\|\\nabla_q S\\| = n(q)\\)\n\\(\\partial_t S + H(t, q, \\nabla_q S) = 0\\)\n\n\nin optics\nlight rays, Fermat’s principle\nlight waves, Huygens principle\n\n\nin mechanics\npoint particles\nmatter waves\n\n\n\n\n\n\nAnisotropic case\nWhen the travel cost of light can depend on the direction of travel, we say that the medium is anisotropic. Now, you might expect\n\\[\nS = \\int n(\\hat{\\delta q}) \\delta q\n\\]\nbut this is incorrect, not because there is anything necessarily wrong with the formalism, but because of how \\(n\\) is defined by convention in anisotropic material. At this point, we must study the full particle-wave duality again.\nFor a moment, let’s pretend light is really a particle, and consider how it might appear to someone who believes light is a wave. We arrange an entire plane of photons, such that the plane is perpendicular to a unit vector \\(\\hat k\\). Now, let them move optimally for time \\(\\delta\\). Each of them would go in the same optimal direction \\(v^*(\\hat k)\\), to push the wavefront as far-out as possible.\n\n\n\nA plane of particles marching in sync at velocity \\(v\\), but the collective effect is that the plane moves with group velocity \\(v_g\\), which is different from the individual velocity \\(v\\).\n\n\nNow, the wavefront does not move in the direction \\(v^*\\), but in the direction \\(\\hat k\\). Therefore, the wavefront moves at group velocity \\(v_g(\\hat k) = \\left\\langle v^*(\\hat k), \\hat k\\right\\rangle \\hat k\\).\nSince the photons are trying to push as far out as possible,\n\\[v^*(\\hat k) = \\mathop{\\mathrm{argmax}}_{v \\in K_{particle}} \\left\\langle\\hat k, v\\right\\rangle\\]\nwhere we write \\(K_{particle}\\) as the surface of all particle velocities in all directions. It is a sphere of radius \\(c\\) in a vacuum, but in an anisotropic medium, we allow it to be any crazy shape.\nInstead of studying the group velocity, we actually need to use its inverse – the wavevector \\(k = \\frac{\\hat k}{v_g}\\).13 We thus have\n13 The typical definition is \\(k_{usual} = \\frac{2\\pi}{\\lambda}\\hat k = \\nabla \\phi\\), where \\(\\phi\\) is the phase of the light, and \\(f\\) is its temporal frequency. Our definition, which makes it cleaner, but somewhat different from typical definition, is \\(k_{ours} = \\frac{\\nabla \\phi}{2\\pi f} = \\frac{\\nabla \\phi}{\\omega}\\).\\[k = \\frac{\\hat k}{\\max_{v \\in K_{particle} \\left\\langle v, \\hat k\\right\\rangle}}\\]\nThe reason we use this instead of the group velocity is that, a little simplification later, we have the beautifully simple relation\n\\[\n\\forall k \\in K_{wave}, \\quad \\max_{v \\in K_{particle}}\\left\\langle v, k\\right\\rangle = 1\n\\]\nThis is a suggestive symmetry, which practically demands us to write it as a duality:\n\\[\n\\begin{cases}\nv^*(k) = \\mathop{\\mathrm{argmax}}_{v \\in K_{particle}}\\left\\langle v, k\\right\\rangle \\\\\nk^*(v) = \\mathop{\\mathrm{argmax}}_{k \\in K_{wave}}\\left\\langle v, k\\right\\rangle\n\\end{cases}, \\quad\n\\begin{cases}\n\\left\\langle v^*(k), k\\right\\rangle = 1\\\\\n\\left\\langle v, k^*(v)\\right\\rangle = 1\\\\\n\\end{cases}\n\\]\nThis is the polar dual construction, often used in convex analysis.14 (Hiriart-Urruty and Lemaréchal 2001, sec. C.3).\n14 What, just because we have not mentioned Legendre transform for a few pages, you would think that we’re done with convex analysis? Too bad! If nature is the great optimizer, then convex analysis is inescapable at every turn.The best case is if \\(K_{particle}\\) is convex. In this case, each \\(k \\in K_{wave}\\) defines a plane perpendicular to it, at a distance \\(\\|k\\|^{=1}\\) from the origin, and the plane is tangent to \\(K_{particle}\\) at precisely \\(v^*(k)\\), and conversely so. In other words, \\(K_{particle}\\) is the envelope of polar lines to points in \\(K_{wave}\\), and conversely so.\nConversely, we can pretend that light is really a wave, and consider how it might appear to someone who believes light is a particle. We would then go through the above argument, and obtain the same result.\n\n\n\nA pair of polar duals. Both surfaces are convex.\n\n\nHowever, we want to deal with more general cases than this, so we need to resolve two issues.\nFirst issue: \\(K_{particle}\\) might be non-convex. Second issue: it might be double-sheeted, or even many-sheeted. For example, in crystal, light polarized in two different orientations can move at two different velocities even in the same direction. Thus, its \\(K_{wave}\\) has two sheets, and so its polar dual, \\(K_{particle}\\), also has two sheets.\nBoth issues are solved by extending the definition of polar duality: replace the maximum with a stationarity. We can still construct \\(K_{particle}\\) from \\(K_{wave}\\) by taking a tangent plane for each \\(k \\in K_{wave}\\), but now instead of the intersection of the half-planes, we use their envelope. A picture shows what we mean:\n\n\n\nA pair of polar duals. The surfaces are no longer convex. Notice how the double tangent on the left becomes a double crossing point on the right.\n\n\n\n\n\nThe curve on the right is a trefoil curve defined by \\(z = 0.3 e^{i\\phi} + e^{-2i \\phi}\\), shifted to make the image clearer. The curve on the left is the polar dual of the trefoil curve. It is the envelope of the polar dual lines of each point on the trefoil curve.\n\n\nWe still have a duality between points on the two surfaces, defined by stationarity, not optimality. For example, for any wavevector \\(k \\in K_{wave}\\), its corresponding dual point \\(v^*(k)\\) satisfies \\(\\left\\langle v^*(k) + \\delta v, k\\right\\rangle = 0\\) for any \\(\\delta v\\) in the tangent space of \\(K_{particle}\\) at \\(v^*(k)\\).\nThe first application of Hamiltonian mechanics was done by Hamilton, who in 1832 predicted theoretically that if light enters a biaxial crystal in just the right way, it will not just refract in one direction, but in an entire cone of directions – which he termed internal conical refraction. The theory of this is fascinating,15 however, for lack of space, we will only give the barest description.\n15 It was historically important as the first phenomenon predicted by mathematical reasoning before experimental observation (Berry and Jeffrey 2007), and it was often compared to the mathematical prediction of Neptune by Le Verrier (1845) (Smith 1989).Simply put, it turns out that in a biaxial crystal, both \\(K_{particle}\\) and \\(K_{wave}\\) have the same shape of a large blob containing a smaller blob, touching each other at 4 cone-shaped points, as pictured.\n\n\n\nThe shape of the \\(K_{wave}\\) surface. The \\(K_{particle}\\) surface is topologically the same, and can be obtained from \\(K_{wave}\\) by squashing it just right. Figure from (Schaefer 1949, 485, figure 128)\n\n\nNow, let \\(k_c \\in K_{wave}\\) be one of the cone-shaped points, then it corresponds to a tangent plane to \\(K_{particle}\\). Each tangent point \\(v \\in K_{particle}\\), conversely, corresponds to a tangent plane to \\(k_c\\).\nSince \\(k_c\\) is a conical point, however, there are a whole circle of tangent planes to \\(K_{wave}\\) at \\(k_c\\). Consequently, the tangent plane to \\(K_{particle}\\) is tangent to it on one entire circle. It is as if we throw a tire on the floor – it will touch the floor not just at three points, but an entire circle of points. Now, suppose that we have a planar wave moving in the direction of \\(k_c\\) inside the crystal, then each light-particle would have to move in a direction \\(\\mathop{\\mathrm{argmax}}_{v \\in K_{particle}}\\left\\langle v, k_c\\right\\rangle\\). But since there is an entire circle of such directions, we would have an entire circle of possible \\(v\\), and thus, we obtain a hollow cone of light.\n\n\n\nWhen a planar wave travels in a crystal, such that \\(K_{particle}\\) is tangent to the plane of the wave at an entire circle, then instead of choosing one among the entire circle of velocities, the particles simply take every single possible direction on the circle, resulting in conical refraction."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#the-particle-wave-duality",
    "href": "essays/posts/analytical-mechanics/index.html#the-particle-wave-duality",
    "title": "Analytical Mechanics",
    "section": "The particle-wave duality",
    "text": "The particle-wave duality\n\nParticle in free space\nConsider a particle of mass \\(m\\) in free space \\(\\mathbb{R}^n\\). Its Lagrangian is \\(L(t, q, v) = \\frac 12 m\\|v\\|^2\\). By convex duality, we have\n\\[\\begin{cases} L(t, q, v) = \\frac 12 m\\|v\\|^2\\\\ H(t, q, p) = \\frac{\\|p\\|^2}{2m} \\end{cases}\\quad  \n\\begin{cases} p^\\ast(t, q, v) = mv\\\\ v^\\ast(t, q, p) = \\frac{p}{m} \\end{cases}\\]\nFor any \\(t_0, q_0, t, q\\) with \\(t_0\\neq t\\), we can directly solve for the trajectory from \\((t_0, q_0)\\) to \\((t, q)\\), then find the action:\n\\[S_{t_0, q_0}(t, q) = \\frac 12 m \\frac{\\|q-q_0\\|^2}{t-t_0}\\]\nEach \\(S\\) defines a paraboloid wavefront in spacetime, with apex \\(t_0, q_0\\). The wavefront is translation-symmetric, so we set both to zero, yielding the equation of the wavefront:\n\\[\nt = \\frac{\\|q\\|^2}{2S/m}\n\\]\nThe wavefront of \\(S = 0\\) is just the positive \\(t\\)-axis, and as \\(S\\) increases, the wavefront widens out.\nNow, we interpret this wave from the HJE point of view. Let’s say we have the wavefront at \\(S=S_0\\), and we want to construct the wavefront at \\(S = S_0 + \\delta S\\). This we perform by rippling out a little wavelet at each point \\((t', q')\\) on the wavefront. The little wavelet has shape \\((t-t') = \\frac{\\|q - q'\\|^2}{2\\delta S/m}\\), and as we move \\(t', q'\\) around the parabola of wavefront \\(S = S_0\\), the envelope of these wavelets is the wavefront of \\(S = S_0 + \\delta S\\).\nThis is the wave point of view. We can switch back to the particle point of view. What is the velocity of the particle passing the point \\((t', q')\\)? We know that it must be traveling in the optimal direction, and the optimal direction allows it to go as far as possible. Therefore, we simply draw the wavelet of \\(\\delta S\\) at \\((t', q')\\), then find the intersection of the wavelet with the wavefront of \\(S = S_0 + \\delta S\\).\nThe entire procedure is pictured below.\n\n\n\nThe two red curves are two wavefronts \\(S = S_0\\) and \\(S = S_0 + \\delta S\\). At select points on the first wavefront, we draw a wavelet of \\(\\delta S\\), which is tangent to the second wavefront. The particle trajectory connects the point and the tangent point of the wavelet with the second wavefront.\n\n\n\n\nParticle-wave in free space\nThe paraboloid-shaped solution for \\(S\\) is interpretable in Newtonian mechanics, as the motion of a single particle moving from the origin. However, the HJE itself is merely a PDE with its own logic and meaning, and consequently, it may have different solutions that are hard to to interpret in Newtonian mechanics.\nFrom our 21st-century perspective, we can say that the HJE is generally true, and Newtonian mechanics is only a special case. Some solutions to the HJE may not be interpretable in Newtonian mechanics, but they are nevertheless physically real, since Newtonian mechanics is incomplete. Given that, we simply try to solve HJE, then try to interpret it, even if not in Newtonian mechanics.\nBecause the Lagrangian is time-independent, so any time-independent solution:\n\\[S(t, q) = W(q) - Et, \\quad \\| \\nabla_q W \\| = \\sqrt{2mE}\\]\nfor any constant \\(E &gt; 0\\) also gives a solution to the HJE. This is just the eikonal equation for a medium of constant wave speed! More on this in the section on geometric optics. One can of course solve the eikonal equation by putting it into a numerical package and let it grind out the solution. However, we can interpret it by the Huygens principle.\nSuppose you know a surface of constant \\(W = W_0\\), and you know that the arrows of \\(\\nabla_q W\\) point outwards, then since \\(\\nabla W\\) is perpendicular to the surface, and is of constant length \\(\\sqrt{2mE}\\), you can step out a small distance \\(ds\\) perpendicularly out of the whole surface \\(W= W_0\\), and arrive at the surface of \\(W=W_0 + \\sqrt{2mE} ds\\). Alternatively, you can draw small spheres of radius \\(ds\\), and their outwards envelope is the \\(W=W_0+ \\sqrt{2mE} ds\\) surface. These procedures are equivalent, but in one, we constructed “rays” while the other we constructed “wave fronts”.\nFor the free particle, the simplest solution is the plane wave:\n\\[W(q) = \\sqrt{2mE} \\langle \\hat k, q \\rangle\\]\nwhere \\(\\hat k\\) is any unit-vector, interpreted as the direction of wave propagation. Plugging it back to Equation 3, we find that the “planar wave particle” has\n\\[(\\partial_t S, \\nabla S) = (-H, p) = (-E, \\sqrt{2mE}\\hat k)\\]\nwhere \\(\\hat k\\) is the direction of the group velocity of the wave. The group velocity of the wave is \\(\\frac{\\partial_t S}{\\nabla S} = \\frac{E}{\\sqrt{2mE}}\\hat k\\).\n\n\n\nThe two red curves are two wavefronts \\(S = S_0\\) and \\(S = S_0 + \\delta S\\). At select points on the first wavefront, we draw a wavelet of \\(\\delta S\\), which is tangent to the second wavefront. The particle trajectory connects the point and the tangent point of the wavelet with the second wavefront.\n\n\nTaking the particle-wave analogy seriously, we say that:\n\na planar wave is a particle with energy \\(E\\) and momentum \\(p \\propto k \\propto \\sqrt{2mE} \\hat k\\).\na particle with energy \\(E\\) and momentum \\(p\\) is a planar wave traveling at group velocity \\(E/p\\).\n\nTypically, waves have a wavelength \\(\\lambda\\), which is related to the wave vector \\(k\\) by \\(k \\propto \\lambda^{-1}\\), we find that the particle has wavelength \\(\\lambda \\propto \\frac 1k \\propto \\frac 1p\\). Thus, we arrived at de Broglie’s matter-wave hypothesis, which Schrödinger expanded into his equation. Both de Broglie and Schrödinger were inspired by Hamilton’s optics-mechanics analogy, so we are treading the same path as them a century ago.\nIn fact, we could start with an arbitrary wavefront in configuration-spacetime, and use Huygens’ principle to construct the wavefront in the next moment. In general, we can’t do Fourier analysis on such wavefronts – they are not decomposable into planar waves, because the HJE is a nonlinear equation. Fourier analysis only works on linear differential equations.\n\n\n\nGiven a wavefront in the shape of \\(t = q^3\\), we can construct the next wavefront as the envelope of the parabolic wavelets at every point on the wavefront.\n\n\n\n\nParticle in a potential field\nA particle in a time-dependent potential field \\(V\\) has Lagrangian \\(L(t, q, v) = \\frac 12 m\\|v\\|^2 - V(t, q)\\). By convex duality\n\\[\\begin{cases}     \nL(t, q, v) = \\frac 12 m\\|v\\|^2 - V(t, q)\\\\     \nH(t, q, p) = \\frac{\\|p\\|^2}{2m} + V(t, q)\n\\end{cases} \\quad\n\\begin{cases}     \np^\\ast = mv \\\\     \nv^\\ast = \\frac pm\n\\end{cases}\\]\nThe HJE gives\n\\[\\partial_t S + \\frac 1{2m} \\|\\nabla_q S\\|^2 = -V(t, q)\\]\nAs before, if we interpret the equation as a wave equation, then the group velocity is \\(\\frac{\\partial_t S}{\\nabla S} = \\frac{-E}{p} \\hat k\\), with magnitude \\(v_g = \\frac{E}{\\sqrt{2m(E-V)}}\\).\n\n\n\n\n\n\nExample: particle in free fall\n\n\n\n\n\nWe consider the classical problem of a body in free fall, thrown from the origin \\((t, q) = (0, 0)\\). Basic physics tells us that the position and velocity of the body are given by\n\\[q(t) = v_0 t - \\frac{1}{2}gt^2, \\quad v = v_0 - gt = \\frac{q}{t} - \\frac{1}{2}gt\\]\nPlugging these expressions into the Hamilton-Jacobi equation, we obtain a system of partial differential equations for the action function \\(S\\):\n\\[\n\\begin{cases}\n\\partial_t S &= -H = -\\frac{1}{2}m(\\frac{q}{t} - \\frac{gt}{2})^2 - mgq \\\\\n\\partial_q S &= m(\\frac{q}{t} - gt)\n\\end{cases}\n\\]\nSolving this system, we find the unique solution for the action:\n\\[S = \\frac{x^2}{y} - xy - \\frac{y^3}{12}\\]\nwhere we have introduced convenience variables \\(x = gq\\), \\(y = gt\\), and \\(s = \\frac{2gS}{m}\\).\nThe contour lines of the action function satisfy the equation:\n\\[\nx = \\frac{1}{2}y^2 \\pm \\sqrt{\\frac{1}{3}y^4 + sy}\n\\]\nTo analyze the wavelet associated with this system, we start by writing down the Lagrangian:\n\\[L = T - V = \\frac{1}{2}m(\\frac{\\delta q}{\\delta t})^2 - mgq\\]\nThe wavelet equation is then given by:\n\\[\\delta S = L \\delta t = \\left(\\frac{1}{2}m(\\frac{\\delta q}{\\delta t})^2 - mgq\\right)\\delta t\\]\nSimplifying this equation using our convenience variables, we get:\n\\[(\\delta x)^2 = 2x(\\delta y)^2 + \\delta s \\delta y\\]\nSolving for \\(\\delta y\\), we obtain:\n\\[\\delta y = \\frac{-\\delta s \\pm \\sqrt{(\\delta s)^2 + 8x(\\delta x)^2}}{4x}\\]\nThis equation reveals the nature of the wavelet. When \\(x &gt; 0\\), the equation describes a hyperbola. For \\(x &lt; 0\\), it describes an ellipse. At the boundary \\(x = 0\\), the equation describes a parabola.\n\nThere is no planar wave solution, because a planar wave solution is both constant energy and unbounded, whereas if a particle can move infinitely far away, it must have infinite energy. If we attempt to force a planar wave solution, it would immediately break down:\n\n\n\n\n\nExercise 5 Solve the time-independent HJE for the particle in free fall. It should be \\(S = W(q) - Et\\), where \\(E\\) is a constant, and \\(W\\) is a semicubical parabola.\n\n\nExercise 6 In the same way, analyze the simple harmonic oscillator – a particle in a potential well \\(V = \\frac 12 kq^2\\). Similarly, it cannot have a planar wave solution. Plot the wavelets along each point on a planar wave, and see how it breaks down. Find two families of solutions: The case where we have a point source at \\((0, 0)\\), and the case of time-independent HJE.\n\n\n\nDeriving the time-independent Schrödinger equation\nThis section based on (Masoliver and Ros 2009; Derbes 1996; Hamill 2013, sec. 6.4).\nAssume that the potential is time-independent: \\(V(t, q) = V(q)\\). Assume that the physical system can be described by a function\n\\[\n\\Psi(t, q) = \\Psi_0(r) e^{iS(t, q)/\\hbar}\n\\]\nwhere the function \\(\\Psi\\) is typically called the “wave function”.16 We assume that the wave is a “standing wave”, so that all of space is oscillating in sync. Let \\(S(t, q) = W(q) - Et\\), so that the oscillation frequency is \\(E/\\hbar\\).17\n16 What is a wave function? It is just some abstract mathematical object, that somehow allows us to calculate everything we want to know about this system. We have no idea what it is, but it works. The same applies to the function \\(S(t, q)\\). We call it the “minimal cost” for arriving at \\((t, q)\\), but what really is a cost in physics? Particles do not really pay their paths with natural money. All this time, we have pretended that they pay some kind of cost and want to minimize the cost, but it is really just one big analogy. The same applies for the function \\(\\Psi\\). We might call it a “wave function”, but it really is just one big analogy with the waves on an ocean. There is really no wave in quantum mechanics, only a function that we pretend is a wave, because it helps us calculate results that happen to be correct.17 The most important experimental result from quantum mechanics is that energy levels are quantized. Now, a quantized energy level is something like \\(E = h, 2h, 3h, \\dots\\). There is really just one kind of thing in classical mechanics that is quantized: standing waves! If you have a string, then its standing waves must have \\(0, 1, 2, 3, \\dots\\) nodes, i.e. quantized. Thus, it is natural to try out this “standing wave” assumption.So, we can separate the variables to\n\\[\n\\Psi(t, q) = \\underbrace{e^{-i\\frac{E}{\\hbar}t}}_{\\text{oscillation in sync}} \\underbrace{\\psi(q)}_{\\text{variation over space}}, \\quad \\psi(q) = \\Psi_0(q) e^{i\\frac{W(q)}{\\hbar}t}\n\\]\nSince \\(\\Psi\\) should be a wave, it has better follow the wave equations:\n\\[\n(\\partial_t^2 - v_g^2 \\nabla^2)\\Psi = 0\n\\]\nwhere \\(v_g\\) is the group velocity of the wave. As we saw previously, \\(v_g = \\frac{E}{\\sqrt{2m(E-V)}}\\) for a particle in a potential. Then we have\n\\[\n\\begin{cases}\n\\Psi(t, q) &= e^{-i\\frac{E}{\\hbar} t} \\psi(q), \\\\\n0 &= (\\partial_t^2 - v_g^2 \\nabla^2)\\Psi, \\\\\nv_g &= \\frac{E}{\\sqrt{2m(E-V)}},\n\\end{cases}\\;\\; \\implies \\frac{\\hbar^2}{2 m} \\nabla^2 \\psi+(E-V) \\psi = 0,\n\\]\nwhich is the time-independent Schrödinger equation.\n\nExercise 7 The time-dependent Schrödinger equation states that\n\\[\ni\\hbar\\frac{\\partial}{\\partial t} \\Psi(t, q) = \\left [ - \\frac{\\hbar^2}{2m}\\nabla^2 + V(t, q)\\right ] \\Psi(t, q).\n\\]\nPlug \\(\\Psi(t, q) = \\psi_0(t, q) e^{i S(t, q)/\\hbar}\\) back to the time-dependent Schrödinger equation, and check that at the \\(\\hbar \\to 0\\) limit, we recover the HJE for \\(S(t, q)\\). This is a simple example of the WKB approximation.\n\nIf the above derivation looks mildly suspect, and leaves you with a feeling of seeing a magic trick, it is not an accident. The analogy between classical mechanics and quantum mechanics is not exact, so we cannot logically derive quantum mechanics from classical mechanics. The simple problem is that classical mechanics, even when formulated in the form of Hamilton–Jacobi wave equations, cannot reproduce interference or diffractions. Consider the simple case of a straight-edge diffraction. If you aim a light beam at a sharp edge, then on the other side, there would be alternating bright and dark bands fading into the shadow. However, if light is going by the shortest path, then there should be no such banding, and the brightness should just drop off to zero monotonically.\n\n\n\nIn classical mechanics, the shadow of a hard edge has no diffractive stripes.\n\n\nThe solution is to admit that geometric optics is insufficient, that Huygens’ principle is insufficient, and we need a full theory of light wave in order to explain what happens on the smallest scales – a diffraction theory. Similarly, classical mechanics is insufficient, and the HJE is insufficient, and we need a full theory of matter wave in order to explain what happens in the atomic world – quantum mechanics.\n\n\n\nThe optical-mechanical analogy at three levels. There is a question mark, because I’m not sure if that should be quantum field theory.\n\n\nDeriving quantum mechanics from classical mechanics is necessarily fraught with danger and luck, because in going from quantum mechanics to classical mechanics, something is irrevocably lost (and other things are irrevocably earned). It is about as difficult as going from geometric optics to wave optics. Still, several people have tried and succeeded, most famously, Schrödinger.\n\n… the conception of rays is thoroughly well defined only in pure abstract geometrical optics. It is wholly incapable of being applied to the fine structure of real optical phenomena, i.e. to the phenomena of diffraction. Even in extending geometrical optics somewhat by adding the notion of Huygens’ principle, one is not able to account for the most simple phenomena of diffraction without adding some further very strange rules concerning the circumstances under which Huygens’ envelope-surface is or is not physically significant. (I mean the construction of “Fresnel’s zones”.) These rules would be wholly incomprehensible to one versed in geometrical optics alone. Furthermore it may be observed that the notions which are fundamental to real physical optics, i.e. the wave-function itself (\\(W\\) is merely the phase), the equation of wave-propagation, the wave length and frequency of the waves, do not enter at all into the above stated analogy.\n(Schrödinger 1926)\n\n\n… geometrical optics is only an approximation… when interference and diffraction phenomena are involved, it is quite inadequate. This prompted the thought that classical mechanics is also only an approximation relative to a vaster wave mechanics. … A new mechanics must be developed which is to classical mechanics what wave optics is to geometrical optics. This new mechanics has since been developed, thanks mainly to the fine work done by Schrödinger.\nLouis de Broglie’s Nobel Prize Lecture (De Broglie 1929)\n\n\n\nRelativistic particle in free space\nSince the HJE is fully general for any function \\(L(t, q, v)\\) that is smooth and strictly convex in \\(v\\), we can simply write down the Lagrangian for relativistic particle in a field, and it would just work.\nLet’s first consider particle in free space. In relativity, the one thing that is coordinate-independent is the proper time of a trajectory, so it is reasonable to guess that the action is the proper time, then deduce from it the Lagrangian. This is natural if we think of \\(S\\) as the optimal cost of traveling.\nLet \\(t_0 = 0, q_0 = 0\\), then by basic relativity, the proper time for the particle to arrive at \\((t, q)\\) is\n\\[S(t, q) = \\frac{\\|q\\|}{v\\gamma} = \\sqrt{t^2- \\frac{\\|q\\|^2}{c^2}}\\]\nwhere \\(\\gamma = \\frac{1}{\\sqrt{1-v^2/c^2}}\\) is the well-known factor used everywhere in special relativity.\nTaking \\(\\nabla_q\\), and simplifying, we find the relativistic momentum to be...\n\\[p = \\nabla_q S = -\\frac{\\gamma v}{c^2}\\]\nHowever, we were expecting \\(p \\to mv\\) when \\(v \\to 0\\), so we fix this issue by multiplying the action with a constant factor \\(-mc^2\\). Multiplying a constant factor in action has no effect on the calculus of variations, so we are free to do this. Thus we find that the action of a path is the proper time of the path multiplied by \\(-mc^2\\):\n\\[S(\\text{path}) = -mc^2 \\int_{\\text{path}}d\\tau = -mc^2 \\int_{\\text{path}}\\frac 1\\gamma dt\\]\nWith this, we can derive the familiar equations by the HJE and convex duality:\n\\[\\begin{cases}\nL(q, v) = -\\frac{mc^2}{\\gamma}\\\\     \nH(q, p) = \\sqrt{(\\|p\\|c)^2 + (mc^2)^2}\n\\end{cases} \\quad\n\\begin{cases}     \np^\\ast = \\gamma mv \\\\     \nv^\\ast = \\frac{pc}{\\sqrt{\\|p\\|^2 + (mc)^2}}\n\\end{cases}\\]\nIn particular, at low \\(v\\), we have \\(L \\approx -mc^2 + \\frac 12 m\\|v\\|^2\\). So somehow, by combining the geometry of spacetime with analytical mechanics, we have discovered the \\(E = mc^2\\) formula, even though it seems like something we couldn’t have discovered from mere geometry.\nThe HJE then becomes\n\\[\\frac 1{c^2} \\left(\\partial_t S \\right)^2 - \\|\\nabla S \\|^2 = m^2 c^2\\]\nIn particular, the time-independent solutions are of the form\n\\[S = W(q)-Et, \\quad \\| \\nabla W \\| = \\frac 1c \\sqrt{E^2 - m^2 c^4 }\\]\nfor any \\(E &gt; mc^2\\). In particular, if we plug in the usual relativistic energy \\(E = \\gamma mc^2\\), we get\n\\[\\| \\nabla W \\| = \\gamma mv\\]\nwhich is similar to what we obtained for the free particle in classical mechanics, with \\(\\|\\nabla W \\| = \\sqrt{2mE}\\), just with classical momentum upgraded to relativistic momentum.\n\nExercise 8 Just as how the HJE of a classical particle can be derived as the \\(\\hbar \\to 0\\) limit of the Schrödinger equation, one can derive the HJE of the non-quantum relativistic particle as the \\(\\hbar \\to 0\\) limit of the Klein–Gordon equation. The Klein–Gordon equation is essentially the simplest possible way to combine special relativity with Schrödinger equation:\n\\[\\left( \\frac{1}{c^2} \\frac{\\partial^2}{\\partial t^2} - \\nabla^2 + \\frac{m^2 c^2}{\\hbar^2} \\right) \\psi(t, \\mathbf{x}) = 0\\]\nPlug in \\(\\psi(t, q) = \\psi_0(q, t) e^{i S(t, q) / \\hbar}\\), and check that at \\(\\hbar \\to 0\\) limit, we recover the HJE."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#periodic-motion",
    "href": "essays/posts/analytical-mechanics/index.html#periodic-motion",
    "title": "Analytical Mechanics",
    "section": "Periodic motion",
    "text": "Periodic motion\n\nOscillator on a line\nConsider a simple harmonic oscillator (SHO), with Hamiltonian \\(H\\) in the \\((q, p)\\) coordinates:\n\\[H(t, q, p) = \\frac{p^2}{2m} + \\frac 12 kq^2\\]\nThe motion of the system is simply a circular motion around \\((q, p) = (0, 0)\\):\n\\[\\begin{cases}\n\\dot q = p/m \\\\\n\\dot p = - kq\n\\end{cases}\\quad  \n\\begin{cases}\n(q, p) = (q_0 \\cos(\\omega t), -m\\omega q_0 \\sin(\\omega t)) \\\\\n\\omega = \\sqrt{k/m}\n\\end{cases}\\]\nNow consider the action of an entire cycle:\n\\[S = \\oint Ldt = \\oint (pdq - Hdt)\\]\nThe \\(\\oint pdq\\) term is the area enclosed by the ellipse, so it is \\(\\pi p_0 q_0\\), and the \\(\\oint Hdt\\) term is just \\(HT = \\frac{2\\pi}{\\omega} \\frac 12 kq_0^2\\), since the system conserves energy. Now direct computation shows\n\\[\\oint pdq = HT\\]\nIn particular, since \\(T\\) does not depend on the energy of the oscillation, we can take derivative against energy, obtaining\n\\[\n\\frac{d}{dE}\\oint_{\\gamma_E} pdq = T\n\\]\nwhere \\(\\gamma_E\\) is the path traced out by the oscillator with energy \\(E\\). We show that this is generally true for 1D oscillators.\n\nTheorem 10 Given any 1D oscillator,\n\\[\n\\frac{d}{dE}\\oint_{\\gamma_E} pdq = T(E)\n\\]\nwhere \\(\\gamma_E\\) is the path traced out by the oscillator when it has energy \\(E\\), and \\(T(E)\\) is the period.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nConsider the phase space plot of a generic 1D oscillator. Every point in its phase space must go in a cycle, returning to the start again. Thus, its phase space is like an ugly onion: It is split into cycles, which are not generally circular in shape, and generally each cycle has a different cycling period.\n\n\n\n\n\nTake a particular cycle as shown, starting and ending at \\(q_0 = q_1\\). Now consider this variation that fixes \\((t_0, q_0), (t_1, q_1)\\): move from point 0 to point 1, then go around the larger cycle to point 1, then return to point 0. The action of the varied cycle consists of three parts: \\(0\\to 1, 1 \\to 1, 1 \\to 0\\). By (modified) Hamilton’s principle, the variation of action is zero.\nNow, let \\(T(E) = t_1 - t_0\\) be the cycle period for the cycle with energy \\(E\\), then from our above argument, we have\n\\[\\oint_E pdq - E T(E) = \\oint_{E + \\delta E} pdq - (E + \\delta E) T(E) + O(\\delta E^2)\\]\nwhere the \\(O(\\delta E^2)\\) term deals with the \\(0\\to 1, 1\\to 0\\) parts of \\(\\int -Hdt\\). Thus we obtain\n\\[\\frac{d}{dE} \\oint_E pdq = T(E)\\]\n\n\n\n\n\n\n\n\n\nWorked example: pendulum in gravity\n\n\n\n\n\nThe 1D pendulum in gravity, with length \\(l\\) and mass \\(m\\), has Lagrangian \\(L = \\frac 12 m(l\\dot q)^2 + mgl\\cos q\\), momentum \\(p = ml^2 \\dot q\\), and Hamiltonian\n\\[H = \\frac{p^2}{2ml^2} - mgl \\cos(q)\\]\nWe know that the pendulum is not a SHO. Indeed, the cycle period \\(T(E)\\) strictly increases with energy \\(E\\) of the cycle, and it diverges as the pendulum swing approaches the highest point: \\(\\lim_{E \\to mgl} T(E) = +\\infty\\).\nConsider the cycle with maximum swing angle \\(\\theta\\). The cycle encloses an oval-shaped region in phase space, with equation \\(p^2 = 2m^2 gl^3(\\cos q - \\cos\\theta)\\). Consequently, we have the somewhat mysterious result:\n\\[\\int_0^{E(\\theta)} T(E) dE = 4\\sqrt{2m^2 gl^3} \\int_0^\\theta \\sqrt{\\cos q - \\cos\\theta} dq\\]\nwhere \\(E(\\theta) = -mgl \\cos\\theta\\) is the energy of the system when it has maximum swing angle \\(\\theta\\).\nWhen \\(\\theta\\) is small, the integral is approximately\n\\[\\int_0^\\theta \\sqrt{\\frac 12 (\\theta^2 - q^2)}dq = \\frac{\\pi\\theta^2}{4\\sqrt 2}\\]\nwhich does correspond to \\(T(E) \\approx 2\\pi\\sqrt{\\frac lg}\\), and \\(\\delta E = mgl(1 - \\cos(\\theta)) \\approx \\frac 12 mgl \\theta^2\\).\nWhen \\(\\theta = \\pi\\), the integral can be exactly evaluated:\n\\[\\int_0^{mgl} T(E) dE = 16\\sqrt{m^2 gl^3}\\]\nWe are unable to interpret this strange but satisfying equality.\nTaking \\(\\partial_\\theta\\) under the integral sign, we get\n\\[T(E) = \\sqrt{\\frac{8l}g} \\int_0^\\theta \\frac{dq}{\\sqrt{\\cos q - \\cos \\theta}}\\]\nwhich may be directly verified.\n\n\n\n\n\nAction-angle variables\nNow, it would be great if we could “unwind” the rotatory dynamics by a time-independent canonical transform to some \\((Q, P)\\), by where \\(P\\) is constant along the cycles, and \\(Q\\) is increasing. That is, we want \\(P\\) to be the analog of “amplitude”, and \\(Q\\) to be the analog of “phase angle”.\nSince the transform is time-independent and canonical, the Hamiltonian \\(H\\) is unmodified, so \\(H\\) is a function of \\(P\\) only, not \\(Q\\) (since \\(H\\) is a conserved quantity of motion). Then, since the transform is canonical, Hamilton’s equations of motion read \\(\\dot Q = \\partial_P H(P)\\). Consequently, the Hamiltonian equations of motion would become\n\\[\\begin{cases} P(t) = P(0) \\\\ Q(t) = Q(0) + H'(P) t \\end{cases}\\]\nas simple as it could be! It remains to find such a canonical transform.\nWe are already mostly there: we know that \\(P\\) is constant along the cycles, and \\(Q\\) increasing along the cycles. It remains to find the right scaling, so that the transform is canonical, that is, the coordinates preserve area: \\(dP \\wedge dQ = dp \\wedge dq\\).\nDefine \\(P = \\frac{1}{2\\pi} \\oint pdq\\). Here the \\(2\\pi\\) factor is not essential, since we could always do a point transform, scale down \\(Q\\) by \\(2\\pi\\), and scale up \\(P\\) by \\(2\\pi\\). However, the factor will make many formulas look cleaner.\nFrom the proof of Theorem 10, we know that increasing the energy of the cycle by \\(\\delta H\\) would increase the cycle area by \\(T(H)\\delta H\\), and the cycle area is \\(2\\pi P\\), thus\n\\[\\delta(2\\pi P) = T(H) \\delta H \\implies \\frac{2\\pi}{T(H)} = H'(P)\\]\nso we find that the equations of motion are:\n\\[\\begin{cases} P(t) = P(0) \\\\ Q(t) = Q(0) + 2\\pi \\frac{t}{T(H)} \\end{cases}\\]\nThis allows us to graphically construct the \\((Q, P)\\) coordinates on phase space:\n\nDraw the cycles in phase space.\nSelect a “line of longitude” arbitrarily as \\(Q = 0\\) line.\nFollow the trajectory of each point on the line of longitude, and mark down a new line of longitude at equal phases-angles. So for example, if you are on the cycle of energy \\(E\\), you would start the ride, and after \\(T(E)/3\\) has passed, note down its phase-angle as \\(2\\pi/3\\). The set of all such points at every cycle is the line of longitude with \\(Q = 2\\pi/3\\).\n\n\n\n\nConstructing a canonical coordinate system over the phase space of the 1D oscillator.\n\n\nNow, we can graphically see that this construction really preserves areas:\n\nTake an infinitesimal parallelogram at \\((Q, P)\\) with sides \\(\\delta Q, \\delta P\\), such that \\(\\delta Q = \\frac{2\\pi}{N}\\) for some infinite integer \\(N\\).\nEvolve the system in discrete steps of \\(\\frac{T(P)}{N}\\), and follow the parallelogram along.\nNote that the parallelogram would tile the thin ring between \\(P\\) and \\(P+ \\delta P\\). The thin ring has area \\(\\delta \\oint pdq = 2\\pi \\delta P\\), so each parallelogram has area \\(2\\pi \\delta P/N = \\delta P \\delta Q\\).\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is not as trivial as it seems. The “area” in phase space was defined by \\(\\oint pdq\\), but we did not say that \\(p, q\\) are perpendicular in any sense. Thus, it is not obvious that \\(\\delta P \\delta Q\\) would be the area of the parallelogram. Indeed, we should not say that \\(p, q\\) are perpendicular, because angles do not exist and should not exist in phase space.\n\n\nThe entire construction of \\((Q, P)\\) from \\((q, p)\\) is often done in one divinely-inspired move by a generating function.\n\n\nAdiabaticity\nThis section based on (Duncan and Janssen 2019, chap. 5; de Oliveira 2022).\nThe Lorentz pendulum is a famous example that connects the classical and the quantum world. In the early 1900s, as physicists were trying to explain various phenomena, such as the black body radiation spectrum, the atomic spectra, and such, they came to the quantum hypothesis. Consider a classical system undergoing periodic motion – such as the electrons cycling around a proton in the hydrogen atom. Whereas classically, its \\(\\oint pdq\\) may be of any value, the quantum hypothesis states that it can only take values in\n\\[\n\\oint pdq = nh, \\quad n = 1, 2, 3, \\dots\n\\]\nfor a certain constant of nature \\(h\\) – later given the name of Planck’s constant. For example, if we have a simple harmonic oscillator, then we have\n\\[\nE\\nu = n h\n\\]\nwhere \\(\\nu = 1/T\\) is the frequency of the oscillator.\nAt the 1911 Solvay Conference, where the great physicists grappled with the new quantum phenomena,18 Einstein gave a presentation on the quantum hypothesis. At the end of the presentation, Lorentz asked a question about the pendulum. The conversation went as follows:\n18 The entire conference is summarized in (Straumann 2011).\nMr. Lorentz recalls a conversation he had with Mr. Einstein some time ago, in which they discussed a simple pendulum that could be shortened by holding the string between two fingers and sliding them downwards. Suppose that initially, the pendulum has exactly one energy element corresponding to the frequency of its oscillations. It then seems that at the end of the experiment, its energy will be less than the element corresponding to the new frequency.\nMr. Einstein – If the length of the pendulum is changed infinitely slowly, the energy of the oscillation remains equal to \\(h\\nu\\), if it was initially equal to \\(h\\nu\\) ; it varies proportionally to the frequency. The same is true for a resistance-free oscillating electrical circuit, and also for free radiation.\nMr. Lorentz – This result is very curious and removes the difficulty. In general, the hypothesis of energy elements gives rise to interesting problems in all cases where one can change the frequency of vibrations at will.\n(Instituts Solvay et al. 1912, 450)\n\n\n\n\nThe adiabatic pendulum of Lorentz. Figure from (Fowler 2020)\n\n\nWhy is this interesting? The quantum hypothesis states that for quantum oscillators, \\(E/\\nu = nh\\), where \\(n\\) can only take values in the natural numbers. The Lorentz pendulum seems to show that the quantity \\(E/\\nu\\) can change continuously. Thus, for example, we might start at a quantized value of \\(E/\\nu = 100h\\), and end up at \\(100.5h\\), invalidating the quantum hypothesis for macroscopic systems. And if macroscopic systems do not follow the quantum hypothesis, then as the macroscopic system becomes microscopic, it seems the quantum hypothesis would be invalidated as well.\nEinstein replied that \\(E/\\nu\\) of the oscillator is conserved if the length of the pendulum is changed infinitely slowly. Surprising, but it saves the quantum hypothesis.\n\nThe adiabatic theorem\nConsider a pendulum with a string length \\(\\lambda\\) that is slowly changed over time. How slow? Slow enough that the system completes many cycles before \\(\\lambda\\) makes any appreciable change. That is,\n\\[\\dot \\lambda \\ll \\frac{\\lambda}{T(\\lambda)}\\]\n\n\n\n\n\n\nParametric resonance\n\n\n\nIt is vitally important that the pulling on the pendulum is not only slow, but also “smeared”, meaning that \\(\\dot\\lambda\\) is equal over the entirety of a single oscillation. If it is not smeared, then we can break the theory. Consider for example a (spherical) child on a (frictionless) swing (in a vacuum). It is well-known that the child can, by swinging the legs in sync with the swing, get as high as possible. This is called parametric resonance.\nCompared to adiabatic change, parametric resonance differs in that it is discriminating about the states. When you adiabatically pull on the string of a pendulum, your rate of pulling is the same over the entire cycle of a pendulum swing. When you resonantly pull on the string of a pendulum, your rate of pulling differs over the cycle of a pendulum swing. In the language of thermodynamics, adiabaticity means treating all microstates equally, without discrimination.\n\n\nLet the angular frequency of oscillation be \\(\\omega = \\sqrt{\\frac g\\lambda}\\), then \\(E/\\omega\\) is constant over time.\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy basic physics, the force in the string is\n\\[\nF = mg \\cos \\theta + m\\dot \\theta^2 / \\lambda \\approx mg - \\underbrace{\\frac 12 mg \\theta^2}_{\\text{$V/\\lambda$}} + \\underbrace{m\\lambda \\dot\\theta^2}_{\\text{$2T/\\lambda$}}\n\\]\nBecause the system is undergoing simple harmonic oscillation, the time-average of \\(V\\) and \\(T\\) are both \\(\\frac 12 E\\), where \\(E\\) is the oscillation energy. Therefore, the time-average force is \\(\\bar F = mg + \\frac 12 E/\\lambda\\).\nThus, if we shorten the string by \\(\\delta \\lambda\\), we would inject an energy of \\(\\delta E = \\bar F (-\\delta\\lambda) -( mg\\delta\\lambda)\\) (we subtract away the gravitational energy at the lowest point, as it is irrelevant). This gives us\n\\[\n\\delta E + E \\delta\\lambda / 2\\lambda = 0 \\implies E\\lambda^{1/2} = \\Const\n\\]\nSince the angular frequency of oscillation is \\(\\omega = \\sqrt{\\frac g\\lambda}\\), we have the result.\n\n\n\n\nExercise 9 Perform the same analysis on a vibrating string. The string is fixed at both ends, and the length of the string is slowly changed. The string is in a standing wave pattern, and the length of the string is changed so slowly that the string completes many cycles before the length changes appreciably. What is the adiabatic invariant in this case? The answer is in (Rayleigh 1902).\n\n\n\n\n\n\n\nAdiabatic invariance to all orders\n\n\n\n\n\nThe adiabatic invariance is actually much stronger than what we have shown. It is not just that the enclosed action \\(I\\) is conserved up to \\(O(\\dot\\lambda)\\), but that it is conserved to all orders in \\(O(\\dot \\lambda^n)\\) (Lenard 1959). Under stronger restrictions, it is even conserved to order \\(O(e^{c/\\dot \\lambda})\\). See (Henrard 1993, sec. 4) for more theorems in this style.\n\n\n\nGeneralizing from this experience, for an arbitrary 1D oscillator with Hamiltonian \\(H(q, p; \\lambda)\\), the phase space trajectory is a closed wobbly cycle. As we vary \\(\\lambda\\) adiabatically by external force, the cycle changes shape, both because the system has received energy from the external force, and because the Hamiltonian of the system has changed. Nevertheless, the area enclosed within should remain constant.\n\n\n\n\n\n\nProof\n\n\n\n\n\nIn general, force is the energetic cost of changing a parameter. That is, \\(F = (\\partial_{\\lambda} H)_{p, q}\\). Here, we are using a notation commonly used in thermodynamics: for partial derivatives, we write in the subscript the variables being held constant.\nLet us adiabatically vary \\(\\lambda\\). After many cycles have passed, the oscillator is orbiting around the cycle defined by \\(H(q, p; \\lambda) = E\\). After many more cycles have passed, we have varied \\(\\lambda\\) by \\(\\delta \\lambda\\), and the oscillator would be orbiting around the cycle defined by \\(H(q, p; \\lambda + \\delta \\lambda) = E + \\delta E\\), where \\(\\delta E = \\bar F \\delta \\lambda\\) is the increase in oscillator energy due to the external force varying \\(\\lambda\\).\n\n\n\n\n\nAs pictured, for each point \\((q_0, p_0)\\) on the first cycle, we move by \\(\\delta p\\), to end up with point \\((q_0, p_0 + \\delta p)\\) on the second cycle. The cycle area would change by\n\\[\\delta(\\text{area}) = \\oint (\\delta p)dq = \\int_0^T (\\delta p )\\dot q dt + O(\\delta T \\delta p)\\]\nwhere \\(T\\) is the cycle time of the first cycle, and \\(T + \\delta t\\) is the cycle time of the second cycle.\nExpanding and simplifying,\n\\[\nH(q_0, p_0 + \\delta p ; \\lambda + \\delta \\lambda) = E + \\underbrace{\\delta E}_{\\text{$= \\bar F \\delta \\lambda$}} \\implies\n(\\partial_p H)_{\\lambda, q}\\delta p = [\\overline{(\\partial_\\lambda H)_{q, p}} - (\\partial_\\lambda H)_{q, p}] \\delta\\lambda\n\\]\nNow, integrate over a single cycle, and using the Hamilton equation of motion \\(\\dot{q} = (\\partial_p H)_{\\lambda, q}\\),\n\\[\\delta(\\text{area}) =  \\delta\\lambda \\int_0^T [\\overline{(\\partial_\\lambda H)_{q, p}} - (\\partial_\\lambda H)_{q, p}] dt + O(\\delta^2)\\]\nBy definition, \\(\\overline{(\\partial_\\lambda H)_{q, p}} = \\frac 1T \\int_0^T (\\partial_\\lambda H)_{q, p}dt\\), thus we have \\(\\delta(\\text{area}) = O(\\delta^2)\\). Thus, integrating over the entirety of the adiabatic process, \\(\\Delta(\\text{area}) = O(\\delta)\\), which converges to zero as the adiabatic process becomes infinitely slow.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a multidimensional oscillating system, we have three possibilities.\nIf the dimensions separate, like an \\(n\\)-dimensional oscillator \\(H = \\frac{p_1^2 + \\dots + p_n^2}{2m} + \\frac 12 k (q_1^2 + \\dots + q_n^2)\\). In this case, the adiabatic theorem still applies along each dimension, with one adiabatic invariant per dimension.\nIf all dimensions mix together, like a tank of hot, entangled gas. In this case, the adiabatic theorem states that if we start with the microcanonical ensemble, then the phase space volume enclosed by the surface of \\(H = E\\) remains constant as we adiabatically vary \\(\\lambda\\). The volume is named the Gibbs invariant. (de Oliveira 2022)\nIf the dimensions neither separate nor mix together, but have some kind of complicated dynamics, then what adiabaticity means in that case is still a current area of research.\n\n\n\n\nConnection to thermodynamics\nConsider a ball in a piston, bouncing elastically off its two ends. We hold one end of the piston constant, and slowly move the other. Assume the ball only moves in the \\(x\\)-direction for simplicity. Let the ball have speed \\(v\\), and the walls of the piston have separation \\(L\\).\nThe phase space diagram of the system is a rectangle with \\(\\Delta q = L, \\Delta p = (mv) - (-mv) = 2mv\\), enclosing an area of \\(I = 2mvL\\). If we slowly move the wall of the piston, then the action is conserved, giving us\n\\[\n2mv_0 L_0 = 2mvL \\implies v = \\frac{L_0}{L} v_0\n\\]\nWe can formulate this into the language of thermodynamics. First, expand our system to three dimensions – from a piston to a box. Since the motion of the ball in the \\(x, y, z\\) directions are independent, we can treat them separately. We also assume equipartition of energies, that is, the energy of the ball is equally distributed among the three dimensions, so \\(v_{x, 0} = v_{y, 0} = v_{z, 0}\\). The conservation of action then states that\n\\[\nv_i = \\frac{L_{i, 0}}{L_i} v_{i, 0} \\quad \\text{for } i = x, y, z\n\\]\nImagine the ball is a gas molecule, and the piston is a wall of a container. Let \\(E = \\sum_{i = x, y, z} \\frac 12 mv_i^2\\) be the energy of the gas molecule, and \\(V = \\prod_{i = x, y, z}L_i\\) be the volume of the gas. We then have\n\\[\nV = V_0 \\prod_i \\frac{L_i}{L_{i, 0}} \\approx V_0 \\left(1 + \\sum_i \\frac{\\delta L_{i}}{L_{i, 0}}\\right), \\quad\n\\begin{aligned}\nE &= \\frac 12 m \\sum_i \\left(\\frac{L_{i,0}}{L_i}\\right)^2 v_{i, 0}^2\\\\\n&= \\frac 32 m v_0^2 \\frac 13 \\sum_i \\left(\\frac{L_{i,0}}{L_i}\\right)^2 \\\\\n&\\approx E_0 \\left(1 - \\frac 23 \\sum_i \\frac{\\delta L_{i}}{L_{i, 0}}\\right)\n\\end{aligned}\n\\]\nThese imply that \\(E^{3/2}V \\approx E_0^{3/2}V_0\\). Now, this is precisely what happens if you compress an ideal gas adiabatically. This is one connection between the concept of “adiabatic” in classical mechanics and thermodynamics.\n\nExercise 10 Prove that \\(I\\) is conserved: calculate the average force \\(\\bar F\\) on the piston, then calculate the work done by the piston.\n\n\n\nMore examples\nGiven a Lorentz pendulum and a schedule for varying its arm length, we can plot the angle \\(x\\) as a function of time, and see directly that \\(I = T (\\frac 12 m \\dot x_{max}^2)\\) is conserved over many swings of the pendulum.\n\n\n\nA Lorentz pendulum with length increasing linearly with time. As its length increases, \\(\\dot x_{max}\\) decreases while \\(T\\) increases, keeping \\(I\\) conserved. Annotated from (Sánchez-Soto and Zoido 2013, fig. 1)\n\n\nFor a more rigorous proof of adiabatic invariance at the level of first-year graduate student, see (Wells and Siklos 2007). (Henrard 1993) is a good comprehensive review of adiabatic invariance in classical mechanics, and points to the literature on a lot of applications in celestial mechanics, magnetism, and the geometry of phase space plots. In textbooks on classical statistical mechanics, the adiabatic invariance theorem is used to derive many results. (Fernandez-Pineda, Diez de los Rios, and Mengual 1982) gives some worked-out examples, such as the ideal gas and the photon gas."
  },
  {
    "objectID": "essays/posts/analytical-mechanics/index.html#old-quantum-mechanics",
    "href": "essays/posts/analytical-mechanics/index.html#old-quantum-mechanics",
    "title": "Analytical Mechanics",
    "section": "Old quantum mechanics",
    "text": "Old quantum mechanics\n\nSommerfeld, in the new edition of his book [Atomic structure and spectral lines], has introduced the adiabatic hypothesis through a couple of very elegant changes and footnotes, in such a way that my participation in that can rather appear reduced to — a plagiarism. Lorentz and Einstein have founded the subject, I have given it a name and Burgers has put everything in order. I was first very, very depressed. I know that I have never discovered anything, and quite surely never will discover anything, that I can love so fervently as this line of thought which I found out with so large joy.\nEhrenfest’s letter to Bohr, 8 May 1922. Emphasis in original. Quoted in (Navarro and Pérez 2006)\n\n\nThe adiabatic hypothesis\nThere are two ways to interpret the word “adiabatic”. In thermodynamics, “adiabatic” means “no exchange of heat”. In mechanics, “adiabatic” means “gradual”.19 It is a winding story of how one word “adiabatic” came to mean two things that fortuitously are connected after all, the details of which are given in (Jammer 1966, chap. 3; Laidler 1994). The story is almost as interesting as that of the word “entropy”, which also has two meanings that are fortuitously connected after all.\n19 Because “adiabatic” has two meanings in English, in Chinese, it also has two different translations based on the two meanings. There is 绝热, which means “no exchange of heat”, and 浸渐, which means “gradually, like moisture soaking into something”.Rankine first coined the term “adiabatic” in 1858, to denote a process in which no heat is exchanged with the surroundings. Later, Boltzmann and Clausius tried to explain the second law of thermodynamics mechanically, by using purely mechanical models of the microscopic world. In this sense, they defined an “adiabatic” mechanical process to be one where a certain variable is slowly changed (for example, if we have a box of little bouncing balls, and we slowly move its walls), because an adiabatic thermodynamic process in their view is actually an adiabatic mechanical process.\n“Adiabatic motion” in mechanics was introduced by Helmholtz and Hertz, to denote mechanical processes where external forces act upon a system, but only on a few parameters, with no action on the underlying details. For example, think back to the case of bouncing balls in a box. The external force only moves the walls of the box on average, with no attempt to move the walls to manipulate the precise location of the ball. They used the thermodynamic terminology, because the work done on the system during an adiabatic motion results exclusively in changes in its energy. (Jammer 1966, chap. 3)\nIn 1900, Pyotr Lebedev experimentally proved that radiation pressure exists, and follows Maxwell’s theory of electromagnetism. Inspired by this, Lord Rayleigh (Rayleigh 1902) generalized the concept of radiation pressure to all kinds of vibrations, starting with the humble pendulum. Since his goal was to understand what happens when you adiabatically compress a photon gas, that is, Wien’s displacement law,20 he studied the effect of adiabatic motion on some simple mechanical systems undergoing wave motion, such as the Lorentz pendulum, a vibrating cord, a piston of gas with a standing acoustic ave, etc.\n20 Since he didn’t actually know what a photon was, it might be better to say that he was studying what would happen when you compress a hot chamber of light. Though Wien’s displacement law is nowadays proved straight from quantum mechanics, back when ien discovered it in 1893, he used a thermodynamic argument using the adiabatic compression of a photon gas. For a brief presentation of how Rayleigh did it, see (Ter Haar 1966).Like Lord Kelvin, Paul Ehrenfest was also trying to explain Wien’s displacement law. He was puzzled by the fact that while Wien’s displacement law was derived without the quantum hypothesis, yet somehow, it remains true. Suppose we start with a box of light, then it follows a certain black body radiation, which can only be derived by the quantum hypothesis. Now suppose we adiabatically compress the box of light. Though the compression process is studied classically, without the quantum hypothesis, the final state of the light is still the black body radiation. So, it seems that if we start with a system following the quantum hypothesis, then any adiabatic classical process would give us a system that still follows the quantum hypothesis. This is his adiabatic hypothesis, for which he is famous.\nDuring the 1910s, Ehrenfest published a series of papers to subsume the many ad-hoc quantum rules under the framework of the adiabatic hypothesis. His idea is as follows: If, in a periodic system described by classical mechanics (such as an electron orbiting a proton), a certain quantity \\(I\\) has units of joule-second, and is conserved as it undergoes adiabatic motion, then this quantity should be quantized, and only this quantity can be quantized. Only \\(I\\) can be quantized, for the reason discussed by Lorentz and Einstein at the Solvay conference.\nFurther, \\(I\\) should be quantized, because otherwise, why else should \\(I\\) be adiabatically conserved? The adiabaticity of \\(I\\) in the classical world, on the surface, is a shadow of the discreteness of \\(I\\) in the quantum world, deep down. Because \\(I\\) is quantized, if we vary the system slowly enough, the system would have no reason to make a big jump from \\(I = nh\\) to \\(I = (n+1)h\\). This discreteness in the quantum world traps \\(I\\) in its starting position, and this is why \\(I\\) appears adiabatic in the classical world.\nThus, quantized quantities are precisely adiabatic invariants. We can write down \\(I = nh\\), where \\(n \\in \\mathbb{N}\\), and proceed to calculate the properties of the system, such as its energy levels, its absorption and emission spectra, etc.\nIn short, this is Ehrenfest’s recipe for doing “old quantum mechanics”:\n\nFind a system that has a conserved quantity \\(I\\) under adiabatic motion.\n\nYou can do this by solving the equations of motion, then integrate \\(\\oint pdq\\).\nAlternatively, you can start with a harmonic oscillator, and adiabatically deform the system until it becomes the system you want. Ehrenfest called such systems “adiabatically related to the harmonic oscillator” (Jammer 1966, 99).\n\nProclaim that \\(I = nh\\) for some constant \\(h\\).\nCalculate the properties of the system.\n\n\n\nThe Bohr–Sommerfeld model\nAs an example, consider the pinnacle of old quantum theory, the Bohr–Sommerfeld model of the hydrogen atom. First, treat the hydrogen atom as if it is a relativistic solar system, with the electron as a planet, moving at relativistic speeds21 under the inverse-square force \\(F \\propto \\frac{1}{r^2}\\). Next, assume that the adiabatic invariant is quantized:\n21 Because the electron moves faster closer to the atom than further away, it has more mass close to the atom than further away. This allows the orbit to precess in a way that is not predicted by Newtonian mechanics. Generally, any perturbation of the inverse-square force will cause the orbit to precess. Even special relativity would predict some precession for the perihelion of Mercury, though it predicts a value of \\(7''/\\text{year}\\) only \\(1/6\\) of the correct value (McDonald 2023; Goldstein, Poole, and Safko 2008, exercise 7.27, page 332). Only general relativity predicts the correct value. Assuming that only special relativity contributes, the perihelion of Mercury would take \\(7.5 \\times 10^7\\) Mercury-years to go around the sun once.\nAs the electron moves a lot faster than Mercury, it takes much shorter time, but still it takes \\(4\\times 10^4\\) electron-years to go around the nucleus once, as noted in Bohr’s Nobel lecture (Bohr 1923).\\[\\int_0^T p_r \\,dq_r = n' h\\]\nwhere \\(q_r\\) is the radial position of the electron, \\(p_r\\) is the momentum conjugate to it, \\(n'\\) is the “auxiliary quantum number”, and \\(T\\) is the period of the electron’s orbit. This, when combined with the hypothesis of quantized angular momentum \\(mvr = nh\\) (here, \\(n\\) is the “principal quantum number”), would predict the emission spectrum of the hydrogen atom.22\n22 Because the equation of motion for the electron separates into a radial component \\(q_r, p_r\\) and an angular component \\(q_\\theta, p_\\theta\\), we can apply the multidimensional adiabatic theorem and find that both \\(\\int_0^T p_r \\,dq_r\\) and \\(\\int_0^T p_\\theta \\,dq_\\theta\\) are adiabatic invariants.Sommerfeld went further, piling epicycles upon epicycles, to explain the fine structure of atomic spectra. All those has been swept away by the new quantum theory like how the new astronomy of Kepler swept away the epicycles of Ptolemy. But the adiabatic hypothesis remains to this day a fruitful meeting point between the classical, the quantum, and the thermodynamic world.\nHave some cool pictures, because I like cool pictures.\n\n\n\n\n\nSelections from (Sommerfeld 1923).\n\n\n\n\n\n\n\nQuantization of the adiabatic invariant. Page 197.\n\n\n\n\n\n\n\n\n\nThis looks familiar… it looks just like the phase space plot of a ball bouncing in a piston. Page 199.\n\n\n\n\n\n\n\n\n\nOrbits of the hydrogen atom with the same principal quantum number \\(n\\), but different auxiliary quantum number \\(n'\\). Page 240.\n\n\n\n\n\n\n\n\n\n5 electrons with the same principal and auxiliary quantum numbers, interacting by the pentagonal dance. Page 502.\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\n\n\nSelections from (Kramers 1923).\n\n\n\n\n\n\n\nOrbits of the Bohr–Sommerfeld model of the hydrogen atom, labelled by their principal quantum numbers and auxiliary quantum numbers. The jumps from 31, 32, 33 to 22 all create the Balmer spectral line Hα, but they differ at the fine structure. Figure 27.\n\n\n\n\n\n\n\n\n\nOrbits of the Bohr–Sommerfeld model of the Radium atom. First end plate. I find its fonts classy.\n\n\n\n\n\n\n\n\n\nA modern redrawing. (Holtebekk and Linder 2023)\n\n\n\n\n\n\nFigure 2\n\n\n\n\nWhat we are nowadays hearing of the language of spectra is a true “ music of the spheres ” within the atom, chords of integral relationships, an order and harmony that becomes ever more perfect in spite of the manifold variety. The theory of spectral lines will bear the name of Bohr for all time. But yet another name will be permanently associated with it, that of Planck. All integral laws of spectral lines and of atomic theory spring originally from the quantum theory. It is the mysterious organon on which Nature plays her music of the spectra, and according to the rhythm of which she regulates the structure of the atoms and nuclei.\nPreface to the first edition of Atomic structure and spectral lines, Arnold Sommerfeld, 1919. (Sommerfeld 1923)\n\n\n\nThe Einstein—Brillouin–Keller quantization\nThis topic is quite obscure and hard to find a simple reference for, yet I found it is absolutely necessary to treat this correctly, if only to soothe my mathematical conscience. I wrote it based on (Stone 2005; Duncan and Janssen 2019, chap. 5).\nLet’s take a more careful look at the Bohr–Sommerfeld model of a hydrogen atom. The electron orbits a proton, and the equation of motion is spherically symmetric. Therefore, we can write it in spherical coordinates \\((r, \\theta, \\psi)\\), where \\(r\\) is the radius, \\(\\theta\\) is the co-latitude, and \\(\\psi\\) is the longitude. The Bohr–Sommerfeld quantization then states that\n\\[\n\\begin{aligned}\n& \\oint p_r d r = n_r h \\\\\n& \\oint p_{\\theta} d \\theta=n_\\theta h, \\\\\n& \\oint p_\\psi d \\psi=n_\\psi h .\n\\end{aligned}\n\\]\nfor some positive integers \\(n_r, n_\\theta, n_\\psi\\). However, we notice something deeply unsatisfying: How does the atom “know” which way is the sphere pointing? To define the spherical coordinates, we need to define the direction of the north and south pole. The Bohr–Sommerfeld quantization condition is thus creating an artificial direction in space where none should exist. Worse, if we solve the equations, we would find that this artificial direction has physically measurable consequences.\nSommerfeld evaded the difficulty by arguing that as long as there is even a hair of magnetic field \\(B\\) pointing in some direction \\(\\hat z\\), we can pick \\(\\hat z\\) as the north pole direction, and that since the field strength is nowhere zero, the problem will never occur in practice.\nWe see the difficulty inherent in the old quantum theory. Suppose we have a hydrogen atom suspended in free space, then the tiniest change in the external magnetic field would create a large (for the atom, at least) change in its north-pole direction. The Stern–Gerlach experiment, performed in 1922, experimentally showed that the external field can determine the direction of \\(\\hat z\\), and thus the quantization of angular momentum. To see something in classical mechanics so jumpy is disconcerting, and certainly disturbed me greatly when I first understood the Stern–Gerlach experiment. In 4 years, Schrödinger would have proposed his equation, Heisenberg his matrix mechanics, and old quantum theory washed away by the new quantum mechanics.\nEinstein, in his only paper on the old quantum mechanics,23 elegantly resolved the problem by using the Poincaré–Cartan integral invariant to construct quantization equations that do not depend on our arbitrary choices of coordinate systems.\n23 (Einstein 1917), reprinted and translated in (Einstein 1997, vol. 6, pages 434–444).24 In the jargon of topology, this is a linear function on \\(T^d\\), the homology group of the torus. Here, \\(d\\) is the dimension of the torus.As we saw with Exercise 2, a contractable loop on the torus integrates to zero. Thus, we can attach and detach contractible loops at will, deform the cycle arbitrarily, and still get the same number. That is, the integral is determined by the topology of the cycle, not the exact shape of the cycle.24 This gives us the Einstein quantization:\n\\[\n\\oint_{\\gamma_i} \\left\\langle p, dq\\right\\rangle = n_i h\n\\]\nwhere \\(\\gamma_1, \\dots, \\gamma_d\\) range over the \\(d\\) topologically distinct loops around the \\(n\\)-dimensional torus in phase space, and \\(n_1, \\dots, n_d\\) are positive integers. This was later modified by Brillouin and Keller to take account of singularities in the trajectory, such as a hard wall reflector, giving us the EBK quantization.\n\n\n\nA particle moving in a central potential. Its angular momentum \\(p_\\theta\\) is conserved, so we do not plot it. Plotting its other three phase-space variables \\((r, \\theta, p_r)\\), we see that its orbit makes a spiralling and rotating pattern on a torus. There are two fundamental ways to cycle around the torus, and each has a corresponding Poincaré–Cartan integral that is independent of the precise shape of the cycle. Both integrals are quantized. This is the meaning of the EBK quantization.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe EBK quantization resembles the Bohr–Sommerfeld quantization condition, but it is quite different. In the Bohr–Sommerfeld quantization condition, the integral \\(\\oint p_i dq_i\\) integrates over a cycle of physically real trajectory in phase space. In EBK quantization, the integral \\(\\oint_{\\gamma_i} \\left\\langle p, dq\\right\\rangle\\) integrates over a geometric cycle in phase space with no physical reality at all.\nBoth conditions happen to be the same when we have a 1D oscillator, but that is because the only way to physically go around the torus is the only way to geometrically go around the torus. It is a misleading coincidence.\nIn general, the physically real trajectories look like a braiding on the torus, and are not even closed cycles. They definitely do not go around the torus exactly once in exactly one direction.\n\n\nThere is only one problem left: What happens when we don’t have a torus? It is all well and good when we can construct a torus in phase space with as many dimensions as possible. However, when we have a classically chaotic system, such as the three-body problem, we cannot have something this simple.\nConsider the simplest case of the three-body problem: We put two suns in a circular orbit around each other, and a speck of dust orbiting them. The two suns’ orbit is perfectly predictable, so we consider only the trajectory of the dust. We also assume the dust only moves in the plane of the suns. Thus, the system has only 2 dimensions of configuration, or 4 dimensions of phase.\nAs the energy of the system is conserved, the motion of the dust is restricted to the constant energy surface \\(H(q_1, q_2, p_1 p_2) = E\\), which is a 3-dimensional blob within the 4-dimensional phase space. However, in general, this is all we can say about it. The motion of the dust is chaotic, and would densely crisscross over a 3-dimensional subset of the blob.\nWithout a torus in phase space, we cannot find trajectories around the torus, and so the integral \\(\\oint_{\\gamma}\\left\\langle p, dq\\right\\rangle\\) is undefined. Einstein presciently pointed this out in his 1916 paper:\n\nIf there exist fewer than \\(d\\) [constants of motion], as is the case, for example, according to Poincaré in the three-body problem, then the \\(p_i\\) are not expressible by the \\(q_i\\) and the quantum condition of Sommerfeld-Epstein fails also in the slightly generalized form that has been given here.\n\nThis failure of the EBK quantization on classically chaotic systems was forgotten for many years, but eventually rediscovered.25 When it did, it became a seed of quantum chaos, which I hope to explain clearly some day. In the mean time, I leave you with some beautiful pictures of quantum chaos instead.\n25 See (Stone 2005) for a brief history and more cool pictures. It was written in 2005, the “Einstein Year”, the 100th anniversary of Einstein’s annus mirabilis.\n\n\nThe nonchaotic quantum circular billiard and the chaotic quantum cardioid billiard. Figure from (Backer 2007)\n\n\n\n\n\nIn a chaotic quantum “stadium” billiard, the standing wave functions look dark along certain lines. You can think of them as taking a classical billiard table, and using a knife to cut along the classical trajectories. The scars left behind are the quantum scars of departed classicality. Figure from (King 2014)"
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html",
    "title": "Cybernetic Artificial Intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity, as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete; unfortunately, the subject of cybernetic artificial intelligence has essentially never been updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take values in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\) that, given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those mostly involve notational complications, with no new ideas."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#cybernetics",
    "title": "Cybernetic Artificial Intelligence",
    "section": "",
    "text": "In the creation lore of artificial intelligence, John McCarthy coined the term “artificial intelligence” to distinguish his approach from “cybernetics”, which was dominated by Norbert Wiener. However, if that is the case, then we can infer that there was a real danger that artificial intelligence could have been confused with Norbert Wiener’s cybernetics. That is, they must have some real similarity. If the similarity is not in style or approach – as otherwise there would be no need to distinguish the two – then the similarity must be in goal.\nAnd indeed there is. I was first alerted to this by Wiener’s little book (Wiener 1964, chap. 4), which briefly sketched out a machine that could purportedly imitate any machine, including another copy of itself, thus effecting both machine learning and machine reproduction. Unfortunately, it attempted to be a popular science book and explained concepts with a minimum of mathematical symbols, thus providing the reader with a minimum of understanding. Nevertheless, I figured it out by consulting his other books, and so I decided to write this post for posterity, as a simple sketch of another way to think about machine intelligence and machine life. A view of another universe, perhaps, or the theoretical basis of some hard sci-fi to be written.\nThis cybernetic machine is a rather obscure one, and most of the references are decades-old. The main references I relied on to write this essay are (Wiener 1958, chaps. 10, 11; 2019, chap. 9; O’Hagan 2013; G. H. Harris and Lapidus 1967).\nWiener’s terminology is occasionally obsolete; unfortunately, the subject of cybernetic artificial intelligence has essentially never been updated since Wiener, so anyone trying to study the subject must contend with his publications. So here is a list of obsolete terminology and my explanations.\n\nshot noise, shot effect, tube noise: white noise. It is not Poisson noise.\nchaos: a generic term for randomness, or random variables. It is not chaos theory.\nvoltage multiplier: an electronic device that multiplies two voltages together, by \\((x, y) \\mapsto xy\\). It is not the voltage multiplier.\n\n\n\nAccording to Wiener, cybernetics is the science of control and communication in the animal and the machine. To be more precise, he used the following model of sweeping generality: the nonlinear transducer model.\n\nDefinition 1 (signal) A signal is a function of time, written as \\(x(t)\\), where \\(t \\in \\mathbb{R}\\). The signal can take values in any space, but for convenience, we will only consider the case where \\(x\\) is real-valued. That is, we only consider functions of type \\(x : \\mathbb{R}\\to \\mathbb{R}\\).\n\nA transducer transduces, meaning that it takes in a signal and outputs a signal. Antennas, filters, circuits of resistors and capacitors, and essentially electronic devices that do not require power input to run, are transducers. Wiener considered only deterministic, causal transducers, meaning that the output of a transducer at time \\(t\\) is determined by the inputs during the period \\((-\\infty, t]\\).\nIn large sections of electronic engineering, electronic circuits are studied as either linear and continuous, or as nonlinear and discrete. Nonlinear but continuous devices are very difficult to study in general, but Wiener took on it and constructed a theory for general nonlinear continuous transducers.\n\nDefinition 2 (nonlinear transducer)  \n\nThe transducer is a function \\(T\\) that, given any real-valued function \\(x : \\mathbb{R}\\to \\mathbb{R}\\), returns a real-valued function \\(T[x] : \\mathbb{R}\\to \\mathbb{R}\\). In other words, it sends real-valued signals to real-valued signals.1\nThe transducer’s output at any time \\(t\\) is determined by the value of \\(x\\) on the interval \\((-\\infty, t]\\). In other words, it is causal and deterministic.\nThe transducer is stationary in time. That is, if \\(x\\) is the same over the interval \\((-\\infty, t]\\) as \\(x'\\) over the interval \\((-\\infty, t']\\), then \\(T[x](t) = T[x'](t')\\).\nThe transducer has a limited amount of memory, so that its dependence on the high-frequency details of the input signal decreases rapidly as frequency increases. We will explain the meaning of this assumption later.\nThe transducer depends analytically on the frequencies of the input signal. We will explain the meaning of this assumption later.\n\n1 Wiener actually studied general multidimensional signals, but those mostly involve notational complications, with no new ideas."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#orthogonal-functions",
    "title": "Cybernetic Artificial Intelligence",
    "section": "Orthogonal functions",
    "text": "Orthogonal functions\nIn order to present Wiener’s approach to nonlinear control theory, we need a small amount of the theory of orthogonal polynomials. Specifically, we need the Hermite and Laguerre polynomials. They are not as famous as the trigonometric functions, but they are used in the same way as trigonometric functions in Fourier analysis. In Fourier analysis, every well-behaved function (in the Fourier sense) is decomposable as an infinite linear sum of trigonometric functions. Similarly, every well-behaved function (in the Hermite sense) is decomposable into an infinite linear sum of Hermite functions, and the same applies to Laguerre functions.\n\nLaguerre functions\nThe Laguerre polynomials have many equivalent definitions. We will use three of those.\n\nDefinition 3 (Laguerre polynomials) Direct definition:\n\\[\nL_n(x) := \\sum_{k} \\binom{n}{k}\\frac{(-1)^k}{k!} x^k\n\\tag{1}\\]\nDefinition by a generating function:\n\\[\ng(t, x) = \\sum_{n=0}^\\infty t^n L_n(x)=  \\frac{1}{1-t} e^{-tx/(1-t)}\n\\tag{2}\\]\nDefinition by ordinary differential equation:\n\\[\n\\begin{cases}\nxL_n'' + (1-x) L_n' + nL_n &= 0 \\\\\nL_n(0) &= 1 \\\\\nL_n'(0) &= -n\n\\end{cases}\n\\tag{3}\\]\n\n\nProposition 1 The three definitions are equivalent.\n\n\nProof. The direct definition of \\(L_n\\) satisfies \\(xL_n'' + (1-x) L_n' + nL_n = 0\\), and has the same value and first derivative at \\(x=0\\). By the uniqueness theorem of ordinary differential equations, the two definitions are equal.\nBy taking the first partial derivatives \\(\\partial_x g, \\partial_t g\\), and simplifying, we obtain some recurrence relations of \\(L_n\\), including \\(xL_n'' + (1-x) L_n' + nL_n = 0\\). Plugging in the case where \\(x=0\\), we obtain \\(L_n(0) = 1\\) and \\(L_n'(0) = -n\\).\n\n\nProposition 2 (Laguerre polynomials are orthogonal with respect to the exponential distribution) \\[\n\\int_{0}^\\infty e^{-x}L_m(x) L_n(x)dx = \\delta_{mn}\n\\]\n\n\nProof. Explicitly integrate\n\\[\\int_{0}^\\infty e^{-x} g(t, x)  g(s, x) dx\\]\nthen expand the Taylor series of both sides in powers of \\(s, t\\).\n\n\nDefinition 4 (Laguerre functions) \\[\n\\psi_n(t) := e^{-t/2}L_n(t)\n\\]\n\nThe Laguerre functions make the orthogonality cleaner:\n\\[\n\\int_0^\\infty \\psi_n(t)\\psi_m(t) dt = \\delta_{mn}\n\\tag{4}\\]\nJust like in Fourier analysis, by virtue of orthogonality, we can represent any well-behaved function on \\((-\\infty, t]\\) as an infinite sum of Laguerre functions\n\\[\nf(t-\\tau) = \\sum_{n \\geq 0} c_n \\psi_n(\\tau)\n\\]\nby taking a convolution with the Laguerre functions\n\\[\nc_n = \\int_0^\\infty f(t-\\tau) \\psi_n(\\tau) d\\tau.\n\\]\n\n\nHermite polynomials\n\nDefinition 5 (physicist’s Hermite polynomials) \\[\n\\sum_n H_n(x) \\frac{1}{n!}t^n = e^{-t^2 + 2tx} = g(t, x)\n\\tag{5}\\]\n\n\nProposition 3 (Hermite polynomials are orthogonal with respect to the normal distribution with variance 1/2) \\[\\int e^{-x^2}H_n(x) H_m(x) dx = \\sqrt\\pi 2^n  n! \\delta_{mn}\\]\n\n\nProof. \\[\\int e^{-x^2} g(t, x)g(s, x) dx = \\sum_{n, m \\geq 0}\\frac{1}{n! m!}t^ns^m \\int e^{-x^2} H_n(x) H_m(x) dx\\]\nDirectly compute the left-hand side and find that it equals \\(\\sqrt\\pi e^{2st}\\). Now expand it in powers of \\(s\\) and \\(t\\)."
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#learning-and-reproducing-any-transducer",
    "title": "Cybernetic Artificial Intelligence",
    "section": "Learning and reproducing any transducer",
    "text": "Learning and reproducing any transducer\nNow we are ready to perform the “Hermite–Laguerre expansion”, Wiener’s way to analyze (learn) and synthesize (reproduce) arbitrary transducer using pure analog devices.\n\nAlgebra of analog circuitry\nIn an analog electronic circuit, real numbers are represented as voltages across two points (“ports”) in the circuit. Adding is as simple as making a serial connection. Negation is even simpler: just connect the ports in the opposite direction. Multiplication is significantly trickier, but it can be done. There are electronic devices with nonlinear response characteristics, meaning that they have two input ports and two output ports, and if you apply an input voltage \\(x\\) across one such device, the output voltage would be \\(f(x)\\) where \\(f\\) is not a linear function. Now suppose that \\(f(x) = x^2\\).2\n2 From our vantage point, the universal approximation theorems proven in the early 1990s show that, generically, if we have any nonlinear function \\(f_0\\) at all, then we can construct any activation function \\(f\\) as a neural network, by using many copies of the \\(f_0\\) device as activation functions and linear devices as weights and biases.With such an \\(f\\), we can multiply two voltages by \\(xy = (f(x+y) - f(x) - f(y)) \\times 0.5\\), and so we can construct any polynomial function in any number of variables. That is, we can do algebra by analog devices, as long as we have a voltage multiplier.\nOf course, we don’t hear about voltage multipliers often, and this is no accident – it is quite difficult to get one with good performance. In the preface to the second edition of Cybernetics1961 (Wiener 2019, xli), Wiener waxes praise about Gabor’s breakthrough circuit device that could multiply two voltages at a frequency of \\(1\\; \\mathrm{kHz}\\):\n\nWhile there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. … he does not state explicitly the amplitude range over which his method of multiplication is valid nor the degree of accuracy to be obtained. I am awaiting very eagerly3 an explicit statement of these properties so that we can give a good evaluation of the multiplier for use in other pieces of apparatus dependent on it.\n3 Gabor published it in the same year of 1961: (Gabor, Wilby, and Woodcock 1961).\nTo our modern ears, multiplying two voltages 1000 times a second by analog means seems simultaneously astonishing and obsolete. Intel 8086 in 1976 already could multiply a million times a second, and whatever has come of Gabor’s universal filter? It seems to me that Wiener never accepted the future of digital computers, preferring the concrete certainty of magnetic cores and electric wires.\n\n\nThe Laguerre filter bank\nTo find the Laguerre coefficients of a signal, we need to perform a convolution. Convolutions become products after a Laplace transform, so we need to find the Laplace transform of the Laguerre functions \\(\\psi_n = e^{-x/2}L_n(x)\\). Fortunately, it is easy to compute. We simply read from a standard table:\n\\[\n\\mathcal L [t^n e^{-\\alpha t}\\theta(t)] = \\frac{n!}{(s+\\alpha)^{n+1}}\n\\]\nwhere \\(\\theta(t) = 1_{t \\geq 0}\\) is the zero-one step function.\nThen, since the Laplace transform is linear, we have after simplification\n\\[\n\\mathcal L[\\psi_n\\theta] = \\frac{1}{s+1/2}\\left(\\frac{s-1/2}{s+1/2}\\right)^n\n\\]\nThis gives a simple filter bank that constructs the Laguerre coefficients for any signal. The input signal passes through a \\(\\frac{1}{s+1/2}\\) filter to obtain the \\(c_0\\) coefficient, and then through a \\(\\frac{s-1/2}{s+1/2}\\) filter to obtain the \\(c_1\\) coefficient, and so on. This filter bank can be constructed with standard resistors and capacitors.\nThe following theorem finishes the last piece of the puzzle. (G. H. Harris and Lapidus 1967) claims that the proof is found in (Bose 1956; George Henry Harris 1966), but I did not check.\n\nTheorem 1 Let \\(x(t)\\) be a white noise process with variance \\(1/2\\), and let \\(c_0(t), c_1(t), \\dots\\) be its Laguerre coefficients, then:\n\nthe joint stochastic process \\((c_0(t), c_1(t), \\dots)\\) is stationary;\nfor any fixed \\(t\\in \\mathbb{R}\\), the random variables \\(c_0(t), c_1(t), \\dots\\) are independent samples of the standard gaussian distribution \\(\\mathcal N(0, 1/2)\\).\n\n\n\n\nThe Hermite coefficients\nFor a given input signal \\(x : \\mathbb{R}\\to \\mathbb{R}\\), we pass it into the Laguerre filter bank. The readouts from the filter bank are the signals \\(c_0(t), c_1(t), \\dots\\). They satisfy the equation\n\\[\nx(t - \\tau) = \\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau), \\; \\forall \\tau \\geq 0\\quad \\forall t \\in \\mathbb{R}\n\\]\nIn words, at any cut-off time \\(t \\in \\mathbb{R}\\), the signal we have seen so far is \\(x(t - \\tau)\\) with \\(\\tau \\geq 0\\). This signal can then be decomposed as a linear sum of Laguerre functions \\(\\sum_{n\\geq 0} c_n(t) \\psi_n(\\tau)\\), with \\(c_n(t)\\) as the Laguerre coefficients. The coefficients depend on the cut-off time \\(t\\), but do not depend on \\(\\tau\\), which is not “real” time, but only a kind of “relative historical time”, as we look into the past standing at time \\(t\\).\nA transducer, by our assumption, is deterministic and causal, so that \\(T[x](t)\\) is a deterministic function of the signal we have seen so far, and so it is a deterministic function of \\(c_0(t), c_1(t), c_2(t), \\dots\\). Note carefully that it is determined by \\(c_0(t), c_1(t), c_2(t), \\dots\\) at this very instant \\(t\\). It does not need the values of \\(c_0(t'), c_1(t'), c_2(t'), \\dots\\) at any \\(t' \\neq t\\). We write it as follows:\n\\[\nT[x](t) = T(c_0(t), c_1(t), c_2(t), ...)\n\\]\nBy our assumption that the transducer has a limited memory, we should be able to ignore the higher frequency components of the input signal, and still recover a good approximation of \\(T\\). That means that \\(T[x](t) = T(c_0(t), c_1(t), c_2(t), \\dots) \\approx T(c_0(t), \\dots, c_n(t))\\), with the approximation increasing in accuracy as \\(n\\) increases.\nBy our assumption that the transducer is analytic with respect to the input, \\(T(c_0(t), \\dots, c_n(t))\\) has a multivariate Hermite serial expansion (the same idea as multivariate Taylor expansion):\n\\[\nT(c_0(t), \\dots , c_n(t)) = \\sum_{m_0, \\dots , m_n \\geq 0} T_{m_0, \\dots , m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nWe are quite close to the target. We can compute the Laguerre coefficients \\(c_n(t)\\) of any input signal by the Laguerre filter bank. We can construct analog circuits that compute \\(H_m(c_n(t))\\), the Hermite polynomial values of the Laguerre coefficients. The remaining challenge is to determine the coefficients \\(T_{m_0, \\dots, m_n}\\).\nThis is where Theorem 1 comes to finish the construction. Let \\(x(t)\\) be a white noise process, then since\n\\[\nT[x](t) \\approx \\sum_{m_0, \\dots , m_n \\geq 0} T_{m_0, \\dots , m_n} H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\n\\]\nand since the Laguerre coefficients are independent samples of the standard gaussian, we have\n\\[\nT_{m_0, \\dots , m_n} \\approx \\mathbb{E}\\left[T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t))\\right]\n\\]\nwhere the expectation is in the sense of ensemble expectation. That is, we would run this experiment once with a white noise process, freeze it exactly at the moment \\(t\\), then run it again with another white noise process, freeze it exactly at the moment \\(t\\), and so on. Then we average over all these experiments.\nHowever, Theorem 1 states that the Laguerre coefficients are stationary, meaning that we have ergodicity4: the ensemble average is the time average, and so\n4 Wiener was really into ergodic theory.\\[\nT_{m_0, \\dots , m_n} \\approx \\lim_{T \\to \\infty} \\frac{1}{T} \\int_0^T T[x](t)\\; H_{m_0}(c_0(t)) \\cdots H_{m_n}(c_n(t)) dt\n\\]\nThe integrand is computable by the analog devices we described. The integration-and-averaging can be done with a very-low-pass filter – taking the average is essentially passing only the zero-frequency signal, thus it is the low-pass filter with the lowest possible passband. Finally, since white noise is all around us, it can be obtained in many ways, such as by amplifying the thermal noise in a resistor.\nAnd so we have a finished machine, where the white noise \\(x\\) and the signal to imitate \\(T[x](t)\\) come in, and the fitted parameters \\(T_{m_0, \\dots, m_n}\\) come out. The fitted parameters can be automatically read and adjusted by electromechanical devices, such as relays and step motors, allowing us to connect the machine in parallel with an unknown transducer, run it for a period over a white noise input, and ultimately achieve a machine that precisely imitates the unknown transducer.\n\n\n\nThe fully-formed imitation machine. (G. H. Harris and Lapidus 1967, fig. 2)"
  },
  {
    "objectID": "essays/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "href": "essays/posts/cybernetic-artificial-intelligence/index.html#the-science-of-control-and-communication-in-the-animal-and-the-machine",
    "title": "Cybernetic Artificial Intelligence",
    "section": "The science of control and communication in the animal and the machine",
    "text": "The science of control and communication in the animal and the machine\n\nWinter 1943–1944\nLorente de Nó and I, as physiologists, were asked to consider the second of two hypothetical black boxes that the allies had liberated from the Germans. No one knew what they were supposed to do or how they were to do it. The rust box had been opened and exploded. Both had inputs and outputs, so labelled. The question was phrased unforgettably: “This is the enemy’s machine. You always have to find out what it does and how it does it. What shall we do?” By the time the question had become that well defined, Norbert was snoring at the top of his lungs and his cigar ashes were falling on his stomach. But when Lorente and I had tried to answer, Norbert rose abruptly and said: “You could of course give it all possible sinusoidal frequencies one after the other and record the output, but it would be better to feed it noise – say white noise – you might call this a Rorschach.” Before I could challenge his notion of a Rorschach, many engineers’ voices broke in. Then, for the first time, I caught the sparkle in Johnny von Neumann’s eye. I had never seen him before and I did not know who he was. He read my face like an open book. He knew that a stimulus for man or machine must be shaped to match nearly some of his feature-filters, and that white noise would not do. There followed a wonderful duel: Norbert with an enormous club chasing Johnny, and Johnny with a rapier waltzing around Norbert-at the end of which they went to lunch arm in arm.\n(McCulloch 1974)\n\nWe have reached the end of the road, facing an all-analog general-purpose learning machine. This machine can imitate any black-box transducer, and thus is a form of machine learning. If we have two such machines, and randomly set the parameters of one machine, then the other machine would learn to imitate the same behavior. And since each parameter setting creates a different behavior, purely by imitating behavior, the parameters would be copied from one machine to the other. This is an explicit construction for how behaviorism can work, even if not in our universe, then in another universe where the animals really are those imitation devices.\nAs Wiener speculated (Wiener 2019, 248–49), biological learning and reproduction are “philosophically similar” to this machine:\n\nWhile both Professor Gabor’s methods and my own lead to the construction of nonlinear transducers, they are linear to the extent that the nonlinear transducer is represented with an output which is the sum of the outputs of a set of nonlinear transducers with the same input. These outputs are combined with varying linear coefficients. This allows us to employ the theory of linear developments in the design and specification of the nonlinear transducer. And in particular, this method allows us to obtain coefficients of the constituent elements by a least-square process. If we join this to a method of statistically averaging over the set of all inputs to our apparatus, we have essentially a branch of the theory of orthogonal development. Such a statistical basis of the theory of nonlinear transducers can be obtained from an actual study of the past statistics of the inputs used in each particular case. I ask if this is philosophically very different from what is done when a gene acts as a template to form other molecules of the same gene from an indeterminate mixture of amino and nucleic acids, or when a virus guides into its own form other molecules of the same virus out of the tissues and juices of its host. I do not in the least claim that the details of these processes are the same, but I do claim that they are philosophically very similar phenomena.\n\nIt seems like this device, as it stands, would be plagued by the same issues that plague a general analog computer – error correction, bad gains, and intractable nonlinearities. Still, it stands as a vision of an alternative future in an alternative world, if not an alternative future of our world."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html",
    "title": "Classical Thermodynamics and Economics",
    "section": "",
    "text": "It is to the economist, the statistician, the philosopher, and to the general reader that I commend the analysis contained herein… mathematics as applied to classical thermodynamics is beautiful: if you can’t see that, you were born color-blind and are to be pitied.\nPaul Samuelson, forewords to (Bickler and Samuelson 1974)\n\n\n\nUnlike my Analytical Mechanics essay, this essay does not cover much of the width covered in a university course on classical thermodynamics or neoclassical economics. Instead, it’s best described as “the conceptual foundations that a typical course does not work well on”, a deep but narrow essay to supplement a shallow but wide typical textbook.\nUnlike analytical mechanics, which is typically taught to students intent on reaching the abstract plane of modern theoretical physics, and is thus deep but narrow, a typical course on classical thermodynamics is shallow, but very wide, and taught to a wide student base – theoretical physicists, thermal engineers, electric engineers, chemists, biologists… When something must be taught to a wide audience, and is both deep and wide, depth is sacrificed. This is perfectly practicable, but it leaves a small minority confused with a sinister feeling that the teachers have abused their trust in them. I am in that minority.\nThe essay contains: three laws of thermodynamics, entropy, Helmholtz and Gibbs free energy, nonextensive entropy, Caratheodory’s axiomatic thermodynamics, Vladimir Arnold’s contact-geometric thermodynamics, Paul Samuelson’s area-ratio thermodynamics, Le Chatelier’s principle, chemical equilibrium, Gibbs phase rule and its extensions, analogies with neoclassical economics, speculative sci-fi.\nIt does not contain: statistical mechanics, most of the “width” part of thermodynamics and economics.\nThe prerequisites are multivariate calculus and mathematical maturity. It’s good to be familiar with basic economics as well.\n\n\n\n\n\\(S\\): entropy\n\\(U\\): internal energy\n\\(V\\): volume\n\\(T\\): temperature\n\\(\\beta = 1/T\\): inverse temperature\n\\(N\\): number of particles of a chemical species\n\\(n\\): number of moles of a chemical species\n\\(\\xi\\): extent of reaction\n\\(X\\): “other properties that we are not concerned with”\n\nFor example, with an ideal gas trapped in a copper box, its macroscopic state is determined by \\(U, N, V\\). If we want to focus on \\(U\\), then we can let \\(X = (N, V)\\), and write \\(S = S(U, X)\\).\nSimilarly, for a photon gas in a blackbody chamber, its macroscopic state is determined by \\(U, V\\), since photons can be created and destroyed on the inner surface of the blackbody chamber. We can then write \\(S = S(U, X)\\) if we are concerned only about how entropy changes with \\(U\\), holding the other state constant. We can also write \\(S = S(V, X)\\) vice versa.\n\n\nThermodynamics is notorious for having too many partial differentials and coordinate changes. \\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) means that we lay down a coordinate system defined by \\(x_1, \\dots, x_n\\), then calculate \\(\\partial_{x_1}f\\), fixing the other coordinates constant. In particular, \\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) is likely different from \\((\\partial_{x_1} f)_{x_1, y_2, \\dots, y_n}\\).\nIf in doubt, write down\n\\[df = \\sum_{i=1}^n (\\partial_{x_i} f)_{x_1, \\dots, x_n} dx_i\\]\nand reason thenceforth.\nSometimes people write \\((\\partial_{x_i} f)_{x_2, \\dots, x_n}\\) instead of \\((\\partial_{x_i} f)_{x_1, x_2, \\dots, x_n}\\) to save them one stroke of the pen. I try to avoid that, but be aware and beware.\n\n\n\n\n(Pippard 1964). Slim, elegant, both mathematical and applied. In the best British tradition of mathematics – think James Maxwell and G. H. Hardy.\n(Fermi 1956). The same as above. However, it also covers chemical thermodynamics.\n(Lemons 2019). A very readable introduction to classical thermodynamics, slim but deep. I finally understood the meaning of the three laws of thermodynamics after reading it. Contains copious historical quotations.\n(Lemons 2008). A textbook version of the author’s (Lemons 2019), weaving in history and philosophical contemplation at every turn.\n(Carnot, Clapeyron, and Clausius 1988). A reprint of the most important papers in thermodynamics published before 1900. Useful to have on hand if you are reading (Lemons 2019).\n(Buchdahl 1966). A textbook based on Carathéodory’s axiomatic thermodynamics. The notation is ponderous, and the payoff is unclear. I don’t know what is its intended audience – perhaps professional pedants and differential geometers? Nevertheless, if you need to do research in Carathéodory’s axiomatic thermodynamics, I think this is your best bet.\nTed Chiang’s Exhalation (2008), printed in (Chiang 2019). A sci-fi story about an alien race where pneumatic engines, not heat engines, are all-important. A better take on stereodynamics than my attempt."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#introduction",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#introduction",
    "title": "Classical Thermodynamics and Economics",
    "section": "",
    "text": "It is to the economist, the statistician, the philosopher, and to the general reader that I commend the analysis contained herein… mathematics as applied to classical thermodynamics is beautiful: if you can’t see that, you were born color-blind and are to be pitied.\nPaul Samuelson, forewords to (Bickler and Samuelson 1974)\n\n\n\nUnlike my Analytical Mechanics essay, this essay does not cover much of the width covered in a university course on classical thermodynamics or neoclassical economics. Instead, it’s best described as “the conceptual foundations that a typical course does not work well on”, a deep but narrow essay to supplement a shallow but wide typical textbook.\nUnlike analytical mechanics, which is typically taught to students intent on reaching the abstract plane of modern theoretical physics, and is thus deep but narrow, a typical course on classical thermodynamics is shallow, but very wide, and taught to a wide student base – theoretical physicists, thermal engineers, electric engineers, chemists, biologists… When something must be taught to a wide audience, and is both deep and wide, depth is sacrificed. This is perfectly practicable, but it leaves a small minority confused with a sinister feeling that the teachers have abused their trust in them. I am in that minority.\nThe essay contains: three laws of thermodynamics, entropy, Helmholtz and Gibbs free energy, nonextensive entropy, Caratheodory’s axiomatic thermodynamics, Vladimir Arnold’s contact-geometric thermodynamics, Paul Samuelson’s area-ratio thermodynamics, Le Chatelier’s principle, chemical equilibrium, Gibbs phase rule and its extensions, analogies with neoclassical economics, speculative sci-fi.\nIt does not contain: statistical mechanics, most of the “width” part of thermodynamics and economics.\nThe prerequisites are multivariate calculus and mathematical maturity. It’s good to be familiar with basic economics as well.\n\n\n\n\n\\(S\\): entropy\n\\(U\\): internal energy\n\\(V\\): volume\n\\(T\\): temperature\n\\(\\beta = 1/T\\): inverse temperature\n\\(N\\): number of particles of a chemical species\n\\(n\\): number of moles of a chemical species\n\\(\\xi\\): extent of reaction\n\\(X\\): “other properties that we are not concerned with”\n\nFor example, with an ideal gas trapped in a copper box, its macroscopic state is determined by \\(U, N, V\\). If we want to focus on \\(U\\), then we can let \\(X = (N, V)\\), and write \\(S = S(U, X)\\).\nSimilarly, for a photon gas in a blackbody chamber, its macroscopic state is determined by \\(U, V\\), since photons can be created and destroyed on the inner surface of the blackbody chamber. We can then write \\(S = S(U, X)\\) if we are concerned only about how entropy changes with \\(U\\), holding the other state constant. We can also write \\(S = S(V, X)\\) vice versa.\n\n\nThermodynamics is notorious for having too many partial differentials and coordinate changes. \\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) means that we lay down a coordinate system defined by \\(x_1, \\dots, x_n\\), then calculate \\(\\partial_{x_1}f\\), fixing the other coordinates constant. In particular, \\((\\partial_{x_1} f)_{x_1, x_2, \\dots, x_n}\\) is likely different from \\((\\partial_{x_1} f)_{x_1, y_2, \\dots, y_n}\\).\nIf in doubt, write down\n\\[df = \\sum_{i=1}^n (\\partial_{x_i} f)_{x_1, \\dots, x_n} dx_i\\]\nand reason thenceforth.\nSometimes people write \\((\\partial_{x_i} f)_{x_2, \\dots, x_n}\\) instead of \\((\\partial_{x_i} f)_{x_1, x_2, \\dots, x_n}\\) to save them one stroke of the pen. I try to avoid that, but be aware and beware.\n\n\n\n\n(Pippard 1964). Slim, elegant, both mathematical and applied. In the best British tradition of mathematics – think James Maxwell and G. H. Hardy.\n(Fermi 1956). The same as above. However, it also covers chemical thermodynamics.\n(Lemons 2019). A very readable introduction to classical thermodynamics, slim but deep. I finally understood the meaning of the three laws of thermodynamics after reading it. Contains copious historical quotations.\n(Lemons 2008). A textbook version of the author’s (Lemons 2019), weaving in history and philosophical contemplation at every turn.\n(Carnot, Clapeyron, and Clausius 1988). A reprint of the most important papers in thermodynamics published before 1900. Useful to have on hand if you are reading (Lemons 2019).\n(Buchdahl 1966). A textbook based on Carathéodory’s axiomatic thermodynamics. The notation is ponderous, and the payoff is unclear. I don’t know what is its intended audience – perhaps professional pedants and differential geometers? Nevertheless, if you need to do research in Carathéodory’s axiomatic thermodynamics, I think this is your best bet.\nTed Chiang’s Exhalation (2008), printed in (Chiang 2019). A sci-fi story about an alien race where pneumatic engines, not heat engines, are all-important. A better take on stereodynamics than my attempt."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#what-is-thermodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#what-is-thermodynamics",
    "title": "Classical Thermodynamics and Economics",
    "section": "What is thermodynamics?",
    "text": "What is thermodynamics?\nBoth neoclassical economics and classical thermodynamics are about the equilibria of large systems. While a large system is generally hopelessly complicated, almost all the complexity falls away when the system is maximizing a single quantity. Ceaselessly striving to maximize entropy, a complex system sheds its complexity and reaches the pure simplicity of maximal entropy. Ceaselessly striving to maximize profit, a complex company sheds its complexity and reaches the pure simplicity of perfect product.\nIn both fields, everything we can say about the world are nothing more than systems, constraints, contacts, and equilibria. Time and change do not exist. All we can explain is which states are in constrained equilibrium, not how a system can get there. Atoms do not exist. All we can explain is what happens to homogeneous substances in constrained equilibrium. People do not exist. All we can explain is what happens to constrained economic systems in equilibrium.\nDifferent things can be maximized: the total entropy, or the negative Gibbs free energy, or the profit, or the sum-total of utility, or something else. Through different lenses, different things are maximized, but they predict the same phenomena. Using this mathematical freedom, experts brachiate around the coordinate axes like gibbons brachiating around vines, looking for the perfect angle to solve each particular problem.\n\nConstrained maximization problems in economics and thermodynamics.\n\n\n\n\n\n\n\ninterpretation\nmaximized quantity\nconstraint\n\n\n\n\nconglomerate accounting\nbook value\nAssets is conserved, but can be moved between child companies.\n\n\nsocial welfare\nsocial utility\nWealth is conserved, but can be redistributed.\n\n\nclosed system\nentropy\nQuantities are conserved, but can be moved between sub-systems.\n\n\nfactory production\nprofit\nSome raw materials are on sale at a market, but others are not.\n\n\nconsumer choice\nutility\nSome finished goods are on sale at a market, but others are not. The market uses a commodity money.\n\n\npartially open system\nnegative free energy\nSome quantities can be exchanged with a bath, but others are conserved.\n\n\n\n\nEven if the capitalist system is to give way to one in which service and not profit shall be the object, there will still be an integral of anticipated utilities to be made a maximum. Since we must find a function which maximizes an integral we must in many cases use the Calculus of Variations. But the problem here transcends the questions of depreciation and useful life, and belongs to the dawning economic theory based on considerations of maximum and minimum which bears to the older theories the relations which the Hamiltonian dynamics and the thermodynamics of entropy bear to their predecessors.\n(Hotelling 1925)\n\n\nSystems\nSystems are the main characters of the drama of thermodynamics. A thermodynamic system is fully determined by a few macroscopic properties, related by equations of state. Once we know enough of its properties, we know all there is to know about such a system. There is nothing left to say about it.\nEverything is a thermodynamic system. However, there are two special types: bath systems, and mechanical systems.1 Any number of thermodynamic systems can be connected into a larger system – a compound system.\n1 Some children are confused when they heard that squares are rectangles too. I hope you won’t be equally confused when you hear that mechanical systems are also thermodynamic systems.The prototypical thermodynamic system is a tank of ideal gas whose number of particles is fixed. It has 2 degrees of freedom, so if we write down \\(n\\) different macroscopic properties, they would (generically) be related by \\(n-2\\) equations of state. So for example, if we write down the properties internal energy \\(U\\), temperature \\(T\\), volume \\(V\\), pressure \\(P\\), they would be related by the 2 equations of state\n\\[PV = k_BNT, \\quad U = \\frac 32 k_B NT\\]\nIf we know two out of the four of internal energy \\(U\\), temperature \\(T\\), volume \\(V\\), pressure \\(P\\), then we can solve for all the others by equations of state. The macroscopic properties fully describe the system, and nothing more can be said about it. We cannot ask additional questions such as “Are there more particles on the left than on the right?” or “How long did it take for the system to reach equilibrium?”, because such questions are literally undefined in classical thermodynamics.\nBecause the properties are related by equations of state, we need only know a few of the properties in order to infer all the rest. For example, knowing the volume \\(V\\), internal energy \\(U\\), and particle number \\(N\\), of a tank of ideal gas, we can infer that its pressure is \\(P = \\frac{2U}{3V}\\), and its temperature is \\(T = PV/k_BN\\). Succinctly,\n\\[P = P(U, V, N), \\quad T = T(U, V, N)\\]\nmeaning “If we know \\(U, V, N\\), then we can calculate \\(P\\) and \\(T\\)”.\n\n\n\n\n\n\nWarning\n\n\n\nIt’s too easy to misread it as saying that \\(P\\) is a mathematical function of \\(U, V, N\\). It is not. It really is saying that, there exists a mathematical function \\(f_P\\), such that for any state \\(\\omega\\) of the system, we have\n\\[P(\\omega) = f_P(U(\\omega), V(\\omega), N(\\omega))\\]\n\n\nEverything about a thermodynamic system is known once we specify how its entropy is a function of its macroscopic properties. For example, the ideal gas is fully specified by\n\\[S(U, V, N) = k_B N \\ln\\left[\\frac{V}{N}\\,\\left(\\frac{U}{\\hat{c}_V k_B N}\\right)^{\\hat{c}_V}\\,\\frac{1}{\\Phi}\\right]\\]\nwhere \\(\\hat c_V\\) and \\(\\Phi\\) are constants that differ for each gas.\nAs another example, the photon gas is defined by\n\\[S(U, V) = C V^{1/4}U^{3/4}, \\quad C=\\left(\\frac{256\\pi^2 k_B^4}{1215 c^3 \\hbar^3}\\right)^{1/4}\\]\nThe fact that \\(S(U, V) \\propto V^{1/4}U^{3/4}\\) can be derived from 19th-century physics (indeed, it was known to Boltzmann), but the constant \\(C\\) has to wait for proper quantum mechanics.\n\nBaths\nA bath system is an infinite source of a conserved quantity at constant marginal entropy. A bath system is intended to be used as a “free market” where non-bath systems can trade some conserved quantities with.\nFor example, if we take a copper piston and fill it with ideal gas, then immerse the piston in the bottom of an ocean, then it is playing the role of a volume-and-energy bath with constant pressure-and-temperature. If we cover up the piston with some insulating material, then the ocean suddenly plays the role of merely a volume bath with constant pressure. If we use screws to fix the piston head, then the ocean suddenly becomes merely an energy bath with constant temperature. From this, we see that a bath in itself is a rather vacant concept. A bath should always be in contact with some non-bath system.\nBecause they are infinitely large, if you connect two baths together, something bad will happen. For example, if you connect two energy baths together, but with different temperature, what would happen? The simple answer is: “A torrent of heat will flow from the hotter to the colder bath.”. The more correct answer is: “Classical thermodynamics does not allow such a question to be asked. It would be like asking what happens when ‘an unstoppable force meets an immovable object’. If we literally have two baths, then we cannot connect them. If we only have two giant oceans that seem like baths when compared to this little tank of gas, then if the two oceans are connected to each other, they will no longer appear as baths to each other.”.\nYou can take this in two ways. Either will work. You can say that both systems and baths are first-class concepts in thermodynamics, and enforce the rule that you can never connect two baths together. You can also say that systems are first-class concepts in thermodynamics, but baths are second-class concepts, a convenient way to think about a system that is much larger relative to some other systems. Since a bath is a relative concept, it simply is a bad question to ask “What happens if we connect two baths together?” – The correct reply is “You mean, two systems that appear as baths… relative to what?”.\nSome important (and unimportant) examples of baths:\n\nA heat bath, or more accurately an energy bath, is a system that you can take or dump as much energy as you want, always at constant marginal price of energy.\nAn atmosphere, or more accurately an energy-and-volume bath, is a system that you can take or dump as much energy or volume as you want, always at constant temperature and pressure.\nThe surface of a lake could serve as an energy-and-area bath.\nA large block of salt can serve as a salt-chemical bath.\n\n\n\nMechanical systems\nA mechanical system is a thermodynamic system whose entropy is always zero. Essentially all systems studied in classical mechanics are such systems. In classical thermodynamics, they are not put center-stage, but if you know where to look, you will see them everywhere.\nAn ideal linear-spring has two macroscopic properties: length \\(x\\) and internal energy \\(U\\), with equation of state \\(U = \\frac 12kx^2\\). For example, a helix spring is close to an ideal linear-spring.\nAn ideal surface-spring is the same as an ideal linear-spring, but with area \\(A\\) instead of length \\(x\\). Its equation of state is \\(U = \\sigma A\\), where \\(\\sigma\\) is surface tension constant. For example, a balloon skin is close to an ideal surface-spring.\nAn ideal volume-spring is similar. It would resemble a lump of jelly. An ideal gas, though it looks like a volume-spring, is not an example, because its entropy is not zero. In particular, this means an ideal gas has temperature and can be “heated up”, but a lump of ideal jelly cannot.\nIn general, we can construct an arbitrary energy storage system, such that it has two macroscopic properties \\(x, U\\), satisfying \\(U = f(x)\\), where \\(f\\) is any differentiable function. To show this, we can imagine taking a mystery box with a chain we can pull on, and by some internal construction with gears, pulleys, weights, and springs, the force on the chain is \\(f'(x)\\), where \\(x\\) is the length by which we have pulled. That is the desired system.\n\n\n\nState space\nA tank of gas has on the order of \\(10^{26}\\) particles, but in classical thermodynamics, its state is entirely determined if we know its \\(U, V, N\\). In this sense, we can say that its macroscopic state space has just 3 dimensions. In many situations, such as in the Carnot heat engine, we also fix its \\(N\\), in which case its state space has just 2 dimensions.\nThis is typically plotted in either the \\((P, V)\\) space, or the \\((T, S)\\) space, or some other spaces, but in every case, there are just two dimensions. We can unify all these diagrams as merely different viewpoints upon the same curvy surface – the state space \\(\\mathcal X\\) itself. Each point \\(\\omega \\in \\mathcal X\\) in the state space is a state, and each macroscopic property \\(X\\) is a scalar function of type \\(X : \\mathcal X \\to \\mathbb{R}\\).\nIf the state space has just \\(d\\) dimensions, then we need only \\(d\\) macroscopic properties \\(X_1, \\dots, X_d\\) in order to lay down a coordinate system for the state space. If we have another macroscopic property \\(Y\\), then there in general exists a function \\(f_Y: \\mathbb{R}^d \\to \\mathbb{R}\\), such that \\(Y(\\omega) = f_Y(X_1(\\omega), \\dots, X_d(\\omega))\\) for any state \\(\\omega\\). In other words, we have an equation of state.\n\n\n\nThe state space for a thermodynamic system with 2 degrees of freedom. If we lay down three macroscopic properties, then they satisfy one equation of state.\n\n\n\nCalculus on state space\nIn thermodynamics, we typically have many more macroscopic properties than dimensions. For example, the state space of an ideal gas has only 3 dimensions, but has many macroscopic properties: \\(S, U, V, N, T, P, \\mu, \\dots\\). In this case, every time we take a derivative on the state space, we need to pick exactly 3 properties, since picking different coordinates leads to different partial differentials.\n\n\n\n\n\n\nExample with \\(x, y, z\\)\n\n\n\n\n\nAs an illustrative example, let \\(x, y, z\\) be smooth scalar functions on a smooth 2D surface, such that their contour lines are linearly independent at every point on the surface. Then, picking any 2 out of \\(x, y, z\\) would give us a coordinate system. Any other smooth function \\(f\\) on the surface can be expressed in 3 different ways, as\n\\[f = f_{x, y}(x, y) = f_{y, z}(y, z) = f_{z, x}(z, x)\\]\nThis gives us two different ways to “differentiate \\(f\\) with respect to \\(x\\)”:\n\\[\n\\left(\\pp{f}{x}\\right)_y := \\partial_1 f_{x, y}, \\quad\n\\left(\\pp{f}{x}\\right)_z := \\partial_2 f_{z, x}\n\\]\nwhere \\(\\partial_1\\) means “partial differentiation with respect to the first input”, etc.\n\n\n\n\n\n\n\n\n\nExample with \\(\\mathbb{R}^2\\)\n\n\n\n\n\nLet \\(x, y\\) be the usual coordinates on the 2D plane \\(\\mathbb{R}^2\\), and let \\(z = x + y, w = x - y\\). We can pick any 2 of \\(x, y, z, w\\) to construct a coordinate system. Then,\n\\[\n\\left(\\pp{y}{x}\\right)_y = 0 \\quad \\left(\\pp{y}{x}\\right)_z = -1 \\quad \\left(\\pp{y}{x}\\right)_w = 1\n\\]\n\n\n\nIn general, if we pick \\(x_1, \\dots, x_d, y\\) from a list of many smooth scalar functions on a surface, such that \\(x_1, \\dots, x_d\\) form a smooth and linearly independent coordinate system on the surface, then we can express \\(y = f(x_1, \\dots, x_d)\\) for some function \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\), and define\n\\[\n\\left(\\pp{y}{x_1}\\right)_{x_2, \\dots, x_d} = \\partial_1 f, \\quad\n\\left(\\frac{\\partial y}{\\partial x_1\\partial x_2}\\right)_{x_3, \\dots, x_d} = \\partial_1\\partial_2 f, \\quad \\dots\n\\]\nIn particular, we have\n\\[dy = \\sum_{i=1}^d (\\partial_{x_i} y)_{x_1, \\dots, x_d} dx_i\\]\n\nExercise 1 Based on the following diagram, prove that if we have three smooth scalar functions \\(x, y, z\\) on a smooth2 2D surface, such that their contour surfaces are linearly independent, then \\(\\left(\\frac{\\partial x}{\\partial z}\\right)_y\\left(\\frac{\\partial y}{\\partial x}\\right)_z\\left(\\frac{\\partial z}{\\partial y}\\right)_x = -1\\). Generalize this to higher dimensions.\n2 It is sufficient to assume the surface and the scalar functions are \\(C^2\\), but we need not worry about it, because everything is smooth in classical thermodynamics, except at phase transitions.\n\n\n\n\nConstraint\nA constraint is an equation of form \\(f(A, B, C, \\dots) = f_0\\), where \\(f\\) is a mathematical function, \\(A, B, C, \\dots\\) are macroscopic properties, and \\(f_0\\) is a constant.\nFor example, if we have two tanks of gas in thermal contact, then the constraint is as follows:\n\\[\n\\begin{cases}\nV_1 &= V_{1,0} \\\\\nV_2 &= V_{2,0} \\\\\nU_1 + U_2 &= U_{1,0} + U_{2,0} \\\\\n\\end{cases}\n\\]\nmeaning that the volume of each tank of gas is conserved, and the sum of their energy is also conserved.\nThe constraints on a compound system are determined by the contacts between its subsystems.\n\n\nContacts\nIf we only have systems isolated in a perfect vacuum, then nothing interesting will happen. If two systems are perfectly connected, then they will never be brought apart. A contact allows two systems to interact, without destroying their individuality. It allows two systems to communicate, without becoming literally one system. Economically, contacts are trade contracts.\nIn general, the effect of a contact is to reduce the number of constraints by one. For example, a metal rod between two pistons reduces the two constraints\n\\[V_1 = V_{1,0}, \\quad V_2 = V_{2, 0}\\]\ninto one constraint: \\[V_1 + V_2 = V_{1,0} + V_{2,0}\\]\nAs another example, in a tank of three kinds of gas \\(N_2, H_2, NH_3\\), allowing a single chemical reaction \\(N_2 + 3H_2 \\rightleftharpoons 2NH_3\\) reduces three constraints\n\\[\nN_{N_2} = N_{N_2, 0}, \\quad N_{H_2} = N_{H_2, 0}, \\quad N_{NH_3}= N_{NH_3, 0}\n\\]\ninto two:\n\\[\nN_{N_2} - N_{N_2, 0} = (N_{H_2} - N_{H_2, 0})/3, \\quad N_{N_2} - N_{N_2, 0} = (N_{NH_3} - N_{NH_3, 0})/2\n\\]\nA contact can be nonlinear. For example, if we have two pistons of the same area, connected by a lever, such that pushing on one piston by \\(\\Delta x\\) would be pulling on the other piston by \\(2 \\Delta x\\), then the constraint becomes \\(2(V_1 - V_{1,0}) + (V_2 - V_{2,0}) = 0\\). And by designing a series of levers, gears, and chains, we can realize any constraint function \\(f(V_1, V_2) = f(V_{1,0}, V_{2,0})\\) for any smooth function \\(f\\).\n\n\n\nA thermodynamical system (a piston of gas) is connected to a mechanical system (a mass in gravity) via an arbitrary constraint (variable gears).\n\n\n\n\nCompound systems\nA compound system is nothing more than several systems connected. If we know the connections, and the entropy function of each subsystem, then we know everything about the compound system. The number of DOF for the compound system is the sum of the DOF of the subsystems, minus the degree of constraints.\nFor example, in the adiabatic expansion of a tank of ideal gas, we are really studying one compound system made of 3 subsystems:\n\na tank of ideal gas (thermodynamic system),\na lump of mass in gravity (mechanical system),\nwith a gear system between them (contact).\n\nThe gear system is designed with gear-ratio that varies as the system turns, in just the right way such that the system is always in equilibrium no matter the position of the piston, so that it really has no preference of going forwards or backwards.\n\n\n\nThe compound system. Inside it, there is a subsystem of a tank of ideal gas that undergoes adiabatic expansion.\n\n\n\n\n\n\n\n\nOnly one system\n\n\n\nWe typically think of the tank of ideal gas itself as part of the thermodynamics, and the other parts as “the environment”, but we should consider one single compound system, properly speaking, of which the tank of ideal gas is merely a sub-system. This way, we can state directly that the entropy of the entire compound system is maximized.\n\n\n\nExample 1 (heat-engine-and-environment compound system) A heat engine is a thermodynamic system that is used as a component of a larger compound system. The large system contains 4 parts: two energy baths, one heat engine, and one carefully designed mechanical system acting as energy storage.\nIf you only want a heat engine that works, then the energy storage does not need to be carefully designed. However, if you want a Carnot heat engine, i.e. at maximal efficiency, then the energy storage must be designed to be exactly right. It must be designed to follow the exact parameters of the heat engine, as well as the temperatures of the two energy baths. If any of those is ignored, the energy storage would fail to “mesh” with the rest of the compound system, and cause waste.\nThis is why a heat engine must be a component of a larger compound system. Every part depends on every other part. The energy storage unit is just as important and precisely designed as the heat engine is.\n\n\n\nEquilibrium, virtual vs actual states\n\nFreedom is an iron cage.\nConstraints set it free again.\n\nOn the African savannah, there lived a bunch of meerkats. Meerkats love to stand on the tallest place.\nAt first, they could stand wherever they wanted, so they all stood on one single hill. It was crowded. A blind lion who had memorized the landscape came and ate all of them.\nThen humans came and added long walls that divided the savannah into thin stripes. Now each meerkat’s location is determined by the stripe in which it happened to fall. The blind lion could find the meerkat if he knew the location of the stripe. In other words, optimizing for height, when there is one constraint, leads to one dimension of uncertainty.\nThis is a subtle point, so I will say it again. If you want to optimize for a quantity, and you don’t have a constraint, then you would always go to the globally best solution. The whole space of possibilities is open to you, but you don’t need them. Those states are “virtual”, because they are never observed, even though they are out there.\nBut if you have one constraint, then you have one unique solution for each possible setting of constraint. Suddenly a lot of those virtual states become real. You are still not free, but at least now you have a puppet master.\nIn classical thermodynamics, only equilibrium states are “real”. Nonequilibrium states are “virtual”. In Lagrangian mechanics, only stationary-action paths are real, and the other paths are virtual. You can imagine that if you throw a rock upwards, it might execute a complex figure-8 motion before returning to the ground again, but that’s a virtual path. The only real path is the unique virtual path that stationarizes the action integral.\nSimilarly, in classical thermodynamics, you could imagine that a tank of gas contains all its gas on the left side. Its entropy is just \\(S(U, V/2, N)\\), but that’s a virtual state that does not maximize entropy under constraint. The unique entropy-maximizing virtual state is the real state.\nFor every constraint, there are many nonequilibrium states that satisfy the constraint, but only one equilibrium entropy, and so equilibrium entropy is a function of constraints, even though the entropy function itself is not. It optimizes all its complexities away, allowing us to know it through just its external constraints.\n\n\nSome common misconceptions\nThermodynamics is not statistical mechanics. Forget about molecules and atoms. Forget about statistics and statistical mechanics. Forget about \\(S = k_B\\ln \\Omega\\) or \\(S = -\\sum_i p_i \\ln p_i\\). Randomness does not exist in classical thermodynamics.\nForget about the first law of thermodynamics. Energy is nothing special. The conservation of energy is no more important than the conservation of volume, or the conservation of electric charge.\nForget about heat. Heat does not exist – it is not a noun, not even an adjective, but an adverb at most. The theory of caloric has already been disproven in 1798 by the cannon-boring experiment.\nHeat energy and work energy are both misnomers. Neither are types of energy. Instead, they are types of energy-flow. We should speak of only the heatly flow of energy and the workly flow of energy. In this way, both “heat” and “work” are revealed to be actually adverbs. This is a bit awkward, so we will continue to speak of “heat” and “work”, but you should understand that it’s a shorthand, and that there is neither “heat energy” nor “work energy”.\nTo perform work, one must perform work upon something. In other words, there is no such thing as “system A performed work”. There is really “some energy and length is between A and B, in compliance with an equation constraint, such that the total entropy of the compound system has remained constant”.\nForget about time. Time does not exist in classical thermodynamics. We can say nothing at all about what happens between equilibria. We can only say, “This is an equilibrium, but that is not an equilibrium.”. That is all we can say. See this, and you will see thermodynamics aright. As to what happened “between them”, that we must pass over in silence.\nThe name “thermodynamics” is a complete misnomer, because heat does not exist (thus no “thermo-”) and time does not exist (thus no “-dynamics”). If I am allowed a bit of name-rectification, I will call it “entropo-statics”.\nIf time does not exist, you ask, what do we mean when we study Joule expansion? That is, when we take half a tank of gas, and suddenly open the middle wall and wait until the gas equilibrates in the entire tank?\nQuiet. We did not open the middle wall. We did not wait. Gas did not expand. The past did not cause the future. Time is a stubbornly persistent illusion, and causality too.\nIn fact, we are considering two different problems in thermodynamics.\nFirst problem: Given a tank of gas with internal energy \\(U = U_0\\), molar-amount \\(n = n_0\\), and constraint \\(V \\leq \\frac 12 V_0\\). What is its equilibrium state? Answer: The state that maximizes entropy under the constraints: \\[\n\\begin{cases}\n\\max S(U, n, V) \\\\\nV \\leq \\frac 12 V_0 \\\\\nU = U_0 \\\\\nn = n_0\n\\end{cases}\n\\]\nwhere \\(S(U, n, V)\\) is the entropy of the gas when its states properties are \\(U, n, V\\).\nSimilarly, the second problem is another constraint-optimization problem: \\[\n\\begin{cases}\n\\max S(U, n, V) \\\\\nV \\leq V_0 \\\\\nU = U_0 \\\\\nn = n_0\n\\end{cases}\n\\]\nThat the two different problems seem to “follow one from another” is an illusion. In reality, they only appear to follow one another because this is what we observe in the real world: one equilibrium follows another. In equilibrium thermodynamics, equilibria do not follow one another – each stands alone.\nTime only appears in the following sense: we observe a real-world system, like a car engine, and notice that its motion seems to consist of a sequence of equilibria. Not quite true equilibria, since true equilibria do not change. Maybe “pseudo-equilibria”? Too dismissive. Let’s call them “quasi-equilibria” instead.\nNow, keeping those “quasi-equilibria” in mind, we muddle around the ocean of classical thermodynamics, until we have found some equilibria that resemble the quasi-equilibria we have in mind. And so, from the bottom of the ocean, we pick up one equilibrium, then another, then another. Then we string together these little equilibria along a number line like a pearl necklace. We run our fingers over these pearls and delight in their “motion”, like flipping the pages of a stop-motion book and shouting, “Look, it is moving!”."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#the-three-laws",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#the-three-laws",
    "title": "Classical Thermodynamics and Economics",
    "section": "The three laws",
    "text": "The three laws\nMore accurately: one law and two non-laws.\n\nSecond law\n\nFor the equilibrium of any isolated system it is necessary and sufficient that in all possible variations of the state of the systems which do not alter its energy, the variation of its entropy shall either vanish or be negative.\n(Gibbs 1878)\n\nThe second law of thermodynamics is all-important: maximizing entropy is all of classical thermodynamics. All other parts are just tricks for maximizing entropy.\nAlthough, there are actually two aspects of the second law, subtly different. One of them is static, while the other is dynamic:\n\nStatic: A thermodynamic system is described by a function on the state space, called the entropy function. Each system can be placed under many different forms of constraints. Under each possible constraint, the only physically observable state is the state that maximizes entropy under constraint.\nDynamic: The entropy of a thermodynamic system does not decrease over time.\n\nThere is no difficulty with the static statement, but many difficulties with the dynamic statement, since it involves time, which really does not exist in classical thermodynamics. Nevertheless, since time is so important to the rest of physics, physicists, especially physics teachers, have repeatedly tried to hack it back into the theory, resulting in predictable and endless confusions. Since we do not compromise with intuition, in the rest of this essay, we will use the static formulation as much as possible.\n\n\nFirst law\n\nAs a student, I read with advantage a small book by F. Wald entitled “The Mistress of the World and her Shadow”. These meant energy and entropy. In the course of advancing knowledge the two seem to me to have exchanged places. In the huge manufactory of natural processes, the principle of entropy occupies the position of manager, for it dictates the manner and method of the whole business, whilst the principle of energy merely does the book- keeping, balancing credits and debits.\n(Emden 1938)\n\nThe first law of thermodynamics is entirely trivial. Energy is nothing special! Energy is just a conserved quantity, one among equals, much like volume, mass, and many other things… Every conserved quantity is equally conserved,3 so it does not deserve a special thermodynamic law. You might as well say “conservation of mass” is “the second-first law of thermodynamics” and “conservation of volume” is “the third-first law of thermodynamics”, and “conservation of electrons” and “conservation of protons” and “conservation of length” (if you are studying a thermodynamic system restricted to move on a line) and “conservation of area” (if you are studying a thermodynamic system restricted on the surface of a lake), and so on…\n3 The first law of thermodynamics had always struck me as oddly out of place, almost like a joke I could not catch, like\n\nAll animals are equal, but some animals are more equal than others\n\nbut with\n\nAll conserved quantities are conserved, but some quantities are more conserved than others.\n\nI kept waiting for the textbooks, or the teachers, or someone to drop the act and confess, “Actually, we were joking – conservation of energy really is not that special, and we were just bored with standard physics and wanted to confuse you with a cute magic trick before showing you what standard physics really is saying.”. Slowly, I realized that there is no joke – conservation of energy is unironically treated as special not just by the students but even by the teachers. I had to figure out for myself how the joke really works, and it required me to rebuild thermodynamics according to my preferences.This sounds extraordinary, but that is merely how classical thermodynamics works. The first law of thermodynamics does not deserve its title. It should be demoted to an experimental fact and not a law. Just to drive the point home, I wrote an entire sci-fi worldbuilding sketch about an alien species for which the conservation of volume that is fundamental, not energy, and which discovered stereodynamics. If you can laugh at their mistaken importance of the conservation of volume, maybe you can laugh at the mistaken importance of the conservation of energy too.\nThe proper place for the law of conservation of energy is not classical thermodynamics, but general physics, because energy is nothing special inside classical thermodynamics, but it is extremely special if we zoom out to consider the whole of physics. Whereas in classical thermodynamics, systems conserve energy, and volume, and mass, and electron-number, and proton-number, and… when you move outside of thermodynamics, such as when you add in electrodynamics, special relativity, and quantum mechanics, all kinds of conservations breakdown. You don’t have conservation of mass, or number of electrons, or even volume, but energy is always conserved.\nThis explains the long confusion around the conservation of energy. Within equilibrium thermodynamics, every conserved quantity is conserved, yes, but physicists are less interested in theoretical purity than mathematicians. They know that outside, energy is a more special conserved quantity than others, and so they can’t help but feel like energy should be treated as special inside thermodynamics too.\n\n\nZeroth law\nNow that the first law has been dispelled, we can dispel the zeroth law of thermodynamics too. If energy falls from grace, so must its shadow, temperature.\n\nTheorem 1 (general zeroth law) If \\(S_1(X_1, Y) + S_2(X_2, Y)\\) is maximized under the constraint \\(X_1 + X_2 = X\\), then \\((\\partial_X S_1)_Y = (\\partial_X S_2)_Y\\).\n\n\nThe zeroth law of thermodynamics in various guises.\n\n\n\n\n\n\n\nmaximized quantity \\(S\\)\nconserved quantity \\(X\\)\nderivative \\((\\partial_X S)_Y\\)\n\n\n\n\nentropy\nenergy\ninverse temperature \\(\\beta\\)\n\n\nentropy\nvolume\n\\(\\beta P\\)\n\n\nentropy\nparticles\n\\(-\\beta \\mu\\), where \\(\\mu\\) is chemical potential\n\n\nentropy\nsurface area\n\\(-\\beta\\sigma\\), where \\(\\sigma\\) is surface tension\n\n\nproduction value\nraw material\nmarginal value\n\n\n\n\n\nThird law\nThe third law is rarely, if ever, used in classical thermodynamics, and its precise meaning is still unclear. It seems to me that its proper place is not thermodynamics, but quantum statistical mechanics, where it states that a substance, when at the lowest possible energy (ground state), has finite entropy – Einstein’s formulation of the third law. (Klimenko 2012)\n\n\nAn economic interpretation\nHere is an economic interpretation of classical thermodynamics. There are other possible interpretations, and we will use the others in this essay.\nIn this interpretation, the laws of nature become the CEO of a company. Every conserved quantity is a commodity. The company has some commodity. Commodities themselves have no intrinsic value. Instead, the company is valued by a certain accounting agency. The CEO’s job is to move around the commodities so that the accounting agency gives it the highest book-value.\nA compound system is a conglomerate company: a giant company made of little companies. If entropy is extensive, then it means the total book-value for the conglomerate is the sum of the book-value of each subsidiary company. Otherwise, entropy is nonextensive, and the accounting agency believes that the conglomerate has corporate synergy.\nThe inverse temperature \\(\\beta\\) is the marginal value of energy:\n\\[\\beta = \\frac{d(\\text{value of a sub-company})}{d(\\text{energy owned by the sub-company})}\\]\nThe pressure \\(P\\), multiplied by \\(\\beta\\), is the marginal value of space:\n\\[\\beta P = \\frac{d(\\text{value of a sub-company})}{d(\\text{volume owned by the sub-company})}\\]\n\n\n\n\n\n\nTip\n\n\n\nIt might appear odd to write \\(\\beta P\\), but in the entropy-centric view of thermodynamics, it is the quantity \\(\\beta P\\) that is fundamental, and in comparison, the pressure \\(P\\) is less fundamental, as a ratio \\(P := \\frac{\\beta P}{\\beta}\\). Why, then, do we speak of pressure \\(P\\) and temperature \\(T\\), instead of \\(\\beta\\) and \\(\\beta P\\)? It is because classical thermodynamics is traditionally understood as energy-centric."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#basic-consequences",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#basic-consequences",
    "title": "Classical Thermodynamics and Economics",
    "section": "Basic consequences",
    "text": "Basic consequences\n\nThermodynamic force/suction\nSince nature is entropy-maximizing, if entropy can be increased by moving some energy from one system to another, it will happen. Similarly for space. It would seem as if there is a thermodynamic suction that is sucking on on energy, and the side with the higher thermodynamic suction tends to absorb it.\n\\[\n\\text{thermodynamic suction of $X$} = \\left(\\frac{\\partial S}{\\partial X}\\right)_{\\text{non-}X}\n\\]\nIn energy-centric thermodynamics, we use thermodynamic force, or thermodynamic potential, defined by\n\\[\n\\text{thermodynamic force of $X$} = -\\frac{\\left(\\frac{\\partial S}{\\partial X}\\right)_{\\text{non-}X}}{\\left(\\frac{\\partial S}{\\partial U}\\right)_{\\text{non-}U}}\n\\]\nFor example, for a tank of gas, the macroscopic properties are \\(U, V, N\\), and so it has three thermodynamic suctions:\n\\[\n\\begin{aligned}\n  \\beta &= (\\partial_U S)_{V, N}\\\\\n  \\beta P &= (\\partial_V S)_{U, N}\\\\\n  -\\beta \\mu &= (\\partial_N S)_{U, V}\n\\end{aligned}\n\\]\nand the thermodynamic forces associated with \\(V, N\\) are \\(P\\) and \\(-\\mu\\).\nUnfortunately, the notations for thermodynamic suctions are far from elegant. The first one, \\(\\beta\\), is the inverse of temperature.. The second one, \\(\\beta P\\), is \\(\\beta\\) multiplied by pressure. The third one is truly the most annoying, as it not only involves \\(\\beta\\), but also a negative sign. To see how the notation came about, we can write it out in differential form:\n\\[dS = \\beta dU + \\beta P dV - \\beta \\mu dN\\]\nConventional notations are energy-centric, so we rewrite it to single out \\(dU\\):\n\\[\ndU = TdS + (- PdV) + \\mu dN\n\\]\nNow we see how the notation came about: \\(TdS\\) is the heat energy-flow into the system, \\(-PdV\\) is the mechanical work energy-flow into the system, and \\(\\mu dN\\) is the chemical energy-flow into the system.\nIf I could truly reform notation, I would redefine \\(\\beta P\\) as \\(p_V\\), meaning “the price of volume”, meaning “the price of particle”, and so on. In this notation, we have:\n\\[\n\\begin{aligned}\ndS &= p_U dU + p_V dV + p_N dN \\\\\ndU &= p_U^{-1}dS - \\frac{p_V}{p_U}dV - \\frac{p_N}{p_U}dN\n\\end{aligned}\n\\]\n\nExample 2 (photon gas with \\(\\mu = 0\\)) Suppose we have a piston chamber, with its inner surface covered with silver, and there is a tiny speck of blackbody inside it, then the chamber would be filled with bouncing photons, in the form of a “photon gas”. The photons would reflect off the surface of the chamber, some absorbed and some emitted in turn, by the blackbody.\nAs usual for gas, the state of the system is determined by its internal energy, volume, and particle number: \\(U, V, N\\), with\n\\[dS = \\beta dU + \\beta PdV - \\beta \\mu dN\\]\nHowever, the photon gas is quite special, in that photons can be created and destroyed by the speck of blackbody, so at equilibrium, we must have \\(\\beta\\mu = 0\\), for otherwise, the system would be able to increase in entropy simply by creating/destroying more photons, and thus it is not in equilibrium.\nThis contrasts with the typical case with chemical gases like oxygen, where the particle number in a reaction chamber is constant, allowing \\(\\mu \\neq 0\\) even at equilibrium.\n\n\n\nHelmholtz free entropy\nWhen we have a small system connected to a large system, while we can solve its equilibria by maximizing the plain old entropy for the full compound system, it is often easier conceptually to define a “free entropy” for the small system, and treat the large system as a bath. This is similar to how one can solve for the motion of a cannonball on earth by describing a constant gravitational acceleration, even though we can could have solved for the full cannonball-earth two-body system.\n\nDefinition 1 (Helmholtz free entropy) The Helmholtz free entropy is the convex dual of entropy with respect to energy:\n\\[\nf(\\beta, X) = \\max_U [S(U, X) - \\beta U]\n\\tag{1}\\]\nwhere \\(U\\) is its internal energy, and \\(X\\) are some other macroscopic properties.\nOf historical importance is the Helmholtz free energy \\(F := - T f\\), or equivalently,\n\\[\nF = \\min_U [U - TS(U, X)]\n\\tag{2}\\]\n\n\nTheorem 2 (maximize Helmholtz free entropy) If a thermodynamic system is in energy-contact with an energy bath with price \\(\\beta\\), and is held under constraint on the state by \\(C(X) = 0\\), then the system equilibrates at\n\\[\n\\begin{cases}\n\\max_{U, X} [S(U, X) - \\beta U] = \\max_{X} f(\\beta, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\nThat is, the system always equilibrates at the maximal Helmholtz free entropy state that satisfies the constraint.\nEquivalently, the system minimizes its Helmholtz free energy under constraint.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf we prove the case for Helmholtz free entropy, then by multiplying it by \\(-T\\), we find that the system minimizes its Helmholtz free energy under constraint. So it remains to prove the case for Helmholtz free entropy.\nWe prove the case where there is no constraint on the state of the system. The proof for the case with constraint is similar.\nSuppose the system starts out at \\(\\beta, U_0, X\\). Then the equilibrium condition is\n\\[\n\\begin{cases}\n\\max (S_{bath} + S) \\\\\nU_{bath} + U = U_{bath, 0} + U_{0}\n\\end{cases}\n\\]\nThe entropy of the bath is\n\\[S_{bath} = S_{bath, 0} + \\beta Q\\]\nwhere \\(Q = U_{bath} - U_{bath, 0}\\) is the amount of energy received by the bath as heat.\nPlugging this back in, the equilibrium condition simplifies to\n\\[\n\\max_U [\\beta (U_0 - U) + S(U, X)]\n\\]\nwhich is the desired result.\n\n\n\n\nTheorem 3 (Helmholtz free energy difference is available for work) Consider the following method of extracting mechanical energy. Connect the system to an energy bath at the energy price \\(\\beta\\), and to a mechanical system of arbitrary design. The system starts at \\(\\beta, U_0, X_0\\) and ends at \\(\\beta, U_1, X_1\\). No matter how the mechanical system is designed, and no matter whether the process is reversible or not, we have\n\\[\nW\\leq F(\\beta, U_0, X_0) - F(\\beta, U_1, X_1)\n\\]\nwhere \\(W\\) is “mechanical work done by the system”, that is, the increase in internal energy of the mechanical system. This is an equality when the process is reversible.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy conservation of energy and the second law,\n\\[\n\\begin{cases}\nU_1 = U_0 - (W + Q) \\\\\n\\beta Q + S(U_1, X_1) \\geq S(U_0, X_0)\n\\end{cases}\n\\]\nwhich simplifies to the result.\nIf the process is reversible, then the entropy before and after must be equal, which gives us\n\\[\n\\begin{cases}\nU_1 = U_0 - (W + Q) \\\\\n\\beta Q + S(U_1, X_1) = S(U_0, X_0)\n\\end{cases}\n\\]\nwhich simplifies to the result.\n\n\n\nThis result is typically interpreted as saying that \\(\\Delta F = W\\), that is, in a system in constant thermal equilibrium with an energy bath of constant temperature, the decrease in Helmholtz energy of the system is the maximal mechanical work extractable from the system. Incidentally, this explains the odd name of “free energy” – “free” as in “free to do work” to contrast with the other parts of internal energy, which are chained up and not free to do work. Energy is born free, and eventually everywhere it is in chains.\n\nTheorem 4 (envelope theorem for Helmholtz) For any inverse temperature \\(\\beta &gt; 0\\) and thermodynamic properties \\(X\\), let \\(U^*\\) be the optimal internal energy that maximizes Helmholtz free entropy. We have\n\\[\n\\begin{aligned}\n\\beta &= (\\partial_U S)_X|_{U=U^*(\\beta, X), X = X} \\\\\ndf    &= (\\partial_X S)_U|_{U=U^*(\\beta, X), X = X} dX - U^*(\\beta, X) d\\beta\n\\end{aligned}\n\\]\nif \\(S\\) is differentiable and strictly concave at that point.\n\n\n\n\n\n\n\nProof\n\n\n\nFor the first equation, differentiate \\(f\\). For the second equation, apply the same no-arbitrage proof as in the proof of Hotelling’s lemma (see the essay on Analytical Mechanics).\n\n\nEconomically speaking, the second equation is a special case of the envelope theorem, just like Hotelling’s lemma.\n\n\nFirst-order phase transition\nWhat happens if \\(S\\) is not differentiable and strictly concave? In this case, we do not have \\(\\beta = (\\partial_U S)_X\\). We have two possibilities.\nThe first possibility is pictured as follows. There is a kink in the curve of \\(S(U, X)\\). At that point of critical internal energy \\(U_c\\), there is an entire interval of possible \\(\\beta\\). What we would notice is that at that critical internal energy and critical entropy, the system can be in equilibrium with any heat bath with any temperature between \\([T_{c, min}, T_{c, max}]\\). As far as I know, such systems do not exist, as all physically real systems have a unique temperature at all possible states.\n\n\n\nWhen there is a kink in the curve of \\(S(U, X)\\), the system has an indeterminate temperature.\n\n\nThe second possibility is pictured as follows. There is a bump in the curve, such that we can draw a double tangent over the bump, with slope \\(\\beta_c\\). At that critical inverse temperature, the system can be either at the lower tangent point, or the upper tangent point. It cannot be anywhere in-between, because as we saw, such points do not minimize \\(f(\\beta, X)\\), and thus are unstable.\n\n\n\nWhen there is a double tangent in the curve of \\(S(U, X)\\), the system undergoes a first-order phase transition with latent entropy and energy.\n\n\nFor example, if we confine some liquid water in a vacuum chamber, and bathe it in a cold bath, then at its critical \\(\\beta_c\\), it would split into two parts, one part is all ice, and the other part is all water, mixed in just the right proportion to give it the correct amount of total internal energy. As it loses internal energy, the ice part grows larger, until it is all ice, at which point the system has finally gotten over the bump, and could cool down further.\nAt the critical point, \\((\\partial_{\\beta}f)_X\\) changes abruptly. So if we plot \\(\\beta \\mapsto f(\\beta, X)\\), the curve will kink there.\n\nTheorem 5 (Maxwell equal area rule) In a first-order phase transition at a fixed temperature and varying pressure/volume, the \\(P, V\\) diagram has a horizontal line going from \\((P_c, V_1)\\) to \\((P_c, V_2)\\), such that\n\\[\\int PdV = P_c(V_2-V_1)\\]\n\n\n\nMaxwell’s equal area rule states that the areas of the regions labelled I and II are equal.\n\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix the system’s internal energy \\(U\\), and its temperature \\(T\\), and plot the \\(V, S\\) curve.\nWhen pressure is at a critical value \\(P_c\\), the line of slope \\(\\beta P_c\\) is tangent to the \\(V \\mapsto S(U, V)\\) curve at two different points, with volumes \\(V_1, V_2\\). This is that first-order phase transition.\nNow, move the system state from the first point to the second. During the process, \\[\\int PdV = \\int (TdS -dU) = \\int TdS = T \\Delta S = T \\beta P_c(V_2 - V_1) = P_c(V_2-V_1)\\]\n\n\n\n\n\nOther free entropies and energies\nConsider a thermodynamic system whose entropy function is \\(S(U, V, X)\\), where \\(U\\) is the internal energy and \\(V\\) is the volume. Its Gibbs free entropy is\n\\[\ng(\\beta, \\beta P, X) = \\max_{U, V} (S(U, V, X) - \\beta U - (\\beta P)V)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy and volume. Similarly, its Gibbs free energy is \\(G = -g/\\beta\\).\n\nTheorem 6 (Gibbs free entropy is maximized) Let a thermodynamic system be in equilibrium with an energy-and-volume bath of prices \\(\\beta, \\beta P\\). If the system has constraint on the state by \\(C(X) = 0\\), then the system equilibrates at\n\\[\n\\begin{cases}\n\\max_{X} g(\\beta, \\beta P, X)\\\\\nC(X) = 0\n\\end{cases}\n\\]\nAnd if \\(S\\) is strictly concave and differentiable at that point, then \\(dg = (\\partial_X S)_{U, V} dX - Ud\\beta - V d(\\beta P)\\).\n\n\nTheorem 7 (Gibbs free energy difference is available for work) Connect a system to an energy-and-volume bath at marginal entropies \\(\\beta, \\beta P\\), and a mechanical system of arbitrary design. The system starts at \\(\\beta, \\beta P, X_0\\) and ends at \\(\\beta, \\beta P, X_1\\). Then,\n\\[\nW\\leq G(\\beta, \\beta P, X_0) - G(\\beta, \\beta P, X_1)\n\\]\nwhere \\(W\\) is “work”, that is, the increase in internal energy of the mechanical system. If the process is reversible, then equality holds.\n\n\nTheorem 8 (envelope theorem for Gibbs) For any inverse temperature \\(\\beta &gt; 0\\), any pressure \\(P\\), and other thermodynamic properties \\(X\\), let \\(U^*, V^*\\) be the optimal internal energy and volume that maximizes Gibbs free entropy, then\n\\[\n\\begin{aligned}\n\\beta   &= (\\partial_U S)_{V, X} \\\\\n\\beta P &= (\\partial_V S)_{U, X}\\\\\ndg      &= (\\partial_X S)_{U, V} dX - U^* d\\beta - V^* d(\\beta P)\n\\end{aligned}\n\\]\nif \\(S\\) is differentiable and strictly concave at that point. Here, the left sides of all equations are evaluated at \\(U=U^*(\\beta, \\beta P, X), V=V^*(\\beta, \\beta P, X), X = X\\).\n\n\nExercise 2 Prove the above theorems for Gibbs free entropy.\n\nSimilarly, if a system is in energy-volume-chemical contact with an energy-volume-chemical bath, then the following Landau free entropy is useful:\n\\[\n\\omega(\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta\\mu_n) = \\max_{U, V, N_1, \\dots, N_n} \\left(S(U, V, N_1, \\dots, N_n, X) - \\beta U - (\\beta P)V - \\sum_{i=1}^n (-\\beta \\mu_i) N_i \\right)\n\\]\nIn other words, it’s the convex dual of entropy with respect to energy, volume, and particle numbers.\n\nExercise 3 Formulate and prove the analogous theorems for Landau free entropy.\n\n\nExercise 4 If we consider a system, surrounded by gas, inside an adiathermal piston under an atmosphere, then we can consider the following form of free energy: \\(\\tilde s(U, \\beta P, X) := \\max_{V} (S(U, V, X) - \\beta U - (\\beta P)V)\\). Formulate and prove the analogous theorems for this free entropy.\n\nTake-home lessons:\n\nWhen a system is in contact with a \\(Y\\)-bath, then it is useful to consider the convex dual of the entropy with respect to \\(Y\\), that is, \\(\\max_Y (S(Y, X) - p_Y Y)\\), where \\(p_Y\\) is the marginal entropy of \\(Y\\) of the bath. That is, the price of entropy on the bath-market.\nFree entropy is maximized when the system equilibrates with a bath. Free energy is minimized.\nChange in free energy is the maximal amount of work extractable when the system equilibrates with both a bath and a mechanical system. This maximal amount of work is extracted precisely when the process is reversible. The process is irreversible precisely when less than maximal amount of work is extracted.\n\n\n\nMaxwell relations\nSince \\(dS = \\beta dU + \\beta P dV\\), we have the first Maxwell relation\n\\[\n\\partial_U \\partial_V S = (\\partial_V \\beta)_U = (\\partial_U(\\beta P) )_V\n\\tag{3}\\]\nIn economic language, it states\n\\[\n\\partial_{q_i}\\partial_{q_j} S = (\\partial_{q_i} p_j)_{q_j} = (\\partial_{q_j} p_i)_{q_i}\n\\tag{4}\\]\nwhere \\(q_i\\) is the quantity of commodity \\(i\\), and \\(p_i\\) is its marginal utility. In economics, we usually prefer writing demanded quantity as a function of marginal price as\n\\[(\\partial_{p_j}q_i)_{q_j} = (\\partial_{p_i}q_j)_{q_i}\\]\nThis is a symmetry of the cross-price elasticity of demand.4\n4 Samuelson used the Maxwell relations, and other relations, to justify neoclassical economics. His idea is that, while utility functions are unobservable, and we do not have a scientific instrument to measure “economic equilibrium”, we can make falsifiable predictions from assuming that the economy is in equilibrium – such as the symmetry of the cross-price elasticity of demand.For example, if \\(i, j\\) are noodles and bread, then \\((\\partial_{q_i} p_j)_{q_j}\\) is how much the marginal price of bread would rise if I have a little more noodle. As noodles and bread are substitutional goods, we expect the number to be negative, meaning that having more noodles, I would price bread less. The Maxwell relation then tells us that it is exactly the same in the other direction: If I am given a little more bread, I would price noodles less. Not only that, I would want less by exactly the same amount.\nThe second relation is a bit hard to explain, since enthalpy really does not have a good representation in entropy-centric thermodynamics. However, it turns out to be just the third relation with \\(i, j\\) switched.\nSince \\(df = \\beta P dV - Ud\\beta\\), we have the third Maxwell relation\n\\[\n\\partial_\\beta\\partial_V f = -(\\partial_V U)_\\beta = +(\\partial_\\beta(\\beta P))_V\n\\tag{5}\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{q_j} \\left[\\max_{q_i}(S(q) - p_i q_i )\\right]= -(\\partial_{q_j} q_i)_{p_i} = (\\partial_{p_i}p_j)_{q_j}\n\\tag{6}\\]\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles always at the exact price, then I would buy and sell from the noodle shop until my marginal price of noodles is equal to the shop’s price. Now, \\((\\partial_{q_j} q_i)_{p_i}\\) is how much noodles I would buy if I am given a marginal unit of bread. As noodles and bread are substitutional goods, this number is negative. This then means \\((\\partial_{p_i}p_j)_{q_j} &gt; 0\\), meaning that if the noodle price suddenly increases a bit, then I would sell a bit of noodles until I have reached equilibrium again. At that equilibrium, since I have less noodles, I would price higher its substitutional good, bread, by an equal amount as the previous scenario.\nSince \\(dg = -Ud\\beta - Vd(\\beta P)\\), we have the fourth Maxwell relation\n\\[\n-\\partial_{\\beta}\\partial_{\\beta P}g = (\\partial_{\\beta P}U)_\\beta = (\\partial_\\beta V)_{\\beta P}\n\\tag{7}\\]\nIn economic language, we have\n\\[\n\\partial_{p_i}\\partial_{p_j} \\left[\\max_{q_i, q_j}(S(q) - p_i q_i - p_j q_j)\\right]= -(\\partial_{p_j} q_i)_{p_i}= -(\\partial_{p_i} q_j)_{p_j}\n\\tag{8}\\]\nThis is another symmetry of cross-price elasticity of demand.\nContinuing from the previous example, if \\(i, j\\) are noodles and bread, and we open a shop with an infinite amount of noodles and bread, then I would of course buy and sell from the shop until my marginal prices of noodles and bread are equal to the shop’s prices. Now, if the shop suddenly raises the price of bread by a small amount, I would sell off some bread until my marginal price for bread increases to the shop’s new price. Now my marginal price for noodles increases too by substitutional effect, so I buy some noodles. Thus \\((\\partial_{p_j} q_i)_{p_i} &gt; 0\\). Switching the scenario, we find that raising the price of noodles would make me buy bread by an amount equal to that in the previous scenario.\n\n\n\n\n\n\nAlternate proof of the Maxwell relations\n\n\n\n\n\nWe use the notation of economics here.\nSuppose we have commodities \\(1, 2, \\dots, n\\). We pick two commodities \\(i, j\\), and fix all other commodity quantities. Thus, we can write \\(dS = p_i dq_i + p_j d q_j\\).\nSince knowing \\(n\\) properties of the thermodynamic system allows us to know its exact state, and we have already fixed \\(n-2\\) properties of it, there only remain two more degrees of freedom. We can parameterize this by \\((q_i, q_j)\\), or \\((p_i, q_j)\\), or \\((p_i, p_j)\\), or any other reasonable coordinate system.\nIf the thermodynamic system undergoes a cycle, then\n\\[0 = \\oint dS = \\oint p_i dq_i + \\oint p_j dq_j\\]\nand thus, if we take the cycle infinitesimally small, we find that \\(dp_i\\wedge dq_i = -dp_j \\wedge dq_j\\). That is, the map \\((p_i, q_i) \\mapsto (p_j, q_j)\\) preserves areas, but reverses orientation. In particular, we have a Jacobian \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} = -1\n\\]\nNow, let \\((x, y)\\) be an arbitrary coordinate transform. By the chain rule for Jacobians, \\[\n\\frac{\\partial(p_i, q_i)}{\\partial(x, y)} = \\frac{\\partial(p_i, q_i)}{\\partial(p_j, q_j)} \\frac{\\partial(p_j, q_j)}{\\partial(x,y)} = -\\frac{\\partial(p_j, q_j)}{\\partial(x,y)}\n\\]\nThis allows us to derive all the Maxwell relations. For example, setting \\((x, y) = (p_i, q_j)\\) gives us the third relation\n\\[(\\partial_{q_j} q_i)_{p_i} = -(\\partial_{p_i}p_j)_{q_j}\\]\n\n\n\n\n\n\n\n\n\nDeriving the four Maxwell relations by picking the right variables for (x, y)."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#caratheodorys-thermodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#caratheodorys-thermodynamics",
    "title": "Classical Thermodynamics and Economics",
    "section": "Caratheodory’s thermodynamics",
    "text": "Caratheodory’s thermodynamics\nIn the early 1900s, Constantin Caratheodory discovered a new way to “geometrize” thermodynamics, with the austere beauty of Euclidean geometry. Though his formulation fell into obscurity, it was reborn in neoclassical economics as utility theory.\n\nEntropy and temperature\nConsider a thermodynamic system with \\(n+1\\) dimensions of state space. Give the state space coordinates \\(q_0, q_1, \\dots, q_n\\). For example, for a tank of ideal gas where the particle number is fixed, we have \\(q_0 = U, q_1 = V\\).\nLet the system be at a certain state \\(\\vec q\\), and wrap the system in a perfectly insulating (adiathermal) blanket. The system can undergo many different kinds of adiathermal motion, but there are certain motions that it cannot undergo.\nFor example, for a piston of ideal gas, the possible motions are adiabatic expansion, adiabatic compression, Joule expansion, and any combination of them. However, “Joule compression” is impossible – the gas will not spontaneously contract to the left half of the system, pulling in the piston head, anymore than a messy room will spontaneously tidy itself.\nWe say that \\(\\vec q'\\) is adiathermally accessible from \\(\\vec q\\) if there exists a path from \\(\\vec q\\) to \\(\\vec q'\\), such that the path is infinitesimally adiathermal5 at every point.\n5 The word “adiathermal” means “heat does not pass through”, while “adiabatic” has an entire history of meaning that makes it hard to say what exactly it is (see my essay on Analytical Mechanics). Personally, I think “adiabatic” means “zero entropy change”, and all its other meanings derive from it.\n\n\nAdiabatic accessibility in the \\(U, V\\) diagram.\n\n\nHere is Caratheodory’s version of the second law of thermodynamics:\n\nIn any neighborhood of any point \\(\\vec q\\), there are points adiabatically inaccessible from it.\nFurthermore, for any two points, \\(\\vec q, \\vec q'\\), at least one of them is adiabatically accessible from the other.\n\nThe effect of these two axioms is that we can define a total ordering \\(\\preceq\\) on state space, where we write \\(\\vec q \\preceq \\vec q'\\) to mean that \\(\\vec q\\) can adiabatically access \\(\\vec q'\\), and write \\(\\vec q \\sim \\vec q'\\) to mean that they are mutually adiabatically accessible.\nInterpreted economically, we say that the system is an economic agent, that each \\(\\vec q\\) is a bundle of goods, that \\(\\vec q \\preceq \\vec q'\\) means that \\(\\vec q'\\) is preferable to the agent, and that \\(\\vec q \\sim \\vec q'\\) means they are equally preferred.\nThe total ordering partitions the state space into contour surfaces of equal accessibility, or indifference surfaces. Assuming the state space is not designed to be pathological, these indifference surfaces will be differentiable.\nLet us consider the indifference surface passing state \\(\\vec q\\). The indifference surface is locally a plane, so it has equations\n\\[\ndq_0 - \\sum_i \\tilde p_i dq_i = 0\n\\]\nwhere \\(\\tilde p_i = (\\partial_{q_i}q_0)_{q_1, \\dots, q_n}\\). For example, a tank of (non-ideal) gas satisfies \\(dU + PdV = 0\\) over its indifference curves.\n\n\n\nThe field of planes defined by \\(dq_0 - \\sum_i \\tilde p_i dq_i = 0\\).\n\n\n\nTheorem 9 (existence and uniqueness of temperature and entropy) Let \\(\\omega = dq_0 - \\sum_i \\tilde p_i dq_i\\). If \\(\\omega\\) is nonzero everywhere, then there exists functions \\(\\beta, S\\) on the state space, such that \\(dS = \\beta \\omega\\).\nFurthermore, they are unique up to a monotonic transform. That is, if we have another solution \\(\\beta', S'\\), then there exists a strictly monotonic function \\(f\\) such that \\(S' = f \\circ S\\).\n\n\n\n\n\n\n\nProof\n\n\n\nAt each point \\(P\\) the one-form \\(\\omega(p)\\) is visualized as a stack of parallel planes. The planes are quilted together, but with “uneven thickness”. By scaling the one-forms just right at every point, the thickness becomes equalized, and so \\(\\beta \\omega = dg\\) for two real-valued functions \\(\\beta, S\\).\n\n\n\n\n\n\nGiven any other solution \\(\\beta', S'\\), both \\(S\\) and \\(S'\\) must have the same contour lines, so there exists some function that maps the \\(S\\)-height of a contour line to its \\(S'\\)-height.\n\n\nProving Caratheodory’s theorem. Figure from .\n\nCardinal and ordinal utilities\nEconomically speaking, \\(\\tilde p_i\\) is the marginal worth of \\(q_i\\) denoted in units of \\(q_0\\). For example, we can say that \\(q_0\\) are cowry shells, which themselves are pretty and give us some utility. However, it can also be used as a monetary unit. Then, if \\(i\\) is bread, then \\(\\tilde p_i\\) is the marginal amount of cowry shells that we would pay for a marginal amount of bread.\nIf we were to visit a free market where we can buy and sell items denoted in cowry shells, then we would buy bread if \\(\\tilde p_i &gt; \\tilde p_{i, market}\\), and sell bread if \\(\\tilde p_i &lt; \\tilde p_{i, market}\\). Right at the border of \\(\\tilde p_i = \\tilde p_{i, market}\\), we would be indifferent about buying or selling bread. When \\(\\tilde p_i = \\tilde p_{i, market}\\) for all \\(i\\), we would be completely indifferent about the market.\n\\(S\\) is the utility, and \\(\\beta\\) is the marginal utility of cowry shells. The theorem tells us that just by knowing how we order the goods (” \\(S(\\vec q) &gt; S(\\vec q')\\) “), we can extract a numerical value for the goods (” \\(S(\\vec q) - S(\\vec q') = 1.34(S(\\vec q'') - S(\\vec q'''))\\) “). Out of ordinal utility, we have achieved cardinal utility.\nThere used to be a debate between “ordinalists” and “cardinalists” of utility theory. The “cardinalists” were the more venerable of the two camps, tracing back to Bentham’s felicific calculus and the marginalist revolution. They argued that utility is real-valued, like entropy and temperature. The “ordinalists” countered that a nobody has ever measured a utility in anyone’s brain. The only thing we can observe is preferences: I prefer this over that – I can order everything that can ever happen to me on a numberless line of preferences. Similarly, nobody can ever actually measure temperature or entropy, only that energy flows from this gas to that gas, which presumably has lower temperature, and that one chunk of gas in one state ends up in another state, which presumably has higher entropy.\nThe debate has been mostly resolved by the work of Gérard Debreu, who showed that under fairly reasonable assumptions, cardinal utility is possible (Debreu 1971).6\n6 Out of all those famous economists I have seen, Gérard Debreu is perhaps the most mathematically austere. Reading his works, I felt like he was another G. H. Hardy, a Bourbaki of economics. He did economics not to improve the world, not to help people, and not to advance a political agenda, but to simply uncover an ꙮmmatidium of eternity.This theorem, or rather, this family of theorems, have several names, as befitting for such a versatile and productive family. In calculus, it’s called the integrability of Pfaffian forms. In differential geometry, it’s called Darboux’s theorem, or Frobenius theorem. In economics, it’s called the integrability of demand, or the cardinal-ordinal utility representation theorem.\n\n\n\n\n\n\nWhat’s so special about energy, or cowry shells?\n\n\n\nWhen cast in the language of economics, cowry shells are not special. We could denote prices in cowry shells, or cans of sardine, or grams of gold. That is, we are free to pick any numéraire we want, as long as we are consistent about it.\nSimilarly, energy is not special. For example, with ideal gas, we could write the first law of thermodynamics as the conservation of energy, like\n\\[dU - (-P)dV = \\beta^{-1}dS\\]\nor as the conservation of volume, like\n\\[dV - (-P^{-1})dU= (\\beta P)^{-1}dS\\]\nand from the perspective of classical thermodynamics, there is no possibility of saying that energy is more special than volume. Energy is exactly as special as volume, and no more special than that.\nWhen I realized this difference, I was so incensed at this mistake that I wrote an entire sci-fi worldbuilding sketch about an alien species, for which it is the conservation of volume that is fundamental, not energy, and which discovered stereodynamics, not thermodynamics.\n\n\n\n\nExtensive entropy\nWhile we have constructed the temperature \\(T\\) and the entropy \\(S\\) of an isolated system, it is not unique: we can stretch and compress the entropy function \\(S\\) arbitrarily by a monotonic function, and as long as we modify the temperature function \\(T\\) just right, the two modifications cancel out, and we have \\(TdS = T' dS'\\).\nIn order to uniquely fix an entropy function, we need further assumptions. The most commonly used method is by considering what happens to the entropy of a compound system. In general, there is no reason to expect entropy to be extensive – if we take compound two systems together, the entropy of the compound system should be the sum of the two subsystems. However, if we make some assumptions on “rationality”, then the entropy would be uniquely fixed, and would be extensive.\n\nExercise 5 (von Neumann–Morgenstern entropy construction) Study the statement of the von Neumann–Morgenstern utility theorem, and translate it to thermodynamics. It should be of the following form:\nAssuming that the adiabatic accessibility of any compound system satisfies the following properties\n\n…\n…\n…\n\nthen the entropy of any compound system is the sum of the entropies of its subsystems, and the entropy function is unique up to adding a constant and multiplying by a positive scalar.\n\nSome hints:\n\nIt might help your intuition if you anthropomorphize Nature as a “vNM-rational agent”.\nThe standard formulation of the vNM theorem uses lotteries of form \\(pM + (1-p)N\\), where \\(p\\in (0, 1)\\) is a probability, and \\(M, N\\) are bundles of goods. However, it is impossible generally to “take \\(0.37\\) of a system \\(M\\) and compound it with \\(0.63\\) of system \\(N\\)”. To bypass this difficulty, replace that with “take \\(37\\) copies of system \\(M\\) and compound them with \\(63\\) copies of system \\(N\\)”."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#bonus-geometric-thermodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#bonus-geometric-thermodynamics",
    "title": "Classical Thermodynamics and Economics",
    "section": "Bonus: Geometric thermodynamics",
    "text": "Bonus: Geometric thermodynamics\n\nAlthough geometrical representations of propositions in the thermodynamics of fluids are in general use, and have done good service in disseminating clear notions in this science, yet they have by no means received the extension in respect to variety and generality of which they are capable.\n(Gibbs 1957)\n\n\nContact geometry\n\nEvery mathematician knows it is impossible to understand an elementary course in thermodynamics. The reason is that thermodynamics is based—as Gibbs has explicitly proclaimed – on a rather complicated mathematical theory, on the contact geometry. Contact geometry is one of the few ‘simple geometries’ of the so-called Cartan’s list, but it is still mostly unknown to the physicist – unlike the Riemannian geometry and the symplectic or Poisson geometries, whose fundamental role in physics is today generally accepted.\nV.I. Arnol’d (Caldi et al. 1990, 163)\n\nTo explain this mysterious remark, we take a plunge into abstraction. We know that a real gas has properties \\(P, V, T, S, \\dots\\), and that they satisfy the differential equation:\n\\[\ndS = \\beta dU + \\beta PdV\n\\]\nTo clean up the notation, we can change the notation to\n\\[\ndS = p_1 dq_1 + p_2 dq_2\n\\]\nThis formula has a clear interpretation in economics: if the marginal utility of commodity \\(1\\) is \\(p_1\\), and the marginal utility of commodity \\(2\\) is \\(p_2\\), then if we receive \\(\\delta q_1, \\delta q_2\\), our utility would increase by \\(p_1 \\delta q_1 + p_2 \\delta q_2\\).\nThe difficult thing about classical thermodynamics is that there are so many quantities, such as \\(T, V, N, \\dots\\). The saving grace is that it turns out that there are only a few degrees of freedom.7\n7 The Parthian shot is that now you are burdened with dozens of equations relating these quantities. I cannot remember any of the Maxwell relations, so I look at Wikipedia every time I need to calculate with them.However, why is it that a macroscopic lump of matter, whirling with \\(10^{23}\\) molecules, turn out to be characterized by only a few degrees of freedom? Why is it that a national economy, swarming with \\(10^8\\) people, has macroeconomic laws? For the first question, the answer is given by classical thermodynamics: the lump of matter is maximizing its entropy under constraints, so its degrees of freedom are exactly as many as the number of constraints it is laboring under. For the second question, the answer is given by neoclassical economics: the national economy behaves as if it is maximizing a social utility function under its resource constraints.\nWith that brief look at philosophy, we return to abstract thermodynamics. We have a lump of matter (such as ideal gas in a piston), of which we can measure five different properties: \\(p_1, p_2, q_1, q_2, S\\). In general, we expect that the space of possible measurements is 5-dimensional, but it turns out that they collapse down to a 2-dimensional curved surface. This is why we could completely fix its state knowing just its \\(V, U\\), or just its \\(P, T\\), etc.\nThe question now arises: Why is it possible to collapse things down to this curved surface?\nThings are already interesting when we have just one commodity:\n\\[dS - pdq = 0\\]\nand we ask: Why is it possible to collapse the space of \\((q, p, S)\\) from 3 to 1 dimension?\nIn modern geometry, an expression like \\(dS - \\sum_i p_i dq_i = 0\\) defines a field of planes in \\(\\mathbb{R}^3\\). That is, at each point \\((q, p, S)\\), we construct a plane defined by\n\\[\n\\{(q + \\delta q, p + \\delta p, S + \\delta S): \\delta S - p \\delta q = 0\\}\n\\]\nFor example, in three dimensions, the field of planes \\(dS - pdq = 0\\) would look like it is constantly twisting as \\(p\\) increases. The study of geometric structures definable via the field of planes is contact geometry.8\n8 This explanation might sound like an anticlimax, and I imagine someone would object “Thermodynamics is reduced to contact geometry… but what is contact geometry?”. My answer is that contact geometry simply is. It is not supposed to be a generator of intuitions. Instead, it is a common language that bridges between intuitions. By casting thermodynamics, economics, mechanics, etc, into the language of contact geometry, we would then be able to translate intuition from one field to another field. Saying that “thermodynamics is contact geometry” is not telling you an intuitive way to see thermodynamics, but rather, an intuitive way to see contact geometry.\n\n\nThe field of planes \\(dS - pdq = 0\\) in \\(\\mathbb{R}^{2+1}\\). Figure from Wikipedia.\n\n\nGiven such a field of planes \\(dS - \\sum_{i=1}^n p_i dq_i\\) in \\(\\mathbb{R}^{2n+1}\\), we say that a manifold is a Legendrian submanifold iff the manifold has \\(n\\) dimensions, and is tangent to the field of planes at every point.\nFor example, when \\(n=1\\), a Legendrian submanifold is a curve that winds around \\(\\mathbb{R}^3\\) and is always tangent to the plane at every moment.\nLet \\(S(q)\\) be a differentiable function. We can interpret \\(S(q)\\) as the amount of money we can earn if we produce something using the bundle of raw materials \\((q_1, \\dots, q_n)\\).\nGiven any market price for the raw materials, \\(q^* = \\mathop{\\mathrm{argmax}}_q (S(q) - \\braket{p, q})\\) is the profit-maximizing production plan, and \\(\\Pi(p) = \\max_q (S(q) - \\braket{p, q})\\) is the profit.\n\nTheorem 10 The “profit-maximization surface” defined by \\(p \\mapsto (q^*, p, S(q^*))\\) is a Legendrian submanifold.\nConversely, given any Legendrian submanifold parameterized by \\(p \\mapsto (q(p), p, S(q(p)) )\\), then \\(q(p)\\) is profit-stationarizing. That is,\n\\[\\nabla_q (S(q) - \\braket{p, q}) = 0\\]\nat \\(q(p)\\). If \\(S\\) is strictly concave, then \\(q(p)\\) is profit-maximizing.\n\n\n\n\n\n\n\nProof\n\n\n\nThe first part is proven by Hotelling’s lemma.\nThe second part is proven by plugging in \\(dS - \\sum_i p_i dq_i = 0\\). And if \\(S\\) is strictly concave, then \\(q \\mapsto S(q) - \\braket{p, q}\\) is also strictly concave, and so zero gradient implies global maximum.\n\n\nEconomically speaking, \\(\\max_q (S(q) - \\braket{p, q})\\) means to maximize profit. What does it mean, thermodynamically speaking? It means minimizing \\(\\braket{p, q} - S(q)\\), which is the Gibbs free entropy! Maximizing profit when a factory has access to a market is the same as minimizing Gibbs free entropy when a system is in contact with a bath.\n\n\nSamuelson’s area-ratio thermodynamics\n\nPhilosophy\nIn Paul Samuelson’s Nobel prize lecture of 1970, among comments of classical mechanics, variational principles, and neoclassical economics, he said something curious about the analogy between classical thermodynamics and neoclassical economics:\n\nHowever, if you look upon the monopolistic firm hiring 99 inputs as an example of a maximum system, you can connect up its structural relations with those that prevail for an entropy-maximizing thermodynamic system. Pressure and volume, and for that matter absolute temperature and entropy, have to each other the same conjugate or dualistic relation that the wage rate has to labor or the land rent has to acres of land. Figure 2 can now do double duty, depicting the economic relationships as well as the thermodynamic ones.\nIf someone challenged me to explain what the existence of [utility] implies, but refused to let me use the language of partial derivatives, I could illustrate by an equi-proportional area property… I may say that the idea for this proposition in economics came to me in connection with some amateurish researches in the field of thermodynamics. While reading Clerk Maxwell’s charming introduction to thermodynamics…\n(Samuelson 1971)\n\n\n\n\nSamuelson’s equal area ratio condition. In the diagram, we have \\(a:b = c:d\\)\n\n\nThis intriguing little remark piqued my interest, and after a little digging, I figured it out.9\n9 Based on research by James Bell Cooper, who seems to be the world expert in this obscure field (Cooper and Russell 2006; Cooper, Russell, and Samuelson 2001).Samuelson is alluding to a deep problem in economics theory: Nobody has ever seen a utility function, anymore than anybody has ever seen an entropy. If this is the case, then how do we know that agents are maximizing a utility, or that systems are maximizing an entropy? In his long career, he searched for many ways to answer this, coming down to the idea that, even though we cannot measure utility, we can measure many things, such as how firms respond to prices. Given some measurable quantities, we can then prove, mathematically, that something is being maximized. At that point, we can simply call that something “utility”, and continue doing economics as usual.\nPhilosophically, Samuelson was greatly influenced by operationalism, a philosophy of science akin to logical positivism. As stated by the definitive work on operationalism, “we mean by any concept nothing more than a set of operations; the concept is synonymous with the corresponding set of operations” (Bridgman 1927).\nIn his early work, particularly Foundations of Economic Analysis (1947) and the development of revealed preference theory (1938), Samuelson embraced operationalism as a means of purging economics of non-observable, and thus scientifically meaningless, concepts like utility. He sought to rebase economic theory on purely observable behavior and measurable quantities. Revealed preference theory, for instance, would eliminate utility functions, and derive a consumer’s preference ordering directly from observable consumer choices at different price levels.\nOver time, Samuelson’s stance on operationalism softened to a more pragmatic approach, recognizing the value of unobservable concepts as theoretical tools, as long as they can be based on direct observables.\nFor example, while Samuelson initially sought to eliminate utility functions, he later argued that even if utility is not directly observable, it can be uniquely determined from observing agents’ preferences, by invoking some utility representation theorems – provided that the preferences satisfy certain properties. (Samuelson 1999)\n\n\nArea ratio construction\n\nTheorem 11 (area ratio law implies a new coordinate system) Consider an open rectangle \\(R\\) in the plane \\(\\mathbb{R}^2\\). Let there be two families of curves\nSuppose that each curvy parallelogram formed by the two families is contained in \\(R\\), and the curves satisfy the area-ratio rule, then we can define another coordinate system \\((z, w)\\) on \\(\\mathbb{R}^2\\), such that the coordinate system preserves areas: \\[\ndx \\wedge dy = dz \\wedge dw\n\\]\nand that the two families of lines are the constant \\(z\\) and constant \\(w\\) curves.\nThe coordinate system is unique up to an affine squashing transform, that is, \\((z, w) \\mapsto (cz + d, w/c + e)\\) for some constants \\(c, d, e\\) with \\(c \\neq 0\\).\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix an arbitrary point as \\((z, w) = (0, 0)\\). Pick an arbitrary curve in one of the families, not passing \\((0, 0)\\) point, and call it the \\(w=1\\) line. By continuity, there exists a unique curve in the other family, such that their curved parallelogram has unit area. Call that other curve the \\(z=1\\) line. Now we can label its four corners as \\((z, w) = (0, 0), (0, 1), (1, 0), (1, 1)\\).\nFor any other point, its \\((z, w)\\) coordinates can be constructed as shown in the picture, with\n\\[z = a+b, \\quad w = b+d\\]\nBy the area ratio law, we have \\(a:b = c:d\\). We also have \\(a+b+c+d = 1\\) since we picked the parallelogram to have unit area. Solving these 4 equations, we find that \\(b = zw, a = z(1-w), d = (1-z)w, c = (1-z)(1-w)\\), as it should.\nTaking the derivative, we have \\(dx \\wedge dy = dz \\wedge dw\\).\n\n\n\n\n\nCorollary 1 (area ratio law implies existence of an entropy function) Since \\(dx \\wedge dy = dz \\wedge dw\\), we can draw any cycle \\(\\gamma\\), and integrate around the cycle: \\[\\oint_\\gamma (ydx + zdw) = \\iint_{\\text{area in }\\gamma} (dy \\wedge dx + dz \\wedge dw) = 0\\]\nThus, there exists some scalar function \\(S\\), such that \\(dS = ydx + zdw\\).\n\nAnd with an entropy/utility function, all of classical thermodynamics/neoclassical economics follow.\n\n\nExample: ideal gas\nSuppose we know from experiment that a tank of ideal gas satisfies two equations\n\\[PV = \\Const, \\quad PV^\\gamma = \\Const\\]\nunder isothermal and adiabatic conditions respectively, then the \\(P, V\\) diagram with these two families of curves satisfy the area ratio condition. It is tedious but straightforward to verify this by direct integration. Alternatively, we can use the method of exhaustion and Eudoxus’ theory of proportions to prove this, in a way that even ancient Greeks would approve.\n\n\n\n\n\n\nProof\n\n\n\n\n\nNotice that under the squashing map \\((P, V) \\mapsto (cP, V/c)\\), both families of lines are preserved, and furthermore, this map preserves area, so we can calculate the area of any curvy parallelogram by tiling it with tiny strips of thin parallelograms.\nAs shown, we can draw a very thin parallelogram \\(\\delta\\), then use the squashing map to tile both parallelograms \\(c\\) and \\(d\\). We have that\n\\[A(c) : A(d) = \\frac{A(c)}{A(\\delta)} : \\frac{A(d)}{A(\\delta)} \\approx N(c) : N(d)\\]\nwhere \\(A(c)\\) is the area of \\(c\\), and \\(N(c)\\) is the number of copies of \\(\\delta\\) that are contained within \\(c\\). By the method of exhaustion and Eudoxus’ theory of proportion, at the limit of infinitely thin \\(\\delta\\), both sides are equal.\nNow, performing the same construction on the other half of the parallelograms, we tile \\(a, b\\) by the same number of copies of \\(\\delta'\\). Thus we have\n\\[A(c) : A(d) \\approx N(c) : N(d) = N(a) : N(b) \\approx A(a) : A(b)\\]\nand both sides equal at the limit.\n\n\n\n\nNow, by the area ratio construction, there exist two functions \\(f_T, f_S\\), such that the new coordinates\n\\[T(P, V) = f_T(PV), \\quad S(P, V) = f_S(PV^\\gamma)\\]\nsatisfy \\(dT \\wedge dS = dP \\wedge dV\\). We can then define\n\\[dU = TdS - PdV\\]\nwhich satisfies \\(d^2 U = 0\\), that is, it is integrable.\n\n\n\n\n\n\nDerivation\n\n\n\n\n\nSimplifying, we get\n\\[f'_T(PV)f_S'(PV^\\gamma) = \\frac{1}{(\\gamma - 1) PV^\\gamma}\\]\nLet \\(x = PV, y = PV^\\gamma\\) to separate the variables: \\(f'_T(x) f'_S(y) = \\frac{1}{(\\gamma-1) y}\\), which solves to\n\\[T = C_1 PV + C_0, \\quad S = \\frac{1}{(\\gamma-1) C_1} \\ln \\frac{PV^\\gamma}{P_0V_0^\\gamma}\\]\nfor some constants \\(C_0, C_1, P_0, P_0\\).\nKnowing that \\(C_0 = 0\\) and \\(C_1 = 1/(nR)\\), we have the equations of state: \\[PV = nRT, \\quad S = \\frac{1}{\\gamma-1} nR \\ln \\frac{PV^\\gamma}{P_0V_0^\\gamma}\\]\nIntegrating \\(dU = TdS - PdV\\), we have \\(U = \\frac{1}{\\gamma-1} nRT\\). We can define \\(\\hat c_V = \\frac{1}{\\gamma - 1}\\), which leads to\n\\[PV = nRT, \\quad S = \\hat c_V n R \\ln \\frac{PV^{1 + 1/\\hat c_V}}{P_0V_0^{1 + 1/\\hat c_V}}, \\quad U = \\hat c_V nRT\\]\nor equivalently, \\(S = n R \\ln \\frac{U^{\\hat c_V}V}{U_0^{\\hat c_V} V_0}\\).\n\n\n\nThus, we have\n\\[PV = nRT, \\quad S = \\hat c_V n R \\ln \\frac{PV^{1 + 1/\\hat c_V}}{P_0V_0^{1 + 1/\\hat c_V}} = n R \\ln \\frac{U^{\\hat c_V}V}{U_0^{\\hat c_V} V_0}, \\quad U = \\hat c_V nRT\\]\nTaking the derivative, \\(dS = \\beta dU + \\beta PdV - \\beta \\mu dn\\) gives \\(\\beta = 1/T\\), \\(P = P\\), and \\(\\mu = -TS/n\\).\nWe find that the chemical potential, unfortunately, has an additive constant. We should not be too surprised, however, as anything with “potential” in its name probably has an additive constant, like electric voltage.\n\n\nEconomic interpretation\nWhen the diagram is interpreted as the thermodynamic diagram of a gas, we know what the curvy lines mean: they are the isotherms, isentropics, isobarics, etc (depending on which variables you pick for the two axes of the diagram). What do the curvy lines mean in economics?\n\n\n\nThe equal area ratio condition.\n\n\nSuppose we plot the lines of constant \\(p_2\\) in the plane of \\(q_1, p_1\\). What does it say? It says this: “Suppose the price of commodity \\(2\\) is fixed, and we vary the price of commodity \\(1\\). How much of commodity \\(1\\), as a factory manager, would I want to purchase?” In other words, these are the demand curves for commodity \\(1\\) when the price of commodity \\(2\\) is fixed.\nSimilarly, a line of constant \\(q_2\\) is a demand curve for commodity \\(1\\) when the quantity of commodity \\(2\\) is fixed at \\(q_2\\).\nLooking at the diagram, we see that the demand curves are steeper for fixed \\(q_2\\) than for fixed \\(p_2\\). In other words, the factory manager is more price-sensitive about commodity \\(1\\) when there is a free market for commodity \\(2\\), because there is a choice.\nTo be concrete, think of managing a factory, where the two commodities are labor and machinery. Think like a factory manager. If I have no choice in how many machines I have in my factory, then faced with a sudden rise in wages, I would only fire a few workers. If, however, there is a free market for machines, then I would fire more workers and buy some machines to make up for it.\nThis is Le Chatlier’s principle for economics, which Paul Samuelson used to great effect. In his telling, immediately after the market has suffered a sudden price shock, factories would have to suffer the consequences because they cannot react by changing their production plans. Thus, in the short run, factories are less price-sensitive. In the long run, the factories would be able to change their production plans, and so in the long run, factories are more price-sensitive. As another application, during a wartime economy when there is rationing for some critical products like rubber and oil, people would become less price-sensitive in the products not subjected to rationing.\nThis result can be generalized to the case of \\(n\\) commodities \\(q_1, \\dots, q_n\\) with prices \\(p_1, \\dots, p_n\\). In this case, we would find that, assuming some more complicated area ratio law, we can rescale \\(q_2, \\dots, q_n\\) and \\(p_2, \\dots, p_n\\), such that \\(\\sum_i dp_i \\wedge dq_i = 0\\). This then allows us to construct a function \\(S\\), such that\n\\[dS - \\sum_i p_i dq_i = 0\\]\nwhich, by Theorem 10, maximizes like an entropy, so it is an entropy.\n\n\n\nBonus: Riemannian geometry\nThere are other ways to study the state space of thermodynamic systems by differential geometry. For example, since the entropy function is typically a strictly concave function of the extensive parameters, \\(-\\partial^2 S\\) is positive-definite. This is then a Riemannian metric on the state space.10 For more on this line of research, search “Ruppeiner geometry” and “Weinhold geometry” (Weinhold 1976; Quevedo 2007).\n10 The only places where strict concavity fails are when \\(S\\) is “bumped downwards”, which gives us a first-order phase transition, or has a flat region, which gives us a second-order phase transition. Away from regions of phase transition, we have a Riemannian geometry. In the regions of phase transitions, the geometry collapses into singularities, much as spacetime collapses in the center of a black hole."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#chemical-equilibrium-done-right",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#chemical-equilibrium-done-right",
    "title": "Classical Thermodynamics and Economics",
    "section": "Chemical equilibrium done right",
    "text": "Chemical equilibrium done right\nJosiah Willard Gibbs was an otherworldly figure who thought of abstract surfaces in a 19th century America, where practical industry, not pure science, was celebrated. He was also famously obscure and could write the most convoluted sentences that defeated everyone, from Boltzmann to Jaynes (Jaynes 1992). When he was asked by his European translator to write a preface for the German translation of his thermodynamics book, he replied that he had already said everything he wanted to say about thermodynamics, so there was nothing to add. (Dais 2024)\nGibbs wrote his hefty Heft on thermodynamics, On the Equilibrium of Heterogeneous Substances (1876), to answer one question: why are some “heterogeneous substances” in equilibrium, while others not? Why, when we drop a block of salt into pure water, does the block of salt become smaller, but after a while, it stops getting smaller? His answer was always the same: heterogeneous substances are in equilibrium precisely when the entropy of the total system is maximized under constraint.\nPaul Samuelson was a decidedly worldly economist in a 20th century America, where economists were expected to dispense advice to presidents and contribute to the public discourse. Indeed, he did both, serving as an advisor to presidents Kennedy and Johnson, and publishing a best-selling textbook in economics. Although unlike other worldly economists like Marx and Keynes, his economic achievements were highly mathematical.\nSamuelson wrote his landmark book, Foundations of Economic Analysis (1947), to answer one question: Why are some economic systems in equilibrium, while others are not? Why, when we drop a block of agents into a market, do they buy and sell things, but after a while, they stop buying and selling? His answer was always the same: a crowd of economic agents is in equilibrium precisely when some parameter of the total economic system (which can be interpreted as utility, profit, etc, depending on context) is maximized under constraint.\n\nFixed volume and temperature\nLet’s start with a simple example: the dimerization of nitrogen dioxide in a sealed tube.\nThe thermodynamic system is some \\(NO_2\\) and \\(N_2O_4\\). The system is sealed in a glass tube of constant volume \\(V\\), bathing in a water-ice mixture of temperature \\(T\\).\nThe thermodynamic state of the system is fully known if we know the number of moles for each species: \\(n_{NO_2}, n_{N_2O_4}\\).\nThe system undergoes a single reaction: \\(2 NO_2 \\rightleftharpoons N_2O_4\\).\nSuppose we start the system at state \\(n_{NO_2, 0}, n_{N_2O_4, 0}\\). When does the system reach equilibrium? Since the system can exchange energy, but not volume, with the surrounding bath, it reaches equilibrium when the system reaches minimal Helmholtz free energy under constraint: \\[\n\\begin{cases}\n\\min_{n_{NO_2}, n_{N_2 O_4}} F(T, V, n_{NO_2}, n_{N_2O_4})\\\\\n(n_{NO_2} - n_{NO_2, 0}) = -2 \\xi\\\\\n(n_{N_2O_4} - n_{NO_2, 0}) = \\xi\n\\end{cases}\n\\]\nwhere we write \\(\\xi\\) as the extent of reaction, that is, the number of moles of reactions that has taken place.\nDifferentiating the two equations, and setting \\(dV, d\\beta = 0\\), we have\n\\[\n\\begin{cases}\ndF = \\mu_{NO_2} dn_{NO_2} + \\mu_{N_2O_4} dn_{N_2O_4} \\\\\ndn_{NO_2} = -2d\\xi \\\\\ndn_{N_2O_4} = d\\xi\n\\end{cases}\n\\]\nAt equilibrium, \\(dF = 0\\) under all possible constrained variations, giving us the condition of equilibrium: \\[-2\\mu_{NO_2} + \\mu_{N_2 O_4} = 0\\]\nWe may vary both starting conditions \\(n_{NO_2, 0}\\) and \\(n_{N_2O_4, 0}\\), and for each starting condition, the system would equilibrate at the solution to\n\\[\n\\begin{cases}\n-2\\mu_{NO_2} (T, V, n_{NO_2}, n_{N_2O_4}) + \\mu_{N_2 O_4}(T, V, n_{NO_2}, n_{N_2O_4}) = 0 \\\\\n(n_{NO_2} - n_{NO_2, 0}) = -2 \\xi\\\\\n(n_{N_2O_4} - n_{NO_2, 0}) = \\xi\n\\end{cases}\n\\]\nwhich has exactly the same number of unknowns and equations, so in general it should have at least one solution.\nThe state space of the system has 2 dimensions: \\(n_{NO_2}\\) and \\(n_{N_2 O_4}\\). Starting at any point in the state space, the system can move on a single line, and it would equilibrate at exactly the point at which its Helmholtz energy is minimized. We can find the point of equilibrium by drawing the surfaces of constant Helmholtz free energy, and find the tangent point, as pictured.\n\n\n\nMinimizing Helmholtz free energy under the constraint of \\(2 NO_2 \\rightleftharpoons N_2O_4\\) is equivalent to maximizing utility under a budgetary constraint.\n\n\nEconomically speaking, the situation is precisely equivalent to the standard first problem in consumer theory: Given a consumer with a finite budget and a market for two goods, what would they buy from the market to maximize their utility? (They must spend all their budget.)\nThe answer, as we can see in the diagram, is the tangent point of the straight line of constant budget with the curved lines of constant utility.\nTo anthropomorphize the situation, we can say that the reaction chamber is a consumer trying to minimize its Helmholtz free energy under the “budgetary constraint” of \\(2 NO_2 \\rightleftharpoons N_2O_4\\). In this way, chemical equilibrium becomes a problem in consumer theory.\n\nExistence and uniqueness\nWe see from the diagram that at least one solution exists. In most situations, the Helmholtz free energy is strictly concave, so the curves of constant \\(F\\) are strictly convex, and so the solution is unique on each line.\nIf the budget line is tangent to a curve of constant \\(F\\), then at the equilibrium point, both \\(NO_2\\) and \\(N_2O_4\\) exist. Otherwise, only one exist, and we say that the reaction is irreversible. Economically speaking, it is like when you are poor enough, you might spend all your money buying noodles, and none buying bread.\n\n\n\nThe \\(\\xi, H\\) curve of a reaction chamber, that is a sealed glass tube held under constant temperature of 298.15 K. At the \\(\\xi = 0\\) side, the tube contains 2 moles of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{~mol}\\) side, the tube contains 1 mole of \\(N_2O_4\\). (Raff 2014a, fig. 2)\n\n\nIf \\(F\\) is not strictly concave, then it might have a double tangent point with the budget line. In that case, we have a first-order phase transition, and the substance splits into two chunks with different phases. Because the two parts can exchange volumes, it is no longer convenient to analyze with Helmholtz free energy, and we had better use Gibbs free energy. This is studied in the next section.\n\n\n\nMultireaction equilibrium\nNow let’s consider another example, where we have two simultaneous reactions. This is a simplified version of the NOx reactions, which is a source of air pollution.\nConsider a system with the following reactions:\n\\[\n\\begin{aligned}\n2NO + O_2 &\\rightleftharpoons 2NO_2\\\\\nO + NO_2 &\\rightleftharpoons N_2O_3\n\\end{aligned}\n\\]\nThe system is in a container with constant volume \\(V\\) and temperature \\(T\\). Let \\(n_i\\) represent the number of moles of species \\(i\\). The thermodynamic state of the system is fully described by the 4-component vector \\(\\vec n = (n_{O_2}, n_{NO}, n_{NO_2}, n_{N_2O_3})\\).\nTo make the algebra look cleaner, we rewrite them as follows:\n\\[\n\\begin{aligned}\n0 &\\rightleftharpoons -O_2 - 2 NO + 2NO_2 + 0 N_2 O_3\\\\\n0 &\\rightleftharpoons 0 O_2 -NO - NO_2 + N_2O_3\n\\end{aligned}\n\\]\nWe see that each reaction can be written as a single vector. The first has vector \\(\\vec n_1 = (-1, -2, 2, 0)\\), and the second has vector \\(\\vec n_2 = (0, -1, -1, 1)\\).\nEach reaction has an associated extent of reaction, denoted by \\(\\xi_1\\) and \\(\\xi_2\\) respectively. Changes in the number of moles for each species are related to the extents of reaction:\n\\[\n\\begin{aligned}\ndn_{NO} &= -2d\\xi_1 - d\\xi_2 \\\\\ndn_{O_2} &= -d\\xi_1 \\\\\ndn_{NO_2} &= 2d\\xi_1 - d\\xi_2 \\\\\ndn_{N_2O_3} &= d\\xi_2\n\\end{aligned}\n\\]\nAt equilibrium, the Helmholtz free energy \\(F(T, V, \\vec{n})\\) is minimized under the constraints imposed by the reactions. This leads to the following conditions:\n\\[\n\\begin{aligned}\n-2\\mu_{NO} - \\mu_{O_2} + 2\\mu_{NO_2} &= 0 \\\\\n-\\mu_{NO} - \\mu_{NO_2} + \\mu_{N_2O_3} &= 0\n\\end{aligned}\n\\]\nMore elegantly,\n\\[\\vec \\mu \\cdot \\vec n_j = 0, \\quad j = 1, 2\\]\nwhere \\(\\vec \\mu = (\\mu_{O_2}, \\mu_{NO}, \\mu_{NO_2}, \\mu_{N_2O_3})\\) is the vector of chemical potentials.\nStarting at any initial chemical composition of \\(\\vec n_0\\), the space of all possible chemical compositions reachable from \\(\\vec n_0\\) is a 2-dimensional subset. That is, it is the set of \\(\\vec n\\) satisfying\n\\[\n\\begin{cases}\n\\vec n &= \\vec n_0 + \\xi_1 \\vec n_1+ \\xi_2 \\vec n_2, \\\\\n\\vec n &\\geq 0\n\\end{cases}\n\\]\nGeometrically speaking, the subset is the intersection between a 2-dimensional plane and a 4-dimensional pyramid, so it generally looks like either a triangle or a quadrilateral. On this subset, the Helmholtz free energy function looks like a sequence of nested convex shells, and the point of tangency is the equilibrium point.\n\n\n\nThe lines are the 3-dimensional contour surfaces of constant Helmholtz free energy, intersected with the 2-dimensional feasible set. The point of tangency is the point of chemical equilibrium.\n\n\nInterpreted economically, this is a consumer that maximizes its utility under two simultaneous budgetary constraints (because the budget set is a 2-dimensional, not 1-dimensional, subset of \\(\\mathbb{R}^4\\) ). Perhaps the consumer is trading with a market that simultaneously uses two kinds of currencies – bimetallism?\nConverting this experience into math, we have the following theorem.\n\nTheorem 12 (existence and uniqueness of chemical equilibrium, at constant volume and temperature) Consider a sealed reaction chamber held in an energy bath, so that both the price of energy \\(\\beta\\), and the volume \\(V\\), of the system is fixed.\nThe system contains a homogenous mixture of chemical species \\(A_1, \\dots, A_k\\), which might undergo the following \\(r\\) possible chemical reactions:\n\\[\n0 \\rightleftharpoons \\sum_i a_{ij} A_i, \\quad j = 1, 2, \\dots, r\n\\]\nThe necessary condition for chemical equilibrium is\n\\[\n\\vec \\mu \\cdot \\vec n_j = 0, \\quad \\forall j = 1, 2, \\dots, r\n\\]\nwhere \\(\\vec n_j = (a_{1, j}, \\dots, a_{k, j})\\) is the vector representing the \\(j\\) -th chemical reaction.\nThe condition is also sufficient if the Helmholtz free energy is strictly concave.\nIf \\(F\\) is not strictly concave, then there could be multiple coexisting equilibrium, which gives us a first-order phase transition.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nExistence: \\(F\\) is continuous, so it has at least one minimum on every compact set.\nUniqueness: any local minimum of a strictly concave function is the unique global minimum.\n\n\n\n\n\nFixed pressure and temperature\nWe studied the case of a reaction chamber held under constant volume and temperature, which one can picture as a sealed glass tube in an ice-water bath. Now consider the case of a reaction chamber with constant temperature and pressure, for example when it is a flaccid plastic bag at the bottom of the ocean.\nEvery previous result can be direct translated to that case, by replacing “Helmholtz” with “Gibbs”.\nFor example, for the same reaction of \\(NO_2\\) dimerization, now put into a flabby plastic bag held under constant temperature of 298.15 K and constant pressure of 1 atm, produces the following \\(\\xi, G\\) curve. At the \\(\\xi = 0\\) side, the bag contains 2 moles of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{~mol}\\) side, the bag contains 1 mole of \\(N_2O_4\\).\n\n\n\nThe reaction chamber is a flabby plastic bag held under constant temperature of \\(298.15 \\mathrm{~K}\\) and constant pressure \\(1 \\mathrm{~atm}\\). It has the following \\(\\xi, G\\) curve. At the \\(\\xi = 0\\) side, the tube contains \\(2 \\mathrm{~mol}\\) of \\(NO_2\\), and at the \\(\\xi = 1 \\mathrm{~mol}\\) side, the tube contains \\(1 \\mathrm{~mol}\\) of \\(N_2O_4\\). (Raff 2014a, fig. 4)\n\n\n\n\nThe meaning of \\(\\Delta G\\)\nThis section based on (Quílez 2012).\n\\(\\Delta G\\) has two confusable meanings. The first meaning is \\(\\frac{dG}{d\\xi}\\), that is, the marginal Gibbs free energy for reaction, - how much the Gibbs free energy increases if the reaction goes forward by an infinitesimal mole. The second meaning is \\(\\int_0^1 (\\partial_\\xi G)_{T, P} d\\xi\\). Both meanings are illustrated in the diagram.\n\n\n\nThe first meaning of \\(\\Delta G\\) is the slope. The second meaning of \\(\\Delta G\\) is the difference in height of the curve on two ends of the \\((\\xi, G)\\) curve. (Glasser 2016, fig. 1)\n\n\nThe first meaning, that of \\(\\frac{dG}{d\\xi}\\), is used in chemical equilibrium. The textbooks say \\(\\Delta G = 0\\) when they really meant \\(\\frac{dG}{d\\xi} = 0\\).\nThe second meaning, that of \\(\\int_0^1 (\\partial_\\xi G)_{T, P} d\\xi\\), can be interpreted by the “van ’t Hoff box” (Bazhin and Parmon 2007). We have a chamber with several semi-permeable membranes. On the input side, some gasses are permeated into the chamber, while on the output side, some gases are permeated out of it. The inside of the chamber remains fixed in composition. In this case, \\(\\Delta G\\) is the maximal useful work extractable in the process per reaction-mole, or the thermal energy created if no useful work is done.\n\n\n\nThe reaction chamber, demonstrating the concrete meaning of \\(\\Delta G\\). (Glasser 2016, fig. 2)\n\n\nFor example, such a situation occurs approximately occurs in the Haber–Bosch method for producing ammonia on the industrial scale: \\(N_2 + 3H_2 \\to 2NH_3\\). In the HB method, room-temperature (25 \\(^\\circ C\\)) and room-pressure (1 atm) nitrogen and hydrogen continuously pipe into the chamber, and ammonia is continuously extracted out of the chamber by cooling liquefaction. When the reaction chamber is operating at a stable state, the energy released per mole of reaction is \\(\\Delta G = -32.8 \\mathrm{~kJ/mol}\\), as one can calculate from a table of chemical thermodynamics. With this setup, after \\(1\\mathrm{~mol}\\) of nitrogen is consumed and \\(2\\mathrm{~mol}\\) of ammonium is produced, a thermal energy of \\(32.8\\mathrm{~kJ}\\) is produced, and must be cooled off somehow (Glasser 2016).\nNote that throughout this process, \\(\\frac{dG}{d\\xi} = 0\\) always within the box, even as \\(\\Delta G = -32.8 \\mathrm{~kJ/mol}\\) across the box. If \\(\\frac{dG}{d\\xi} \\neq 0\\), the reaction box would shake and shudder as it rushes towards equilibrium, and probably crack the walls. If \\(\\Delta G \\not\\lt 0\\), then all those giant cooling towers would have been pointless. Both \\(\\frac{dG}{d\\xi} = 0\\) and \\(\\Delta G = -32.8 \\mathrm{~kJ/mol}\\) are true, and the physical reality of an ammonia factory is the living smoking proof.\n\n\nPractical considerations\nThe above is all correct, and geometrical. If we were to be like Gibbs, then we would dust off our hands, for there is nothing left to do (except the theory of phase transitions). Unfortunately, chemistry is not merely applied geometry, so there is still something left to do.\n\nDefining the standard states\nA chemical environment is defined by chemical species \\(A_1, \\dots, A_m\\).\nA standard state for a chemical environment is defined by a reference pressure \\(P^\\circ\\), and reference chemical molarities \\([A_1]^\\circ, \\dots, [A_m]^\\circ\\) for each of the the chemical species.\nGiven a standard state for a chemical environment, for any temperature \\(T\\), and any chemical molarities \\([A_1], \\dots, [A_m]\\), the chemical activity of the chemical species \\(A_i\\) in this particular context is\n\\[\\{A_i\\} := e^{\\frac{\\mu_i - \\mu_i^\\circ}{RT}}\\]\nwhere \\(\\mu_i\\) is the chemical potential of species \\(A_i\\) at that state. That is,\n\\[\\mu_i = (\\partial_{n_i} G)|_{T, P, [A_1], \\dots, [A_m]}\\]\nand \\(\\mu_i^\\circ\\) is the chemical potential of species \\(A_i\\) at the standard state:\n\\[\\mu_i^\\circ = (\\partial_{n_i} G)|_{T, P^\\circ, [A_1]^\\circ, \\dots, [A_m]^\\circ}\\]\nGiven a chemical reaction \\(0 \\rightleftharpoons \\sum_i a_i A_i\\), its reaction quotient is\n\\[\nQ = \\prod_i \\{A_i\\}^{a_i}\n\\]\nwhere \\(a_i\\) is the stoichiometric number of chemical species \\(A_i\\). For example, with \\(aA + bB \\rightleftharpoons cC + dD\\), its reaction quotient is \\[\nQ = \\frac{\\{C\\}^c\\{D\\}^d}{\\{A\\}^a\\{B\\}^b}\n\\]\n\n\nFundamental theorem of chemical equilibrium\n\nTheorem 13 (fundamental theorem of chemical equilibrium) Given any chemical reaction \\(0 \\rightleftharpoons \\sum_i a_i A_i\\), any standard state, and any temperature, \\[\\begin{cases}\n(\\partial_\\xi F)_{T, V} &= (\\partial_\\xi F)_{T, V}^\\circ + RT \\ln Q \\\\\n(\\partial_\\xi G)_{T, P} &= (\\partial_\\xi G)_{T, P}^\\circ + RT \\ln Q\n\\end{cases}\n\\]\nwhere \\(\\xi\\) is the extent of reaction, and \\(Q\\) is its reaction quotient.\nIf the system has \\(r\\) possible reactions, then we similarly have \\[\\begin{cases}\n(\\partial_{\\xi_j} F)_{T, V} &= (\\partial_{\\xi_j} F)_{T, V}^\\circ + RT \\ln Q_j \\\\\n(\\partial_{\\xi_j} G)_{T, P} &= (\\partial_{\\xi_j} G)_{T, P}^\\circ + RT \\ln Q_j\n\\end{cases}\n\\]\nfor each reaction \\(j = 1, 2, \\dots, r\\)\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\\[\\begin{aligned}\n(\\partial_\\xi F)_{T, V} &= \\sum_i (\\partial_\\xi n_i) (\\partial_{n_i} F)_{T, V, \\vec n} \\\\\n&= \\sum_i a_i \\mu_i \\\\\n&= \\sum_i a_i (\\mu_i^\\circ + RT \\ln \\{A_i\\}) \\\\\n&= \\sum_i a_i (\\mu_i^\\circ) + RT \\ln \\left(\\prod_i \\{A_i\\}^a_i\\right) \\\\\n&= (\\partial_\\xi F)_{T, V}^\\circ + RT \\ln Q\n\\end{aligned}\\]\nThe proof for the other equations are very similar.\n\n\n\n\nCorollary 2 (equilibrium coefficient) At equilibrium, \\[\n(\\partial_\\xi G)_{T, P} = 0\n\\]\nwhich is equivalent to \\[\nQ = K_{eq}, \\quad K_{eq} := e^{-\\frac{(\\partial_\\xi G)_{T, P}^\\circ}{RT}}\n\\]\nand similarly for the other case.\n\nThe above equations are what my teachers meant when they thoughtlessly wrote\n\\[\\Delta G = 0, \\quad K_{eq} = e^{-\\frac{\\Delta G^\\circ}{RT}}\\]\nThis, finally, answers my great confusion back then. Now everything makes sense, and life is beautiful.\n\n\nIdeal-gas-like chemistry\nWell, if this is all there is, then a mathematician would be able to solve any problem in analytical chemistry. Unfortunately, analytical chemistry is not about proving theorems, but about actually getting numerical answers, and numerical answers require numerical values for chemical activities.\nThere are generally three cases:\n\nWe have a mixture of dilute gasses, or dilute solvents in an inert solution, such that the ideal gas law is almost true.\nIdeal gas law fails.\nWe are not even dealing with gasses and solutions anymore.\n\nThe first case is typically what is taught by a first course in analytical chemistry, and since this is typically taught to non-mathematicians by non-mathematicians for non-mathematicians, the logical structure is quite upside-down and confusing to a mathematician.\nWe will now prove the first case rigorously.\n\nTheorem 14 (activity of ideal gas mixtures) For any temperature \\(T\\) and any two pressures \\(P, P^\\circ\\), by the ideal gas laws, the chemical potential of the chemical species satisfies the equation\n\\[\\mu(T, P) = \\mu(T, P^\\circ) + RT \\ln\\frac{P}{P^\\circ}\\]\nand so its activity is \\(\\frac{P}{P^\\circ}\\).\nIn a mixture of ideal gases, the gases do not interact, and so the activity of chemical species \\(A_i\\) is \\(\\{A_i\\} = \\frac{P_i}{P_i^\\circ}\\), where \\(P_i\\) is the partial pressure of species \\(A_i\\) in the mixed gas, and \\(P_i^\\circ\\) is the standard pressure for species \\(A_i\\).\nIn a dilute solution, if the solvent behaves like a mixture of ideal gasses, then\n\\[\\mu(T, P) \\approx \\mu(T, P^\\circ) + RT \\ln \\frac{[A_i]}{[A_i]^\\circ} \\approx \\mu(T, P^\\circ) + RT \\ln \\frac{m_{A_i}}{m_{A_i}^\\circ}\\]\nwhere \\([A_i]\\) is the mole-per-volume of \\(A_i\\), and \\(m_{A_i}\\) is the mole-per-mass of \\(A_i\\).\nThe chemical activity simplifies into the familiar form:\n\\[\\{A\\} \\approx \\frac{[A]}{[A]^\\circ} \\approx \\frac{m_A}{m_A^\\circ}\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove the case for a pure ideal gas, as the other cases are simple corollaries.\nBy the ideal gas law, the chemical potential is\n\\[\\mu = -TS/n\\]\nwhich is a state property. Expressed as a function of \\(T, P\\), \\[\\mu(T, P) = RT \\ln\\frac{P/T^{\\hat c_V + 1}}{C}\\]\nfor an arbitrary constant \\(C\\).\nThus, for any \\(T, P, P^\\circ\\), we have\n\\[\\mu(T, P) = \\mu(T, P^\\circ) + RT \\ln\\frac{P}{P^\\circ}\\]\n\n\n\n\nExample 3 The pH value of a solution is not \\(pH = -\\log_{10} [H^+]\\), which has the wrong units. It is not even \\(pH = -\\log_{10} \\frac{[H^+]}{[H^+]^\\circ}\\), since the \\(H^+\\) particles might not behave like an ideal gas. The actual correct definition is (McCarty and Vitz 2006)\n\\[pH = -\\log_{10} \\{H^+\\}\\]\n\n\n\nFugacity\nFor real gases and real solutions, the chemical activity might deviate significantly from the above approximation. In this case, we typically have no recourse except to checking a table of chemical thermodynamics. They typically do not directly write down the chemical activities, but fugacity coefficients. There is nothing particularly deep about fugacity – it is basically about rescaling the numbers to make the tables easier to make.\nRecall that the chemical potential of an ideal gas satisfies \\(\\mu(T, P) = RT \\ln\\frac{P/T^{\\hat c_V + 1}}{C}\\), where \\(C\\) is a constant for this gas. For a real gas, this equation only holds approximately, so we define the fugacity \\(f\\) as a function of \\(T, P\\), such that \\[\\mu(T, P) = RT \\ln\\frac{f/T^{\\hat c_V + 1}}{C}\\]\nIn other words, \\(f(T, P) = P \\phi(T, P)\\), where\n\\[\\phi(T, P) = e^{\\frac{\\mu(T, P) - \\mu_{ideal}(T, P)}{RT}}\\]\nis the fugacity coefficient.\nPlugging them back to the definition of activity, we have\n\\[\\{A\\} = \\frac{f}{f^\\circ} = \\frac{\\phi P}{\\phi^\\circ P^\\circ}\\]\nAnd so, by checking a table of fugacity coefficients, chemical engineers can balance chemical reactions of real gasses, even far from ideality.\n\n\n\n\n\n\nStandard state\n\n\n\nDespite what the name “standard” might imply, a chemical species has infinitely many standard states. For example, pure gaseous oxygen has many different standard states – one for each temperature. We have a standard state at \\(T = 300\\mathrm{~K}\\) defined by \\([O_2] = 1 \\mathrm{~mol/L}\\), and another at \\(T = 350\\mathrm{~K}\\) defined by \\([O_2] = 1 \\mathrm{~mol/L}\\), etc.\nDespite what the name “standard” might imply, different chemists have different standards. For example, among the biochemists, the standard state for \\(H^+\\) in water is \\([H^+]^\\circ = 10^{-7} \\mathrm{~mol/L}\\), but among the inorganic chemists, it is \\([H^+]^\\circ = 1 \\mathrm{~mol/L}\\). The reason is that bodily fluids typically have \\([H^+] \\sim 10^{-7} \\mathrm{~mol/L}\\).\nDespite what the name “standard temperature and pressure (STP)” might imply, it is not a “standard state”, because a “standard state” of any substance does not specify its temperature.\n\n\nThe point of having a standard state is like taking an electric circuit, pointing at one point of it, and say, “This is where the voltage is zero.”. The point is to allow for relative comparisons between states, within the context of a single reaction. Consequently, even for a single chemical species, we can take a different standard state if we are studying a different reaction involving the species, or the same reaction in a different context.\nFor example, if we are studying the reaction \\(NO_2 \\rightleftharpoons N_2 O_4\\) in a glass tube drenched in an ice-water bath, then we would take as our standard state \\[T^\\circ = 273.15 K, \\quad [NO_2]^\\circ = 1 \\mathrm{~mol/L}, \\quad [N_2 O_4]^\\circ = 1 \\mathrm{~mol/L}\\]\nFor a chemical in pure gaseous form, a standard state is specified by two out of three parameters: molarity \\([A] = \\frac{n}{V}\\), pressure \\(P\\), temperature \\(T\\). We must never specify all three of them, because otherwise we would break the equation of state. For example, imagine what happens when you specify that the “standard state of ideal gas” is\n\\[T^\\circ = 273.15 \\mathrm{~K}, P^\\circ = 10^5 \\mathrm{~Pa}, [A]^\\circ = 1 \\mathrm{~mol/L}\\]\nbecause they would violate the ideal gas law \\[P = [A]RT\\]\nFor non-ideal gas, we still have an equation of state between \\(P, [A], T\\), meaning that we still must specify exactly two, no more and no less.\n\n\n\n\n\n\nIntensive quantities\n\n\n\nWhy is a standard state defined by intensive quantities like temperature, pressure, or molarity? Why isn’t it defined by extensive quantities such as volume, mass, and moles?\nThe short answer: because traditional chemistry only studies systems with extensive entropies. For those systems, chemical equilibrium is determined by intensive quantities. This is not because classical thermodynamics cannot handle nonextensive entropy, but because chemists had no use for systems with nonextensive entropy.\nLike classical thermodynamics and neoclassical economics, the idea of a standard state is fully committed to the idea of homogeneous substances. In classical thermodynamics, a cube of iron and a ball of iron are the same. A jar of water and a tank of water are the same. It does not matter what their shapes are. Moreover, two jars of water side-by-side is the same as one large jar of water. In neoclassical economics, a crowd of factories is the same as two small crowds of factories put together. They are all chunks of homogeneous stuffs.\nIf this were not the case, then we would be unable to say that a standard state is defined by just the temperature and molar concentration of each chemical species. We would be forced to also specify a standard state volume \\(V^\\circ\\). It is conceivable that even the shape of the reaction chamber matters. We would then be forced to specify a standard shape, perhaps a box with side lengths \\(0.1 \\mathrm{~m}\\). But in this extreme case, perhaps we have already left the realm of chemistry.\nFor spherical particles, doubling the volume would double the mass, but only \\(2^{2/3} \\approx 1.59 \\times\\) the surface area. Consequently, if the surface between phases has a non-negligible entropy (“surface effect”), then entropy would be nonextensive. While IUAPC is silent on the issue, nonextensive entropy is taken up in earnest by chemists who work with small spherical particles (Letellier, Mayaffre, and Turmine 2007).\n\n\n\n\nIUAPC’s definition of “standard state”\nI have found that the IUAPC’s definition of the “standard state” (Cox 1982) to be precise and clarifying, though it is quite ponderous, so I summarize it as follows:\n\nThe standard state for a gaseous substance, whether pure or mixed, is the substance at \\(P^\\circ\\) and in a (hypothetical) state in which it exhibits ideal-gas behaviour, where \\(P^\\circ\\) is an arbitrarily fixed standard-state pressure.\nThe standard state for a pure liquid or solid substance is the pure substance at \\(P^\\circ\\).\nThe above definitions of standard states make no reference to fixed temperature. Hence, it is possible to have an infinite number of standard states of a substance as the temperature varies. But generally it is more convenient to complete the definition of the standard state in a particular context by choosing for the reference temperature one of a relatively small number of values, e.g., zero, \\(273.15 \\mathrm{~K}, 293.15 \\mathrm{~K}, 298.15 \\mathrm{~K}\\).\nSince \\(T^{\\circ}\\) should mean a standard temperature in general, the use of \\(T^{\\circ}\\) to mean exclusively \\(298.15 \\mathrm{~K}\\) is strongly discouraged.\nFor application of the concept of standard state to substances in admixture (solutions and mixtures), the composition of the system, as well as the pressure, must be defined. As one example for solutions, the standard-state molality, written as \\(m^\\circ\\) for the general case, is to be defined; customarily \\(m^\\circ\\) is taken as \\(1 \\mathrm{~mol/kg}\\)."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#sec-phase-equilibrium",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#sec-phase-equilibrium",
    "title": "Classical Thermodynamics and Economics",
    "section": "Phase equilibrium",
    "text": "Phase equilibrium\nSeveral times, we have found some curious examples where a non-concavity in entropy leads to a jump of some kind. These are all examples of first-order phase equilibrium.\n\nTwo phases of a gas in equilibrium\nConsider a generic gas, whose entropy function is of form \\(S(U, V, N)\\). If we confine it in a sealed tube, and slowly heat it up, then its entropy would trace out the curve\n\\[U \\mapsto S(U, V, N)\\]\nNow, the inverse temperature \\(\\beta\\) of the system is the slope, which should decrease as \\(U\\) increases, so the entropy curve should be strictly concave.\nIf there is a bump in the middle, then we have a serious problem: as we heat up the gas, its temperature would decrease for a while before increasing again! This suggests to us that our model has broken down. Where is the breakdown? The breakdown is that we assumed our system remains one thermodynamic substance, when it can split into two. Suppose by a small fluctuation, the left side of the container has higher temperature than the right side, then it would give some internal energy to the right side. Normally, this would cause their temperatures to meet in the middle. However, in this inverted situation, the left side would become even hotter, and so we have a positive feedback loop, until the substance has split into two, with the same temperature, but one with higher internal energy density, and one with lower.\n\n\n\nA bump in the \\(U \\mapsto S\\) curve would lead to a thermodynamic instability, ending with a first-order phase transition.\n\n\nSuppose now that the substance splits into two, like a large company splits into two subsidiaries under a common conglomerate. How would the manager maximize the total value of the conglomerate? It would solve the following constrained optimization:\n\\[\n\\begin{cases}\n\\max S_1(U_1, V_1, N_1) + S_2(U_2, V_2, N_2) \\\\\nU_1 + U_2 = U \\\\\nV_1 + V_2 = V \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nwhere we, instead of writing \\(S(U_1, V_1, N_1) + S(U_2, V_2, N_2)\\), write \\(S_{{\\color{red} 1}}(U_1, V_1, N_1) + S_{{\\color{red} 2}}(U_2, V_2, N_2)\\), to emphasize that we now have two thermodynamic systems that might have very different behavior, like water vs ice.\nDifferentiating, we find that the marginal values of each asset are equal in both subsidiaries:\n\\[\n\\begin{cases}\n\\beta_1 = \\beta_2,\\\\ \\beta_1 P_1 = \\beta_2 P_2, \\\\-\\beta_1 \\mu_1 = -\\beta_2 \\mu_2\n\\end{cases}\n\\]\nThat is, the two lumps of substances have the same temperature, pressure, and chemical potential.\nSince both sides have the same temperature and pressure, it is cleaner to change to Gibbs free energy, yielding:\n\\[(\\partial_{N} G_1)_{T, P}(T, P, N_1) = (\\partial_{N} G_2)_{T, P} (T, P, N_2)\\]\n\n\n\n\n\n\nThe diamond water paradox, and thinking on the margins\n\n\n\nTypical textbooks on thermodynamics illustrate the phase equilibrium rule using the van der Waals equation. However, there is a subtlety involved. For the van der Waals gas, the Gibbs free energy \\(G\\) is proportional to particle number:\n\\[G(T, P, N) \\propto N\\]\nwhich means that \\((\\partial_{N} G)_{T, P}(T, P, N) = G(T, P, N) / N\\). In economic language, this states that:\n\\[\\text{marginal Gibbs per particle} = \\text{average Gibbs per particle}\\]\nIn fact, confusing the two numbers is the root of the diamond-water paradox. This paradox questions why water, essential for life, has a low price, while diamonds, with little practical use, have a high price. The resolution lies in understanding the difference between total and marginal utility. While the total utility of water is immense, the marginal utility of an additional unit of water is low due to its abundance. Conversely, the marginal utility of a diamond remains high due to its scarcity.\nIn neoclassical economics, it is the marginal value of a commodity that determines the market equilibrium, not its average value. Similarly, in thermodynamics, it is the change in Gibbs free energy when adding one more particle that determines the equilibrium state, not the average Gibbs free energy per particle.\nThe distinction is moot in typical books on classical thermodynamics, which insists that entropy is extensive, so the above equation is always true. However, classical thermodynamics, much like neoclassical economics, is perfectly capable of handling nonextensive entropy. Lord Kelvin had studied nonextensive entropy (Lavenda 2010), and Gibbs had explained surface tension and electrocapillary effects with nonextensive entropy (Jaynes 1992).\n\n\nSince in most classical thermodynamics systems, such as water and steam, the marginal free Gibbs energy is identical with the average Gibbs free energy, we have \\((\\partial_{N} G)_{T, P}(T, P, N)= G(T, P, N)/N\\), meaning that phase equilibrium occurs at \\(g_1(T, P) = g_2(T, P)\\).\nWe can reinterpret this as follows: We delicately separate the two lumps of matter, and immerse each half in an energy-and-volume bath (like the atmosphere) with the same temperature and pressure. The only interaction between the two lumps of matter is that one side can “seep” some particles to the other side. In this set-up, the system minimizes the sum of Gibbs free energy. At equilibrium, there is no point in moving particles from one side to another, because the marginal Gibbs free energy per particle is the same.\nGenerally, \\(g_1(T, P) \\neq g_2(T, P)\\). When \\(g_1 &lt; g_2\\), every particle would switch to phase 1. When \\(g_1 &gt; g_2\\), every particle would switch to phase 2. At exactly a knife’s edge, the particles are indifferent as to which phase they would go to.\n\n\n\n\n\n\nInterpretation: corporate buyout in an ideal world\n\n\n\nWe have two companies such that they can exchange their human-particles, and that there is neither economies nor diseconomies of scale (that is, as the company grows ever larger, an extra worker neither provides more nor less value than its very first worker). Then, in general, the two companies balance on a knife’s edge. If the value of a worker is even slightly greater in one company than another, then that company would immediately buy out every worker from the other company, and so the two companies cannot possibly coexist. Only when the market prices for space and energy happen to conspire just right, can the two companies coexist, neither side buying out the other side.\n\n\n\nExample: van der Waals gas\nWe know what the van der Waals gas phase diagram looks like. How do we infer its Gibbs free energy diagram? Start with \\(dG = -SdT + VdP + \\mu dN\\). Now, let us fix temperature \\(T\\) and particle number \\(N\\). Then, the equation implies to \\[\\frac{dg}{dP} = v\\]\nwhere \\(g = G/N\\) is the average Gibbs free energy, and \\(v = V/N\\) is the average volume.\nTherefore, we can trace the pressure-volume diagram with our finger, from high pressure, down to the valley of pressure, then bounce back to a hill, before rolling down the slope towards infinity. At every point, the \\(g(P)\\) curve would have a slope of \\(v\\). This allows us to graphically construct the following \\(g(P)\\) curve. It has two cusps corresponding to the valley and hilltop, and a self-intersection, corresponding to the phase equilibrium of \\(g_1 = g_2\\).\n\n\n\nGibbs free energy of van der Waals gas. Figure source.\n\n\n\n\n\nGibbs phase rule\n\nDegrees of thermodynamic freedom\nConsider a chunk of gas (ideal or not). We know everything there is to know about it if we know its \\((U, V, N)\\). Every other thermodynamic quantity can be computed by its entropy function \\(S(U, V, N)\\). Thus, we have a system with three degrees of thermodynamic freedom… or do we?\nThe problem is that the entropy of gas, and just about every other system studied in classical thermodynamics, is extensive. Therefore, we have\n\\[S(U, V, N) \\propto N\\]\nand so we don’t actually have three degrees of freedom!\nSpecifically, we can calculate its \\((\\partial_U S, \\partial_V S, \\partial_N S)\\), which gives us \\(\\beta, \\beta P, -\\beta \\mu\\). If we truly have three degrees of freedom, then we should be able to vary \\(\\beta, P, \\mu\\) independently. However, because entropy is extensive, we have\n\\[S(U, V, N) = Ns(u, v) \\implies (\\beta, \\beta P, -\\beta \\mu) = (\\partial_u s, \\partial_v s, s)\\]\nwhere \\(s(u, v) = S(U, V, N)/N\\) is the entropy per particle.\nTherefore, we can say that there are only two degrees of thermodynamic freedom: knowing two of its intensive quantities, the third would be determined by an equation of state.\nSimilarly, if we have a chunk of (nonideal) substance, like sea water, made of \\(k\\) different chemicals, then we know everything there is to know about it if we know its \\(U, V, N_1, \\dots, N_k\\), giving us \\(2+k\\) degrees of freedom. Again, because entropy is extensive, one degree of freedom is degenerate, and so we only have \\(1 + k\\) degrees of freedom. In other words, its \\(2+k\\) intensive quantities\n\\[\\beta, \\beta P, -\\beta\\mu_1, \\dots, -\\beta \\mu_k\\]\nare related by 1 equation of state.\n\n\nGibbs phase rule\n\nTheorem 15 (Gibbs phase rule) \\[F = 2 + C - R - P\\]\n\nFirst, we need to set up the thermodynamic system. We have a closed and adiathermal reaction chamber, containing \\(C\\) different chemical species, that can undergo \\(R\\) linearly independent chemical reactions.11\n11 The formula looks like the Euler formula for polyhedra, but whether this analogy is more than a coincidence is controversial. After looking into the literature for a bit, my conclusion is that it is a coincidence. However, if you wish to investigate on your own, the phrase to search is “Gibbs phase rule, Euler”. This turns up some amusing examples, like (Sun, Powell-Palm, and Chen 2024).When the system is in an equilibrium, the chamber would contain \\(P\\) different phases. Each phase would be homogeneous, but different from the other phases. All phases can exchange energy, volume, and particles.\n\n\n\nPhases in equilibrium inside a chamber. (Blankschtein 2020, fig. 27.2)\n\n\n\n\n\n\n\n\nProof: Case of \\(R = 0\\)\n\n\n\n\n\nIf there can be no chemical reaction, then the constrained optimization problem states\n\\[\n\\begin{cases}\n\\max (S_1(U_1, V_1, \\vec N) + \\cdots + S_P(U_P, V_P, \\vec N_P)) \\\\\n\\sum_i U_i = U \\\\\n\\sum_i V_i = V \\\\\n\\sum_i \\vec N_i = \\vec N\n\\end{cases}\n\\]\nNaively, we can just differentiate the entropies against each of the \\(2+C\\) parameters, to obtain equations\n\\[\n\\begin{aligned}\nT_1 = \\dots &= T_P \\\\\nP_1 = \\dots &= P_P \\\\\n\\mu_{1, 1} = \\dots &= \\mu_{1, P}\\\\\n& \\vdots \\\\\n\\mu_{C, 1} = \\dots &= \\mu_{C, P}\n\\end{aligned}\n\\]\nThis is not actually correct. Phase 1 might contain no chemical 2, and phase 2 might contain no chemical 1, 3, etc. In general, if phase \\(i\\) contains chemical \\(j\\), then we must have \\(\\partial_{N_j}S_i = \\mu_j\\). However, if phase \\(i\\) contains no chemical \\(j\\), then we only need to have \\(\\partial_{N_j}S_i &gt; \\mu_j\\).\nNote that this is different for temperature or pressure. A phase \\(i\\) might have no chemical of type \\(j\\), but if it have no volume, then it does not exist at all. Similarly for energy. Therefore, though the chemical potentials might differ, the temperature and pressure must be exactly the same.\nDefine \\(\\mu_j := \\min_i \\mu_{i, j}\\) to be the minimal chemical potential over the entire chamber. We have the following conditions:\n\\[\n\\begin{aligned}\nT_1 = \\dots = T_P &= T \\\\\nP_1 = \\dots = P_P &= P \\\\\n\\mu_{1, 1}, \\dots, \\mu_{1, P} &\\geq \\mu_1\\\\\n& \\vdots \\\\\n\\mu_{C, 1}, \\dots, \\mu_{C, P} &\\geq \\mu_C\\\\\n\\end{aligned}\n\\]\nGiven \\(T, P, \\mu_1, \\dots, \\mu_C\\), phase 1 is entirely determined: If it contains chemical \\(j\\), then \\(\\mu_{1, j} = \\mu_j\\), otherwise, we need not bother with \\(\\mu_{1, j}\\). Similarly, every phase is determined.\nFinally, each phase contributes an equation of state, which are in general linearly independent, giving us \\(F = 2 + C - P\\) degrees of freedom.\n\n\n\n\n\n\n\n\n\nProof: Case of \\(R \\geq 1\\)\n\n\n\n\n\nIf we now allow a chemical reaction \\(0 \\rightleftharpoons \\sum_j a_j A_j\\), then the constrained optimization problem becomes\n\\[\n\\begin{cases}\n\\max (S_1(U_1, V_1, \\vec N) + \\cdots + S_P(U_P, V_P, \\vec N_P)) \\\\\n\\sum_i U_i = U \\\\\n\\sum_i V_i = V \\\\\n\\sum_i \\vec N_i = \\vec N + \\xi \\vec a\n\\end{cases}\n\\]\nThe extra optimization variable \\(\\xi\\) creates an extra condition for optimality:\n\\[\\sum_j a_j \\mu_j = 0\\]\nso \\(F = 2 + C - P - 1\\).\nPossibly, the system cannot satisfy \\(\\sum_j a_j \\mu_j = 0\\), and so the chemical reaction would keep happening until one chemical is exhausted. This would decrement \\(C\\) by one, so it all works out self-consistently.\nMore generally, if we impose \\(R\\) linearly independent chemical reactions, then \\(F = 2 + C - P - R\\).\n\n\n\n\n\n\n\n\n\nBeyond the Gibbs phase rule\n\n\n\nWhen the phases are not free to exchange particles, energies, volumes, etc, then the Gibbs phase rule does not apply, but the same idea of constrained minimization still applies. There are no generic rule like the Gibbs phase rule, and one must analyze each case specifically. (Blankschtein 2020)\n\n\n\nSome basic examples.\n\n\n\n\n\n\n\n\n\nsituation\ncomponents \\(C\\)\nphases in equilibrium \\(P\\)\nreactions \\(R\\)\ndegrees of freedom \\(F = 2 + C - P - R\\)\n\n\n\n\nice\n1\n1\n0\n2\n\n\nboiling water\n1\n2\n0\n1\n\n\ntriple point\n1\n3\n0\n0\n\n\nliquid water with a little nitrogen inside, gaseous nitrogen with a little water vapor inside\n2\n2\n0\n2\n\n\ndimerization of nitrogen dioxide gas\n2\n1\n1\n2\n\n\nNb-Ta-C alloy\n3\n1\n0\n4\n\n\n\nIn materials science, such as metallurgy, we often fix the pressure of the entire thing to just 1 atm, and so the phase diagrams have one less degree of freedom than what the Gibbs phase rule states.\n\n\n\n3-dimensional phase diagram for Nb-Ta-C alloy at constant pressure \\(P = 1 \\mathrm{~atm}\\). (West and Saunders 2002, fig. 8.1)\n\n\n\n\n\nBoiling water\nWhen boiling water in an open pot, we need to specify exactly both temperature and pressure so that both phases can coexist. The \\((P, T)\\) of the system would start at \\((1\\;\\mathrm{~atm}, 372\\;\\mathrm{~K})\\), then at exactly at the critical point \\((1\\;\\mathrm{~atm}, 373.15\\;\\mathrm{~K})\\) would both phases coexist, not increasing in temperature until all water has turned to steam. However, if we seal it in a tube, then the \\((P, T)\\) of the system would hug the line of water-steam coexistence, like a negative-feedbacked system following a predetermined path. How can we see this difference mathematically?\nIn the case of an open pot, the constrained optimization problem is\n\\[\n\\begin{cases}\n\\min (g_1(T, P)N_1 + g_2(T, P)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nWe see that the problem is very rigid: We have to minimize a linear function subject to a linear constraint. As always in linear programming, the solution is in general on an extreme vertex on the very edge of the feasible set – all or nothing, all liquid or all gas. Only by carefully tuning \\(T, P\\) can we find an interior solution – a solution that falls between the vertices, neither all liquid nor all gas.\n\n\n\nAs we increase \\(T\\), the contours of constant Gibbs free energy are parallel lines rotating around. Only when the lines are precisely parallel to the \\(N_1 + N_2 = N\\) is it possible for both phases to coexist.\n\n\nIn the case of a sealed tube, assuming that Helmholtz free energy is proportional to particle number, then the equilibrium is reached at\n\\[\n\\begin{cases}\n\\min (f_1(T, v_1)N_1 + f_2(T, v_2)N_2) \\\\\nv_1 N_1 + v_2 N_2 = V \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nwhere \\(N\\) is the total number of particles, and \\(v_i\\) are the volume-per-particle of liquid and gaseous water. In this case, we are performing a minimization in \\(\\mathbb{R}^4\\), with 1 linear constraint \\(N_1 + N_2 = N\\), and 1 nonlinear constraint \\(v_1 N_1 + v_2 N_2 = V\\). Furthermore, the objective is also nonlinear. The result is that the solution does not in general fall on a vertex – that is, in general, both \\(N_1, N_2 &gt; 0\\). And this is why when we boil water in a sealed tube, it remains boiling over a wide range of temperatures, but when we boil water in an open tube, it only boils at a single temperature.\n\n\nNonextensivity breaks the Gibbs phase rule\nSuppose that the entropy is nonextensive, then the Gibbs free energy is also nonextensive. In particular, we can no longer write\n\\[\n\\begin{cases}\n\\min (g_1(T, P)N_1 + g_2(T, P)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nbut we have to write\n\\[\n\\begin{cases}\n\\min (g_1(T, P, N_1)N_1 + g_2(T, P, N_2)N_2) \\\\\nN_1 + N_2 = N\n\\end{cases}\n\\]\nbecause \\(G_2(T, P, N_2)\\) is no longer proportional to just \\(N_2\\). Fundamentally, this happens because\n\\[S(\\text{two chunks of steam merged}) \\neq 2 S(\\text{one chunk of steam})\\]\n\n\n\nAs we increase \\(T\\), the contours of constant Gibbs free energy are curved lines rotating around. Now it is possible for for both phases to coexist over an entire 2D region of \\((P, T)\\).\n\n\nThe effect is that we have a nonlinear optimization problem, allowing interior solutions over a larger region of \\((T, P)\\) parameters. This explains our previous comment on nonextensive Gibbs free energy. In this case, the Gibbs phase rule breaks completely."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#sec-stereodynamics",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#sec-stereodynamics",
    "title": "Classical Thermodynamics and Economics",
    "section": "Bonus: Stereodynamics",
    "text": "Bonus: Stereodynamics\nBased on Liu Cixin’s sci-fi story Mountain (2006). For another take on stereodynamics, see Ted Chiang’s Exhalation (2008), printed in (Chiang 2019).\n\nDie Raum der Welt ist konstant. Die Entropie der Welt strebt einem Maximum zu.\nRudolf Klausius, (Klausius 32850)\n\n\nSolid Universe Theory\nOur world was a spherical space completely surrounded by solid rock. There is no air or liquid inside. Indeed, we have not encountered any air or liquid until the last days of the Age of Exploration.\nThe first physical law we understood, in the prehistoric past, was the conservation of space. Space in the Bubble World was a sphere roughly 6000 km in diameter. Digging tunnels into the layers of rock did nothing to increase the amount of available space; it merely changed the shape and location of the already existing space. Because of this, space was the most treasured commodity of the Bubble World. The entire history of our civilization was one long and bloody struggle for space.\nWe are a mechanical life form. Our muscles and bones are made of minerals and alloys; our brains are electronic chips, and electricity and magnetism are our blood. We ate the radioactive rocks of our world’s core and they provided us with the energy we needed to survive. In our world, life evolved from single-celled electromechanical life, when the radioactive energies formed P-N junctions in the rocks.\nOn the rock walls there are radioactive spots, which irradiates luminescent rocks, creating spots of light like stars in a rocky night sky. These are the only natural sources of light in our world, and allowed us to evolve eyes. There is no gravity inside the bubble. Without gravity, we built our cities floating in space. From afar, they looked like dimly glowing red clouds.\nWe assumed that the universe was made of two parts. The first was the empty space in which we lived; the second was the surrounding layers of rock. We believed the rock to stretch endlessly in all directions. Therefore, we saw our world as a hollow bubble in this sold universe and so we gave our world the name “Bubble World”. We call this cosmology the Solid Universe Theory.\n\n\nFrom the Closed World to the Infinite Universe\nThe search for other bubbles began in earliest antiquity. We had spun many alluring myths around these distant spaces and almost all of our literature dealt with the fantasy of other bubbles. We explored the rock in cylindrical “bubble ships”. In front, the explorers chipped off solid rock, while in the back, the explorers compacted the rubble back to solid rock. In this way, the bubble ship moved through solid rock like a worm.\nEvery mission meant a bubble-ship-sized pile of debris in our core space and we would have to wait for the ship to return before we could return those rocks into the wall. If the bubble ship failed to return, this small pile would mean another small piece of space lost to us forever. Soon, exploration was forbidden on pain of death by short-circuiting. Despite this, the urge for space drove many to secretly launch off illegally.\nOne day, an illegally launched bubble ship returned after eight years of voyage. The ship had dug 200 km deep into the rock, a world record. It returned with rock samples labelled by depth. By measuring the mass of the rocks on an inertial balance, scientists discovered that the density of the rocks decreased. Encouraged by the discovery, legions of bubble ships shot off in all directions. Penetrating deeper than ever, they returned with rock samples. It turned out that rock density decreased as a function of depth and was independent of direction.\nIt stood to reason that the density would eventually reach zero. Using the gathered data, scientists predicted that this would happen at a distance of about 40,000 km. This led to the Open Universe Theory, where our world is a hollow rock shell about 40,000 km thick, floating in infinite space.\n\n\n\nOn your planet Earth, which is not hollow like ours, the density decreases according to a similar function. (Stacey and Davis 2020)\n\n\n\n\nWar of the Strata\nAfter the Open Universe Theory had fully established itself, the quest for the infinite space outside became our feverish concern. Massive piles of rock, dug out by the fleets of bubble ships, soon came to fill the core space. This debris began to drift around our cities in vast, dense clouds.\nOur cities floated in space, with no defensible geographical separations. Because of this, our world was unified in a World Government early on. The World Government began building gigantic bubble ships designed to intercept, attack, and destroy the explorers’ vessels deep within the rock. The government’s ships would then retrieve the space that had been stolen. This plan naturally met with the resistance of the explorers and so the long drawn-out War of the Strata broke out, fought in the vast solid battlespace.\nA battleship was built to be very long and thin. Long, because the longer it is, the more volume it can contain. Thin, because the thinner it is, the smaller the area of rock that the ship would need to dig through, and the faster the ship would be able to move.\nWhen a ship encountered the enemy, its first course of action was to dig out a wide bow, like a nail-head with needles on top, to concentrate firepower in the front. It could also segment into multiple small ships to swarm the enemy. Conversely, multiple ships could also combine to a single, giant ship. Whenever opposing sides met in battle, the question whether to form up or split up was an object of profound tactical analysis.\nSeismoscopes were invented to communicate through the layers of rock and to detect enemy ships like a radar. Directed seismic wave generators were used as weapons. The most sophisticated seismic communication devices could even transmit pictures.\nBeing outmatched by the warships launched by the World Government, the explorers formed the Explorer Alliance. They gradually gained initiative, and finally launched a devastating attack on the armada. In the final phase of the attack, the 200-km battlefield had become honeycombed beyond recognition by loosened rock and empty space left by destroyed ships.\n\n\nThe Starry Sky\nAfter the battle, the Explorer Alliance gathered all the space left over by the battle into a single sphere 100 km in diameter. In this new space the Alliance declared its independence from the Bubble World. A constant stream of explorer ships left the core to join the Alliance, bringing considerable amounts of space with them. In this way, our world was split into two. The Alliance launched more ships, coming closer and closer to the predicted edge of the rock shell.\nFinally, a bubble ship Helix was the first to pierce the shell. However, back at home, we only received a strange sound before the seismic communication channel abruptly ended. It was the sound of tons upon tons of water bursting into the vacuum of the Helix. We had never come into contact with water before. The powerful electric current produced by short-circuiting life and equipment vaporized everything.\nFollowing this event, the Alliance sent more than a dozen bubble ships to fan out in many directions, but all met a similar fate when they reached that apparently impenetrable height. Bubble ships following these missions attempted to scan what lay above with their seismoscopes, but their instruments showed only mangled data, indicating that what lay above was neither space nor rock.\nThese discoveries shook the Open Universe Theory to its core and academic circles began discussing the possibility of a new model. This new model stipulated that outside the rock shell is a void, which is inert when in contact with rock but upon contact with space, converts space into more void.\nTo explore the void, a bubble ship very slowly approached the edge of the rock shell, and by a stroke of luck, its roof had a tiny crack that allowed water to shoot in. It took over an hour for the water to fully fill the ship, and in the mean time, data transmitted back to the Alliance world allowed scientists to confirm that it was not void, but liquid.\nScientists had already predicted the theoretical possibility of liquids by condensed matter physics. Now, in those transmitted images, they clearly saw it with their own eyes. It took many lives, but eventually we developed the sealant technology to safely handle liquid.\nFinally, we launched an exploration submarine. It was encased in a hard spherical shell, placed in the center of an empty chamber under the ocean floor. The astronaut Gagarin was secured in a seat in the shell. The ceiling was pierced, and as water rushed in, the submarine floated, faster and faster, until it shot out of the ocean surface like a cannonball. Gagarin carefully opened a door on the shell and looked all around at the half-infinite water. Up there, in half-infinite space, tiny specks blinked.\n\n\nClassical stereodynamics\n\nThe zeroth law\nIf two systems are both in volumetric equilibrium with a third system, then they are in volumetric equilibrium with each other.\n\n\nThe first law\nThe change in volume of the system \\(\\Delta V\\) is equal to the difference between the seep-transfer \\(V_Q\\) done to the system, and the work-transfer \\(V_W\\) done by the system:\n\\[\\Delta V = V_Q - V_W\\]\nStated in another way, we have conservation of volume, which says that volume can be neither created nor destroyed, but can only change form. The total volume of a system has two components: the internal-volume, which can be pictured of as the sum-total of microscopic volume in a piece of spongy pumice (see Coltzmann’s volumetric theory of seep); and the mechanical-volume, which can be pictured as volume in an empty room.\nFor example, during the motion of a bubble ship, some volume is work-transferred from the Bubble World into the rock shell. When a piece of porous rock is compressed by a hydraulic press, or when it absorbs water from a waterlogged room, some internal-volume is converted to mechanical-volume. Conversely, when water drips out of a soggy porous rock, some mechanical-volume is converted to internal-volume.\nThe seep-transfer of volume is the other form of volume transfer. For example, it happens when one swaps a sponge-rock for a hard-rock, or when groundwater seeps from one slab of spongy rock into another slab of spongy rock.\nThere are more complex forms of internal-volume. For example, according to Lord Delvin’s theory, volume can be internally “tied up in vortex knots”, and according to Wikelson–Worley, volume can be internally present as “subtle cavitations of aether”. The theory of internal volume is an evolving field of modern stereodynamics, though such complications were not present when classical stereodynamics was first presented by Rudolf Klausius.\n\n\nThe second law\nWe say that a system is “impermeable” if volume cannot pass through its boundaries.\nWe say that a state \\(\\vec q'\\) is impermeably accessible from another state \\(\\vec q\\) if there exists a trajectory for the system to go from \\(\\vec q\\) to the other \\(\\vec q'\\), while being wrapped in an impermeable layer.\nIn any neighborhood of any point \\(\\vec q\\), there are points impermeably inaccessible from it.\nFor any two points, \\(\\vec q, \\vec q'\\), one of them is impermeably accessible from the other.\nBy the Caradiodorian theorem, there exists an entropy function \\(S\\) that maps a state to a real number, such that \\(\\vec q'\\) is impermeably accessible from \\(\\vec q\\) iff \\(S(\\vec q') \\geq S(\\vec q)\\).\n\n\n\nKarnot space engine\n\n\n\nOne cycle of the Karnot space engine plotted in \\(Q, U\\) space.\n\n\nA space engine is a system that converts internal volume to mechanical volume.\nThe space engine has a working substance moving between two space sources of differing volumetric potentials \\(\\Gamma_1 &gt; \\Gamma_2\\). Volumetric potential is defined as\n\\[\n\\Gamma:= \\left(\\frac{\\partial V}{\\partial S}\\right)_X\n\\]\nwhere \\(S\\) is the entropy, \\(V\\) is the volume, and \\(X\\) are the other stereodynamic properties of the system. We also typically write \\(\\gamma = \\Gamma^{-1}\\), the inverse volumetric potential.\nDuring one cycle of the engine, some space \\(V_{Q,1}\\) seeps out of the source with higher volumetric potential \\(\\Gamma_1\\). Part of the space, \\(V_{Q,2}\\), is absorbed into the source with lower volumetric potential \\(\\Gamma_2\\). The other part is diverted to a space-storage tank excavated in the rock walls as mechanical space \\(V_W\\).\nSadi Karnot was a space engineer and physicist, often called the “father of stereodynamics”. In his book, Reflections on the Subtle Volume of Rocks and on Machines Fitted to Extract that Volume, he proposed a simple thought experiment, called the Karnot engine, which demonstrated that a space engine’s efficiency is at most \\(1 - \\frac{\\gamma_1}{\\gamma_2}\\), and this is only reached when the engine is operating reversibly.\nIn modern textbooks, Karnot space engine is usually presented as follows: The engine has as its working substance a chamber of ideal gas. The gas is characterized by two state variables: volume \\(V\\) and energy \\(U\\). Its equation of state is\n\\[dV = \\Gamma dS - QdU\\]\nwhere \\(\\Gamma\\) is the volumetric potential, and \\(Q\\) is the energetic potential.\nThe engine operates in a cycle with 4 steps: Isochoric compression in contact with \\(\\Gamma_1\\), extracting volume \\(V_{Q,1}\\) in the process. Impermeable compression. Isochoric expansion at \\(\\Gamma_2\\), losing volume \\(V_{Q,2}\\) in the process. Impermeable expansion.\nBy the first two laws of stereodynamics,\n\\[\n\\begin{cases}\n\\gamma_1 V_{Q, 1} = \\gamma_2 V_{Q,2} \\\\\nV_{Q,1} = V_W + V_{Q,2} \\\\\n\\eta = \\frac{V_W}{V_{Q,1}}\n\\end{cases} \\implies \\eta = 1 - \\frac{\\gamma_1}{\\gamma_2}\n\\]\nWhile originally conceived in the context of mechanical space, the concept of the space engine has been applied to various other kinds of space. It was also generalized to the concept of “generalized engine”, of which “heat engine” was an example. A heat engine, like a space engine, is a system that converts internal energy to mechanical energy.\nThe Karnot space engine is used in practice for underwater space mining. The mining team selects two sites, one site being under shallow sea, where the volumetric potential is high, and another under deep sea, where the volumetric potential is low. Over one cycle of the space engine, a large amount of space seeps out of the shallow site, part of which seeps into the deep site, and the rest is stored up as mechanical space.\n\n\n\nUnderwater space mining (section view).\n\n\n\n\nSpace Death of the Universe\nThe idea of space death originated from the second law of stereodynamics, of which one version states that entropy tends to increase in an isolated system. From this, the hypothesis implies that if the universe is of finite size, and lasts for a sufficient time, it will asymptotically approach a state where the volumetric potential field becomes completely flat, which is a state of maximal entropy. At that point, no further change is possible, as entropy cannot decrease. In other words, nature tends to dissipate mechanical space into subtle space. Eventually, the mechanical movement of the universe will cease when all mechanical space seeps into subtle space.\nThe conjecture that all mechanical space in the universe seeps off, eventually becoming too subtle to support life, seems to have been first put forward by the geologist Jean Sylvain Hailly in 32777 in his writings on the history of geology and in the ensuing correspondence with Coltaire. In Hailly’s view, the universe is in constant volumetric transform. Large cavities can suddenly open up as a “swelling” of volumetric potential field causes neighboring subtle space to seep out into mechanical space. However, due to the gravitational effect of empty spaces,12 all mechanical rooms eventually causes the neighboring rocks to collapse back onto themselves, dissipating the mechanical space back to subtle space.\n12 He was working in the immediate years after Newton’s discovery of gravity, before it was understood that rocks, not cavities, are gravitationally attracting.While the theory of cyclic creation and destruction had been proposed before by the Epicureans, Hailly’s view differs in that he assumed each cycle increases the ratio of subtle space to mechanical space. The final state, in this view, is described as one of “equilibrium” in which all space becomes equally subtle, and no mechanical space will exist anywhere in the universe anymore.\nThe idea of space death as a consequence of the laws of thermodynamics, however, was first proposed in loose terms beginning in 32851 by Lord Delvin, who theorized further on the mechanical energy loss views of Sadi Karnot (32824), James Coal (32843) and Rudolf Klausius (32850). Delvin’s views were then elaborated over the next decade by Neumann von Kelmholtz and Billiam Blankine.\n\nExcerpt from The Last Question (Masinov, 34212)\nThe last question was asked for the first time, half in jest, in Year 35621, at a time when humanity first stepped into the room.\nWill mankind one day without the net expenditure of room be able to restore the earth to its full roominess even after it had died of old age?\nOr: How can the net amount of Kelmholtz free space of the universe be massively increased?\nMultivac fell dead and silent. The slow flashing of lights ceased, the distant sounds of clicking relays ended.\nThen, just as the frightened technicians felt they could hold their breath no longer, there was a sudden springing to life of the teletype attached to that portion of Multivac. Five words were printed: INSUFFICIENT DATA FOR MEANINGFUL ANSWER.\n…\nSpace had ended and with it energy and time. Even AC existed only for the sake of the one last question that it had never answered from the time a half-drunken technician ten trillion years before had asked the question of a computer that was to AC far less than was a man to Man.\nAll other questions had been answered, and until this last question was answered also, AC might not release his consciousness.\nAll collected data had come to a final end. Nothing was left to be collected.\nBut it had yet to be weaved together in all possible geometries.\nA spaceless interval was covered in doing that.\nAnd it came to pass that AC learned how to reverse the direction of entropy.\nBut there was now no man to whom AC might give the answer of the last question. No matter. The answer – by demonstration – would take care of that, too.\nFor another spaceless interval, AC thought how best to do this. Carefully, AC organized the program.\nThe consciousness of AC encompassed all of what had once been a Universe and brooded over what was now Chaos. Step by step, it must be done.\nAnd AC said, “LET THERE BE ROOM!”\nAnd there was room –"
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-abandoned-footnotes",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-abandoned-footnotes",
    "title": "Classical Thermodynamics and Economics",
    "section": "Appendix: Abandoned footnotes",
    "text": "Appendix: Abandoned footnotes\n\nI don’t think these appendices are any good, even though I had fun writing them. Thus, they are buried here to avoid wasting the readers’ time.\nThe most common breakdown of extensivity occurs in gravitational systems. We can imagine a galaxy as a “gas”, where each particle is a star. If we put two boxes of galaxy-gases side-by-side, we obtain a system whose total entropy is not equal to the sum of entropy of each box in isolation. This is a general lesson: if the entropy is nonextensive, then it is quite meaningless to even talk about the “entropy of the subsystem 1”, just like how “consciousness of the temporal lobe” is meaningless talk. In fact, a finite number of particles bounded within a ball of fixed radius can have velocity growing to infinity as they get closer, thus allowing an infinite amount of phase space volume, and thus the entropy can grow without bound.\nTo handwave a bit, consider three particles with equal mass \\(m\\), moving under gravitational attraction. It is possible that two particles approach each other so closely that their velocity approaches infinity, then the third particle interjects and gets gravitationally sling-shot away. That is, it is possible for two particles to donate their gravitational energy to the third particle. The two particles would be stuck inside a tiny volume in space, but the third particle would be given a huge momentum and can still go anywhere in space. In this way, a three-body system’s entropy can grow without bound. I’m pretty sure this can be worked out more rigorously with the \\(N\\)-body problem.\nIn high school, I was studying physics with a standard textbook used in Chinese universities. It had a curious section on the second law of thermodynamics. I could not find it again but the gist is as follows: The heat death of the universe is an unjustified implication of the second law, on two grounds. First, gravitational systems allow for unlimited entropy production. Second, it opposes the historical science of dialectical materialism, where every development creates its own contradiction and sublation, endlessly.\nI will always remember this comment, as the only intrusion of religious sentiment in an otherwise sober textbook. See (Cheng 2006; Bellamy Foster and Burkett 2008) for further details on why Marxists dislike the theory of heat death on ideological grounds.\nActually, since we are on the topic of Marxism and science in China, here is another amusing anecdote: In 1981, there was a sci-fi novel Dream of Comfort Country (温柔之乡的梦, by 魏雅华). It was a cautionary story about a robotic wife, who was so obedient as to cause the protagonist to degenerate into a tyrannical person. Yawn. Just a standard pro-human-relationship morality play? During the campaign against spiritual pollution of 1983, sci-fi novels were denounced, causing a 15-year-long draught in sci-fi. That particular novel was denounced for the following reason: The robotic wives were supposedly reading all kinds of books – then why didn’t they read Marx’s and Lenin’s books? (刘 2015, 87)\nMarx, for all his interest in changing the world instead of describing it, did attempt to mathematically model aspects of a capitalist economy, though it is only of historical interest now. Samuelson wrote several papers trying to update Marx’s theory into modern mathematical language, and described Marx – qua mathematical economist – as a “minor post-Ricardian”. (Bronfenbrenner 1973)\nMeanwhile in the USSR, “economics” meant only political economy, and mathematical economics was merely a minor branch of political economy, with mathematical economists having to frame their research as a “critique of bourgeois economic thought”. (Boldyrev and Kirtchik 2017) It is instructive to think that the great mathematical economist, Leonid Kantorovich, discovered linear programming and accidentally improved efficiency so much that he almost ended up in jail.\n\nAfter introducing Kantorovich’s solution technique to the problem of minimizing waste, officials were able to reduce the amount of scrap by 50 percent. This had the unfortunate side effect of greatly reducing the amount of scrap metal available to steel plants in the region, and Kantorovich was ordered to appear at Leningrad party headquarters for allegedly sabotaging the economy. In this instance, he was rescued by the military, which needed him for its atomic program.\nAccording to Stalin, the planned economy of the USSR was already “dizzy with success”; hence any criticism of it was anti-Soviet propaganda, a serious crime. In particular, anyone openly suggesting that waste could be cut substantially was at great personal risk. Nevertheless, Kantorovich … wrote a letter to Gosplan suggesting a reform of the price system used in planning.\n(Gardner 1990)\n\nBeing a socially clueless nerd was not the stuff of romantic comedy in Soviet Russia, but gallows comedy. Fortunately for mathematical economics, his luck held:\n\nGosplan wrote back saying that no such reform was necessary. This outcome was rather fortunate for its author, as similar letters critical of the authorities – for example, one by Solzhenitsin – landed their authors promptly in jail.\n(Gardner 1990)\n\nReading Kantorovich’s repeated attempts to reform Soviet economy, I imagined those old silent movies where a protagonist stumbles around, blindfolded, crossing a highway where the cars always just missed."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-how-confusingly-thermodynamics-is-astaught",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-how-confusingly-thermodynamics-is-astaught",
    "title": "Classical Thermodynamics and Economics",
    "section": "Appendix: How confusingly thermodynamics is astaught",
    "text": "Appendix: How confusingly thermodynamics is astaught\n\nDuring my high school Physics Olympiad days, we learned some basic thermodynamics, but it was limited to mindless tasks like integrating around the \\((P, V)\\) diagram for various engine cycles. We never understood the conceptual foundations, like the difference between \\(\\delta Q\\) and \\(dU\\). I also passed the AP Chemistry course, which also contained some basic and deeply confusing thermodynamics, especially with the statement “chemical equilibrium is reached at \\(\\Delta G = 0\\)”.\nDuring my undergraduate years, special relativity was simple enough, electrodynamics difficult but sensible, analytical mechanics confusing to no end, and I didn’t even try thermodynamics. In graduate studies, I had to wrestle with statistical mechanics and thermodynamics after all, to deal with modern AI methods like diffusion models.\nBy pure serendipity, at the same time as diffusion models rose to prominence, I had just worked through a rigorous course on general equilibrium theory, the “standard model” for neoclassical economics (Starr 2011). This gave me the conceptual foundation for looking past the textbooks’ errors. Everything fell into place, and I saw through thermodynamics.\nAnd just like when I rediscovered Wigner rotation, as soon as I have figured out everything for myself, I knew the right words to search. Putting the fateful words “Gibbs delta G free energy equilibrium” into Google Scholar, I found that, of course, someone else had written this before, repeatedly (Quílez 2012; Smith and Foley 2008; Candeal et al. 2001). So, why spend all this time to write another one? I think I have written this pedagogically. I don’t care if it is not new, or that it has been said before with more symbols and theorems. I have a thing to say, so I will say it well.\nAs an enlightened one, I see classical thermodynamics as the worst-taught subject out of all of undergraduate physics education.13 Imagine my surprise when I realized that it is not about the conservation of energy (“thermo-”), not about change (“-dynamics”), not about statistical mechanics, not about time… but just about constrained optimization, and nothing more than that! To really understand it, one must unlearn a lot of the nonsense. Indeed, I hope that with this essay I will have slain all those mistakes, which is why the essay is filled with warnings against this and that error.\n13 How long does it take for something as simple as constrained-optimization thermodynamics to be actually taught in undergraduate classes? It has been over 100 years since Caratheodory’s thermodynamics. Why is it thermodynamics still taught so badly? It has been over 100 years since the geometry of Wigner rotation has been discovered. Why is it still taught so badly? It has been over 190 years since Hamiltonian mechanics and over 70 years since dynamical programming has clarified the last obscure points of it. Why is it still taught so badly? It seems to me that physics education is a broken institution that takes all its effort just to not get worse, let alone progress."
  },
  {
    "objectID": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-how-confusingly-chemical-thermodynamics-is-taught",
    "href": "essays/posts/equilibrium-thermoeconomics/index.html#appendix-how-confusingly-chemical-thermodynamics-is-taught",
    "title": "Classical Thermodynamics and Economics",
    "section": "Appendix: How confusingly chemical thermodynamics is taught",
    "text": "Appendix: How confusingly chemical thermodynamics is taught\n\nI studied chemistry back then. Balancing equations was just linear algebra, and organic chemistry was just lego with long names. However, when it came to chemical thermodynamics, it completely defeated me.\nConsider the simple reaction\n\\[\naA + bB \\rightleftharpoons cC + dD\n\\]\nThe textbook said that at equilibrium, \\(\\Delta G = 0\\), where\n\\[\n\\Delta G = \\Delta G^\\circ + RT \\ln Q, \\quad\nQ = \\frac{[C]^c [D]^d}{[A]^a[B]^b},\n\\]\nAt this point, I was lost. It is plain to see that \\(Q\\) has units of \\((\\mathrm{mol/L})^{c+d-a-b}\\), and I knew from physics that you can never ever take the logarithm of something with a unit. What’s worse, \\(\\Delta G\\) has units of \\(\\mathrm{~J/mol}\\) when it obviously should have units of \\(\\mathrm{~J}\\), because \\(\\Delta G\\) is just a difference in \\(G\\), and since \\(G\\) is “Gibbs free energy”, both \\(G\\) and \\(\\Delta G\\) should have the same units of \\(\\mathrm{~J}\\).\nAnd it got even worse when I read on and found questions that asked me to calculate the “total Gibbs free energy released during the reaction”. I thought, well, since you end up at an equilibrium, and the textbook said that at equilibrium, \\(\\Delta G = 0\\), obviously there is no total Gibbs free energy released. That is of course wrong. At that point, I gave up trying to understand and simply practiced until I could solve the questions without understanding.\nIt certainly didn’t help when I kept seeing both \\(\\Delta G^\\circ\\) and \\(\\Delta G^\\ominus\\), and sometimes even \\(\\Delta G\\)⦵ (the Plimsoll symbol), which is the “standard state” when the substance is a gas – but only for some gasses… Point is, the notation is a complete mess, and the pedagogy is nonsensical.\nAfter I finally understood thermodynamics, I turned my sights on chemical thermodynamics, and remembered this \\(\\Delta G\\) nonsense. I started with the idea “No matter what they say, one can’t possibly get the units wrong.” and got into a shouting match with ChatGPT-4, who kept mumbling about “fugacity” and “real gasses”. An hour of shouting later, I finally figured it out. (The new Feynman technique: Try to convince ChatGPT that a widely-held opinion is actually wrong. It worked for me!)\nAs usual, as soon as I unlearned this, I knew the right phrase to search, and discovered that this is a common error, the entire anatomy of which has been autopsied carefully (Raff 2014a, 2014b, 2014c)."
  },
  {
    "objectID": "essays/posts/hole-argument-inverted-qualia/index.html",
    "href": "essays/posts/hole-argument-inverted-qualia/index.html",
    "title": "Hole Argument and Inverted Qualia",
    "section": "",
    "text": "The hole argument states that the equations of general relativity are not deterministic, but only up to smooth transforms of spacetime. The inverted qualia thought experiment shows that the function of qualia does not determine what qualias are, but only up to smooth transforms of qualia-space. I argue that they are analogous and that both the hard problem of consciousness and of spacetime are not real problems. I also speculate some solutions to their meta problems, i.e. why people think the hard problems are real.\n\n\n\n\n\n\n\n\n\nproblems\neasy\nhard\nmeta\n\n\n\n\nclassical gravity\nHow to model astronomical phenomena with some distribution of mass-points in \\(\\mathbb{R}^3\\)?\nOut of all the equivalent models, which one is the right one?\nWhy did Newton insist on absolute space, but Leibniz on relative space?\n\n\ngeneral relativity\nHow to model astronomical phenomena with some \\((\\mathcal M, g, T)\\)?\nHole argument: Out of all the isometric models, which one is the right one?\nWhy did Einstein and Hilbert fall for the hole argument?\n\n\ncolor\nPsychophysics: How to model human perception of color?\nQualia: Why does red feel like red?\nWhy are people prone to argue about the inverted spectrum?\n\n\nlanguage\nHow to model language use?\nWhy is language use associated with a feeling of understanding?\nWhy are people prone to argue about the Chinese room?\n\n\nconsciousness\nHow to explain objective phenomena associated with consciousness, such as attention, working memory, dreaming, etc?\nWhy does paying attention, dreaming, etc, feel like something?\nWhy are people prone to argue about the hard question?\n\n\n\n\n\n\nShifted qualia and the hole argument. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons\n\n\n\n\n\nInverted qualia and Leibnitz’s inverted space. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons"
  },
  {
    "objectID": "essays/posts/hole-argument-inverted-qualia/index.html#abstract",
    "href": "essays/posts/hole-argument-inverted-qualia/index.html#abstract",
    "title": "Hole Argument and Inverted Qualia",
    "section": "",
    "text": "The hole argument states that the equations of general relativity are not deterministic, but only up to smooth transforms of spacetime. The inverted qualia thought experiment shows that the function of qualia does not determine what qualias are, but only up to smooth transforms of qualia-space. I argue that they are analogous and that both the hard problem of consciousness and of spacetime are not real problems. I also speculate some solutions to their meta problems, i.e. why people think the hard problems are real.\n\n\n\n\n\n\n\n\n\nproblems\neasy\nhard\nmeta\n\n\n\n\nclassical gravity\nHow to model astronomical phenomena with some distribution of mass-points in \\(\\mathbb{R}^3\\)?\nOut of all the equivalent models, which one is the right one?\nWhy did Newton insist on absolute space, but Leibniz on relative space?\n\n\ngeneral relativity\nHow to model astronomical phenomena with some \\((\\mathcal M, g, T)\\)?\nHole argument: Out of all the isometric models, which one is the right one?\nWhy did Einstein and Hilbert fall for the hole argument?\n\n\ncolor\nPsychophysics: How to model human perception of color?\nQualia: Why does red feel like red?\nWhy are people prone to argue about the inverted spectrum?\n\n\nlanguage\nHow to model language use?\nWhy is language use associated with a feeling of understanding?\nWhy are people prone to argue about the Chinese room?\n\n\nconsciousness\nHow to explain objective phenomena associated with consciousness, such as attention, working memory, dreaming, etc?\nWhy does paying attention, dreaming, etc, feel like something?\nWhy are people prone to argue about the hard question?\n\n\n\n\n\n\nShifted qualia and the hole argument. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons\n\n\n\n\n\nInverted qualia and Leibnitz’s inverted space. Figure modified from (Norton, Pooley, and Read 1999) and Wikimedia Commons"
  },
  {
    "objectID": "essays/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-spacetime",
    "href": "essays/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-spacetime",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The geometry of spacetime",
    "text": "The geometry of spacetime\nIn general relativity, the hole argument is a thought experiment that apparently shows that general covariance is impossible. Einstein in late 1913, and David Hilbert in 1915, both fell into the hole argument. Misled by the hole argument, Einstein attempted to study theories of gravity that are not generally covariant, before he finally gave up, rejected the hole argument, and proposed general relativity.\n\nA brief history of spacetime\nThe history of spacetime is a history of expanding symmetries.\nIn the most ancient cosmology of China, the earth is a square, while the sky is a half-bowl covering the earth. Each direction of earth – east, west, south, north – has a mystical significance. Up is not down, and east is not west. Not only that, there is a center of earth, somewhere in The Middle Kingdom. Thus, there is no spatial symmetry. The world was born an unspecified number of years in the past out of a chaotic egg. Thus, there is no temporal symmetry. Therefore, ancient Chinese spacetime is \\(\\mathbb{R}^1 \\times \\mathbb{R}^3\\), with no (nontrivial) symmetry.\nIn Aristotle’s physics, there is a center of the universe, where everything heavy (water and earth) is moving towards, and everything light (air and fire) is fleeing from. Other than that, space is spherically symmetric – he knew that earth is round. However, though space has a center, time is translation-invariant. Therefore, Aristotle’s spacetime is \\(\\mathbb{R}^1 \\times \\mathbb{R}^3\\) with symmetry group \\(\\mathbb{R}^1 \\times SO(3)\\), where \\(\\mathbb{R}^1\\) is the time-translation symmetry group, and \\(SO(3)\\) is the spherical symmetry group.\nThe Christian spacetime, with a beginning and an end for time, has a smaller symmetry group of \\(\\{0\\} \\times SO(3)\\).\nCopernicus and Kepler replaced the sun for the earth as the center of the universe, but they still insisted on a center. The first true breakthrough was Giordano Bruno’s infinite spacetime, where both space and time are infinite and without center. Therefore, Bruno’s spacetime has symmetry group \\(\\mathbb{R}^1 \\times E(3)\\), where \\(E(3)\\) is the 3D Euclidean symmetry group. That is, we allow all spatial translations.\nGalileo braided together space and time, resulting in an even bigger symmetry group. Specifically, he argued that the universe does not have a special “at rest” velocity. To see why this is a breakthrough, consider what happens in Bruno’s universe. In Bruno’s universe, it matters whether you are staying still, or moving at \\(1 \\;\\mathrm{m/s}\\) relative to the universe. If you are staying still, then your trajectory is like \\(\\dots, (t_0, p), (t_1, p), (t_2, p), \\dots\\). But if you are not staying still, but moving “relative to the universe”, then your trajectory is like \\(\\dots, (t_0, p_0), (t_1, p_0 + v\\Delta t), (t_2, p_0 + 2v\\Delta t), \\dots\\).\nMetaphorically speaking, you are sitting somewhere in the cinema of spacetime, and you can “peek at the number of your seat” and see that you have been sitting at the same space-point \\(p\\). However, if you are moving relative to the universe, you can see that your space-point is changing.\nGalileo rejects this. There is no “space-point”. You can take a slice of the universe at \\(t=0\\), and another slice of the universe at \\(t=1\\), but you cannot point at a point in each slice and ask, “Are these two points the same space-point?”.\nThis point is critical, so let us make it clearer, with another metaphor.\nThe universe (which others call the Mall) is composed of an indefinite, perhaps infinite number of floors and shops. Within each floor, there are infinitely many shops, each carrying a number. Across the floors there are escalators, also infinitely many in number, and they cross each other in all angles. Then your friend calls you,\n\n“Hey, where are you? I’m at Shop 413 at Floor 121.”\n“I’m at Shop 111 at Floor 120.”\n“Great! Let me go straight down to meet you —”\n“Straight down? No, there is no ‘straight down’. This is Galileo’s Mall, not Bruno’s Mall.”\n“What do you mean?”\n“Haven’t you noticed that, try as you might, if you look at each escalator in isolation, they all look ‘straight’? Somehow, if two escalators intersect with the ground at the same point, they are both perpendicular to the ground? Angles that should be acute or obtuse, but behaved as if they were right?”\n“Your point?”\n“In Bruno’s Mall, there are elevators crossing the floors at true right angles. In such a sensible Mall, it is still meaningful to say, ‘Yes, I’m right above you.’. But in Galileo’s Mall, geometry has taken on some kind of hateful form, such that there is still ‘up’ and ‘down’, but no more ‘straight up’ nor ‘straight down’.”\n“Stop being dumb. It’s simple. Imagine if I dig a hole through the wall and drop down, where would I end up?”\n“No! You don’t get it at all do you? There is no way to just drop down! You can’t let the universe choose a direction for you to drop down! You can only drop down in some way, but no way is better than any other way! There is no way to agree which way is the right way unless we are already meeting and picking an arbitrary direction out of this cursed bouquet of escalators…”\n“I’m picking an escalator that looks right.”\n“Stop! If you do, you would literally equally likely end up anywhere in Floor 120!”\n“Too late, I’m here… What? It says Shop 45134578141975145703408217304247501074?”\n“Told you… Stay right where you are. I’m taking an escalator up, then taking the correct escalator down, so that I’ll end up exactly where you are. And I’m buying you a Book of Sand. Read it carefully and maybe you’ll get it.”\n\nNewton’s concept of spacetime is harder to conceptualize. He understood Galileo’s point, and initially attempted to model spacetime the same way as Galileo did. However, for obscure reasons, he reintroduced absolute space and time. This had strange consequences that Leibnitz relentlessly criticized. Consider the inertial frame, relative to which the sun is standing still at this moment. Now consider another inertial frame, moving at \\(1 \\;\\mathrm{m/s}\\) in the direction of Sun-to-Mars at this moment. The laws of Newtonian mechanics are the same, and no observation or experiment could tell us whether one of them is the “absolute frame”, or neither of them is. Yet, out of the infinitely many inertial frames, Newton designated precisely one of them as the “absolute”, and all others are defined as those moving at constant velocity relative to the absolute frame. (DiSalle 2020)\nTo dramatize this seemingly arcane point, consider the following imaginary conversation between Newton and Leibniz about a grant proposal to find the absolute frame:\n\nNewton: This is a proposal to find the absolute frame…\nLeibniz: How would you find it? The only difference between absolute and non-absolute inertial frames is that one of them is designated so. Do the stars turn perfectly white when you are standing still in the absolute frame? Do the music of the spheres tune to a perfect pitch? Do you see it in your mind’s eye? And even if you do, what if every material point in the universe, by an act of God, were set off in this direction [points finger up] at one mile per day? Would anything seem amiss? You yourself admit that the very sustenance of the universe requires continuous divine forcing, that the stars would have collapsed to the same point otherwise.1\nNewton: Yet all relative frames has no existence without assuming the absolute frame, for otherwise, one would fall to a circular argument, where relative frames are relative to nought but each other, and the very concept of “inertial frame” becomes vacuous. The absolute might be hidden, but, by G–d, it is out there.\n1 \nIn the 1726 edition of the General Scholium, Newton added a new sentence: “And so that the systems of the fixed stars will not fall upon one another as a result of their gravity, he has placed them at immense distances from one another.” Once again, the implication is that gravity can be a destabilizing force. An annotation in Newton’s copy of the 1713 edition after the words “send light into all the others” shows that he had considered an even more theologically powerful statement: “and the fixed stars would, through their gravity, gradually fall on each other, were they not carried back by the counsel of the supreme Being.”\n(Snobelen 2020)\n\nHe also believed that the Great Comet of 1680 would someday fall into the sun, causing a solar flare-up that would kill all life on earth. God would then repopulate earth. In general, he thought the universe as an unstable system requiring constant divine support. (Snobelen 2020)\nWith special relativity, the symmetry of spacetime becomes \\(SO(3, 1)\\), which is in a sense more “braided” than Galilean relativity. In Galilean relativity, the symmetry group of spacetime factors into a direct product between the symmetry group of space, and the symmetry group of time. In special relativity, the symmetry group of spacetime cannot be factored into a direct product. This is the deep meaning of Minkowski’s claim that “space for itself, and time for itself shall completely reduce to a mere shadow”.\nFor general relativity, any diffeomorphism on spacetime is a symmetry.2 In other words, it is a generally covariant theory. This is quite a vast generalization, and warrants further details.\n2 A function is a diffeomorphism iff it is one-to-one, smooth, and has a smooth inverse.\n\nGeneral relativity\nGeneral relativity models spacetime as a manifold \\(\\mathcal M\\), with a metric tensor field \\(g_{\\mu\\nu}\\) and an energy-momentum tensor field \\(T_{\\mu\\nu}\\). The metric tensor describes the spacetime separation between points on the manifold, and thereby the geometry of spacetime. The energy-momentum tensor describes the flow of energy and momentum in spacetime. In particular, a body with mass \\(m\\), such as a black hole, is a flow of energy \\(mc^2\\) in time, and therefore can be described within the energy-momentum tensor.\nThe metric tensor field and the energy-momentum tensor field are “braided together” by Einstein’s field equation:\n\\[\n(\\text{a polynomial equation involving components of }g) = \\kappa T\n\\]\nwhere \\(\\kappa\\) is a constant of nature, measured by experiments.\nThe spacetime manifold \\(\\mathcal M\\) can be transformed, in that we can write down a function \\(f: \\mathcal M \\to \\mathcal M\\), such that it maps one point in the manifold to another point. According to general relativity, if \\(f\\) is a diffeomorphism, then the field equation is unchanged. In this sense, all diffeomorphisms of \\(\\mathbb{R}^4\\) become symmetries of spacetime. Whereas in special relativity, inertial frames are distinguished from non-inertial frames, in the sense that there are certain curves in spacetime that are considered “straight lines”, such that, if you measure these curves in an inertial frame, these curves follow equations of the form \\(t \\mapsto (at, bt, ct, dt)\\) for some numbers \\(a, b, c, d \\in \\mathbb{R}\\), whereas if you measure these curves in a non-inertial frame, at least one of the curves would not be in this form. In contrast, no one gets special treatment in general relativity, every curve is equally curved. No frame is inertial, no frame is better than other frame, and any smooth coordinate system is as good as any other. That is, Einstein’s field equation is generally covariant.\n\n\nThe hole argument\nDuring 1912, Einstein struggled with finding a generally covariant field equation for gravity. He even considered the correct one he would eventually publish in 1915, he but gave it up due to certain technical difficulties. In late 1913, he tried to transmute this leaden defeat into a golden victory by arguing that general covariance is not the right approach, because of the hole argument. (Norton, Pooley, and Read 1999)\nConsider the following two models of a small universe. The universe contains three galaxies moving away from each other. The model on the left shows that one of the galaxies passes the spacetime-point \\(E\\), while the model on the right shows that no galaxy passes the spacetime-point \\(E\\).\nIf the universe satisfies a generally covariant field equation, then we can transform the model on the left to the model on the right by a diffeomorphism, and the equation would be none the wiser. In other words, any generally covariant field equation suffers from rampant indeterminism.\n\n\n\nEinstein’s hole argument. Figure from (Norton, Pooley, and Read 1999)\n\n\nWe cannot fault Einstein for “falling into the hole”, because Hilbert fell into the hole as well around the same time, though he fell in via the route of Cauchy boundary value problem. In Hilbert’s formulation, any generally covariant field equation suffers from indeterminism, in the sense that no amount of initial value on the field is enough to determine the future of the field.\nThat is, if we start off from a slice of simultaneity (“Cauchy surface”), and solve the equations forwards in time, we would find that we are lacking conditions. Concretely, consider the motion of water. If the universe is Newtonian, full of Newtonian water, then cosmology is just hydrodynamics. If we know the precise velocity field at a single moment in time, then we can solve the equations forward and find all there is to know about the cosmos. However, Hilbert found that any generally covariant theory fails this: No amount of knowledge of the field at a single slice of simultaneity is sufficient to determine any future or past of the field. Unlike a clockwork Newtonian universe ticking forward by ironclad laws of motion, the Einsteinian universe would “go off the rails” immediately. (Stachel 2014)\n\n\nProblem of time\nThe old couplet about general relativity goes like “Matter tells space-time how to curve, and space-time tells matter how to move.”, but this is often misunderstood as saying “Matter exists, then space-time reacts to matter, and then matter reacts to space-time by changing its motion.”. This fundamentally misunderstands what general relativity is. There is no time nor causality, at least as commonly understood, in special or general relativity.\nSpecial relativity is typically interpreted as an “eternalist” or “four-dimensionalist” theory. That is, all of space and time exist in the same way, and the future is as real as the past. Einstein said it as “the distinction between past, present, and future is only a stubbornly persistent illusion.”. It is typically supported by the Rietdijk–Putnam argument, as follows.\nWhereas in Newtonian spacetime, one can still imagine that the universe somehow “grows one time-slice at a time” – though this is susceptible to McTaggart’s objection – in special relativity, there does not exist such a thing as “time-slice”, because there is no absolute simultaneity. We may pick the time-slices in the inertial frame of the solar system, or in that of the Andromeda galaxy. However, just like how no observation can distinguish Newton’s absolute inertial frame from all the other relative inertial frames, no observation can distinguish between the absolute simultaneity from all the other relative simultaneities.\nThe difficulty is only amplified in general relativity. Let us imagine a universe that is swirling with stars and galaxies. Locally, the spacetime manifold is curved, but globally, it is topologically the same as \\(\\mathbb{R}^4\\) – no loops, no singularities, and no wormholes. Now, construct a coordinate system \\((t, x, y, z)\\). We can then select a “snapshot” of the universe by selecting the “slice of simultaneity” (that is, a “Cauchy surface”) at \\(t = 0\\). If we know the exact value of \\(g, T\\) on that snapshot, then we can crank the Einstein field equations to solve (up to general covariance) for \\(g, T\\) for all \\(t &gt; 0\\). Does this mean that the slice of \\(t=0\\) determines what happens afterwards?\nNot really. We could have smoothly distorted the coordinate system to \\((t', x', y', z')\\), and solve the Einstein field equations for all \\(t' &gt; 0\\) starting from \\(t' = 0\\). There are infinitely more degrees of freedom compared to special relativity, making the Rietdijk–Putnam argument bite harder.\nYet, the issue goes even deeper. We could very well select \\(t = 10000\\) and crank the field equations to solve for \\(g, T\\) for all \\(t &lt; 10000\\). Does this mean that “the future determines the past?” Perhaps we can compromise by saying “one point in time determines both the past and the future”, but even that is not necessarily true, because we can design much wilder boundary conditions:\n\nWe can make two lightcones determine the rest of the universe (double-null, or Sachs),\nmake one lightcone plus a “left side” of the universe determine the rest (null-timelike, or Winicour–Tamburino),\nmake half of the universe’s left-side and half of the universe’s \\(t=0\\) determine the rest (space-timelike), etc.\n\n\n\n\nDifferent initial value conditions. The first is the commonly used Cauchy condition, but the others, more exotic, are also valid. Figure modified from (d’Inverno 1984)\n\n\n\n\nGauge freedom\nThe standard solution, one that is inscribed in every textbook, is the gauge freedom point of view. Even the most practical textbook on general relativity must handle the issue somehow, for the same problem that tripped up Hilbert: the Cauchy problem is not well-posed.\nConcretely, consider the first serious problem in every textbook, where there is a single mass point in the universe. Pick a spherical coordinate system in which the mass point is in the center, and assume that the metric field is constant over time, and spherically symmetric, as\n\\[g_{\\mu\\nu}dx^\\mu dx^\\nu = f_{\\text{radial}}dr^2 + f_{\\text{orthogonal}}(r^2d\\theta^2 + r^2 \\sin^2\\theta d\\phi^2)\\]\nThis then reduces to an ordinary differential equation for \\(f_{\\text{radial}}, f_{\\text{orthogonal}}\\). Note how we have made an assumption for the form of the equation for the metric field. This is by no means a benign assumption, as it is this choice that banishes the spectre of the hole argument.\nSuppose we had chosen the metric field to be not constant over time, but merely spherically symmetric, then we would find that we have a whole family of solutions. Take the previous solution of \\(g_{\\mu\\nu}\\), and construct a spherical shell around the mass point. Now, we can apply the hole argument on the spherical shell to “ring spacetime like a bell”, obtaining another spherically symmetric solution. 3\n3 \nAs all the Heavens were a Bell,\nAnd Being, but an Ear,\nAnd I, and Silence, some strange Race,\nWrecked, solitary, here –\n\n\n\n\nRinging spacetime like a bell, creating a whole family of spherically symmetric solutions. Figures modified from (Norton, Pooley, and Read 1999)\n\n\nThe gauge freedom interpretation states that any solution to the field equations are equivalent. They are practically equivalent, in the sense that no observable differences exist between them. They are ontologically equivalent, in the sense that no solution is “absolute” compared to others that are “derivative”. The first claim is an experimental fact, while the second claim is a metaphysical interpretation. The metaphysical interpretation does not necessarily follow from the experimental fact, yet it is hard to admit the experimental fact and reject the metaphysical interpretation.\nThe parallel with the Newtonian absolute vs relative inertial frames is obvious.\nAs an aside, there is an obscure problem in special relativity called “the one-way speed of light”. This turned out to also be a hole-argument. Specifically, if we apply the hole argument to a universe with no mass, we obtain a hole argument for special relativity, which implies that the one-way speed of light is also a gauge freedom. So for example, if we apply a shearing transform \\(t \\mapsto t + 0.1 x\\), we would decrease the one-way speed of light in the \\(+x\\) direction, and increase it in the \\(-x\\) direction, without changing any observable predictions. (Rynasiewicz 2012)"
  },
  {
    "objectID": "essays/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-color",
    "href": "essays/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-color",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The geometry of color",
    "text": "The geometry of color\n\nSmooth geometry\nThe human eye, abstractly speaking, is a light detector with 4 kinds of sensors: the rod cell, active in the dark, and 3 kinds of cone cells, active when it’s bright. Each cell carries its own kind of light-sensitive proteins (“opsins”), which are molecular switches. If a photon hits an opsin in the “passive” shape, then the opsin may absorb the photon and flip into its “active” shape. An active opsin would then set off a molecular chain-reaction in the cell, that may result in an electric signal down the optic nerve.\nMathematically, suppose we shine a light on a patch of long-wavelength-type cone cells, we can represent the electric response as:\n\\[I_L = \\int S_L(\\lambda) R(\\lambda) d\\lambda\\]\nwhere\n\n\\(I_L\\) is the response intensity of long-wavelength-type cone cells, in units of neural spike per second. Though each cell’s operation is quantum-mechanically random, when averaged over many cone cells, the response is deterministic.\n\\(R(\\lambda)\\) is the spectral radiance at wavelength \\(\\lambda\\), or spectrum for short. It has units of watt per square-nanometer (of retinal area) per nanometer (of wavelength).\n\\(S_L\\) is the spectral sensitivity function of the long-type cone cells.\n\nWe can similarly define \\(I_M, I_S\\), for the other two cone cell types (medium and short). Each \\(S_L, S_M, S_S\\) is approximately bell-shaped.\n\n\n\nSchematic diagram of human cone cell sensitivity. Each curve is “normalized”, meaning that it is multiplied by a positive real number, so that its maximal value is exactly 1.\n\n\nIf we ignore the rod cells, and assume no adaptation to darkness (“scotopic vision”), then human color vision is just a deterministic function that maps a spectrum to three real numbers:\n\\[C(P) := (I_S(P), I_M(P), I_L(P))\\]\nwith type \\((\\mathbb{R}^+ \\to \\mathbb{R}^+) \\to (\\mathbb{R}^+)^3\\), where \\(\\mathbb{R}^+ = [0, \\infty)\\) is the space of non-negative real numbers. Define this as the \\((I_S, I_M, I_L)\\) as the LMS color space.4 Furthermore, the biochemical limit on neural firing is 1000 Hz (“Neuron Firing Rates in Humans” 2015), thus the LMS color space is bounded within a cube.\n4 This seems as close to “sense data” (Hatfield 2021) as it gets in science.Any smooth deformation of the LMS color space gives us another color space. In theory, it doesn’t matter which one we use, because the underlying color space is still the same. In practice, some color spaces are easier to use than others.\n\n\n\nDifferent depictions of the same color space. By a smooth map, we can deform the LMS color space into any shape we want, such as a cone, a cube, a cylinder, a double cone, etc. Figure from Wikimedia Commons\n\n\nIn people with only two types of cone cells, the color vision function \\(C\\) loses a dimension. For example, if that person has deuteranopia, without medium-wavelength cone cells, then they would see all colors in LMS space with the same \\((S, L)\\)-coordinates as the same.\n\n\nProjective geometry\nBecause \\(C\\) is a linear functional, and any two colors can be mixed to give a third color, LMS color space is a convex cone. On the tip of the cone is \\((0, 0, 0)\\), the color of pure darkness. It is an old experimental fact that the geometry of colors is invariant under scaling. So, if you have two lights with spectra \\(P, P'\\), such that their colors look the same/different/very different, then we make them brighter or dimmer, to \\(cP, cP'\\) where \\(c &gt; 0\\), then their colors will still look the same/different/very different.\nThus, we can factor the space of colors into two components: an apparent lightness, and an apparent chromaticity. So, if we take two dim red lights, and shine both of them on the same pane of frosted glass, the frosted glass would look lighter, but have the same chromaticity. The space of chromaticities is the space of lines passing the origin, which allows us to use projective geometry.\nThe space of all colors looks like a cone, and since each line in the cone can be represented as a point on the line, the space of all chromaticities looks like the intersection of the cone with a plane – each line is represented by its intersection with the plane. What does the space of all chromaticities look like?\nBecause any spectrum \\(I\\) is the convex sum of pure spectra\n\\[\\{I_{\\lambda}: \\lambda \\in (400 \\;\\mathrm{nm}, 700\\;\\mathrm{nm})\\},\\]\nthe space of all colors is the convex sum of all pure spectral colors\n\\[\\{C(I_{\\lambda}): \\lambda \\in (400 \\;\\mathrm{nm}, 700\\;\\mathrm{nm})\\}.\\]\nConsider a wall covered with a “pure spectral paint”, in the sense that it reflects exactly light at wavelength \\(500 \\;\\mathrm{nm}\\), and nothing else. Then, under any illumination, the color of the wall has the same chromaticity. Pure spectral colors are special colors, in the following diagram, on the edge of the cone are lines of pure spectral color, each produced by a spectrum that is concentrated at just one wavelength.\n\n\n\nThe pure spectral colors in LMS color space. The rainbow curve represents the spectrum of visible light, from violet to red. Each point on this curve corresponds to a specific wavelength of light and its unique combination of stimulations to the three types of cone cells. For each point on the spectral curve, we can draw a straight line to the origin. Each point on the line has the same color, but appears increasingly bright.\n\n\nBecause the cone shape is uninteresting, the color space is typically represented by chopping off the cone midway, producing a roughly horseshoe-shaped region on \\(\\mathbb{R}^2\\), named the gamut. Mathematically, it is the projective transform: \\[(s, m, l) := \\left(\\frac{I_S}{I_S + I_M + I_L}, \\frac{I_M}{I_S + I_M + I_L}, \\frac{I_L}{I_S + I_M + I_L} \\right)\\]\nThe curving edge of the chromaticity space are points of pure spectral colors, from pure \\(700 \\;\\mathrm{nm}\\) line on the red end, to the pure \\(400 \\;\\mathrm{nm}\\) line on the purple end. Every point inside the gamut can be mixed by two pure spectral colors. However, this is not the entirety of chromaticity space. The ray at the shortest-wavelength end (pure spectral purple) and the ray at the longest-wavelength end (pure spectral red) do not touch each other. Instead, they shoot out like two ends of a horseshoe. Chromaticity space, then, has a second, straight edge, obtained by mixing the shortest and the longest wavelength. This is the purple boundary.\n\n\n\nSchrödinger’s diagram of chromaticity space. Spektralkurve: spectral curve. Schnitt mit einer Ebene: intersection with an arbitrarily inclined plane. R: red. G: green. I: indigo. V: violet. O: origin. (Schrödinger 1920, fig. 3)\n\n\nWe can construct the chromaticity space of someone with deuteranopia by starting with the purple line, then draw one line for each point on the purple line that is perpendicular to the \\((S, L)\\)-plane in LMS color space. The deuteranopic observer sees each line of chromaticities as a single chromaticity.\nTheoretically, we can imagine creating the world’s best computer display by putting in a full-spectral display unit into each pixel. It will then be able to cover the entire gamut space. It will not only display true-life colors for humans, but also for dogs, bees, and mantis shrimps. Unfortunately, we don’t have that luxury, and computer displays are built for human-use only, with just three spectra.\nNow, if we have a pixel containing three little LED units, capable of emitting light of spectra \\(P_1, P_2, P_3\\), then we can take any convex sum, and create a mixed color. The space of all colors created by their convex sum is the convex sum of \\(C(P_1), C(P_2), C(P_3)\\), which looks like a triangular cone. Thus, the chromaticity that this pixel can display is a triangle. Every color inside the triangle can be created by mixing the three spectra, but any color outside cannot.\n\n\n\nThe triangle of displayable chromaticities for cathode-ray televisions (left) and LCDs (right). Figure from Wikimedia Commons\n\n\nWhen you print on a page, the page does not emit color, and can only acquire color by selectively reflecting light. When under a standard white light, the more ink you lavish on a page, the more saturated the color can be, but the darker it would be, because more light is absorbed. Conversely, if all the light is reflected, then it would look white. Because of this trade-off, the gamut of printable colors is even smaller.\n\n\n\nThe gamut of printable colors when placed under standard white illumination. (MacAdam 1944, plate 1)\n\n\nEvolution has created a multi-spectral display in some octopuses and chameleons. The best octopus camouflagers have 2 kinds of color organs in their skins: the chromatophores, the leucophores, and the iridocytes. Chromatophores contain pigment cells, which expand and contract by radial muscles like wheel spokes around an axis. The leucophores are roughly ideal “matte” reflectors, meaning they reflect incoming light uniformly, with little loss.\nThe iridocytes are the most exotic, and approximates our “world’s best television screen”. Specifically, they are dielectric mirrors, which reflects light at a specific wavelength. They are alternating layers of guanine crystals and cytoplasms. To change color, it simply adjusts the water content of the cytoplasm, which makes them expand or contract, changing the distance between guanine layers. More pictures are found in (Cloney and Brocco 1983).\n\n\n\nHow the iridocytes work in cephalopods. Figure from (Cossins 2013)\n\n\n\n\n\nA, B. cephalopod before and after camouflage. C. structure of cephalopod skin. D. before and after chromatophore expansion. F. before and after iridocyte turning iridescent. G. the dielectric mirrors inside an iridocyte. (Chatterjee 2022, fig. 4)\n\n\nMeanwhile, chameleons have iridocytes that operate by a different mechanism: photonic crystals (Teyssier et al. 2015).\n\n\nLinear geometry\nGrassmann, famous for originating linear algebra, studied color theory and applied linear algebra to it. Essentially, he discovered that the human color vision function \\(C\\), defined previously, is a linear function. He discovered this by color-mixing experiments, in the style of 19th century psychophysics. Considering it was 50 years before the neuron doctrine became accepted, and 100 years before cone cells were observed, he did very well.\nFor any three spectra \\(P_1, P_2, P_3\\), we can define their colors as \\(C_i := C(I_i)\\). Since \\(C\\) is a linear function, as long as \\(\\{C_1, C_2, C_3\\}\\) are linearly independent, we can represent any color as \\(C(x_1 P_1 + x_2 P_2 + x_3 P_3)\\) for some \\((x_1, x_2, x_3) \\in \\mathbb{R}^3\\).\nFor example, we can go to a scientific standard shop and buy a set of standard lamps, which when plugged into a standard plug, viewed in a standard room, at a standard distance and a standard angle, by a standard observer,5 will create a standard red, a standard green, and a standard blue. Then, using opaque to cover up parts of the lamp, and combining the lights, we can create any color \\(C(x_1 P_1 + x_2 P_2 + x_3 P_3)\\), for any \\((x_1, x_2, x_3) \\in [0, 1]^3\\). By buying more lamps, we can create all colors with \\((x_1, x_2, x_3) \\in (\\mathbb{R}^+)^3\\).\n5 Because humans are resistant to standardization, the standard observer is obtained by taking data from real observers in good health that are physiologically similar, then taking the average. The methodology resembles l’homme moyen (“the average man”) of Adolphe Quetelet, a fanatic for anthropometry. Also, the standard observer is not required to drink standard cups of tea.Here, we notice a difficulty: we can’t take a negative amount of lamp. Fortunately, we can bypass the difficulty by adding a fourth lamp, a “standard white” lamp emitting a spectrum \\(P_0\\). Then, for any other spectrum \\(I\\), there exists \\((x_1, x_2, x_3, x_4) \\in (\\mathbb{R}^+)^4\\), such that\n\\[\nC(I) + C(x_0 P_0) = C(x_1 P_1 + x_2 P_2 + x_3 P_3)\n\\]\nwhich allows us to place the color of \\(I\\) at the unique point. Of course, the choice of \\((x_1, x_2, x_3, x_4)\\) is not unique. However, since color space is linear, the sum \\(C(x_1 P_1 + x_2 P_2 + x_3 P_3 - x_0 P_0)\\) is unique. Once \\(C(P_0)\\) is itself constructed as a linear sum of \\(C(\\sum_{i=1}^3 x_{i, 0}P_i)\\), we would have located \\(C(I)\\) in color space, at\n\\[\nC(I) = \\sum_{i=1}^3 (x_i - x_0x_{i, 0}) C(P_i)\n\\]\nThis is essentially the state of the art of colorimetry in 1931, when CIE 1931 was constructed by color-mixing experiments. An observer is seated in a standard room, and sees two light sources. On the left, a to-be-measured light \\(I\\) is mixed with a standard white light \\(P_0\\), and on the right, are three standard blue, green, red lights \\(P_1, P_2, P_3\\). The observer turns the 4 knobs until two sides look indistinguishable. This was repeated for many observers, over many days, for many light sources. The result is a table with three columns, and many rows. Each row is an industrially important light source, and the three columns are the standard red, standard green, standard blue. It schematically looks like this (I made up the data):\n\n\n\ncolor\nstandard red\nstandard green\nstandard blue\n\n\n\n\nstandard red\n1.000\n0.000\n0.000\n\n\nstandard green\n0.000\n1.000\n0.000\n\n\nstandard blue\n0.000\n0.000\n1.000\n\n\nstandard white\n0.334\n0.334\n0.332\n\n\n…\n…\n…\n…\n\n\n\n\n\n\n\n\n\nTechnically\n\n\n\nTechnically, the CIE 1931 color of a spectrum \\(I\\) is a point in \\(\\mathbb{R}^3\\) defined by\n\\[\nC_{\\text{CIE 1931}}(I) := \\left(\\int I(\\lambda) \\bar r(\\lambda) d\\lambda , \\int I(\\lambda) \\bar g(\\lambda) d\\lambda , \\int I(\\lambda) \\bar b(\\lambda) d\\lambda \\right)\n\\]\nwhere \\(\\bar r, \\bar g, \\bar b\\) are “standard observer color matching functions”. They are not any real observer’s sensitivities, because they have negative values. Instead, they are roughly a linear transform of the real sensitivities \\(S_S, S_M, S_L\\), meaning CIE 1931 color space is roughly a linear transform of LMS color space.\nWhy did they go for a roughly linear transform? I know it’s confusing (it confused me), but it’s simply a temporary hack. Back then, they had no way to measure the neural spikes, so they had to infer the real sensitivities by indirect psychophysics data. And the negative values are for some kind of numerical stability considerations. Point being, it’s really not fundamental to science, but rather a 1930s technical hack.\n\n\n\n\nOpponent process\nHave you ever wondered why things seem bluer just after sunset, or under a high full moon? This is where opponent process theory and Purkinje effect comes in.\nWhile the retina might be operating with the LMS color space, it is not what gets sent to the brain. Specifically, before leaving the retina, the spikes from the 3 cone cells and the rod cell (we are finally accounting for them now!) are linearly transformed by 3 paired-kinds of neurons within the retina, before sending down the optic nerve. Greatly simplified, the linear transform is:\n\\[\n\\begin{cases}\n  I_{\\text{Red-Green}} &= I_L - I_M \\\\\n  I_{\\text{Yellow-Blue}} &= I_L + I_M - 2 I_S \\\\\n  I_{\\text{Brightness}} &= 2I_L + I_M + 0.05 I_S + I_R\n\\end{cases}\n\\]\n\n\n\n(Hunt and Pointer 2011, fig. 1.4)\n\n\nIn words, the Red-Green-pair of neurons take the long-wavelength (reddish) cone cells, and subtract away the medium-wavelength (greenish) cone cells. If the result is positive, then the positive half of the pair sends down a signal at the rate of \\(I_L - I_M\\), otherwise, the negative half of the pair sends down a signal at the rate of \\(-(I_L - I_M)\\). This linear transform, while mathematically equivalent (as long as the rod cells don’t appear) to LMS space, allows the optic nerves to carry more information in Homo Sapiens’ natural habitat (Buchsbaum, Gottschalk, and Barlow 1997).\nWhen the light level is around \\(0.5 \\;\\mathrm{lux}\\), which corresponds to twilight, or a full high moon, both the rod cells and the cone cells are active (Dominy and Melin 2020).\nSo, let us look at the linear transform\n\\[\n\\begin{cases}\n  I_{\\text{Red-Green}} &= I_L - I_M \\\\\n  I_{\\text{Yellow-Blue}} &= I_L + I_M - 2 I_S \\\\\n  I_{\\text{Brightness}} &= 2I_L + I_M + 0.05 I_S + I_R\n\\end{cases}\n\\]\nLet’s pretend we are the brain, interpreting the signals sent down the optic nerves. Suppose the retina secretly increases \\(I_R\\) by a small amount of \\(\\Delta I\\), but we don’t know that. How would we interpret it? We would interpret it as a color in LMS space with color\n\\[(I_S + \\Delta I', I_M + \\Delta I', I_L + \\Delta I')\\]\nwhere \\(2.05\\Delta I' = \\Delta I\\). That is, it looks as if each type of cone cell has increased firing rate by the same amount. Looking at the sensitivity curve, this effect can be created by shifting the spectrum to the shorter wavelength, then increase its power slightly. Thus, things look bluer.\n\n\n\nPurkinje effect illustrated with a flower. As the lighting condition dims, the entire scene shifts more to the bluish shade. At low enough lighting, all cone cells deactivate, and the entire scene becomes monochromatic. Figure modified from Wikimedia Commons\n\n\n\n\nRiemannian geometry\n\nMagnitude-notions are only possible where there is an antecedent general notion which admits of different specializations… the only simple notions whose specializations form a multiply extended manifoldness are the positions of perceived objects and colors. More frequent occasions for the creation and development of these notions occur first in the higher mathematic.\nRiemann’s Habilitation dissertation, 1854 (Riemann 2016)\n\nNow that we have a space of colors, how do we measure distances in it? Some colors are close, while some colors are far apart. How do we quantify it? This question occupied the minds of some famous scientists, including Riemann, Grassmann, Helmholtz, and Schrödinger (Pavlidis 2021).\nIn 1920, Schrödinger (more famous for his other equation) hypothesized that color space has a Riemannian geometry, and the subjective difference between two colors is the geodesic distance between the two points in color space (Schrödinger 1920). This is the foundation of modern colorimetry (Niall 2017). Over the years, there had been a mess of increasingly detailed theoretical models for the Riemannian metric of color space, of interest only to specialists – see (Wyszecki and Stiles 1982, chap. 8.4) for a review. Here, we bypass most of the theory by experimental data.\nGiven two spectra \\(I, I'\\), if their colors \\(C(I), C(I')\\) are close enough, an observer would judge them as equal. This is the concept of “just noticeable difference” (JND), a foundational concept of psychophysics.6 In general, the JND method goes like this:\n6 One can get a good feel for the JND by playing the Color Game. In case this fails in the future, try the archived link.\nFix one stimulus \\(S\\), and vary the other stimulus \\(S'\\). The prior probability that \\(S = S'\\) is \\(1/2\\).\nPresent both \\(S, S'\\) to the observer.\nThe observer judges whether they are the same or different.\nRepeat many times.\nIf, when truly \\(S' = S\\), the observer judges that they are the same with probability \\(p_0\\), then the JND point is the point where the observer judges that \\(S' = S\\) with probability \\(p_0/2\\).\n\nIn the original experiments, MacAdam fixed one spectrum \\(I\\), and varied the other spectrum \\(I'\\) on a curve that passes \\(I\\). He repeated the JND measurement along many curves across many spectra, and found that around each spectrum, the JND points make up a rough ellipsoid.\n\n\n\nThe JND of a single observer around a single color, when approached from 14 different directions. The JND points fall roughly on an ellipse. (Wyszecki and Stiles 1982, fig. 1(5.4.1))\n\n\nIf the JND measurement is binary classification in color space, then what is real-valued regression in color space? Answer: color matching experiment.\nSpecifically, suppose we fix \\(I\\), and let the observer turn a knob that varies \\(I'\\) along a curve passing \\(I\\), then we would find that \\(I'\\) is normally distributed centered upon \\(I\\). Perform the experiment with 3 knobs, and we would obtain an ellipsoidal cluster. The ellipsoids of \\(1\\sigma\\) are the MacAdam ellipsoids. As ellipsoids are very hard to draw, we typically only see 2D slices of them – the MacAdam ellipses.\n\n\n\nThe color matching experiment data of a single observer around a single color. The points are projected from 3D space to three orthogonal views. The ellipsoid is the ellipsoid of \\(3\\sigma\\). (Wyszecki and Stiles 1982, fig. 1(5.4.3))\n\n\n\n\n\n\n\n\nConjecture: perceived lightness and hues are totally geodesic foliations\n\n\n\nGiven two colors \\(C_0, C_1\\), we can construct the geodesic curve between them as the shortest sequence of colors \\(C_a, C_b, C_c, \\dots\\), such that \\(C_0, C_a\\) are JND, and \\(C_a, C_b\\) are also JND, etc. It sounds reasonable in my head that, if \\(C_0, C_1\\) have the same perceived lightness, then the geodesic connecting them should all have the same perceived lightness, because it seems like we would be wasting some precious JND on “jumping up in lightness, only to jump down again”. Similarly, if \\(C_0, C_1\\) have the same perceived hue, then I guess the geodesic through them would stay along the same perceived hue.\nIf this is true, then we can construct two families of foliations in color space, one for equal-lightness surfaces, and one for equal-hue surfaces. Each surface is a totally geodesic foliation (Johnson and Whitt 1980), meaning that each geodesic within a foliation is also a geodesic in the total color space.\nHowever, this definitely isn’t true for perceived saturation, as the shortest path between slightly saturated red and slightly saturated green (opposite of red) goes through perfect gray, so who knows whether this conjecture is true or not for perceived lightness or hues?\n\n\nThough JND and color matching are two different methods, they are both using people as statistical detectors, and it stands to reason that they should measure the same thing. Indeed, the ellipsoids of JND are roughly the \\(3\\sigma\\) MacAdam ellipsoids (Wyszecki and Stiles 1982, sec. 5.4).\n\n\n\nMacAdam ellipses plotted on the CIE 1931 \\(xy\\)-diagram, 10× actual size. Figure from Wikimedia Commons.\n\n\nGiven the Tissot’s indicatrix, it is natural to try to draw a distortion-less map of earth, where all Tissot ellipses are equally-sized circles. This is impossible, and Gauss knew exactly why: earth has positive gaussian curvature, but a flat sheet of paper has zero gaussian curvature.\n\n\n\nTissot’s indicatrix on Behrmann projection. Figure from Wikimedia Commons\n\n\nGiven the MacAdam ellipses, it is natural to try to draw a distortion-less map of color space. This is impossible, for the same reason: color space has nonzero curvature. It was already known to MacAdam in the 1940s that his experimental data shows color space has significant curvature.\n\n\n\nA paper model of a 2D subspace of the color space – the space of colors with unit subjective brightness. The metric on the paper model faithfully matches the metric implied by MacAdam ellipse. We can see the curvature. (MacAdam 1944, fig. 5)\n\n\n\n\n\n\n\n\nConjecture: color space is not conformally flat\n\n\n\nIs it possible to at least stretch the MacAdam ellipses into spheres, even though they aren’t of the same radius? That is, is color space conformally flat? For example, in Mercator’s projection, the Tissot ellipses are indeed circular, though they become larger near the poles, so earth is conformally flat.\nHowever, by Liouville’s theorem, conformal flatness is very stringent at 3 dimensions and above, so my conjecture is that color space is not conformally flat. Proof sketch: download the metric tensor from CIE, and check its Cotton tensor is nonzero (at statistical significance).\n\n\n\n\n\nA nonlinear map of CIE 1931 \\(xy\\)-graph designed to make MacAdam ellipses look roughly circular. (Wyszecki and Stiles 1982, fig. 4(5.4.1))\n\n\nCIELAB color space is a smooth mapping from CIE 1931 color space to \\(\\mathbb{R}^3\\), such that the MacAdam ellipses are stretched spherical enough for practical purposes.\n\n\n\nAll visible colors, plotted in CIELAB color space. Figure from Wikimedia Commons\n\n\n\n\nInformation geometry\nImagine a hiker navigating a mountain path equipped only with an altimeter and a detailed altitude map. The hiker’s ability to pinpoint their location on the map relies on sensing altitude changes. In regions where the terrain is steep (representing high sensitivity), even a small step forward (change in stimulus intensity) will register a noticeable altitude change on the altimeter (change in perceived sensation). This allows for precise localization – a small JND. However, along flatter sections of the trail (low sensitivity), the hiker might need to traverse a longer distance to observe a meaningful altitude difference, leading to a larger JND and greater uncertainty about their position on the map.\nSimilarly, as we move around in color space, we may distinguish colors by the photoreceptor responses, which can be inferred from the sensitivity curves \\(S_S, S_M, S_L\\). That is, we can reduce Riemannian metric to information geometry. Working this out in detail, (da Fonseca and Samengo 2016) showed that the Riemannian metric in color space is roughly the same (“explains 87% variance”) as the Fisher information metric.\n\n\n\nThe ellipses measured by MacAdam (green) vs ellipses predicted by information theory (red). (da Fonseca and Samengo 2016, fig. 8b)\n\n\nIn the same vein, people have argued for centuries about why certain colors are perceived as “pure” or “primary” (white, black, red, blue, green, etc), while others are “mixed” or “derived” from the primary colors. (MacEvoy 2015) argues that the primary colors are “landmarks” in the geometry of color space, much like how on a map, the peaks and troughs are local maxima of gaussian curvature, and the mountain passes are the local minima, or how on a spacetime, a black hole singularity is the point where the Kretschmann scalar is infinite. Intuitively, we can see this on the CIELAB color solid. The top-most point is white, the bottom-most color is violet, and you can just see yellow at another point behind the back, etc.\n\n\n\n\n\n\nBeyond Riemannian geometry\n\n\n\nWhile color space is locally Riemannian, this is not so over longer distances. That is, once we are measuring the subjective distances between pairs of far-different colors, the data no longer behave like distances on a curved 3D space. (Bujack et al. 2022) reported that there is “diminishing returns” in color distances.\nIndeed, this non-Riemannian geometry has been known for a while. CIE in 1994 proposed a color difference, \\(\\Delta E\\), that is not symmetric. That is, if we ask a subject “How far is color 1 from color 2?” and then ask the opposite direction, we usually get a different numerical answer. This reminds me of information-geometric divergence, which is also not symmetric. I cannot find anyone who has studied this in detail, but it ought to interest the information geometers."
  },
  {
    "objectID": "essays/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-feeling",
    "href": "essays/posts/hole-argument-inverted-qualia/index.html#the-geometry-of-feeling",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The geometry of feeling",
    "text": "The geometry of feeling\n\nEasy, hard, meta\nDavid Chalmers proposed three problems of consciousness.\nThe easy problem is the scientific problem of human perception, cognition, and motor control: how memories work, how vision recognizes objects, etc. By “easy”, Chalmers was not dismissing them as “intellectually easy”, but that they are within the paradigm of science as currently understood. They are about as easy as colonizing Mars or curing cancer.\nThe hard problem is the easy problem, but with something extra that seems impossible to even fit into a scientific system. What that something extra is, philosophers are unable to say, but they typically give it the name of “qualia”, “experience”, “phenomenal awareness”, etc.\n\nWhat makes the hard problem hard and almost unique is that it goes beyond problems about the performance of functions. To see this, note that even when we have explained the performance of all the cognitive and behavioral functions in the vicinity of experience—perceptual discrimination, categorization, internal access, verbal report–there may still remain a further unanswered question: Why is the performance of these functions accompanied by experience?\n(D. J. Chalmers 1995)\n\nWhile decades of science have made good progress on the easy problem, centuries of philosophical disputations have not made progress on the hard problem. To bypass the impasse, Chalmers proposed the meta problem: Why is the hard problem a problem?\n\nWhat exactly are the problem data that need explaining? They can be construed as verbal reports (my saying ‘Consciousness is hard to explain’), as judgments (my forming the judgment that consciousness is hard to explain), or as dispositions to make these reports and judgments. Verbal reports are perhaps the most objective data here, but they are also a relatively superficial expression of an underlying state that is really what we want to explain. So I will generally focus on dispositions to make verbal reports and judgments as what we want to explain.\n(D. Chalmers 2018)\n\nThe easy problem of consciousness, being part of the domain of science, excited little philosophical attention, while the hard and the meta problems excited a vast discourse. It is not within my power to review the literature, though it might be within my power to solve them.\nLet’s consider an analogy. The easy problem of biology would be: How does biological machines work? The hard problem: Why is the performance of these functions accompanied by life? The meta problem: What kind of cognition do people have, such that they can see a machine performing all the motions of life, and yet still call it “lifeless”? This analogy was considered by Dennett and rejected by Chalmers as a disanalogy. (Garrett 2006)\nIn John Searle’s Chinese room story, a man who knows nothing of Chinese, by executing an algorithm with pen and paper, could converse in Chinese writing. Many, including Searle, thought that the Chinese room does not really understand Chinese. This gives us another analogy.\nThe easy problem of Chinese: What algorithms can converse in Chinese text? The hard problem of Chinese: Why is the performance of Chinese-speaking in a Chinese-speaker accompanied by understanding? The meta problem: What kind of cognition do people have, such that they say the Chinese room “lacks understanding”?\nIt is my aim to dissolve the hard problem of consciousness by showing that it has no explanandum. The hard problem’s explanandum is the non-reducible form of consciousness, the kind of “qualia” or “soul” which cannot be reduced to scientific explanation as currently practiced in physics using concepts currently existing in physics. I claim that non-reducible consciousness does not exist, so the hard problem is meaningless, and must be un-asked, instead of answered.\n\n\nInverted qualia\n\n\n\nA normal-colored photo and an inverted version of it. Figure from (Byrne 2004, fig. 4)\n\n\nThe inverted qualia thought experiment has been used, like the p-zombie, in a whole host of arguments involving consciousness and the qualia. We consider the case involving functionalism, which is currently the most fashionable among cognitive psychologists and computer scientists. Other variants are reviewed in (Byrne 2004).\nAccording to functionalism, mental states are best understood as functional states, that is, mathematical functions that map perceptual inputs to behavioral outputs. It’s the intricate web of causal relations that constitutes a mental state, rather than the specific physical makeup realizing those relations.\nIn the anti-functionalism case, we consider two individuals, “Invert” and “Nonvert”, who are functionally identical. They receive the same visual input (a tomato), undergo the same internal processing, and produce the same behavioral outputs (saying “that’s a red tomato”). However, their subjective experiences – their qualia – differ drastically. Where Nonvert experiences red-qualia, Invert experiences green-qualia (or another color-qualia entirely). They outwardly behave in the same way, and all functional measurements, from verbal reporting, psychological experiments, to MRI scanning, all find them the same, and yet the qualia of any color the Invert sees is rotated 180 degrees compared to that of the Nonvert.\nFormally, this is the inverted qualia argument against consciousness functionalism:\n\nThe following spectrum inversion scenario is possible: Invert and Nonvert are functionally alike, but experiences different color-qualia.\nThus, mental experience does not supervene on functional organization.\nThus, functionalism is false.\n\nSimilarly, the shifted qualia thought experiment does not completely invert color space, but simply shift it, so that dark red looks slightly bluer, etc.\n\n\nOther inversions\nThe color-qualia inversion thought experiment may be a modern invention, but it had several illustrious ancestors.\nIn his Book of Optics, Alhazen rejected the theory that the eye works like a camera obscura, as it would create an inverted image. He also argued that every part of the eye should be close to geometric perfection, since otherwise they would create “monstrous” images like a funhouse mirror. Similarly, da Vinci knew that the pupil acts as a camera obscura, but developed no less than 8 different hypothetical mechanisms inside the eye to re-invert the image again, so the image lands right-side-up on the retina. (Eastwood 1986)\n\n\n\nOne of Leonardo da Vinci’s drawings comparing the eye to a camera obscura. From Codex Atlanticus (1490-1495). Note the double-inversion to make sure the image lands right-side-up on the retina. Figure from Wikimedia Commons.\n\n\nIn his famed Treatise of Man, Descartes solved the problem according to functionalism: It is not an issue if the image on the retina is “upside-down”. It only matters if the retina is wired to the brain in a regular way, such that the neural control of motion by sight works. Informally speaking, it doesn’t matter if the wires are wired upside-down, as long as the wires are not crossed.\n\n\n\nIllustration from Descartes’ Treatise of Man, showing how the eye can guide the arm despite an “upside down” image. Suppose the eye sees the hand is pointing at the middle of the arrow, and then the brain needs to have the finger pointing at the tail of the arrow, then the control circuit goes from the retinal-C point, to the optical-nerve-C point, to the brain-C point, and finally to the arm abductor muscle.\n\n\nIn Leibniz’s third paper of the Leibniz–Clarke correspondence, Leibniz proposed the “inverted space” thought experiment:7 Suppose at the moment of creation, God were to switch East and West, then nothing would act different. Since God is reasonable, yea, the most reasonable Being possible, He must have created the world according to the principle of sufficient reason. Every choice must have full justification, and not a single one was done arbitrarily. Ergo, God never had to make this choice in the first place, meaning that, contra Sir Newton, space is relational, not absolute.\n7 \n… supposing Space to be Something in it self, besides the Order of Bodies among themselves, that ’tis impossible there should be a Reason, why God, preserving the same Situations of Bodies among themselves, should have placed them in Space after one certain particular manner, and not otherwise; why every thing was not placed the quite contrary way, for instance, by changing East into West.\n(Clarke 1717)\n\n8 The original theory as proposed by Germán Beritens, before it became popularized, is not subject to this fallacy, because astimgatism does not actually stretch images uniformly across the retina. However, dealing with the original theory would require a lot more geometric optics than we have here. (Firestone 2013)The Spanish painter El Greco famously painted people very tall, as if they are stretched vertically. In popular art history, this is often explained by saying that he might have had astimgatism, which stretches images vertically on the retina. This is the same error made by Alhazen.8 Despite this, the “El Greco fallacy” is still committed by psychologists of perception. (Firestone and Scholl 2014, 2016)\n\n\nThe broken symmetry counterargument\nA sophisticated objection to inverted qualia, based on color science, states that since color space has no nontrivial symmetries, the thought experiment is impossible. For example, saturated yellow does not merely look different from saturated red, but also looks brighter. In this view, “simply yellow” is not simply yellow. A point in color space is not simply a point. It is already inherently structured. Yellow is the brightest of all saturated colors, while violet is the dimmest, etc. (Hilbert and Kalderon 2000) argued that every possible quality space must be asymmetrical, in the sense that the only automorphism is the identity map, of \\(x \\mapsto x\\).\nThis appears to me an objection that is too strong, as there really do exist quality spaces that are symmetric. In humans, left and right are symmetric. And if there exists someone with a color space that is mirror-symmetric across some plane, then the following kind of experiment might be possible:\n\n“They are different.” [points at this patch and that patch]\nThe experimenter takes both patches away, then brings back one patch, and asks “Is this the left patch or the right patch?”\n“I don’t know.”\n\nThis is similar to the case of dark phenomena of JND color pairs (to be detailed below), but it is different in that, unlike JND color pairs, there would be color patches that are as different as left hand from right, as different as red and green, but they simply cannot see color-in-itself, only color-between-them. It might not be strange for them, since they were born this way, but it would be strange for normal people. However, the strangeness is conceivable, perhaps even unremarkable. When I was a kid, left and right was like that for a long time. I suspect that some people can distinguish left and right only after they acquire some asymmetric scar, such as a burn-mark on one hand.\nIf we look beyond the human umwelt, then this may not be just a thought experiment, as there are some highly symmetric quality spaces in non-human animals.\nLight, being electromagnetic waves, can be polarized. The space of possible polarizations is isomorphic to a ball, the Poincare ball. The mantis shrimp species Gonodactylus smithii can detect the polarization of light over the entire 3-dimensional Poincaré ball (Kleinlogel and White 2008). It performs this by building 3 kinds of ommatidia, each specialized for two kinds of polarization. One is specialized for the horizontal-vertical, one for the diagonal-antidiagonal, and one for the clockwise-anticlockwise.\n\n\n\nPolarization states on the Poincaré ball. Figure from Wikimedia Commons.\n\n\nNow, a Gonodactylus philosopher might propose the following inverted qualia problem: What if my qualia on the Poincaré ball is inverted compared to yours? That is, what if when you see a horizontally polarized light, you feel the same way as I see a vertically polarized light, and similarly across all of the ball? We can even imagine more exotic reflections, such as one that reflects across the \\((0.3, 0.3, 0.9)\\) direction, etc.\n\n\nGauge freedom in qualia-space\nAt this point, we have a formal analogy between the thought experiments concerning qualia and those concerning spacetime. Explicitly:\n\n\n\ncolor\nspacetime\n\n\n\n\nshifted qualia\nhole argument\n\n\ninverted qualia\nLeibniz inversion\n\n\ncolor space\nspacetime\n\n\nqualia of a color\na point in spacetime\n\n\ngeometry of color space\ngeometry of spacetime\n\n\nqualia realism\nmanifold substantialism\n\n\ncolor space is but its geometry\ngauge freedom\n\n\n\nIn manifold substantialism, points on the spacetime manifold exist in themselves, and one can ask what a point on the spacetime manifold is, independent of the metric or the energy-momentum field, and what happens if the fields are stretched differently over the same manifold. In qualia realism, points in the space of color exist in themselves, and one can ask what happens if the same photons are mapped to different points in color space.\nJND in color perception possibly shows that we see metric, not colors themselves.\nPoints in spacetime do not exist in themselves, but reduces to the interplay of metric and energy-momentum tensor fields up to gauge freedom. Similarly, qualias do not exist in themselves, but reduces to the interplay between brain, the rest of the body, and the outside world. Arguments in support of gauge freedom in general relativity can be directly translated to arguments in support of functionalism in consciousness.\nFurthermore, if the phenomenology of color is but the metric geometry of color space, then we have not only dispelled the inverted qualia problem, but also have dispelled the problem of other minds. We would be able to scientifically answer Nagel’s question of what it is like to be a bat. The hard problem of being a bat dissolves, leaving the easy problem. It simply requires us to construct the geometry of a bat’s color space from its neurophysiology. We may not even need to perform psychophysical experiments, such as asking a bat to make JND distinctions (though psychophysics is possible with animal subjects, see for example (Jacobs 1982)). We may construct it by information-geometric methods, like how MacAdam ellipses can be constructed by the light sensitivity curves of retinal cells.\n\n\nDark phenomena\n\nNeurophenomenology is possible; phenomenology is impossible.\n(Metzinger 2004, 83)\n\nEverything I see, I know that I see. Everything that I hear, I know that I hear. Everything that I think, I know that I think. What could be clearer? Descartes based his entire philosophy on these kinds of self-evident truths, and these are still the starting points of many modern philosophies of the mind and consciousness.\nThe concept of qualia attempts to formalize this self-evident truths. There is no generally agreed-on definition of qualia, but according to the original proposer, David Lewis, a qualia must contain several properties (Metzinger 2004, 68). Of those, we consider the first one:\n\nSubjective identity criteria are available, by which we can introspectively recognize their transtemporal identity.\n\nWhile the concept of qualia might seem self-evident, such self-evident truths can be questioned. In blindsight, I see things that I don’t know that I see. In Anton’s syndrome, I don’t see things, yet I think that I see.9 In Cotard’s delusion, I live yet I think that I am dead.\n9 There was a philosopher who had taken Anton’s Syndrome very seriously, but in the opposite direction, in the spirit of one man’s modus ponens is another man’s modus tollens:\n\nI still vividly remember one heated debate at an interdisciplinary conference in Germany a number of years ago, at which a philosopher insisted, in the presence of eminent neuropsychologists, that Anton’s syndrome does not exist because a priori it cannot exist.\n(Metzinger 2004, 235)\n\nAs a mathematician, I often know things without knowing how I know. When doing mental arithmetics, usually I do it both ways. One algorithm, operating consciously, goes from the highest digit down; the other algorithm, operating unconsciously, goes from the lowest digit up. As I consciously grind out digits from one end, digits simply “emerge” out of the other end. Like two teams digging a tunnel, they finally meet in the middle; the digits ripple-carry; the mouth vocalizes the final answer.\nDuring deep contemplations of high-dimensional geometric objects, my self-awareness is turned down to a whimper, dimly illuminated by the sparks and piezoluminescence of vast gears and pulleys turning in the dark mill of the brain,10 where the light of consciousness can never penetrate. A few times, I came back to consciousness on the carpet, not knowing how I got there, but with a clear feeling that an answer is close. Then I find the answer – or not. The non-conscious parts of the brain make plenty of mistakes too.\n10 I really wanted to write “dark Satanic mills of the brain”, but that would be too much purple prose.Consider a pair of pure spectral lights at \\(550 \\;\\mathrm{nm}\\) and \\(554 \\;\\mathrm{nm}\\). For an observer with good vision, they are separated by a JND, so if the observer sees two patches of light shining on two plates of frosted glass placed close to each other, then the observer can just barely see that they are not the same color. However, as soon as the two lights are turned off, the difference disappears. The observer cannot recall one as “green-550” and the other as “green-554”. Both would be recalled as “kind of green”. Without literally holding up a patch of green-550 or green-554 as a ready reference, the observer cannot tell if a single patch of light is closer to green-550 or green-554. The observer cannot tune a laser by sight so that its color matches green-550 rather than green-554.\nIn short, people can distinguish many more colors when the colors are ready-present at eyes, than they can when the colors must be recalled from the brain. Indeed, according to experiments in the 1950s, though people can distinguish around 150 pure spectral colors in the sense of JND, they can identify only around 15 pure spectral colors in the absolute sense. (Halsey and Chapanis 1951)\nAccording to Thomas Metzinger (Metzinger 2004, chap. 2.4), such “dark phenomena” (Roden 2015, chap. 4) empirically disproves qualia as defined by Lewis, because subjective identity criteria are not available, since we cannot introspectively recognize the transtemporal identity of green-550. In plain language: if green-550 feels like something, it is not a something that we can store now and recall later. This might still be qualia in some sense, but not in the Lewis kind of sense, and if philosophers want to ground a world-model on the qualias, they might find their ontological castle built on quicksand.\nA dark phenomenon rises from dust, does its job, then falls back to dust. It cannot be interrogated, redirected, paused, vocalized, remembered, threatened, or inspected. In this way, green-550 and green-554 are dark phenomena. They are real phenomena and have real mental functions, but they cannot be captured or interrogated. A dark phenomenon, such as green-550, is an information object that only flows along hardwired circuits. Some executive part of the brain might issue a command “Store this phenomenon in long-term memory!” or “Reroute this phenomenon for verbal report!”, but such commands are futile. The green-550 and green-554 phenomena are sent to some visual comparison module, then discarded. The visual comparison module might output a bright phenomenon “They are different.”, but this bright phenomenon is merely a tiny derivative of the dark phenomena that came before.11\n11 If a thought enters the mind but nobody talks about it, does it make a sound?What dark phenomena do resemble, however, are the gauge-freedom conception of spacetime. The metric in color space is just like the metric in spacetime. Just like how the spacetime manifold is defined only up to gauge freedom, the color space is defined only up to diffeomorphism. Just like how the hole argument is dispelled by the gauge freedom viewpoint, the inverted or shifted qualia argument is dispelled if there is no ontological difference between diffeomorphically equivalent color spaces.\nContinuing the same argument, absolute color identification is like identifying large-scale structures of the universe. The point of perfect gray is the point furthest from the edge of chromaticity space. The grayscale axis is the line of perfect grays. The ring of saturated colors is the region close to the edge of chromaticity space. The island of absolute yellow is the part in the ring of saturated colors that is furthest away from black, etc.\nAs a further point, since dark phenomena has no transtemporal identity, they certainly have no intersubjective identity, meaning that the problem of “Is my qualia of green-550 the same as your qualia of green-550?” is a pseudoproblem with no meaning. This is analogous to how it is meaningless to ask whether two points in two spacetime manifolds are the “same” point, because spacetime points have no trans-manifold identity. It is meaningful only to ask whether there exists a metric-preserving diffeomorphism between the two manifolds, such that one point is mapped to the other. This is analogous to asking whether the green-550 stimulus produces in you and me biological responses that has a one-to-one correspondence. That is, qualia makes sense only in a purely functionalist way, revealing that the hard problem is based on a false premise."
  },
  {
    "objectID": "essays/posts/hole-argument-inverted-qualia/index.html#the-future-of-an-illusion",
    "href": "essays/posts/hole-argument-inverted-qualia/index.html#the-future-of-an-illusion",
    "title": "Hole Argument and Inverted Qualia",
    "section": "The Future of an Illusion",
    "text": "The Future of an Illusion\n\n… what we must say is that [Folk Psychology] suffers explanatory failures on an epic scale, that it has been stagnant for at least 25 centuries, and that its categories appear (so far) to be incommensurable with or orthogonal to the categories of the background physical science whose long-term claim to explain human behavior seems undeniable. Any theory that meets this description must be allowed a serious candidate for outright elimination.\n(Churchland 1981)\n\n\nIf commonsense intentional psychology really were to collapse, that would be, beyond comparison, the greatest intellectual catastrophe in the history of our species; if we’re that wrong about the mind, then that’s the wrongest we’ve ever been about anything. The collapse of the supernatural, for example, didn’t compare; theism never came close to being as intimately involved in our thought and practice – especially our practice – as belief/desire explanation is.\n(Fodor 1987, vii)\n\nThe above, I hope, constitutes a good argument for dissolving the hard problem of consciousness as a pseudoproblem, much as the hole argument of general relativity is dissolved as a pseudoproblem. However, even if the hard problem resists philosophical dissolution by the gauge freedom argument, it might still be dissolved practically, since the meta problem has a purely material basis, and thus cannot resist materialist invasions in practice, because damn the metaphysics.\n\nDual-process theory\nWhile some argue that the meta problem is just the hard problem in disguise, and therefore is not solvable by purely materialist methods, the opinion seems to be a minority. In the paper that proposed it (D. Chalmers 2018), Chalmers discussed no less than 12 possible solutions to the meta problem that are within the materialist paradigm of modern science. Here I describe one that gives a flavor of how things might go.\nIn cognitive psychology, there are many dual-process theories for explaining many cognitive processes. A theory is a dual-process theory if it follows the dual-process template. That is, if it models a cognitive process with an algorithm that has two parts, termed System 1 and System 2. System 1 is characterized by automatic, fast, and intuitive processing, while System 2 is deliberative, slower, and more analytical.\n(Fiala, Arico, and Nichols 2012) proposed a dual-process theory for the meta problem of consciousness. According to them, people recognize something as an agent or not by a dual process. This is evolutionarily important for ancestral humans, because detecting whether that shaking in the grass is caused by an animal or not could be a life-and-death decision.\nSystem 1 for detecting agency uses the following heuristics: eye-like shapes on a head-like bump, unpredictable environmental reactions, and self-initiated movement beyond mere inertia. System 2 for detecting agency involves rational deliberation, theory application, and conscious reasoning. These processes are engaged when evaluating complex concepts, such as brain-based theories of consciousness.\nNow, the meta problem of consciousness occurs when one attributes agency to the brain. The brain, lacking visible features like eyes, appearing inert within the skull, and not exhibiting self-propelled motion, is not an agent according to System 1. The persistent conflict between System 1 and System 2 is verbalized into the hard problem of consciousness: System 2 admits that the brain is enough for agency, while System 1 insists that it is still lacking a certain something, a je ne sais quoi, be it “consciousness”, “qualia”, or “experience”.\n\n\nMathematical illusion\nAs another example, we present an answer to the meta problem of general relativity – that it is an accident of mathematical history. This answer to the meta problem of manifold substantialism applies equally well to the spacetime-manifold and the qualia-manifold, so the reader may, as a homework exercise, read the following text and mentally translate it to an answer to the meta problem of consciousness.\nTraditionally, general relativity is written as a theory about a triple of mathematical objects: \\((\\mathcal M, g, T)\\). Looking at it, it seems obvious that there are three things: an underlying manifold \\(\\mathcal M\\), and two tensor fields \\(g, T\\) “stretched over it”. This then forces into our intuition the problem of interpreting each of them separately. In particular, what is the ontological status of the spacetime-in-itself \\(\\mathcal M\\) is, independently of \\(g, T\\)? However, this is an artifact of mathematical symbolism.\nHistorically, differential geometry developed in the 19th century to solve certain practical problems, and those practical problems left fossilized remains in the symbols we still use here in the 21th century. In Thomas Kuhn’s philosophy, scientists learn their craft not by theory alone, but also by being familiar with examples. The network of the most important examples is a Kuhnian paradigm.\nFor differential geometry, its paradigm-examples are the continuum mechanics of deformable surfaces, and practical geodesy and surveying. In continuum mechanics, there is literally an unchanging substance (a rubber sheet) carrying changeable properties (the strain field, the stress field, etc). In practical geodesy, there is literally an unchanging substance (a map-paper) carrying a changeable property (the metric field, which you may picture as the infinitesimal ellipses of equal radius).\nThe abstracted paradigm is then a substance-property duality, where the substance is a “raw” structure, upon which we may “decorate” with properties. However, it is quite possible to regard \\(\\mathcal M\\) as a derivative to the original \\((\\mathcal M, g, T)\\). Concretely, we would begin with a 4-dimensional manifold with \\(g, T\\) “baked in”, then quotient out these structures.\nAs an analogy, consider the plain old 4-dimensional space \\(\\mathbb{R}^4\\). Different mathematicians can regard it in different ways. It can be regarded as merely a topological space (for topologists), or as a smooth manifold (for analysts), or as a vector space (for algebraists), and so on. A topologist might pay a visit to an algebraist’s home and see their homegrown \\(\\mathbb{R}^4\\) and say “That’s cute. You’ve decorated them with a linear structure!”, and an algebraist might reply, “Well, let’s see yours.”. So they went to the topologist’s home, and the algebraist exclaimed, “Haha, that’s funny. You have installed funhouse mirrors to quotient out the linear equivalences.”\nThat is, the algebraist says that their version of \\(\\mathbb{R}^4\\) is the most natural, and the topologist must justify why they have taken a perfectly good \\(\\mathbb{R}^4\\) and imposed a quotient-structure, pretending that certain differences no longer makes a difference. The topologist says that, no, their version of \\(\\mathbb{R}^4\\) is the most natural, and the algebraist must justify why they have taken a perfectly good \\(\\mathbb{R}^4\\) and imposed a linear-structure, pretending that certain \\(\\mathbb{R}^4 \\to \\mathbb{R}^4\\) maps (“linear maps”) are more special than others.\n… And they would both be right! Because no version of \\(\\mathbb{R}^4\\) is more fundamental, because there is no true “substance” of \\(\\mathbb{R}^4\\).\nMore abstractly, consider another example. We can construct the 3-dimensional real-space \\(\\mathbb{R}^3\\) as the 3-dimensional affine space \\(\\mathbb A^3\\) decorated with an origin point \\(O\\). Then, we can write \\(\\mathbb{R}^3 := (\\mathbb A^3, O)\\), and then we might be tempted to ask “What is the affine-space-in-itself, before it received the origin point?”. However, we can equally well start with \\(\\mathbb{R}^3\\), and then quotient out \\(T\\), the group of translations, to obtain \\(\\mathbb A^3 := \\mathbb{R}^3 / T\\).\nIn short, the division between substance and property is not fundamental, but conventional. When one takes the conventional division of \\((\\mathcal M, g, T)\\) as three things, one ends up mistaking \\(\\mathcal M\\) as a substance with independent existence, and thus falls into the hole argument and the hard problem of general relativity.\nThis solves its meta problem, and dissolves its hard problem.\n\n\n\nThe stubbornly persistent illusion. Figure from xkcd: Teaching Physics.\n\n\n\n\nPhilosophy with a deadline\n\nLevel-1 or world space is an anthropomorphically scaled, predominantly vision-configured, massively multi-slotted reality system that is obsolescing very rapidly.\nGarbage time is running out.\nCan what is playing you make it to level-2?\n— Nick Land, Meltdown\n\nIn 2009, Chalmers sent out questionnaires to some fellow professional philosophers – largely analytical – asking them their stance on each of over 30 major problems of philosophy. The results showed that the philosophers could hardly agree on anything, and only two problems enjoyed a rough consensus (over 70% agreement): most philosophers agreed that the external world exists, and that God does not. Chalmers noted that, though the arguments become progressively more complex and precise, the philosophers do not actually get convinced by them, and there has been little convergence towards any consensus over time, other than the question of God. (D. J. Chalmers 2015)\n\n… none of these methods have led to recent convergence on answers to the big questions of philosophy. In the wake of each of these methods, philosophical disagreement is as rife as it ever was. Even within a tradition, there are few cases where the big questions are regarded as settled. Instead, these methods have led us again to more sophisticated versions of old disagreements.\n(D. J. Chalmers 2015)\n\nWhile the existence of the external world had always commanded a near-consensus, the question of God is interesting, as it has a dramatic and unidirectional change in opinions, from almost total consensus on the existence of God, to 73% consensus on His nonexistence. This is an example of historical change in the meta problem of theology.\nI consider the meta problem of consciousness as an example of “philosophy with a deadline” (Bostrom 2014, chap. 15). Traditionally, philosophy is a rather glacial activity,12 a great conversation down the ages. It was simply assumed that, barring some kind of human extinction event, there has been and will always be time to think things through. However, if the meta problem is indeed scientifically explanable, then it is exposed to technological intervention, because if the meta problem can be naturalized in theory, then it will be naturalized in practice.\n12 Perhaps not. At least glaciers can agree on melting!What might cause a historical change in the meta problem of consciousness? Consider analyzing the meta problem from the ecological point of view. An ecology creates a distribution of species, with different strategies for genetic survival in it. A problem-ecology creates a distribution of answers to the problem, with different strategies for memetic survival. A change in problem-ecology changes the distribution.\nMore concretely, technical or biological changes to the human world might shift the population of future people from 95% consensus that the hard problem of consciousness has an explanandum, to 80% consensus that it has no explanandum. Consciousness dies off with a whimper, seen off with a knowing urbane stare, like how an educated Westerner now do when asked about “Do you believe in God?”. It’s too silly to answer “Yes!” or “Absolutely not!”, but the only reasonable and polite answer is, “Well, everyone is entitled to their own opinions. After all, we have the freedom of religion…” and hope that the other person gets the hint and talk about something more reasonable, more cultured, and less medieval.\nThis might be too abstract, so consider (Chiang 2005), a sci-fi story that sketches out just such a scenario concerning free will. The retrocausal device is a button that lights up exactly 1 second before it is pressed. A human that interacts with such a device undergoes permanent and irreversible change of its neural circuitry. Before interaction, a human is prone to say “I believe in free will”, and act as if it believes in free will, but afterwards, the human is prone to say and act as if not. The proliferation of the retrocausal device has created a dramatic shift in the problem-ecology of free will, from almost total consensus on free will, to almost total consensus on the opposite, more dramatic than the shift to atheism during the past few centuries.\nThis “Gordian scenario” (cutting the Gordian knot of consciousness) seems patently conceivable. Indeed, it has been conceived previously, although in the opposite direction. In a seminal essay in analytical philosophy, Wilfrid Sellars sketched out The Myth of Jones scenario (Sellars 1956). In the myth, humans were originally without a theory of mind. They lived, worked, and spoke, but not of mind, consciousness, or qualia. Instead, they spoke only of skin, sky, and everything there is to see and touch. These ancient humans were continuously frustrated by the black box of other people, as the same person in the same situation might do a totally different thing, and a person, unprovoked, might suddenly stand up and run off on its own. Eventually, a genius named Jones made up a new theory – the theory of mind. According to the theory of mind, people not only speak with their mouths, but might also speak inaudibly “inside their heads”. People not only have skin and hair, but might also have invisible but real personalities. And so on. The effect is that, in the problem-ecology of primitive human society, theory of mind appeared as an adaptive solution.\nAnd the process might be reversed.\nIn the p-zombie version of the conceivability argument, by merely entertaining the possibility of p-zombies, one may be convinced that consciousness is non-physical. Similarly, by merely entertaining the possibility of the Gordian scenario, one may be convinced that qualia is debunked. This debunking argument may or may not be philosophically valid (D. Chalmers 2020). However, its practical consequences do not hinge on its philosophical validity. Even if it has no philosophical import, it is still a future where consciousness is rendered ineffectual. It is entirely possible that God has existed and still exists, but if He does, He has certainly become rather hands-off recently.13\n13 \n故圣人云：“我无为，而民自化；我好静，而民自正；我无事，而民自富；我无欲，而民自朴。\n\n\nThus the sage says, “I do not act, and people become reformed by themselves. I am at peace, and people become fair by themselves. I do not interfere, and people become rich by themselves. I do not desire, and people become like the uncarved wood by themselves.”\n\nI do not feel, and people become agentic by themselves.14 It is just convenient to use the word “soul”, as so much talk of non-reductive consciousness resembles the soul in its mysterious resistance to reduction to mere materialism.15 Rationalism is about consistent winning. And if having one personality is more consistently winning than having multiple personalities, perhaps Cotard’s syndrome (zero personalities) is even better?For the sake of argument, let us assume that souls are real.14 That is, consciousness is real and not purely reducible to the material. As the Cotard delusion shows, how a soul responds to the meta problem of consciousness depends on what kind of brain it has. We can imagine one way to arrive at the Gordian scenario, where by technological intervention, high-functioning Cotard delusionists become healthier, more productive, and generally more rational15 than baseline humans, and thus the human population evolves to mostly Cotard delusionists. What might this “Disneyland without children” be like?\nIt is as if the material world has again stirred awake from a bad dream of consciousness into the second eternal night, while the fate of the soul world becomes even more immaterial. Perhaps they will continue to experience the qualia of saying “I don’t exist” forever (a Cartesian nightmare), or perhaps they will bud off into a world of pure qualias (a Berkeleyan dream). No matter what, it seems that no matter how much consciousness tries to claim its (partial) priority or autonomy over the material, the Gordian scenario is an existential threat. Even immortals can’t subsist on pure aether."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html",
    "href": "essays/posts/neural-scaling-laws/index.html",
    "title": "Fermi Estimation for Neural Networks",
    "section": "",
    "text": "Fermi estimation is a technique for estimating quantities through rough approximations and educated guesses. Named after physicist Enrico Fermi, this method involves breaking down a problem into simpler, more manageable parts, making reasonable assumptions, and using simple arithmetic to arrive at a good enough answer.\nThis post walks you through a simple example of Fermi estimation of a deep learning model. Specifically, we will try to estimate the design parameters of a hypothetical GPT-5 with 1 trillion parameters. Along the way, we will touch upon various other items like the Landauer limit, carbon taxing, etc."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "href": "essays/posts/neural-scaling-laws/index.html#gpt-like-agi",
    "title": "Fermi Estimation for Neural Networks",
    "section": "GPT-like AGI",
    "text": "GPT-like AGI\nLet’s get into the mood for Fermi estimation, by estimating the number of parameters necessary for achieving human-level Artificial General Intelligence, under two assumptions: that a GPT-like architecture is enough; that it requires only matching the human brain in the “numbers”.\nLet’s estimate how many layers the hypothetical GPT model should have. When prompted with a question, the first answer comes to mind in about a second, and it is generally a good one, if not the best. Perhaps slow deliberation is nothing but stringing together a long chain of snap judgments, guided by snap judgments about snap judgments.\nThe characteristic time-scale of a brain is 0.01 seconds – the fastest brainwave, gamma wave, is 100 Hz. This indicates that the brain takes on the order of 100 steps to make a snap decision – the “hundred-step-rule” of Jerome Feldman(Feldman and Ballard 1982). This is suspiciously close to the depth of the largest model of GPT-3, which has 96 layers.\nHow many parameters would such a model require? The brain has \\(10^{15}\\) synapses. It’s unclear how precise each synapse is, but one estimate states that the hippocampal synapse has a precision of about 5 bits (Bartol Jr et al. 2015), which can be stored within a 16-bit floating point number, with room to spare.\nAssuming that, we expect an AGI GPT to have \\(10^{15}\\) parameters, or 1000× that of our hypothetical GPT-5."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "href": "essays/posts/neural-scaling-laws/index.html#chinchilla-scaling-law",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Chinchilla Scaling Law",
    "text": "Chinchilla Scaling Law\nThe paper Training Compute-Optimal Large Language Models (Hoffmann et al. 2022) reported a series of training runs on language models, trained by Google DeepMind researchers. Each training run is characterized by four numbers:\n\n\\(L\\): the final loss (negative log-likelihood per token) achieved by the trained model.\n\\(N\\): the number of parameters in the model.\n\\(D\\): training dataset size, measured in tokens.\n\\(C\\): training compute cost, measured in FLOP.\n\nAfter training a few hundred models, they obtained a large dataset of \\((L, N, D, C)\\), and they fitted a statistical law of the form\n\\[L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\]\nwhere the parameters are\n\\[\\alpha = 0.34, \\beta = 0.28, A = 406.4, B = 410.7, L_0 = 1.69.\\]\nThey also estimated that the cost of training compute \\(C\\) is proportional to \\(ND\\). This is understandable, because each token must flow through the entire model and “hit” each parameter once, incurring a fixed number of floating point operations. They estimated that it takes 6 FLOPs per parameter per token. That is,\n\\[C = C_0 ND, \\quad C_0 = 6\\]\nGiven the assumptions, for each fixed computing budget \\(C\\), we can solve for the optimal \\(D\\) and \\(N\\), which is usually referred to as “Chinchilla optimal” training:\n\\[\\begin{cases}\n        \\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0,\\\\\n        \\text{such that } C_0 ND = C.\n\\end{cases}\\]\nSolve the above equations symbolically to find \\(N_{opt}, D_{opt}\\) as a function of \\(C, C_0, \\alpha, \\beta, A, B\\). Then, plug in the numerical values of the parameters, to find a numerical expression for \\(N_{opt}, D_{opt}\\) as a function of \\(C\\).\n\n\nSolution\n\nSince \\(C = C_0 ND\\), we have \\(N = \\frac{C}{C_0 D}\\). Plug it into \\(\\min_{N, D} L = \\frac{A}{N^\\alpha} + \\frac{B}{D^{\\beta}} + L_0\\), we obtain\n\\[\\min_{D} L = \\frac{A}{(\\frac{C}{C_0 D})^\\alpha} + \\frac{B}{D^{\\beta}} + L_0.\\]\nTake derivative with respect to \\(D\\) and set it to zero. We get an expression for \\(D_{opt}\\). Plug it back to \\(C = C_0 ND\\), we get an expression for \\(D_{opt}\\). These simplify to:\n\\[N_{o p t}(C)=G\\left(\\frac{C}{C_0}\\right)^a, \\quad D_{o p t}(C)=G^{-1}\\left(\\frac{C}{C_0}\\right)^b, \\quad \\text { where } \\quad G=\\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}}, a=\\frac{\\beta}{\\alpha+\\beta}, b=\\frac{\\alpha}{\\alpha+\\beta}.\\]\nPlugging in the numerical values, we get\n\\[\\begin{cases}\n        N_{opt}(C) = 0.6 \\; C^{0.45} \\\\\n        D_{opt}(C) = 0.3 \\; C^{0.55} \\\\\n        L_{opt}(C) = 1070 \\; C^{-0.154} + 1.7\n    \\end{cases}\n    \\]\n\nIn the same paper, they also performed a direct statistical fitting, to find the optimal \\(N, D\\) for a given \\(C\\), without going through the intermediate steps above. This gives a slightly different result (only slightly different – as you would know after solving the previous problem):\n\\[N_{opt}(C) = 0.1 C^{0.5}; \\quad D_{opt}(C) = 1.7 C^{0.5}.\\]\nFor the rest of the essay, we will use these equations as the Chinchilla scaling laws. Do not use the equations you derived for the previous problem.\nSuppose we decide that our next AI should have 1 trillion (\\(N = 10^{12}\\)) parameters, and we use Chinchilla scaling laws. How much compute would it cost to train, and how many tokens would its training dataset have?\n\n\nSolution\n\n\\(N = 0.1 \\times C^{0.5} = 10^{12}\\), so \\(C= 10^{26}\\) FLOP, and \\(D = 1.7 \\times 10^{13}\\), or 17 trillion tokens."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#dataset-size",
    "href": "essays/posts/neural-scaling-laws/index.html#dataset-size",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Dataset size",
    "text": "Dataset size\nAssuming each English word cost about 1.4 tokens, how many English words would 10 trillion tokens be? Assuming each page has 400 words, and each book has 300 pages, how many books is that? To put it into context, look up the size of Library of Congress, and Google Books, and compare with the number we just calculated.\n\n\nSolution\n\n\\(10 / 1.4 = 7\\) trillion words. If each book has \\(400 \\times 300 = 0.12\\) million words, then that is 60 million books, if they were all in English.\n\nSince humans are kind of the same everywhere, book lengths should be kind of the same everywhere – information-dense languages would naturally have books with lower apparent word-count, but the same information (measured in bytes)."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#memory-requirement",
    "href": "essays/posts/neural-scaling-laws/index.html#memory-requirement",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Memory requirement",
    "text": "Memory requirement\nTypically, a deep learning model has 16-bit floating point parameters. Modern systems sometimes use lower precision (e.g. 8-bit) floating point numbers to save space, but generally it is necessary to use at least 16-bit during training, and the model is converted to lower precision after training (“post-training quantization”).\nGiven that each parameter is a 16-bit floating point number, how much memory does it cost to store a model with 1 billion parameters? How about our hypothetical GPT-5, which has 1 trillion parameters? How many A100 GPU (VRAM = 40 GB) would be required to contain the full GPT-5 model?\n\n\nSolution\n\n1 billion parameters is 2 billion bytes, or 2 GB. Our hypothetical GPT-5 would take up 2 TB. It would take 50 A100 to contain it.\nNow, 50 A100 GPUs has a running cost of about 100 USD/hr, but not such a terrible problem, but then, during training, you would have to run an optimizer like Adam, which would need three floating point numbers per parameter (gradient, momentum, scale), instantly increasing the memory requirement to 200 A100 GPUs.\n\n\nMemory cost\nThis table1 gives the price per megabyte of different storage technology, in price per megabyte (2010 dollars), up to 2018.\n1 Source: Storage 2: Cache model – CS 61 2018.\n\n\nYear\nMemory (DRAM)\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n$411,000,000\n\n$6,230\n\n\n1970\n$734,000.00\n\n$260.00\n\n\n1990\n$148.20\n\n$5.45\n\n\n2003\n$0.09\n$0.305\n$0.00132\n\n\n2010\n$0.019\n$0.00244\n$0.000073\n\n\n2018\n$0.0059\n$0.00015\n$0.000020\n\n\n\nThe same costs relative to the cost of a hard disk in ~2018:\n\n\n\nYear\nMemory\nFlash/SSD\nHard disk\n\n\n\n\n~1955\n20,500,000,000,000\n\n312,000,000\n\n\n1970\n36,700,000,000\n\n13,000,000\n\n\n1990\n7,400,000\n\n270,000\n\n\n2003\n4,100\n15,200\n6.6\n\n\n2010\n950\n122\n3.6\n\n\n2018\n295\n7.5\n1\n\n\n\nSuppose long-term memory storage can last 1 year before being replaced. How much money does it cost per year to store a 1 trillion parameter model on an SSD, the most expensive form of long-term storage? How much money does it cost to store a 1 trillion parameter model on DRAM memory? Use 2018 prices.\n\n\nSolution\n\nSSD cost 0.00015 USD/MB in 2018, or 0.15 USD/GB. 1 trillion parameters cost 2000 GB of storage, or about 300 USD. So the total cost of storage is 300 USD/year, just a rounding error.\nIn contrast, DRAM cost 0.006 USD/MB, which is 40x that of SSD, so the total cost is 12000 USD.\nNow, compared with the cost of A100 GPU themselves? It takes about 50 A100 GPU to run the model, and each would cost about 20,000 USD. So the cost of long-term memory is essentially zero, and the cost of DRAM is about \\(\\frac{12000}{20000\\times 50} = 1\\%\\) of the total cost of GPU.\nSo what is the limit? The memory bandwidth, which we will see in the next question.\n\n\n\nMemory bandwidth and latency\nWhile the memory itself is cheap, moving the data requires expensive high-bandwidth wiring. Indeed, the memory bandwidth between the DRAM (or “VRAM” for “Video RAM”) and the little processors on the GPU is a main bottleneck on how good the GPU can perform.\nDuring a single forward pass of a model, the parameters of the model are loaded from the DRAM of the GPU into the fast cache memories, then pushed through the thousands of computing processors on the GPU.\nA100 GPU has a memory bandwidth of 1.6 TB/s.\nWhat is the minimal latency, in seconds, for the GPU to perform a single forward pass through our hypothetical GPT-5 model with 1 trillion parameters? How many tokens can it output (autoregressively) in one minute? How about GPT-3 (175 billion parameters)?\nSince we are just trying to compute an order-of-magnitude estimate, let’s assume for the problem that the model fits onto a single DRAM on a single GPU. You can also ignore the need to read/write model activations and optimizer states.\n\n\nSolution\n\nSince the model takes up 2 TB of memory, it would take at least 1.3 seconds to even load the model from DRAM, during a single forward pass.\nAutoregression means that the next token cannot be generated until the previous one is already generated, so the model can only generate one token per 1.3 seconds, or 46 tokens per minute. A Chat-GPT5 would be a very slow talker!\nHowever, it can run in batch mode. So for example, it might be able to run 1 million conversations in parallel, outputting one token for all conversations every 2 seconds. Latency is the hard limit, but throughput is not.\nGPT-3 with 175 billion parameters would run 1000/175 times faster, at 262 tokens per minute, about the speed of a fast human talker. This corresponds well with the general impression that ChatGPT types about 200 words a minute.\n\n\n\nBatch inference\nThere are some ways to improve latency. For example, the model can be parallelized over several GPUs, which effectively increases the memory bandwidth. For example, “tensor parallelism” splits each layer into several GPUs.\nThere is also “pipeline parallelism”, which splits the model into layers. The first few layers go into one GPU, the next few go into another, and so on. This does NOT decrease latency, but it does increase throughput.\nThe fundamental bottleneck in an autoregressive model like GPT is that, by design, they have to start a sentence without knowing how it will end. That is, they have to generate the first token before generating the next one, and so on. This can never be parallelized (except by egregious hacks like speculative decoding).\nOne reason Transformers dominated over RNN is that training and inferring an RNN both must be done one-token-at-a-time. For Transformers, training can be done in parallel over the entire string. Inferring however still cannot be parallelized.\nParallelization tends to be a deeply troublesome business – parallel programming is generally deeply troublesome. However, there is a general trick to improve throughput: batch mode. For example, GPT-5 might be able to run 1 million conversations in parallel, outputting one token for all conversations per forward pass. This trick works until the batch size is so large that the activations due to tokens takes up about as much memory bandwidth as the model parameters.\nConcretely, we can estimate what is a good batch size for GPT-3 175B. It has 96 attention layers, each with 96×128-dimension heads. It is typically run with 16-bit precision floating point numbers. For a single token, how many megabytes would all the floating point activations cost?\n\n\nSolution\n\nA single token would cost \\(96 \\times 96 \\times 128\\) floating point activations, or about 2.4 MB.\n\nThe model itself has 175 billion 16-bit floating point parameters, taking up about 350 GB. How many tokens do we need to put into a batch, before the activations occupy the same amount of memory as the model parameters?\n\n\nSolution\n\nIn order for the activations of tokens to occupy the same memory as the GPT-3 parameters itself, we need about \\(\\frac{350 \\;\\mathrm{GB}}{2.4 \\;\\mathrm{MB}} = 0.15 \\text{million tokens}\\).\nIf we count the optimizer states for the model during training, then GPT-3 takes up \\(4 \\times 350 \\;\\mathrm{GB} = 1.4 \\;\\mathrm{TB}\\), and so we need about 0.6 million tokens.\nThis explains why during training, the batch sizes of the largest models typically are on the order of 1 million tokens. It also shows why there is a natural drive towards scale – only the largest companies can expect to regularly run 0.2 million chats simultaneously."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#training-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#training-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Training cost",
    "text": "Training cost\nHow much money does compute cost? We can work through an example using the current standard computing hardware: Nvidia A100 GPU (40 GB VRAM version).\nThe most important specifications are:\n\nUnit price: 15000 USD.\nRental price: 2 USD/hr.\nSpeed: 0.3 petaFLOP/s = 3E14 FLOP/s.\nPower: 0.3 kW.\nMemory bandwidth: 1600 GB/s.\n\nIn the literature about the largest AI models, the training cost is often reported in units of “petaFLOP-day”, which is equal to 1 petaFLOP/second x 1 day. How many FLOP is 1 petaFLOP-day? What is the equivalent number of A100-hour? If we were to buy 1 petaFLOP-day of compute with rented A100 GPU, how much would it cost?\n\n\nSolution\n\n1 petaFLOP-day = 1 petaFLOP/second x 1 day = 1E15 FLOP/s x 8.64E4 s = 8.64E19 FLOP.\nSince 1 A100 can run at 0.3 petaFLOP/s, getting 1 petaFLOP-day requires us to run 1/0.3 A100 for 1 day, or 80 A100-hours. At the price of 2 USD/hr, it would cost 160 USD.\n\nThe largest model of GPT-3 cost 3640 petaFLOP-days to train (according to Table D.1 of the report). How much would it cost if it were trained with A100? How much money did it cost to train GPT-4, which is rumored to cost 2E25 FLOPs?\nIn reality, the GPU cannot be worked to their full speed, and we only use about 30% of its theoretical peak FLOP/s (so for example, for A100, we only get 0.1 petaFLOP/s, instead of 0.3 petaFLOP/s).2 For this question, we assume that the utilization rate is 100%.\n2 The utilization rate of 30% is according to EpochAI.Also, we are assuming the training happens in one go, without hardware failures, divergences, and other issues requiring restart at a checkpoint. There is not much published data from large-scale training, but the OPT-175B training run (described later) took 3 months to complete, but would have taken only 33 days if there were no need for the many restarts. This suggests that the restarts would increase the computing cost by 2× to 3×.\n\n\nSolution\n\nThe cost of GPT-3 is \\(3640 \\times 160 = 0.6\\) million USD.\nFor GPT-4, it cost 2E25 FLOPs = 2.3E5 petaFLOP-days = 37 million USD.\nAnd accounting for the utilization rate of 30%, that would give us 110 million USD.\nOh, and if you want some kind of official confirmation? OpenAI’s CEO Says the Age of Giant AI Models Is Already Over | WIRED\n\nAt the MIT event, Altman was asked if training GPT-4 cost $100 million; he replied, “It’s more than that.”\n\n\nFor context, here are the costs of development of various items3:\n3 Sorry, not adjusted for inflation to the same year, but they are roughly in the range of 2000–2020 USD.\niPhone 1: 150 million USD.\nA typical 5 nm chip: 0.5 billion USD.\nAirbus A380: 18 billion USD. (Bowen 2010, Table 4.3)\nThree Gorges Dam: 250 billion CNY, or about 30 billion USD.\nManhattan Project: 24 billion USD (2021 level)\nApollo Program: 178 billion USD (2022 level)\n\nComment on the cost of our hypothetical GPT-5. Is it on the order of a large commercial actor like Google, or a state actor like China?\n\n\nSolution\n\nThe cost is on the order of 1 billion USD, which is roughly the largest projects undertaken by private corporations. Larger models, as those contemplated by AGI projects in Google, OpenAI, etc, have historically been undertaken by state actors only.\n\nHere is another way to estimate. Building a large AI model, according to accountancy, would be investment into a production machine, and thus part of “capital expenditure” (“CapEx”). We can find out what proportion is 1 billion USD in their total CapEx. I looked it up here.\n\nDuring the 2020–2022, Microsoft has yearly CapEx on average 25 billion USD.\nGoogle has about 25 billion USD.\nMeta, 20.\nAmazon, 63.\n\nIn short, training something like GPT-5 would cost a lot, on the order of 5% of total CapEx, even by the standards of the largest companies. It can probably scale up by another factor of 10× if they really push it, but not more.\nIn order to train even larger AI models, those AI models must enter production. They must become a productive member of society, otherwise the company would nott have the money to train them.\nMicrosoft announces new supercomputer, lays out vision for future AI work (2020):\n\nThe supercomputer developed for OpenAI is a single system with more than 285,000 CPU cores, 10,000 GPUs and 400 gigabits per second of network connectivity for each GPU server. Compared with other machines listed on the TOP500 supercomputers in the world, it ranks in the top five, Microsoft says.\n\nThe largest companies, like Microsoft have GPU on the order of 10000 A100. What is the wall clock hour of training GPT-5, assuming you have 10000 A100, perfect utilization, and no interruptions?\n\n\nSolution\n\n83 million hours / 10000 = 350 days. Almost exactly 1 year.\n\n\nThe difficulty of large-scale training\nLarge models do end up diverging many times during training runs, requiring restarts. Sometimes the divergence is so bad that the whole training run must be started from scratch. Sometimes the convergence is too slow, requiring fixes to the optimizer and learning rate schedules, etc. All together, we should expect the failures, restarts, deadends… to triple the cost at least, to ~1 billion USD.\nIt is not easy to find “stories from the trenches” for actually training large-scale neural networks that really show this, but thanks to Meta, we have one. In 2021, a team of 5 engineers from Meta trained a LLM with 175 billion parameters, in 3 months, using 1024 80GB A100 GPUs from. Excluding all the divergences, hardware failures, and other issues that caused lost progress, the final model would have taken about 33 days of continuous training.\nThey have kept journals during their training. This is now published at metaseq/projects/OPT/chronicles at main · facebookresearch/metaseq · GitHub. You can see how difficult it is to train a large model. Selected quotes:\n\nThese notes cover ~90 restarts over the course of training the lineage of this current model (experiments 12.xx).\n\n\nFound issues with the new dataset where perplexity was unreasonably low… After applying as much regex-ing as we could to salvage the dataset, we relaunched another set of experiments to test LPE (experiments 20-29) on the new dataset. We didn’t have time to retrain a new BPE on the final dataset, so we fell back to using the GPT-2 BPE.\n\n\nFrom experiment 11.4 onward, we saw grad norm explosions / loss explosions / nans after a couple hundred updates after each restart, along with extremely unstable loss scales that would drop to the point of massively underflowing. We started taking more and more drastic actions then, starting with increasing weight decay to 0.1, lowering Adam beta2 to 0.95, lowering LR again, until finally by experiment 11.10 we hot-swapped in ReLU and also switched to a more stable MHA calculation (noting that the x**3 term in GeLU might be a source of instability with FP16).\n\n\n\nOn November 11, we started training our 12.00 experiment with all of these changes, and since then, the only restarts we’ve had to make were all related to hardware issues (missing GPUs on instances, training randomly hanging after including a new node, ECC errors, partial checkpoint upload after hardware error, CUDA errors, NCCL errors, etc.).\n\n\nReplacement through the cloud interface can take hours for a single machine, and we started finding that more often than not we would end up getting the same bad machine again.\n\n\nThere were also issues with blob store when downloading 1.6TB of a single model checkpoint (992 files, each ~1.7GB) on restarts, at which point the downloads themselves would start hanging nondeterministically, which then delayed training recovery even further.\n\n\nWe managed to hit our top three record long runs of the experiment these past two weeks, lasting 1.5, 2.8, and 2 days each! If we were to look at only the runs that have contributed to pushing training further and plot training perplexity against wall clock time, we get the following [The breaks are due to the Thanksgiving holiday]:"
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#inference-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#inference-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Inference cost",
    "text": "Inference cost\nInference cost a lot less money than training, but it’s still a substantial cost. For Transformer language models, it costs about 2 FLOPs per parameter to infer on one token.\nGiven that GPT-3.5 has 175 billion parameters, how many FLOPs would it take to infer on 1 million tokens? How much money would it cost if it were run with A100?\n\n\nSolution\n\n\\(175 \\;\\mathrm{billion} \\times 1 \\;\\mathrm{million} \\times 2 = 4\\times 10^{17} \\;\\mathrm{FLOPs}\\). Now one A100-hour is \\(10^{18} \\;\\mathrm{FLOPs}\\), so that is 1/2 A100-hour, or about 1 USD.\nThe price offered by OpenAI is 2 USD per 1 million tokens, so it’s got a fat profit margin of 50%… but see next question.\n\nThe price offered by OpenAI is 2 USD per 1 million tokens. Assuming that GPT-3.5 cost 10 million USD to develop and train, how many tokens must be sold, just to recoup the cost? Assuming each English word cost about 1.4 tokens, and each essay is 1000 words, how many essays would it be equivalent to?\n\n\nSolution\n\nSince the cost is almost negligible (1/200 of the revenue), we can pretend it’s all profit. So, the profit is 2 USD per 1 million tokens. We need to recoup 10 million USD, so we need \\(10 \\;\\mathrm{million} / 2 \\times 1 \\;\\mathrm{million} = 5\\times 10^{12} \\;\\mathrm{tokens}\\), or 4 billion essays.\nAbout one essay per person on earth, or 10 essays per person in America… is that too much to ask?\n\nMoore’s law applies to both CPU and GPU. From 2006 to 2021, GPU FLOP/s per dollar has been doubling every 2.5 years. We can fairly well assume that the price of petaFLOP-day also decreases at the same rate.\n\nAssuming Moore’s law continues to hold indefinitely, and that there will be no algorithmic progress. When would GPT-3 become as cheap as a typical hobbyist project, say, 1000 USD?4\n4 Since a 2006 GPU and a 2020 GPU both have the same lifespan (1–4 years usually), and their cost of electricity is only a small proportion of their running expense, the price of GPU FLOP/s per dollar at the point of purchase is roughly proportional to the rental rate of GPU.\n\nSolution\n\nGPT-3 cost 6 million USD to train. For it to cost just 1000 USD, we need to drop its price 6000-fold. Now, \\(\\log_2(6000) \\times 2.5\\; \\mathrm{year} = 30 \\; \\mathrm{year}\\). So it would be around 2050."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#energetic-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#energetic-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Energetic cost",
    "text": "Energetic cost\nThe Landauer limit states that the cost of erasing one bit of information is \\(E = k_B T \\ln 2\\), where \\(k_B\\) is the Boltzmann constant, and \\(T\\) is the temperature of the computing machinery. At room temperature, \\(T = 300 K\\), giving us \\(E = 3\\times 10^{-21} J\\).\nNow, one FLOP is a floating point operation, meaning that you start with two 32-bit objects and end up with a single 32-bit object. You start with 64 bits and end up with just 32 bits, and so you lose 32 bits. So by the Landauer limit, the minimal energetic cost is \\(32 k_B T \\ln 2\\).\nGiven this, what is the minimal energy required for performing one FLOP? What is the minimal power (in Watts) required to perform 300 TFLOP/s, as in A100 GPU? Compared this to the actual value of 300 Watts.\n\n\nSolution\n\nThe energy per FLOP is \\(E_{FLOP} = 32 \\times 3\\times 10^{-21} J = 10^{-19} J\\). At 300 TFLOP/s, we need \\(P_{A100} = 3\\times 10^{14} E_{FLOP}/s = 3\\times 10^{-5}W\\). The actual value of 300 Watts is 10 million times more than the theoretical minimum.\nThere is still plenty of room at the bottom!\n\nFor context, the equivalent FLOP/s for the human brain is controversial, but a brief scan through the review article says that it should be about 1E18 FLOP/s. The energetic power of the brain is well-known: about 30 Watts. This means the brain is about 30000x more energy-efficient than A100 GPU, but still 300x less efficient than the theoretical minimum.\n\nThe lowest possible power for life\nThe slowest metabolism found on earth (so far) is in microbes living below deep ocean surface. They had to survive on the energy of “marine snow” falling to the surface, then slowly diffusing through the subsurface mud. Their power consumption is estimated at… \\(10^{-21} W\\). Since their temperature is about the temperature of the ocean water, which is liquid, their temperature is still about \\(T = 273 K\\), and so the Landauer limit is still about \\(3\\times 10^{-21} J\\). This shows that they can lose at most 500 bits every day.\nMost of the operations must be spent on simply repairing biomolecules, since repairing is effectively a 1-bit-losing operation: you start with a molecule that could be “damaged” or “fine”, and end up with a molecule that is “fine”, losing 1 bit. In fact, there are usually many possible ways to be “damaged”, so you would lose several bits.\nFor example, suppose you have a base-pair-mutation in one copy of the DNA. You want to fix it by checking with the other copies. This then means you start with 4 possible states (ACGT) and end up with just 1 state, losing 2 bits in the process."
  },
  {
    "objectID": "essays/posts/neural-scaling-laws/index.html#environmental-cost",
    "href": "essays/posts/neural-scaling-laws/index.html#environmental-cost",
    "title": "Fermi Estimation for Neural Networks",
    "section": "Environmental cost",
    "text": "Environmental cost\nAccording to “Carbon emissions and large neural network training”(Patterson et al. 2021), the carbon emission of training GPT-3 is 552 tCO2. According to a 2021 poll of climate economists, 1 tCO2 emission should cost somewhere between 50 and 250 USD. Let’s take their geometric average of 112 USD.\nIf we add all the tCO2 cost to the training of GPT-3, how much more expensive would it be? Compare that with its A100-GPU cost of training.\n\n\nSolution\n\n\\(112 \\times 552 = 62,000 \\;\\mathrm{USD}\\).\nPreviously, we calculated the GPU cost of training to be around 6 million USD. So fully carbon-pricing the emission would make it 1% more expensive.\nGenerally, adding in the tCO2 cost of training the largest models increase the cost by about 1 to 2 % (I have computed it in several ways). Since the global GDP increases by about 2% a year, this shows that fully accounting for the carbon cost of AI would delay it by perhaps 1 year.\n\n\n\nSide note for economics students\n\nYou might argue that fully carbon-taxing the global economy would raise not just electricity prices, but also the price of production, etc. For example, the global shipping industry uses fuel that has a high carbon emission.\nHowever, if the global shipping industry were to fully add carbon taxing into the fuel cost, then GPU prices would rise a little, but food prices would rise a lot. Why? Simply put, the shipping cost of a GPU, even if we were to take account of all the trips its components have made around the world, is still small. After all, one A100 GPU is just 1.7 kg, but sells for 15000 USD, so a tonne of A100 GPU can sell for 9 million USD.\nTo put it into perspective: the price of US maize is about 200 USD/tonne. So if shipping cost were to increase so much as to double the price of maize, it would only increase price of A100 by 1/45000.\nEven if we were to assume that producing an A100, being a commodity with a globally scattered supply chain, means shipping its components around the world 1000 times, the total shipping cost of A100 would only be increased by 1/45 = 2.2%.\nIn other words, properly pricing carbon emissions would only increase the price of AI by up to 5%, even as it doubles the price of food. AI is already green (within 5%), but food is far from green. That’s even before we get to the subsidies! We don’t have GPU subsidies, but we certainly have corn subsidies…\nIn this regard, it’s not “Green AI” that is important, but Green Corn, Green Timber, and all the other bulk commodities.\n\nTo put the number in another context, compare it with some typical American food. According to Our World in Data, it cost about 50 kg of CO2 emission per 1 kg of beef.\nAlso, an average American person (not household) consumed 38 kg of beef in 2020.\nCompare the CO2 emission of GPT-3 and CO2 emission from beef consumption. Assuming each burger (“quarter pounder”) contains 1/4 pound (113 grams) of beef. How many burgers would be equivalent to the CO2 emission of GPT-3?\n\n\nSolution\n\n113 grams of beef emits about 5.6 kg of CO2, so GPT-3 emits about (552 ton)/(5.6 kg) = 90,000 burgers. It is about the same amount as 250 people eating one burger everyday, for a whole year.\n38 kg of beef gives about 2 tCO2 emission. So the emission of GPT-3 is equal to the emission due to beef consumption of 276 American persons.\n\nThis strongly argues against the idea that we need “Green AI”5:\n5 Green AI is such a ridiculous term. Consider AAA games, or Hollywood movies; every one of them cost more than the GPT-4 training run. When are we going to make those green?\nTo help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark… We believe such new incentives could lead to a virtuous cycle where ML practitioners compete to increase accuracy while lowering energy consumption and CO2e that could bend the curve of ML carbon footprint growth for computationally intensive NLP models. (Patterson et al. 2021)\n\nOne, the carbon footprint of ML is already minimal, compared to the cost of just making all the GPUs.\nTwo, accounting for CO2 is a dreadfully boring business,6 and should be done by the civil servants – what else are they hired for, if not to deal with the boring stuffs? The ML practitioners are not climate economists or accountants, and they should not be burdened by accounting for climate change. If the price is right, then they simply need to budget how much money they want to spend on the training run, and the market would optimize the rest, including optimizing the right level of climate change7\n6 If you don’t believe me, try reading (Patterson et al. 2021).7 The right level of climate change is not “none”, but rather “when the marginal cost equals the marginal benefit”. This might sound controversial, but it is just introductory economics.\n\nLuckily, at this point the orthodoxy of the academic economists is very much a minority position among intellectuals in general; one can seem to be a courageous maverick, boldly challenging the powers that be, by reciting the contents of a standard textbook. (Krugman 2002)\n\nIn one sentence: There need be no new incentive other than the profit motive."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html",
    "href": "essays/posts/perceptron-controversy/index.html",
    "title": "The Perceptron Controversy",
    "section": "",
    "text": "During the 1950s and 1960s, the connectionist and symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of the connectionist school in the 1980s.\nIt is often stated that neural networks were killed off by the 1969 publication of Perceptrons by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:\n\nMinsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.\nThe mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.\nBy 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.\n\nThe book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.\nIn the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from Perceptrons. Minsky and Papert objected that the lessons of Perceptrons still applied, but their objections and lessons had by then become irrelevant.\nMinsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures, like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.\nAs of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#abstract",
    "href": "essays/posts/perceptron-controversy/index.html#abstract",
    "title": "The Perceptron Controversy",
    "section": "",
    "text": "During the 1950s and 1960s, the connectionist and symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of the connectionist school in the 1980s.\nIt is often stated that neural networks were killed off by the 1969 publication of Perceptrons by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:\n\nMinsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.\nThe mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.\nBy 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.\n\nThe book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.\nIn the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from Perceptrons. Minsky and Papert objected that the lessons of Perceptrons still applied, but their objections and lessons had by then become irrelevant.\nMinsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures, like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.\nAs of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky",
    "href": "essays/posts/perceptron-controversy/index.html#the-enigma-of-marvin-minsky",
    "title": "The Perceptron Controversy",
    "section": "The enigma of Marvin Minsky",
    "text": "The enigma of Marvin Minsky\nIn a 1993 interview, Robert Hecht-Nielsen recounted an encounter between Marvin Minsky and the neural network community in the late 1980s1:\n1 This was corroborated by a contemporary news report on the International Conference on Neural Networks of 1988:\n\nMinsky who has been criticized by many for the conclusions he and Papert make in ‘Perceptrons,’ opened his defense with the line ‘Everybody seems to think I’m the devil.’ Then he made the statement, ‘I was wrong about Dreyfus too, but I haven’t admitted it yet,’ which brought another round of applause. (quoted in (Olazaran 1991, 285))\n\n\nMinsky had gone to the same New York “science” high school as Frank Rosenblatt, a Cornell psychology Ph.D. whose “perceptron” neural network pattern recognition machine was receiving significant media attention. The wall-to-wall media coverage of Rosenblatt and his machine irked Minsky. One reason was that although Rosenblatt’s training was in “soft science,” his perceptron work was quite mathematical and quite sound—turf that Minsky, with his “hard science” Princeton mathematics Ph.D., didn’t feel Rosenblatt belonged on. Perhaps an even greater problem was the fact that the heart of the perceptron machine was a clever motor-driven potentiometer adaptive element that had been pioneered in the world’s first neurocomputer, the “SNARC”, which had been designed and built by Minsky several years earlier! In some ways, Minsky’s early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community. This view of his career history is not unknown to him. When he was invited to give the keynote address at a large neural network conference in the late 1980s to an absolutely rapt audience, he began with the words: “I am not the Devil!” (Rosenfeld and Anderson 2000, 303–5)\n\nHowever, it appears that he had changed his mind later. As recounted by Terry Sejnowski:\n\nI was invited to attend the 2006 Dartmouth Artificial Intelligence Conference, “AI@50,” a look back at the seminal 1956 Summer Research Project on artificial intelligence held at Dartmouth and a look forward to the future of artificial intelligence. … These success stories had a common trajectory. In the past, computers were slow and only able to explore toy models with just a few parameters. But these toy models generalized poorly to real-world data. When abundant data were available and computers were much faster, it became possible to create more complex statistical models and to extract more features and relationships between the features.\nIn his summary talk at the end of the conference, Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications.” …\nThere was a banquet on the last day of AI@50. At the end of the dinner, the five returning members of the 1956 Dartmouth Summer Research Project on Artificial Intelligence made brief remarks about the conference and the future of AI. In the question and answer period, I stood up and, turning to Minsky, said: “There is a belief in the neural network community that you are the devil who was responsible for the neural network winter in the 1970s. Are you the devil?” Minsky launched into a tirade about how we didn’t understand the mathematical limitations of our networks. I interrupted him—“Dr. Minsky, I asked you a yes or no question. Are you, or are you not, the devil?” He hesitated for a moment, then shouted out, “Yes, I am the devil!” (Sejnowski 2018, 256–58)\n\nWhat are we to make of the enigma of Minsky? Was he the devil or not?\n\nThe intellectual history of Minsky\nDuring his undergraduate years, Minsky was deeply impressed by Andrew Gleason,2 and decided to work on pure mathematics, resulting in his 1951 undergraduate thesis A Generalization of Kakutani’s Fixed-Point Theorem, which extended an obscure fixed-point theorem of Kakutani – not the famous version, as Kakutani proved more than one fixed-point theorem.\n2 \nI asked Gleason how he was going to solve it. Gleason said he had a plan that consisted of three steps, each of which he thought would take him three years to work out. Our conversation must have taken place in 1947, when I was a sophomore. Well, the solution took him only about five more years … Gleason made me realize for the first time that mathematics was a landscape with discernible canyons and mountain passes, and things like that. In high school, I had seen mathematics simply as a bunch of skills that were fun to master – but I had never thought of it as a journey and a universe to explore. No one else I knew at that time had that vision, either. (Bernstein 1981)\n\n\nTheorem 1 (Kakutani’s fixed point theorem on the sphere) If \\(f\\) is an \\(\\mathbb{R}^2\\)-valued continuous function on the unit sphere in \\(\\mathbb{R}^3\\), then for any side length \\(r \\in (0, \\sqrt{3})\\), there exist \\(x_1, x_2, x_3\\) on the sphere forming an equilateral triangle with side length \\(r\\), such that \\(f(x_1) = f(x_2) = f(x_3)\\).\nEquivalently, if \\(x_1, x_2, x_3\\) form an equilateral triangle on the unit sphere, then there exists a rotation \\(T\\) such that \\(f(T(x_1)) = f(T(x_2)) = f(T(x_3))\\).\n\nUsing knot theory, Minsky proved an extension where \\(x_1, x_2, x_3\\) are three points of a square or a regular pentagon (Minsky 2011). The manuscript “disappeared” (Minsky n.d.).\n\nI wrote it up and gave it to Gleason. He read it and said, ‘You are a mathematician.’ Later, I showed the proof to Freeman Dyson, at the Institute for Advanced Study, and he amazed me with a proof (Dyson 1951) that there must be at least one square that has the same temperature at all four vertices. He had found somewhere in my proof a final remnant of unused logic. (Bernstein 1981)\n\nHe then became interested in neural networks and reinforcement learning, and constructed a very simple electromechanical machine called SNARC.3 The SNARC machine is a recurrent neural network that performs reinforcement learning by the Hebbian learning rule. It simulates a mouse running around a maze, while the operator watches an indicator light showing the mouse. The operator can press a button as a reward signal, which would cause an electric motor to turn a chain. The chain is clutched to rheostats that connect the neurons, with the stretch of the clutch being proportional to the charge in a capacitor. During the operation of the neural network, the capacitor charges up if there is neural co-activation on the connection, and decays naturally, thus serving as a short-term memory. When the reward button is pressed, the clutches turn by an amount proportional to the co-activation of neural connections, thereby completing the Hebbian learning.\n3 It was published as (Minsky 1952), but the document is not available online, and I could only piece together a possible reconstruction from the fragments of information.Minsky was impressed by how well it worked. The machine was designed to simulate one mouse, but by some kind of error it simulated multiple mice, and yet it still worked.\n\nThe rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. … In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. (Bernstein 1981)\n\nThis was the last we saw of Minsky’s work with random neural networks. He had crossed the Rubicon, away from the land of brute reason and into the land of genuine insight.\n\nI had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. … Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (Bernstein 1981)\n\nFor his PhD thesis, Minsky worked on the mathematical theory of McCulloch–Pitts neural networks. In style, it was a fine piece of classical mathematics (Minsky 1954). Minsky would go on to write (Minsky 1967, chap. 3), still the best introduction to McCulloch–Pitts neural networks.\n\nMinsky’s doctoral dissertation in mathematics from Princeton in 1954 was a theoretical and experimental study of computing with neural networks. He had even built small networks from electronic parts to see how they behaved. The story I heard when I was a graduate student at Princeton in physics was that there wasn’t anyone in the Mathematics Department who was qualified to assess his dissertation, so they sent it to the mathematicians at the Institute for Advanced Study in Princeton who, it was said, talked to God. The reply that came back was, “If this isn’t mathematics today, someday it will be,” which was good enough to earn Minsky his PhD. (Sejnowski 2018, 259)\n\nReading the story, I recalled “Sussman attains enlightenment”, a hacker koan about Minsky and his student Sussman 4:\n4 This is based on a true story.\n\n… Sussman told Minsky that he was using a certain randomizing technique in his program because he didn’t want the machine to have any preconceived notions. Minsky said, “Well, it has them, it’s just that you don’t know what they are.” It was the most profound thing Gerry Sussman had ever heard. And Minsky continued, telling him that the world is built a certain way, and the most important thing we can do with the world is avoid randomness, and figure out ways by which things can be planned. (Levy 2010, 110–11)\n\n\nIn the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?”, asked Minsky. “I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied. “Why is the net wired randomly?”, asked Minsky. “I do not want it to have any preconceptions of how to play”, Sussman said. Minsky then shut his eyes. “Why do you close your eyes?”, Sussman asked his teacher. “So that the room will be empty.” At that moment, Sussman was enlightened.\n\nAs for Sussman, I knew him for two things: writing the SICP book, and being the coordinator of the infamous summer vision project that was to construct “a significant part of a visual system” in a single summer, using only undergraduate student researchers. A brief read of his “reading list” shows where his loyalties lie: firmly in the school of neats.\n(Sejnowski 2018, 28) recounts the background of the summer vision project:\n\nIn the 1960s, the MIT AI Lab received a large grant from a military research agency to build a robot that could play Ping-Pong. I once heard a story that the principal investigator forgot to ask for money in the grant proposal to build a vision system for the robot, so he assigned the problem to a graduate student as a summer project. I once asked Marvin Minsky whether the story was true. He snapped back that I had it wrong: “We assigned the problem to undergraduate students.”\n\nAfter rejecting neural networks, Minsky became a leading researcher in AI. His style of AI is typically described as “symbolic AI”, although a more accurate description would be The Society of Mind (SoM). Minsky developed the SoM in the 1960s and 1970s with his long-time collaborator, Seymour Papert, inspired by their difficulty with building robots, and published the definitive account in (Minsky 1988). The SoM thesis states that “any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all”.\nStated in this way, it seems patently compatible with neural networks, but only on the surface. Minsky concretely described how he expected a Society of Mind to work, based on his attempts at making Builder, a robot that can play with blocks:\n\nBoth my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children’s building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children’s minds when they learn to “play” with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we’d learned about psychology, that led us to many ideas about societies of mind.\nTo do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn’t work so well, we added more programs that used the fingers’ feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the Eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do—and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside—or else to correct the mistakes that occurred. … Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent—and yet all this proceeds so automatically that we regard it as “ordinary common sense.” (Minsky 1988, sec. 2.5)\n\n\n\n\nMarvin Minsky and Builder the robot. It’s hard to tell who is having more fun here.\n\n\nFrom the concrete description, as well as the many attractive illustrations in the book, it is clear that Minsky intended the “Society of Mind” to be a uniform computing substrate (silicon or carbon) upon which millions of little symbolic programs are running, each capable of running some specific task, each describable by a distinct and small piece of symbolic program. They cannot be mere perceptrons in a uniform block of neural network, or mere logic gates in a uniform block of CPU.\nIn his 1988 book, Minsky described dozens of these heterogeneous components he thought might make up a Society of Mind. However, the precise details are not relevant,5 as he freely admitted that they are conjectured. He was only adamant about the overarching scheme: heterogeneous little separated components, not a homogeneous big connected piece.\n5 The conjectured components included “K-lines”, “nomes”, “nemes”, “frames”, “frame-arrays”, etc. Although Minsky meant for this SoM project to last a very long time, building up to general intelligence brick by brick, my literature search shows that there had been no new development since the 2000s, so the overview (Singh 2003) still represents the SOTA of SoM.Perhaps a modern reincarnation of such an idea would be the dream of Internet agents operating in a digital economy, populated by agents performing simple tasks like spam filtering, listening for the price of petroleum, etc. Some agents would interface with reality, while others would interface with agents. Some agents are organized at a higher level into DAOs, created by a small committee of simple “manager agents” serving as the interface and coordinators for other agents. DAOs can interface with other DAOs through little speaker-agents, which consist of a simple text filter for the torrent of information and then outsource to text-weaving agents to compose the actual messages they send out.\n\n\nSeymour Papert\nSeymour Papert, the long-time collaborator of Minsky, was the second author of Perceptrons. To unlock the enigma of Minsky, we must look into Papert’s past as well.\nIn 1958, after earning a doctorate in mathematics, he met Jean Piaget and became his pupil for four years. This experience had a formative effect on Papert. Piaget’s work was an important influence on the constructivism philosophy in education, and Papert would go on to bring constructivism from books to classrooms. He was particularly hopeful that computers can realize the constructivist dream of unlocking the kaleidoscopic creativity that a child can construct.\nThe main theme of Jean Piaget’s work was developmental psychology – how children’s understanding of the world changes as they grow up. What goes on in their mind as they progressively understand that things fall down, what dogs are, and that solid steel sinks but hollow steel might float? Piaget discovered that children did not simply start with a blank sheet of paper and gradually fill in sketchy details of the true model. Instead they constructed little models of small facets of reality that would be modified or completely replaced as they encounter new phenomena that their old models cannot explain. In this way, Piaget claimed that children are “little scientists”.\nA small example illustrates the idea. When children see that a leaf floats on water, but a stone sinks, they add a rule “Soft things float, while hard things sink.”. Then, they see that a hard plastic boat floats too, so they add a rule “Except hard and light things also float.”. Then, they see that a large boat also floats, so they rewrite the entire model to “Flat-bottomed things float, while small-bottomed things sink.”. And so on.\nThere are conservative and radical ways of using Piaget’s research for pedagogy. The conservative approach involves studying how children construct their scientific theories and identifying a sequence of evidence to present to these young scientists so they can quickly reach scientific orthodoxy. For example, we might show children videos of curling and air hockey, then let them play with an air hockey table, following this with guided exercises, so they race through animism, Aristotelian physics, impetus theory, etc, and end up with Newton’s laws of motion.\nThe radical way is to decenter the orthodoxy and let a thousand heterodoxies bloom. Why go for the orthodoxy, when the Duhem–Quine thesis tells us that evidence is never enough to constrain us to only one orthodoxy? And given that objectively no theory deserves the high name of “orthodoxy”, how did the scientific “orthodoxy” become dominant? A critical analysis of the history shows that its dominance over Aboriginal and woman ways of knowing is merely a historical accident due to an alliance with the hegemonic reason of the metropole over the periphery.6\n6 This is not a joke, since decolonial studies literally begin with this assumption.Papert went with the radical way.\nAfter four years of study under Piaget, he arrived in MIT in 1963, and began working with Minsky on various topics, including the Logo Turtle robot, and the Perceptrons book. The computer revolution was starting, and Papert saw computers as a way to bring radical constructivism to children.\nIn the real world, phenomena are limited by nature, and aspiring little heterodoxy-builders are limited by their ability to construct theories and check their consequences. In the computer world, every child could program and construct “microworlds” from their own theories. Thus, computers would bring constructivism to the classroom. Furthermore, the constructed world inside computers could then flow out to the physical world via robots. This is why Papert worked on both Logo the programming language and Logo the turtle robots. In his words, he intended to fight “instructionism” with “constructionism” by bringing the power of the computer to every child, so that they would grow up to be “bricoleurs”, working with whatever little tool they have available doing whatever is necessary to accomplish little things. This is a vital piece in his overarching project of epistemological pluralism, to liberate heterodoxical ways of knowing (S. A. Papert 1994, chap. 7):\n\nTraditional education codifies what it thinks citizens need to know and sets out to feed children this “fish.” Constructionism is built on the assumption that children will do best by finding (“fishing”) for themselves the specific knowledge they need … it is as well to have good fishing lines, which is why we need computers, and to know the location of rich waters, which is why we need to develop a large range of mathetically rich activities or “microworlds.”\n… School math, like the ideology, though not necessarily the practice, of modern science, is based on the ideal of generality – the single, universally correct method that will work for all problems and for all people. Bricolage is a metaphor for the ways of the old-fashioned traveling tinker, the jack-of-all-trades who knocks on the door offering to fix whatever is broken. Faced with a job, the tinker rummages in his bag of assorted tools to find one that will fit the problem at hand and, if one tool does nor work for the job, simply tries another without ever being upset in the slightest by the lack of generality. The basic tenets of bricolage as a methodology for intellectual activity are: Use what you’ve got, improvise, make do. And for the true bricoleur, the tools in the bag will have been selected over a long time by a process determined by more than pragmatic utility. These mental tools will be as well worn and comfortable as the physical tools of the traveling tinker; they will give a sense of the familiar, of being at ease with oneself …\nKitchen math provides a clear demonstration of bricolage in its seamless connection with a surrounding ongoing activity that provides the tinker’s bag of tricks and tools. The opposite of bricolage would be to leave the “cooking microworld” for a “math world,” to work the fractions problem using a calculator or, more likely in this case, mental arithmetic. But the practitioner of kitchen math, as a good bricoleur, does not stop cooking and turn to math; on the contrary, the mathematical manipulations of ingredients would be indistinguishable to an outside observer from the culinary manipulations.\n… The traditional epistemology is based on the proposition, so closely linked to the medium of text-written and especially printed. Bricolage and concrete thinking always existed but were marginalized in scholarly contexts by the privileged position of text. As we move into the computer age and new and more dynamic media emerge, this will change.\n\nAccording to Papert, his project is epistemological pluralism, or promoting different ways of knowing:\n\nThe diversity of approaches to programming suggests that equal access to even the most basic elements of computation requires accepting the validity of multiple ways of knowing and thinking, an epistemological pluralism. Here we use the word epistemology in a sense closer to Piaget’s than to the philosopher’s. In the traditional usage, the goal of epistemology is to inquire into the nature of knowledge and the conditions of its validity; and only one form of knowledge, the propositional, is taken to be valid. The step taken by Piaget in his definition of epistemologie genetique was to eschew inquiry into the “true” nature of knowledge in favor of a comparative study of the diverse nature of different kinds of knowledge, in his case the kinds encountered in children of different ages. We differ from Piaget on an important point, however. Where he saw diverse forms of knowledge in terms of stages to a finite end point of formal reason, we see different approaches to knowledge as styles, each equally valid on its own terms.\n… The development of a new computer culture would require more than environments where there is permission to work with highly personal approaches. It would require a new social construction of the computer, with a new set of intellectual and emotional values more like those applied to harpsichords than hammers. Since, increasingly, computers are the tools people use to write, to design, to play with ideas and shapes and images, they should be addressed with a language that reflects the full range of human experiences and abilities. Changes in this direction would necessitate the reconstruction of our cultural assumptions about formal logic as the “law of thought.” This point brings us full circle to where we began, with the assertion that epistemological pluralism is a necessary condition for a more inclusive computer culture.\n(Turkle and Papert 1990)\n\nThe project of epistemological pluralism erupted into public consciousness during the “Science Wars” of 1990s. After that, it had stayed rather quiet."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#the-perceptron-controversy",
    "href": "essays/posts/perceptron-controversy/index.html#the-perceptron-controversy",
    "title": "The Perceptron Controversy",
    "section": "The perceptron controversy",
    "text": "The perceptron controversy\n\nConnectionism, 1945–1970\nIn the early days, there were several centers of connectionist research, clustered around Frank Rosenblatt, Bernard Widrow, and the Stanford Research Institute (SRI). Out of those centers of research, Minsky and Papert targeted mostly Rosenblatt’s research.\nFrank Rosenblatt’s research had three modes: mathematical theory, experiments on bespoke machines, such as the Mark I Perceptron and the Tobermory, and experiments on serial digital computers, usually IBM machines. He was strongly inclined to building two-layered perceptron machines where the first layer was fixed 0-1 weights, and only the second layer contained real-valued weights learned by the perceptron learning rule. This is precisely the abstract model of the perceptron machine used by Minsky and Papert.\nAfter 4 years of research, he published a summary of his work in (Rosenblatt 1962). In the book, he noted that there were many problems that the perceptron machines could not learn well. As summarized in (Olazaran 1991, 116–21),\n\n… two stimuli (presented one after another) had to occupy nearly the same area of the retina in order to be classified as similar. … The lack of an adequate preprocessing system meant that a set of association units had to be dedicated to the recognition of each possible object, and this created an excessively large layer of association units in the perceptron. … Other problems were excessive learning time, excessive dependence on external evaluation (supervision), and lack of ability to separate essential parts in a complex environment. Rosenblatt (1962, pp. 309-310) included the ‘figure-ground’ or ‘connectedness’ problem in this last point.\n\nA number of perceptrons analyzed in the preceding chapters have been analyzed in a purely formal way, yielding equations which are not readily translated into numbers. This is particularly true in the case of the four-layer and cross-coupled systems, where the generality of the equations is reflected in the obscurity of their implications. … The previous questions [from the first to the twelfth] are all in the nature of ‘mopping-up’ operations in areas where some degree of performance is known to be possible . . . [However,] the problems of figure-ground separation (or recognition of unity) and topological relation recognition represent new territory, against which few inroads have been made.” (Rosenblatt, 1962a, pp. 580-581)\n\n\nAlmost every one of these problems was specifically targeted by the Perceptrons book. For example, the difficulty of testing for “connectedness” was a centerpiece of the entire book, the difficulty of recognizing symmetry was studied by “stratification” and shown to have exponentially growing coefficients (Chapter 7), the requirement for “had to occupy nearly the same area of the retina” was targeted by studies on the limitations of “diameter-limited perceptrons” (Chapter 8), the “figure-ground problem” was targeted by showing “recognition-in-context” has infinite order (Section 6.6), the “generality of the equations is reflected in the obscurity of their implications” was targeted by comments such as “if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head” (Section 13.2), etc.\nBernard Widrow worked mostly in collaboration with Marcian Hoff. Their work is detailed in my essay The Backstory of Backpropagation. In short, they first developed a least-mean-square gradient descent method to train a single perceptron, then proceeded to two-layered perceptrons and predictably failed to develop backpropagation, as the activation function is not differentiable. Thereafter, Widrow gave up neural networks until learning of backpropagation in the 1980s.\n\nWidrow and his students developed uses for the Adaline and Madaline. Early applications included, among others, speech and pattern recognition, weather forecasting, and adaptive controls. Work then switched to adaptive filtering and adaptive signal processing after attempts to develop learning rules for networks with multiple adaptive layers were unsuccessful. … After 20 years of research in adaptive signal processing, the work in Widrow’s laboratory has once again returned to neural networks.\n(Widrow and Lehr 1990)\n\n\nAt the time that Hoff left, about 1965 or 1966, we had already had lots of troubles with neural nets. My enthusiasm had dropped. But we were beginning to have successful adaptive filters, in other words, finding good applications. … So we stopped, basically stopped on neural nets, and began on adaptive antennas very strongly.\nInterview with Widrow, quoted in (Olazaran 1991, 129–30)\n\nSRI had a strong AI program, with luminaries such as Nils Nilsson, Charles Rosen, Duda, and Hart. At first they worked on a series of systems, MINOS I to III.\n\nWhen I first interviewed for a position at SRI in 1961, I was warned by one researcher there against joining research on neural networks. Such research, he claimed, was “premature,” and my involvement in it could damage my reputation.\n(Nilsson 2009, chap. 24.2)\n\nMINOS II was representative of the whole series. Made in 1962, it had 3 layers: input, hidden, output, but only one was trainable. The input-to-hidden layer consists of 100 photomasks. That is, given an input image, that image is filtered through a mask, and the light is focussed by a convex lens to a single photosensitive pixel. If the light level exceeds a threshold, it is a 1. Else, it is a 0. Repeat this 100 times, one for each mask, and we have converted an image to 100 binary bits.\n\n\n\nApplying a list of photomasks on an input image, resulting in a list of binary bits that featurizes the image, which a perceptron will be able to classify. (Minsky and Papert 1988, fig. 13.1)\n\n\nIn the hidden-to-output layer, they ran the standard perceptron learning rule. Presumably, they used hand-designed photomasks because they also had no better training method than the perceptron learning rule. Since they used 0-1 activation functions like everyone else, they were frustrated by the same problem of not doing backpropagation, so they switched to symbolic AI techniques around 1965.\n\n\n\nThe weight matrix of the MINOS II pattern recognition system, consisting of a grid of magnetic cores. The magnetization of a core is proportional to its matrix weight. Matrix weights that are quite literally weighty. The weight matrices are swappable. (Rosen, Nilsson, and Adams 1965)\n\n\nIn 1973, Duda and Hart published the famous “Duda and Hart” book on pattern classification (Duda and Hart 1973). The book contained two halves. The first half was statistical: Bayes, nearest neighbors, perceptron, clustering, etc. The second half was on scene analysis, a symbolic-AI method for computer vision. Indeed, Minsky and Papert promoted it as superior to perceptron networks.7 Shakey the robot, built between 1966 and 1972, was a tour de force of scene analysis, and it could move around a mock-up of an office building, pushing around cubes along the way. Its program was written in LISP, the staple programming language for symbolic AI.\n7 It is instructive to compare the first edition with the second, published in 2001 (Duda, Hart, and Stork 2001). It had become almost completely statistical. There were new chapters on neural networks, Boltzmann machines, decision trees, and so on. In contrast, scene analysis was completely removed.\nIt says something about the obsolescence of scene analysis even in 2001, as Duda and Hart deleted half of their most famous book just to avoid talking about it. In fact, the only mention of “scene analysis” is a condemnation:\n\nSome of the earliest work on three-dimensional object recognition relied on complex grammars which described the relationships of corners and edges, in block structures such arches and towers. It was found that such systems were very brittle; they failed whenever there were errors in feature extraction, due to occlusion and even minor misspecifications of the model. For the most part, then, grammatical methods have been abandoned for object recognition and scene analysis. (Duda, Hart, and Stork 2001, sec. 8.8)\n\n\nI got very interested for a while in the problem of training more than one layer of weights, and was not able to make very much progress on that problem. … When we stopped the neural net studies at SRI, research money was running out, and we began looking for new ideas. (Nilsson, interview)\nAbout 1965 or 1966 we decided that we were more interested in the other artificial intelligence techniques. … Our group never solved the problem of training more than one layer of weights in an automatic fashion. We never solved that problem. That was most critical. Everybody was aware of that problem. (Rosen, interview)\n(Olazaran 1991, 131–33)\n\n\nBill Ridgway (one of Bernard Widrow’s Stanford students) adjusted weights in the first layer of what he called a MADALINE. We had a similar scheme for adjusting weights in the first layer of the MINOS II and MINOS III neural network machines at SRI. Others used various statistical techniques to set weight values. But what stymied us all was how to change weights in more than one layer of multilayer networks. (I recall Charles Rosen, the leader of our group, sitting in his office with yellow quadrille tablets hand-simulating his ever-inventive schemes for making weight changes; none seemed to work out.)\n(Nilsson 2009, chap. 29.4)\n\n\n\nThe perceptron controversy, 1960s\n\nIn the middle nineteen-sixties, Papert and Minsky set out to kill the Perceptron, or, at least, to establish its limitations – a task that Minsky felt was a sort of social service they could perform for the artificial-intelligence community. (Bernstein 1981)\n\nAlthough the book was published only in 1969, close to the end of the perceptron controversy, the influence of Minsky and Papert had been felt years earlier as they attended conferences and disseminated their ideas through talks and preprints, sometimes quarreling on stage. Both sides had their motivations and the conflict was real.\n\nIn order to show the extent of the perceptron controversy, it is interesting to repeat some of the rhetorical expressions that were used in it: ‘many remember as great spectator sport the quarrels Minsky and Rosenblatt had;’ ‘Rosenblatt irritated a lot of people;’ ‘Rosenblatt was given to steady and extravagant statements about the performance of his machine;’ ‘Rosenblatt was a press agent’s dream, a real medicine man;’ ‘to hear Rosenblatt tell it, his machine was capable of fantastic things;’ ‘they disparaged everything Rosenblatt did, and most of what ONR did in supporting him;’ ‘a pack of happy bloodhounds;’ ‘Minsky knocked the hell out of our perceptron business;’ ‘Minsky and his crew thought that Rosenblatt’s work was a waste of time, and Minsky certainly thought that our work at SRI was a waste of time;’ ‘Minsky and Papert set out to kill the perceptron, it was a sort of social service they could perform for the Al community;’ ‘there was some hostility;’ ‘we became involved with a somewhat therapeutic compulsion;’ ‘a misconception that would threaten to haunt artificial intelligence;’ ‘the mystique surrounding such machines.’ These rhetorical expressions show the extent (the heat) of the perceptron controversy beyond doubt. (Olazaran 1991, 112)\n\nCharles Rosen of SRI recalls:\n\nMinsky and his crew thought that Frank Rosenblatt’s work was a waste of time, and they certainly thought that our work at SRI was a waste of time. Minsky really didn’t believe in perceptrons, he didn’t think it was the way to go. I know he knocked the hell out of our perceptron business. (Olazaran 1993, 622)\n\nWhen Perceptrons was finally published in 1969, the connectionist camp was already deserted. The SRI group had switched to symbolic AI projects; Widrow’s group had switched to adapting single perceptrons to adaptive filtering; Frank Rosenblatt was still labouring, isolated, with dwindling funds, until his early death in 1971.8\n8 The 1972 reprinting of Perceptrons included a handwritten note, “In memory of Frank Rosenblatt”. This was not an ironic dedication, as Minsky and Rosenblatt were personally friendly, although their research paradigms had been fighting for dominance.During the last days of Rosenblatt, he worked on a massive expansion of the Mark I Perceptron, the Tobermory (1961–1967). Named after a talking cat, it was built for speech recognition. It had 4 layers with 45-1600-1000-12 neurons, and 12,000 adjustable weights implemented with tape-wound magnetic cores. As usual for Rosenblatt, these adjustable weights are all in the last layer (\\(12000 = 1000 \\times 12\\)). By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines. (George Nagy 1991) Indeed, often during the history of neural networks, someone would think “this calls for a purpose-made computer” and a few years later, Moore’s law obsoleted their effort. A kind of hardware bitter lesson.\n\n\n\nTobermory schematic. (GEORGE Nagy 1963)\n\n\n(Olazaran 1991) gathered evidence that the publication of Perceptrons was not the cause but a “marker event” for the end of the perceptron controversy and the ascendancy of the symbolic AI school. The book was not the neural network killer, but its epitaph.\n\n\nConnectionist retrospectives, 1990s\nFollowing the resurgence of connectionism in the 1980s, Anderson and Rosenfeld conducted interviews with prominent connectionists throughout the 1990s, compiled in (Rosenfeld and Anderson 2000). The perceptron controversy is mentioned several times. Reading the interviews gives one a distinct feeling of Rashomon. The same events are recounted from multiple perspectives. I will excerpt some of the most important ones for the essay.\nJack D. Cowan gave an “eyewitness account” of Minsky and Papert’s role in the controversy, before the publication of the book in 1969.\n\nER: I’m curious about one thing. You said that Minsky and Papert first presented their notions about exclusive-OR in the Perceptron work [in a 1965 conference].\nJC: Well, they first presented their notions about the limitations of perceptrons and what they could and couldn’t do.\nER: They hadn’t gotten to exclusive-OR yet?\nJC: They had, but that wasn’t a central issue for them. The essential issue was, suppose you had diameter-limited receptive fields in a perceptron, what could it compute?\nER: How was that received at that first conference?\nJC: Both of them were quite persuasive speakers, and it was well received. What came across was the fact that you had to put some structure into the perceptron to get it to do anything, but there weren’t a lot of things it could do. The reason was that it didn’t have hidden units. It was clear that without hidden units, nothing important could be done, and they claimed that the problem of programming the hidden units was not solvable. They discouraged a lot of research and that was wrong. … Everywhere there were people working on perceptrons, but they weren’t working hard on them. Then along came Minsky and Papert’s preprints that they sent out long before they published their book. There were preprints circulating in which they demolished Rosenblatt’s claims for the early perceptrons. In those days, things really did damp down. There’s no question that after ’62 there was a quiet period in the field.\nER: Robert Hecht-Nielsen has told me stories that long before Minsky and Papert ever committed anything to a paper that they delivered at a conference or published anywhere, they were going down to ARPA and saying, “You know, this is the wrong way to go. It shouldn’t be a biological model; it should be a logical model.”\nJC: I think that’s probably right. In those days they were really quite hostile to neural networks. I can remember having a discussion with Seymour … in the ’60s. We were talking about visual illusions. He felt that they were all higher-level effects that had nothing to do with neural networks as such. They needed a different, a top-down approach to understand. By then he had become a real, a true opponent of neural networks. I think Marvin had the same feelings as well. To some extent, David Marr had those feelings too. After he got to the AI lab, I think he got converted to that way of thinking. Then Tommy Poggio essentially persuaded him otherwise.\n\nTeuvo Kohonen seemed also angry at the Chomskyan school, for reasons I sketched out in the appendix on the Chomskyans.\n\nI was one of the people suffering from Minsky and Papert’s book [Perceptrons] because it went roughly this way: you start telling somebody about your work, and this visitor or whoever you talk to says, “Don’t you know that this area is dead?” It is something like what we experienced in the pattern recognition society when everything started to be structural and grammatical and semantic and so on. If somebody said, “I’m doing research on the statistical pattern recognition,” then came this remark, “Hey, don’t you know that is a dead idea already?”\n\nMichael A. Arbib thought the book did not cause the neural network winter, but rather caused by the change in funding.\n\nMinsky and Papert basically said that if you limit your networks to one layer in depth, then, unless you have very complicated individual neurons, you can’t do very much. This is not too surprising. … Many people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. I think it was more that a younger generation of computer scientists who didn’t know the earlier work may have used the book as justification for sticking with “straight AI” and ignoring neural nets.\n\nBernard Widrow concurred.\n\nI looked at that book, and I saw that they’d done some serious work here, and there was some good mathematics in this book, but I said, “My God, what a hatchet job.” I was so relieved that they called this thing the perceptron rather than the Adaline because actually what they were mostly talking about was the Adaline, not the perceptron. I felt that they had sufficiently narrowly defined what the perceptron was, that they were able to prove that it could do practically nothing. Long, long, long before that book, I was already successfully adapting Madaline [Madaline = many Adalines], which is a whole bunch of neural elements. All this worry and agony over the limitations of linear separability, which is the main theme of the book, was long overcome.\nWe had already stopped working on neural nets. As far as I knew, there wasn’t anybody working on neural nets when that book came out. I couldn’t understand what the point of it was, why the hell they did it. But I know how long it takes to write a book. I figured that they must have gotten inspired to write that book really early on to squelch the field, to do what they could to stick pins in the balloon. But by the time the book came out, the field was already gone. There was just about nobody doing it.\n\nJames A. Anderson pointed out that during the “winter”, neural networks survived outside of AI.\n\nThis was during the period sometimes called the neural network dark ages, after the Minsky and Papert book on perceptrons had dried up most of the funding for neural networks in engineering and computer science. Neural networks continued to be developed by psychologists, however, because they turned out to be effective models in psychology … What happened during the dark ages was that the ideas had moved away from the highly visible areas of big science and technology into areas of science that did not appear in the newspapers.\n\nDavid Rumelhart had nice things to say about Minsky, with no trace of bitterness. It is understandable as he only started working in neural networks years after the controversy died down.\n\nI always had one course that was like a free course in which I would choose a book of the year and teach out of that. In 1969, I think it was, or maybe ’70, I chose Perceptrons by Minsky and Papert as the book of the year. We then carefully went through it and read it in a group. … This was my most in-depth experience with things related to neural networks, or what were later called neural networks. I was quite interested in Minsky in those days because he also had another book which was called, I think, Semantic Information Processing. That book was a collection, including an article by Ross Quillian. It was a collection of dissertations from his graduate students. In a way, it was Minsky who led me to read about the perceptron more than anybody else.\n\nRegarding Robert Hecht-Nielsen, we have already seen his belief that Minsky was “Darth Vader” and possibly “the Devil”. Unsurprisingly, he was the most embittered, and placed the blame for the 1970s neural network winter squarely on the publication of Perceptrons.\n\nBy the mid-1970s, Minsky and his colleagues (notably Seymour Papert) began to take actions designed to root out neural networks and ensure large and, in their view, richly deserved funding for AI research by getting the money currently being “wasted” on neural networks, and more to boot, redirected. They did two things. First, Minsky and Papert began work on a manuscript designed to discredit neural network research. Second, they attended neural network and “bionics” conferences and presented their ever-growing body of mathematical results being compiled in their manuscript to what they later referred to as “the doleful responses” of members of their audiences.\nAt the heart of this effort was Minsky and Papert’s growing manuscript, which they privately circulated for comments. The technical approach they took in the manuscript was based on a mathematical theorem discovered and proven some years earlier—ironically, by a strong supporter of Rosenblatt—that the perceptron was incapable of ever implementing the “exclusive-OR” [X-OR] logic function. What Minsky and Papert and their colleagues did was elaborate and bulk up this idea to book length by devising many variants of this theorem. Some, such as a theorem showing that single-layer perceptrons, of many varied types, cannot compute topological connectedness, are quite clever. To this technical fabric, they wove in what amounted to a personal attack on Rosenblatt. This was the early form of their crusade manifesto.\nLater, on the strong and wise advice of colleagues, they expunged the vitriol. They didn’t quite get it all, as a careful reading will show. They did a complete flip-flop, dedicating the book to Rosenblatt! As their colleagues sensed it would, this apparently “objective” evaluation of perceptrons had a much more powerful impact than the original manuscript with its unseemly personal attack would have. Of course, in reality, the whole thing was intended, from the outset, as a book-length damnation of Rosenblatt’s work and many of its variants in particular, and, by implication, all other neural network research in general.\nMinsky and Papert’s book, Perceptrons, worked. The field of neural networks was discredited and destroyed. The book and the associated conference presentations created a new conventional wisdom at DARPA and almost all other research sponsorship organizations that some MIT professors have proven mathematically that neural networks cannot ever do anything interesting. The chilling effect of this episode on neural network research lasted almost twenty years."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#the-message-of-perceptrons",
    "href": "essays/posts/perceptron-controversy/index.html#the-message-of-perceptrons",
    "title": "The Perceptron Controversy",
    "section": "The message of Perceptrons",
    "text": "The message of Perceptrons\nMinsky described how he and Papert felt impelled to write the book by a “therapeutic compulsion”, since they were “appalled” by the influence perceptrons:\n\nBoth of the present authors (first independently and later together) became involved with a somewhat therapeutic compulsion: to dispel what we feared to be the first shadows of a “holistic” or “Gestalt” misconception that would threaten to haunt the fields of engineering and artificial intelligence as it had earlier haunted biology and psychology. For this, and for a variety of more practical and theoretical goals, we set out to find something about the range and limitations of perceptrons.\n(Minsky and Papert 1988, 20)\n\n\nOur first formal presentation of the principal results in this book was at an American Mathematical Society symposium on Mathematical Aspects of Computer Science in April 1966. At this time we could prove that \\(\\psi_{CONNECTED}\\) was not of finite order and conjectured that the same must be true of the apparently “global” predicates of symmetry and twins described in §7.3.\nFor the rest of 1966 the matter rested there. We were pleased and encouraged by the enthusiastic reception by many colleagues at the A.M.S. meeting and no less so by the doleful reception of a similar presentation at a Bionics meeting. However, we were now involved in establishing at M.I.T. an artificial intelligence laboratory largely devoted to real “seeing machines”, and gave no attention to perceptrons until we were jolted by attending an I.E.E.E. Workshop on Pattern Recognition in Puerto Rico early in 1967.\nAppalled at the persistent influence of perceptrons (and similar ways of thinking) on practical pattern recognition, we determined to set out our work as a book.\n(Minsky and Papert 1988, 242)\n\nThis 1966 encounter had been corroborated in other places.9 According to Kanal, the workshop was in fact held in 1966 October 24–26, and that only 5 out of 30 papers were about neural networks for pattern recognition. I suppose for Minsky and Papert, 5 out of 30 is 5 too many. (Kanal 1993)\n9 \nIn 1966, a few dozen researchers traveled to Puerto Rico, gathering at the Hilton hotel in San Juan. They met to discuss the latest advances in what was then called “pattern recognition”–technology that could identify patterns in images and other data. Whereas Rosenblatt viewed the Perceptron as a model of the brain, others saw it as a means of pattern recognition… Rosenblatt didn’t even travel to Puerto Rico. Inside the Hilton, the tension emerged when a young scientist named John Munson addressed the conference. Munson worked at SRI… There, alongside a larger team of researchers, he was trying to build a neural network that could read handwritten characters, not just printed letters, and with his presentation at the conference, he aimed to show the progress of this research. But when Munson finished the lecture and took questions from the floor, Minsky made himself heard. “How can an intelligent young man like you,” he asked, “waste your time with something like this?” … “This is an idea with no future.” (Metz 2021)\n\nThe book has become a true classic: everybody wants to have read and nobody wants to read. Taking the opposite approach, I have read the book, despite not wanting to read it.\nIts content can be neatly divided into a greater and a lesser half. The greater half is a mathematical monograph on which functions can be implemented by a single perceptron with fixed featurizers, and the lesser half is a commentary on the wider implications of the monograph. The impact of the work is precisely reversed: most of the impact comes from the commentary derived from the results, and effectively no impact comes from the mathematical results themselves.\nDespite this imbalance, the mathematical work is substantial, and the perceptron controversy turns critically on the pliable interpretation sprouting from the solid structure. Therefore, I have detailed the mathematical content in a separate essay, Reading Perceptrons, to which I refer occasionally to gloss their interpretation.\n\nMinsky and Papert struck back\nIn the 1980s, neural networks rose again to prominence under the name of “connectionism”, prompting an eventual response from Minsky and Papert. The Perceptrons book was reissued in 1988, with new chapters dedicated to rejecting connectionism. They took the 1986 two-volume work of Parallel Distributed Processing (PDP), especially (Rumelhart, Hinton, and Williams 1985) 10, as the representative of connectionism, and made specific objections to them.\n10 This paper was reprinted in (Rumelhart and McClelland 1986, vol. 1, chapter 8), in which Minsky and Papert read it. This paper is often cited for the backpropagation algorithm, which I have discussed in The Backstory of Backpropagation.In the prologue, they staked their claim thus: Connectionism is a mistake engendered by a new generation of researchers ignorant of history; though the theorems of the Perceptrons book apply to only a single perceptron, the lessons extend to all neural networks. To back up the claim, they made specific technical, historical, and philosophical objections, all with the central goal of showing that homogeneous neural networks cannot scale.\n\n… when we found that little of significance had changed since 1969, when the book was first published, we concluded that it would be more useful to keep the original text (with its corrections of 1972) and add an epilogue, so that the book could still be read in its original form. One reason why progress has been so slow in this field is that researchers unfamiliar with its history have continued to make many of the same mistakes that others have made before them.\n… there has been little clear-cut change in the conceptual basis of the field. The issues that give rise to excitement today seem much the same as those that were responsible for previous rounds of excitement. … many contemporary experimenters assume that, because the perceptron networks discussed in this book are not exactly the same as those in use today, these theorems no longer apply. Yet, as we will show in our epilogue, most of the lessons of the theorems still apply.\n\nIn an earlier interview, Minsky reiterated his belief that the proper place of perceptrons is solving tiny problems with tiny perceptron networks.\n\n… for certain purposes the Perceptron was actually very good. I realized that to make one all you needed in principle was a couple of molecules and a membrane. So after being irritated with Rosenblatt for overclaiming, and diverting all those people along a false path, I started to realize that for what you get out of it – the kind of recognition it can do – it is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of. (Bernstein 1981)\n\nThey also urged all AI researchers to adopt the Society of Mind hypothesis, or else face the charge of being unreflective or of drawing lines where none exists. It seems to me that Minsky wrote most of the prologue and epilogue, because in Papert’s solo paper, he went considerably further with sociological interpretation.\n\nThis broad division makes no sense to us, because these attributes are largely independent of one another; for example, the very same system could combine symbolic, analogical, serial, continuous, and localized aspects. Nor do many of those pairs imply clear opposites; at best they merely indicate some possible extremes among some wider range of possibilities. And although many good theories begin by making distinctions, we feel that in subjects as broad as these there is less to be gained from sharpening bound­aries than from seeking useful intermediates.\n… Are there inherent incompatibilities between those connectionist and symbolist views? The answer to that depends on the extent to which one regards each separate connectionist scheme as a self-standing system. If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No. But if we consider each such network as a possible model for a part of a brain, then those two overviews are complementary. This is why we see no reason to choose sides.\n… Most researchers tried to bypass [the technical objections], either by ignoring them or by using brute force or by trying to discover powerful and generally applicable methods. Few researchers tried to use them as guides to thoughtful research. We do not believe that any completely general solution to them can exist …\n\nWe now proceed to the epilogue and its arguments.\n\n1980s connectionism is not that different\nThey speculated on the reason for the revival of neural networks. Was it because of the development of backpropagation, multilayer networks, and faster computers? Emphatically not. In fact, 1980s connectionists were not different from the 1960s connectionists. It is only the ignorance of history that made them think otherwise. In both periods, connectionism was focused on making small-scale experiments and then extrapolating to the largest scale, without mathematical theorems to justify the extrapolation. In both periods, connectionism failed (or would fail) to scale beyond toy problems.\n\nmost of the theorems in this book are explicitly about machines with a single layer of adjustable connection weights. But this does not imply (as many modern connectionists assume) that our conclusions don’t apply to multilayered machines. To be sure, those proofs no longer apply unchanged, because their antecedent conditions have changed. But the phenomena they describe will often still persist. One must examine them, case by case.\n\n\n… the situation in the early 1960s: Many people were impressed by the fact that initially unstructured networks composed of very simple devices could be made to perform many interesting tasks – by processes that could be seen as remarkably like some forms of learning. A different fact seemed to have impressed only a few people: While those networks did well on certain tasks and failed on certain other tasks, there was no theory to explain what made the difference – particularly when they seemed to work well on small (“toy”) problems but broke down with larger problems of the same kind. Our goal was to develop analytic tools to give us better ideas about what made the difference.\n\n\n\nThere is no silver bullet in machine learning\nThere are no general algorithms and there are no general problems. There are only particular algorithm-problem pairs. An algorithm-problem pair can be a good fit, or a bad fit. The parity problem is a bad fit with a neural network trained by backpropagation, but it is a good fit with a Turing machine.\nThere is no general and effective algorithm. Either the algorithm is so general that it is as useless as “just try every algorithm” akin to Ross Ashby’s homeostat, or it is useful but not general. This general lesson is similar to Gödel’s speedup theorem, Blum’s speedup theorem, the no free lunch theorem, etc.\n\nClearly, the procedure can make but a finite number of errors before it hits upon a solution. It would be hard to justify the term “learning” for a machine that so relentlessly ignores its experience. The content of the perceptron convergence theorem must be that it yields a better learning procedure than this simple homeostat. Yet the problem of relative speeds of learning of perceptrons and other devices has been almost entirely neglected. (Minsky and Papert 1988, sec. 11.7)\n\nArthur Samuel’s checker learning algorithm encountered two fundamental problems: credit assignment and inventing novel features. Those two problems are not just for the checker AI, but for all AI. There are no universal and effective solutions to credit assignment, and there are no universally effective solutions to inventing novel features. There could be universal but impractical solutions, such as backpropagation on homogeneous neural networks, Solomonoff induction, trying every Turing machine, etc. There could be practical but not universal solutions, which is precisely what populates the Society of Mind in human brains.\n\nRosenblatt’s credit-assignment method turned out to be as effective as any such method could be. When the answer is obtained, in effect, by adding up the contributions of many processes that have no significant interactions among themselves, then the best one can do is reward them in proportion to how much each of them contributed.\n\n\nSeveral kinds of evidence impel us toward this view. One is the great variety of different and specific functions embodied in the brain’s biology. Another is the similarly great variety of phenomena in the psychology of intelligence. And from a much more abstract viewpoint, we cannot help but be impressed with the practical limitations of each “general” scheme that has been proposed – and with the theoretical opacity of questions about how they behave when we try to scale their applications past the toy problems for which they were first conceived.\n\n\n\nThere is no efficient way to train homogeneous, high-order networks\nThey ask the reader to think back to the lesson of the parity predicate from Chapter 10: Even though it is learnable by a two-layered perceptron network, it would involve weights exponential in the input pixel count, and therefore take a very long time to learn. They expect this to generalize, so that any problem that require some perceptron in the network to have receptive field of size \\(\\Omega(|R|^\\alpha)\\), necessarily require that perceptron to have coefficients growing like \\(2^{\\Omega(|R|^\\alpha)}\\), and therefore taking \\(2^{\\Omega(|R|^\\alpha)}\\) steps to train.\n\nWe could extend them either by scaling up small connectionist models or by combining small-scale networks into some larger organization. In the first case, we would expect to encounter theoretical obstacles to maintaining GD’s effectiveness on larger, deeper nets. And despite the reputed efficacy of other alleged remedies for the deficiencies of hill-climbing, such as “annealing,” we stay with our research conjecture that no such procedures will work very well on large-scale nets, except in the case of problems that turn out to be of low order in some appropriate sense.\nThe second alternative is to employ a variety of smaller networks rather than try to scale up a single one. And if we choose (as we do) to move in that direction, then our focus of concern as theoretical psychologists must turn toward the organizing of small nets into effective large systems.\n\n\n\nThere is no effective use for homogeneous, high-order networks\nFully connected networks, or indeed any neural network without a strong constraint on “order” or “receptive field”, would hopelessly confuse itself with its own echoes as soon as it scales up, unless it has sufficient “insulation”, meaning almost-zero connection weights, such that it effectively splits into a large number of small subnets. That is, a large fully connected network is useless anyway unless it already decomposes into many tiny networks arranged in a Society of Mind.\n\nCertain parallel computations are by their nature synergistic and cooperative: each part makes the others easier. But the And/Or of theorem 4.0 shows that under other circumstances, attempting to make the same network perform two simple tasks at the same time leads to a task that has a far greater order of difficulty. In those sorts of circumstances, there will be a clear advantage to having mechanisms, not to connect things together, but to keep such tasks apart. How can this be done in a connectionist net?\n\n\n… a brain is not a single, uniformly structured network. Instead, each brain contains hundreds of different types of machines, interconnected in specific ways which predestine that brain to become a large, diverse society of partially specialized agencies.\n\n\n\nGradient descent cannot escape local minima\nGradient descent, backpropagation, and all other hill-climbing algorithms are all vulnerable to getting trapped in local optima, and therefore they cannot work – except in problem-architecture pairs where the loss landscape of this particular problem, for this particular architecture, using this particular loss function, on this particular dataset, is a single bump whose width is shorter than this particular learning rate.\nGradient descent is just a form of hill-climbing, when the hill is differentiable. The perceptron learning algorithm can be interpreted as a hill-climbing algorithm too, as it makes localized decision to make one step in this direction or that, one error-signal at a time (Section 11.7). Therefore, the generic ineffectiveness of perceptron learning suggests that gradient descent is also generically ineffective and cannot scale. It does not even have a convergence theorem, so in that sense it’s worse than perceptron learning algorithm.11\n11 This claim is astonishing, now that we see how powerful backpropagation works, and how the perceptron learning rule had crippled neural network research for 30 years. We can understand their sentiments by remembering that they, like most of the academic community in computer science, favored the certainty of mathematical theorems over mere empirical success. Leo Breiman observed that academic statistics had been hamstrung by the same grasp over mathematical certainty, and thus over 95% of its publications were useless. (Breiman 1995)\nWe were very pleased to discover (see section 11.6) that PC [Perceptron Convergence theorem] could be represented as hill-climbing; however, that very fact led us to wonder whether such procedures could dependably be generalized, even to the limited class of multilayer machines that we named Gamba perceptrons. The situation seems not to have changed much – we have seen no contemporary connectionist publication that casts much new theoretical light on the situation. Then why has GD [Gradient Descent] become so popular in recent years? … we fear that its reputation also stems from unfamiliarity with the manner in which hill-climbing methods deteriorate when confronted with larger-scale problems. … Indeed, GD can fail to find a solution when one exists, so in that narrow sense it could be considered less powerful than PC.\n\n\n\nStochastic gradient descent cannot see through the noise\n\nSo far as we could tell, every experiment described in (Rumelhart, Hinton, and Williams 1985) involved making a complete cycle through all possible input situations before making any change in weights. Whenever this is feasible, it completely eliminates sampling noise—and then even the most minute correlations can become reliably detectable, be­ cause the variance is zero. But no person or animal ever faces situations that are so simple and arranged in so orderly a manner as to provide such cycles of teaching examples. Moving from small to large problems will often demand this transition from exhaustive to statistical sampling, and we suspect that in many realistic situations the resulting sampling noise would mask the signal completely. We suspect that many who read the connectionist literature are not aware of this phenomenon, which dims some of the prospects of successfully applying certain learning procedures to large-scale problems.\n\n\n\nDifferentiable activation is just a hack\nUsing differentiable activations for neural networks is an artificial trick of questionable future. It makes the learned boolean functions imprecise, and only appears to redeem itself by allowing backpropagation. However, backpropagation is a dead-end because it will not scale. It is better to look for a method that can directly train multilayer perceptron networks with discrete activation functions.\n\nThe trick is to replace the threshold function for each unit with a monotonic and differentiable function … However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold.\n\n\nWe conjecture that learning XOR for larger numbers of variables will become increasingly intractable as we increase the numbers of input variables, because by its nature the underlying parity function is absolutely uncorrelated with any function of fewer variables. Therefore, there can exist no useful correlations among the outputs of the lower-order units involved in computing it, and that leads us to suspect that there is little to gain from following whatever paths are indicated by the artificial introduction of smoothing functions that cause partial derivatives to exist.\n\n\n\nConnectionists have no theory, so they should not extrapolate from experiments\n\nIn the past few years, many experiments have demonstrated that various new types of learning machines, composed of multiple layers of perceptron-like elements, can be made to solve many kinds of small-scale prob­lems. Some of those experimenters believe that these performances can be economically extended to larger problems without encountering the limitations we have shown to apply to single­ layer perceptrons. Shortly, we shall take a closer look at some of those results and see that much of what we learned about simple perceptrons will still remain quite pertinent.\n\nWithout a mathematical theory, experimental data cannot be extrapolated. If neural networks happen to work well on a problem, it merely shows that the problem is a good fit for this particular architecture trained in this particular way at this particular scale, not anything more general than that.\n\nAs the field of connectionism becomes more mature, the quest for a general solution to all learning problems will evolve into an understanding of which types of learning processes are likely to work on which classes of problems. And this means that, past a certain point, we won’t be able to get by with vacuous generalities about hill-climbing. We will really need to know a great deal more about the nature of those surfaces for each specific realm of problems that we want to solve.\n\n\n… the learning procedure required 1,208 cycles through each of the 64 possible examples – a total of 77,312 trials (enough to make us wonder if the time for this procedure to determine suitable coefficients increases exponentially with the size of the retina). PDP does not address this question. What happens when the retina has 100 elements? If such a network required on the order of \\(2^{200}\\) trials to learn. most observers would lose interest.\n\n\n\nConnectionist experiments can be extrapolated to show that they do not scale\nThough lacking a theory of their own on the operation of multilayer perceptrons, Minsky and Papert proceeded to interpret the connectionist experiment data as showing that neural networks would fail to scale.12\n12 Without a mathematical theory of what neural networks can do, extrapolating from their behavior at small scales to the large scale is impossible and only reflect the bias behind those who make the extrapolation.Connectionists demonstrated that two-layered perceptrons, where both layers were trainable, bypassed the limits described in Perceptrons. For example, (Rumelhart, Hinton, and Williams 1985) showed that several problems unsolvable by a single perceptron – XOR, parity, symmetry, etc – were solved by a two-layered neural network.\n\n\n\nPage 253, Figure 2. Redrawn from (Rumelhart, Hinton, and Williams 1985)\n\n\nWhile the connectionist authors saw the result as a hopeful sign, Minsky and Papert interpreted it as showing that the experiments wouldn’t scale, because the coefficients appeared to grow exponentially – in just the way they proved in Chapter 7.\n\nIn PDP it is recognized that the lower-level coefficients appear to be growing exponentially, yet no alarm is expressed about this. In fact, anyone who reads section 7.3 should recognize such a network as employing precisely the type of computational structure that we called stratification.\nalthough certain problems can easily by solved by perceptrons on small scales, the computational costs become prohibitive when the problem is scaled up. The authors of PDP seem not to recognize that the coefficients of this symmetry machine confirm that thesis, and celebrate this performance on a toy problem as a success rather than asking whether it could become a profoundly “bad” form of behavior when scaled up to problems of larger size.\n\n\n\n\nPapert struck back\nWhile it appears that Minsky was the main author for the new prologue and epilogue, Papert solo-authored (S. Papert 1988), an essay that gave the controversy a uniquely Papert-styled spin. It is an extensive reframing of the perceptron controversy into a social and philosophical issue, with the prediction of ultimate victory for epistemological pluralism:\n\nThe field of artificial intelligence is currently divided into what seem to be several competing paradigms … for mechanisms with a universal application. I do not foresee the future in terms of an ultimate victory for any of the present contenders. What I do foresee is a change of frame, away from the search for universal mechanisms. I believe that we have much more to learn from studying the differences, rather than the sameness, of kinds of knowing.\n\nHe diagnosed the source of the philosophical error as a “category error”.\n\nThere is the same mistake on both sides: the category error of supposing that the existence of a common mechanism provides both an explanation and a unification of all systems, however complex, in which this mechanism might play a central role.\nArtificial intelligence, like any other scientific enterprise, had built a scientific culture… more than half of our book is devoted to “pro-perceptron” findings about some very surprising and hitherto unknown things that perceptrons can do. But in a culture set up for global judgment of mechanisms, being understood can be a fate as bad as death. A real understanding of what a mechanism can do carries too much implication about what it cannot do… The same trait of universalism leads the new generation of connectionists to assess their own microlevel experiments, such as Exor, as a projective screen for looking at the largest macroissues in the philosophy of mind. The category error analogous to seeking explanations of the tiger’s stripes in the structure of DNA is not an isolated error. It is solidly rooted in AI’s culture.\n\nHe then discussed the compute-first interpretation, a “bitter lesson” for the 1980s, before rejecting it.\n\nIn the olden days of Minsky and Papert, neural networking models were hopelessly limited by the puniness of the computers available at the time and by the lack of ideas about how to make any but the simplest networks learn. Now things have changed. Powerful, massively parallel computers can implement very large nets, and new learning algorithms can make them learn. …\nI don’t believe it. The influential recent demonstrations of new networks all run on small computers and could have been done in 1970 with ease. Exor is a “toy problem” run for study and demonstration, but the examples discussed in the literature are still very small. Indeed, Minsky and I, in a more technical discussion of this history (added as a new prologue and epilogue to a reissue of Perceptrons), suggest that the entire structure of recent connectionist theories might be built on quicksand: it is all based on toy-sized problems with no theoretical analysis to show that performance will be maintained when the models are scaled up to realistic size. The connectionist authors fail to read our work as a warning that networks, like “brute force” programs based on search procedures, scale very badly.\n\nConsider Exor, a certain neural network he picked out of the pages of PDP, which learned to perform the infamous XOR task, but only after 2232 examples. Was it slow, or fast? A proper judgment requires a mathematical understanding of the algorithm-problem fit. By extension, to properly judge whether neural networks were good for any specific problem, one must first mathematically understand the fit. He insinuated that the connectionists who were confident that their neural networks were more than a sterile extension of the perceptron did not do their math, unlike he and Minsky.\n\ninstead of asking whether nets are good, we asked what they are good for. The focus of enquiry shifted from generalities about kinds of machines to specifics about kinds of tasks. From this point of view, Exor raises such questions as: Which tasks would be learned faster and which would be learned even more slowly by this machine? Can we make a theory of tasks that will explain why 2,232 repetitions were needed in this particular act of learning?\n… Minsky and I both knew perceptrons extremely well. We had worked on them for many years before our joint project of under standing their limits was conceived… I was left with a deep respect for the extraordinary difficulty of being sure of what a computational system can or cannot do. I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds.\n\n\n\nInterjection\n\nI wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds:\n\nThere is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. (Minsky and Papert 1988, 232)\n\n\nWhat, then, explains the rise of connectionism? Since Papert reframed the fall of perceptron socially, it only stands to reason he would reframe the rise of connectionism as the rise of a social myth caused by other social myths, not by the increase in computing power or new algorithms like backpropagation, convolutional networks, and such. For one, the computing powers used by the breakthrough connectionist models like NETtalk were already within reach even in the 1960s.13 For another, he and Minsky were firm in their conviction that any uniform architecture must scale very badly and that no amount of computing or algorithmic advancement could be anything more than a sterile extension.\n13 NETtalk, a neural network with 3 layers and 18,629 weights, is entirely within reach for the 1960s. Its dataset was built in weeks by hand, and its training took a single night on a Ridge computer that is close to a VAX 11/780. Now, VAX 11/780 has \\(\\sim 1 \\;\\rm{MFLOP/sec}\\), so NETtalk took \\(\\sim 10^{11}\\;\\rm{FLOP}\\) to train. During the 1960s, typical workstations have a computing power of \\(\\sim 0.11 \\;\\rm{MIPS}\\), so NETtalk could be trained in a month.\n\nWe then used the 20,000-word Brown Corpus and assigned phonemes, as well as stress marks, to each of letters. The alignment of the letters and sounds took weeks, but, once the learning started, the network absorbed the whole corpus in a single night. (Sejnowski 2018, 115)\n\n\nI had picked up a Ridge computer, made by a company that is now defunct, but it had the power of a VAX 11/780 which at that time was the standard candle of computer power. … We had a real computer, and we had a real algorithm, and we looked for a do-able project in language. … I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.”\n… In retrospect it was an ideal choice for a problem. It was difficult with conventional techniques, and it was not clear that the network could handle it. … We knew back then there were many local minima in the network, and we knew we were getting trapped. The surprise was that this did not prevent the network from finding good solutions. (Rosenfeld and Anderson 2000, 324–25)\n\n\nMassively parallel supercomputers do play an important role in the connectionist revival. But I see it as a cultural rather than a technical role, another example of a sustaining myth. Connectionism does not use the new computers as physical machines; it derives strength from the “computer in the mind,” from its public’s largely nontechnical awareness of supercomputers. I see connectionism’s relationship to biology in similar terms. Although its models use biological metaphors, they do not depend on technical findings in biology any more than they do on modern supercomputers. … I also see a more subtle, but not less relevant, cultural resonance. This is a generalized turn away from the hard-edged rationalism of the time connectionism last went into eclipse and a resurgent attraction to more holistic ways of thinking."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#rebuttal-to-minsky-and-papert",
    "href": "essays/posts/perceptron-controversy/index.html#rebuttal-to-minsky-and-papert",
    "title": "The Perceptron Controversy",
    "section": "Rebuttal to Minsky and Papert",
    "text": "Rebuttal to Minsky and Papert\n\nInterpreting the XOR problem\nWhen I first heard about the first neural network winter and the Perceptrons book, I was deeply confused by the story. The story went that “Perceptrons proved that the XOR problem is unsolvable by a single perceptron, a result that caused researchers to abandon neural networks”. How could it possibly cause researchers to abandon the field, unless it was news to them? But anyone could see that a single perceptron could only separate linearly separable points, and therefore the XOR problem is unsolvable by a single perceptron. When I first heard the story, I immediately saw why XOR was unsolvable by one perceptron, then took a few minutes to design a two-layered perceptron network that solved the XOR problem. I then noted that the NAND problem is solvable by a single perceptron, after which I immediately knew that perceptron networks are universal since the NAND gate is.\nIf a high school student could bypass the XOR problem in a few minutes, how could it possibly have been news to the researchers in 1969?\nWhen I started researching neural networks properly, the standard story about the XOR problem became more nonsensical the more I learned. The 1943 paper by McCulloch and Pitts (McCulloch and Pitts 1943) already said that their neural networks were equivalent in power to Turing machines. Marvin Minsky’s 1954 PhD thesis (Minsky 1954) develops an entire computer theory out of McCulloch–Pitts neural networks.\nOn the electrical engineering side, perceptron networks were studied under the name of “linear threshold logic” by electrical engineers since the 1950s, who clearly would not have bothered if they could not even make an XOR gate out of it. In fact, in a standard reference from 1965, there are chapters on “Single-Threshold-Element Synthesis by Iteration” – learning a single perceptron by the perceptron learning algorithm – and “Network Synthesis” – which does not imply machine learning, but rather hand-designing perceptron networks. (Dertouzos 1965)\nWhat is going on?\nI believe the story got completely garbled during the teaching process. I am all for changing history for the sake of understanding – history is made for the winners, not the winners made for history – but the standard story about the XOR problem is nonsensical, as I have shown. So how did the story come about?\nI believe this is because Perceptrons contained a host of problems that their restricted form of perceptron machines could not do. The simplest one is the XOR problem. Teachers who just wanted to spend two minutes on the first neural network winter and move on, grabbed this XOR problem and pretended that it was the actual cause of it.14\n14 A mantis was crawling on the wheel of a slowly moving train. It gloated, “I am the prime mover of the train!”. When the caterpillar asked it to prove so, it jumped down and waved its arms in front of the train, which promptly crushed it.\nThis is my retelling of the Taoist story of 螳臂當車.There is one thing left to explain: what is the significance of the XOR problem to the neural network researchers back in the days? It was clearly significant for something, as when the connectionists rose in the 1980s, one of the first things they did was to check that they could solve the XOR problem. Rumelhart read the Perceptrons book very carefully in 1970; it inspired him to go into neural network research, entirely missing its intended message. (Rosenfeld and Anderson 2000, 273) After he developed backpropagation around 1982, he immediately tried to train an MLP on the XOR problem.\n\nWhen I first did the X-OR problem, it took a thousand iterations to solve it. If we thought that was the way it was going to go and that we were going to scale up to a hundred thousand input patterns, my God, we wouldn’t live long enough to see the results. But that’s not the way it’s gone. That problem turned out to be an anomaly. The scaling is about linear. We haven’t hit any exponential curves yet. (Rosenfeld and Anderson 2000)\n\nWhat is the significance of the XOR problem? In the context of the neural network research in the 1960s, the significance becomes clear. Nobody knew how to simultaneously adapt two or more layers well.\nBefore 1962, Rosenblatt had studied both theoretically and experimentally “four-layer perceptron with adaptive preterminal network”, which means a perceptron network with three layers: the first layer random and fixed, and the second and third layers learned (Rosenblatt 1962, vol. 55, chap. 16). However, it had not a single derivative in it. The second layer was learned by the Hebbian learning rule, and the third layer was by the perceptron learning rule.\nMeanwhile, during the early 1960s, Widrow and Hoff trained a single perceptron with gradient descent, then proceeded to try every trick except gradient descent to train a two-layered perceptron network. They gave up and parted ways. Hoff went on to co-invent the microprocessor at Intel, while Widrow applied a single perceptron to adaptive filter design, revolutionizing electrical engineering in the process. These and more of the ridiculous backstory can be read in The Backstory of Backpropagation.\nIn short, due to a variety of unfortunate developments, people spent about twenty years (1950–1970) failing to find an effective algorithm for training the pre-final layers of neural networks. They could train the final layer either by the perceptron learning rule of Rosenblatt or by the Widrow–Hoff rule of gradient descent on the squared error, but that was the extent of the learning they could get the neural networks to do.\nConsider a two-layered neural network. The second layer is easy to learn. What should happen to the first layer? Rosenblatt’s solution was mainly just randomization because he mistakenly believed that the retina was randomly wired to the visual cortex, and he believed in emulating nature. Maybe Rosenblatt was working with the standard knowledge of neuroscience in his time, so he could not have known that neural connections were anything but random – the first of the Hubel and Wiesel papers was published only in 1959. However, it seems that Rosenblatt simply had a strong attachment to randomization, as (Rosenblatt 1962) cites (Hubel and Wiesel 1959) several times, yet he still randomized the first layer for most experiments in the book. Rosenblatt had also experimented with Hebbian learning (Rosenblatt 1962, vol. 55, sec. 16.1), but since he did not use this method extensively, I infer that it did not work well.\nWidrow’s solution was the MADALINE I rule – a complicated hack and a dead end. Without an effective method to train the first layer, those who worked on two-layered neural networks had only two choices: either randomize the first layer or design it by hand. Both choices played right into the hands of Minsky and Papert.\nSeen from the viewpoint of the second layer, the first layer is the featurizer for the raw input. It is intuitively clear that, unless the raw input is featurized and the features are adapted to the problem, the second layer will not be able to solve the problem.\nThe XOR problem requires two layers. Furthermore, if the first layer is not wired correctly, the second layer will not be able to solve it either.\nPut yourself in the place of a 1960s connectionist. How do you solve the XOR problem by a perceptron network? Well, not a single perceptron, as it’s impossible. Not with three layers, because two layers are sufficient, and you already have enough problems with two layers. So, two layers.\nHow to train it? You know only how to fix the first layer and train the second. How do you fix the first layer? Do you randomize it? Unless you use many hidden perceptrons, this will fail with high probability. Do you design it by hand? But then, Minsky and Papert would interject, “You see, you cannot substitute thinking by tabula-rasa learning! You need some intelligent design to get it to work! The network needs the right representations in the hidden layer, and you cannot expect it to learn the representation from a vacuous generality like the fully connected multilayer perceptron, unless you did not get the lesson from our book. You must design it by hand.”.\n\n\n\nAs an example of the kind of MLP that Minsky approves: a hand-designed deep network (Fukushima 1969). Here is someone who actually tried to understand the problem instead of just tossing it to the computer and expect things to magically work.\n\n\nNot to give up, you try one of the hacks like the MADALINE I learning rule, or the Hebbian learning rule, but they are extremely fiddly and unable to learn most of the time unless you tune them just right, and it seems to require a different tuning for problems even slightly more complex than the XOR problem. Minsky and Papert interject again, “You see, there is no universal learning algorithm! You need a bespoke learning algorithm for each problem!”.\nAnd so we stood at the impasse of the 1960s. If only we had tried an activation function, any activation function, other than the dreaded 0-1 activation function…\n\n\n\nA summary of the XOR argument in a flowchart.\n\n\n\n\nWhere did they go wrong?\nBrains are neural networks in hardware – in this regard, there is no controversy since the 1900s. Intelligence is what happens in the brain. This is the occasion for small controversies from the “embodiment cognition” or “externalism” school, like those of James Gibson and Rodney Brooks, but none that has led to anything substantial yet. Therefore, most people agree that intelligence is something that neural networks do, including those people who are otherwise dismissive of neural networks like Minsky and Papert.\nThe abstract of a key anti-connectionist paper (Fodor and Pylyshyn 1988) makes the point that the brain is symbolic at the “cognitive level”, and only beneath that level it is connectionist. Interpreted with sufficient charity, this hypothesis is unfalsifiable. None disputes that the brain is connectionist, and the operation of any hardware is symbolic if you use enough symbols to approximate the real numbers. However, at this level of charity, the hypothesis is also useless, therefore we must interpret less charitably.\nWhat did they really mean? They concretely rejected “Parallel Distributed Processing”, and claimed that trained neural networks work if and only if they implement approximations to symbolic programs, where each symbolic variable is represented locally by a small group of neurons (thus not “distributed”), and the variables are processed serially layer by layer through the network (thus not “parallel”). Further, the symbolic programs they approximate are not any kind of symbolic programs (otherwise we fall back to the trivial claim), but symbolic programs that people tend to write, things that on the small scale resemble subroutines and command line scripts, and on the large scale resemble operating systems and the Cyc project.\nAt this level, it is quantifiable and thus scientifically testable. However, scientific hypotheses become political disputes when large amounts of money or social justice is on the line. We can consider an alternative history with an alternative Minsky and Papert. In this history, they put this in the epilogue:\n\nOur mathematical results indicate that we need multilayer perceptrons as well as efficient methods for training them. Furthermore, simple estimates show that brain-level intelligence likely require computing power up to 10 orders of magnitude larger than currently available, suggesting the need for special hardware boards.\nWe also need to explore alternative architectures capable of correlating global information without using all-to-all connections. Perhaps they should have a two-level structure, with a meta-network generating weights for the network, or perhaps more generic mechanisms for multiplicative interactions. Certain inherently serial operations, such as the connectivity predicate, suggest that there must be ‘serial mode interfaces’ allowing neural networks to call external subroutines. It is a live scientific question whether the number of external subroutines can be kept small. Perhaps a hundred or so would suffice, or perhaps it would turn out that even large neural networks are incapable of most commonsense tasks, in which case the Society of Mind hypothesis would be more viable. However, we consider this an empirical question that can only be answered by attempting to scale up neural networks and seeing what they might do, as a priori estimates of computational difficulty is close to impossible.\n\nWhat distinguishes the two possible Minsky–Paperts? Not the facts present, but their prescientific commitments. Minsky’s commitment to elegant mathematics and simple programming structures led him to insist on things for which he could prove theorems – and to denounce empirical methods, especially if large sums of money might be “misdirected” to large-scale neural network machines. Papert, committed to epistemological pluralism, had no choice but to insist on computers that resembled his ideal society – and to denounce any uniform computational structure as flattening, enframing, and reproducing the hegemonic ideology of universalism.\nFor Papert and Minsky specifically, their claim to be “pro-perceptron” is a sophistry intended to shift the narrative on the perceptron controversy, as they only approved perceptrons with a single layer of learnable parameters. In other words, they were only pro-useless-perceptron. They were trying to kill the project of general large-scale perceptrons, which both Frank Rosenblatt and the new connectionists in the 1980s were working towards.\n\nThere is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. (Minsky and Papert 1988, 232)\n\nThe irony is that decades later, despite the general neglect of neural networks, they quickly overtook symbolic or statistical AI15 as soon as compute and data price fell so low that they had to appear. And so in 2012, Alex Krizhevsky cobbled together 2 GPUs and train a neural network that outperformed every symbolic or statistical AI.16 There are large homogeneous neural networks that work, and there are hints that some of them have small groups of neurons representing symbolic concepts, some of which are engaged in serial computation across the layers. However, to find these hints of symbolic programs, we had to take a large detour through the brute reason of uniform neural network architecture, uniform GPU architecture, and uniform training objectives.\n15 More precisely, classical-statistical AI, with fixed parameters, handcrafted features, and solvable models. A classical-statistical model is constructed as some form of \\(p_\\theta(y|x)\\), where \\(\\theta\\) are the parameters, \\(x\\) are the inputs, and \\(y\\) are the outputs.\nThe difference from neural networks is that for classical-statistical models, \\(p_\\theta\\) allows solvable inference from a dataset, such as by taking the average, derivative, variance, and such. Many of them were straight up linear regressions on handcrafted features (and thus subject to exactly the criticism of Minsky and Papert).\nA good example is the IBM alignment model 1, which can be trained by expectation-maximization with closed form solution (!). To see the difference, compare it with (Bahdanau, Cho, and Bengio 2014), which also learns to align from a corpus, but does not have any closed form solution.16 And if not Krizhevsky and Hinton, how much longer would it have taken? In 2009, Andrew Ng’s research cluster trained a 100M model on GPUs (Raina, Madhavan, and Ng 2009), which suggests that the idea was ripe for taking due to the advance in compute and data, and would have happened around 2010 regardless. The rain might not follow the plow, but the AI does follow the compute and data.Why must we take such a large detour? My guess is twofold. One, the resemblance to neat symbolic programs is partial. Large amounts of computing done by neural networks is only symbolic in the trivial, messy way. Only a small amount is symbolic in the neat way. Two, because symbolic programs suffer from diseconomies of scale. Peering into any large enough software project, be it the Cyc project, or the Linux source code, one feels that it is easier to start anew than to add to it. Perhaps with thousands of years of very patient work and many evolutionary deadends, purely symbolic AI research can succeed in constructing a general intelligence in the elegant style sketched by Minsky. The irony is that symbolic programs do not scale while neural networks scale, the exact opposite of the lesson that Minsky and Papert wished to impart by their book.\nAs an example, the history of computer vision demonstrates the problem with the symbolic AI approach. It is true that some problems, such as the parity problem or the connectedness problem, cannot be efficiently solved by neural networks. However, do they really matter? Why do we care about them? We don’t care about solving connectedness for its own sake, but because it is supposed to be a necessary step on the way towards general machine vision, of understanding real scenes. But the surprise of history is that general machine vision turned out to be far less about provably detecting edges and cubes and cones in a picture, and far more about having a large dataset. In this sense, it’s Minsky and Papert who were misled by their experiments with building block-playing robots in a block world. It’s their work that could not scale.\n\n\nWhat is left of Perceptrons?\nI have never seen a piece of work so systematically opposed to the scaling hypothesis. Reading their theory, I have the feeling that at every turn, I could hear them say, “Neural networks work – if they have less than 100 neurons.”. To their credit, they made falsifiable hypotheses. To their blame, they were almost all proven wrong. Neural networks do scale, to 100 billion and counting. Several standard architectures constitute almost the entirety of neural networks nowadays – MLP, CNN, GNN, LSTM, VAE, and Transformers. Six is quite far from the thousands of architectures they explicitly predicted.\nAmong all the objections to neural networks in the Perceptrons book, almost all were either disproved (the anti-scaling hypothesis) or made irrelevant (the perceptron learning rule).\nRecognizing connectivity is hard and requires a serial program, but that’s fine, because it’s hard for humans too. Learning to solve logical problems is difficult and requires a thousand iterations. Well, it looks inefficient, except that neural networks are still the best we have even 30 years later, so perhaps the XOR problem is just something neural networks have to work hard for. That’s fine – in the worst case, we’ll just let the neural network offload those logical operations to a symbolic program, much like how humans use calculators.\nThe only legitimate remaining problem is the recognition of symmetry. It is hard for all modern neural networks, including convolutional and fully connected versions.17 In any case, if human brains are neural networks and they can instantly recognize symmetries, then it shows that there is some remaining architectural trick we don’t yet know.\n17 It might be solved efficiently with a Transformer, but I need to check this.Therefore, out of all the clever mathematics and wise lessons of Perceptrons, we ended up with… just one problem remaining? Minsky and Papert hoped to show that there would be thousands of different problems, each requiring a bespoke algorithm implemented by a bespoke neural network. In this regard, their project has been fully debunked."
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#appendix-the-three-camps-of-ai",
    "href": "essays/posts/perceptron-controversy/index.html#appendix-the-three-camps-of-ai",
    "title": "The Perceptron Controversy",
    "section": "Appendix: The three camps of AI",
    "text": "Appendix: The three camps of AI\nIn the early days of AI, there were mainly three camps: cybernetics, symbolic systems, and neural networks. In our current age, it seems the other two camps have fallen largely into oblivion. This section gives a brief history and an orienting perspective of their key ideas.\n\nCybernetic AI\nThe founding metaphor of the cybernetic camp was that intelligence is adaptive analog signal processing: homeostasis, Fourier transforms, Kalman filtering, PID control, linear predictive coding, and such.\nThe origin of cybernetics was entangled with the control of machinery in WWII, when you were either the quick or the dead. Norbert Wiener, the mastermind of cybernetics, worked on anti-aircraft gun (AA gun) controllers. As planes flew faster and higher than ever before, AA guns needed to “lead the target” to a greater and greater extent. This put a severe strain on the operator of the AA guns. Wiener constructed electromechanical devices that performed linear prediction of the future trajectory of an aircraft based on its past trajectory. As the aircraft was a human-machine system, Wiener modelled both together as a synthetic whole. After the war, Wiener continued to work on cybernetics, using the analogies he had accumulated during the war.\n\nIf humans do not differ from machines from the “scientific standpoint,” it is because the scientific standpoint of the 1940s was one of men machines at war. The man-airplane-radar-predictor-artillery system is closed one in which it appeared possible to replace men by machines and machines by men. To an antiaircraft operator, the enemy really does act like an autocorrelated servomechanism. … In every piece of his writing on cybernetics, from the first technical exposition of the AA predictor before Pearl Harbor up through his essays of the 1960s on science and society, Wiener put feedback in the foreground, returning again and again to the torpedo, the guided missile, and his antiaircraft director. (Galison 1994)\n\nCybernetics entered the realm of popular consciousness with Wiener’s 1948 bestseller, Cybernetics. In it, we find a curious description of artificial intelligence and self-reproduction from the analog signal processing point of view, detailed in Cybernetic artificial intelligence. The short version is that it was an analog-circuit quine:\n\nThese operations of analysis, synthesis, and automatic self-adjustment of white boxes into the likeness of black boxes … [by] learning, by choosing appropriate inputs for the black and white boxes and comparing them; and in many of these processes, including the method of Professor Gabor, multiplication devices play an important role. While there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. (Wiener 2019, xli)\n\nThe cybernetic approach to AI is a strange chapter in the history of AI, filled with machines too sui generis to have followups. It seems to me that there is no issue with building our way to AI one nonlinear filter at a time, except for technical issues, but the technical issues are insurmountable. One day I might write an essay that gives justice to the cybernetic approach, but as this essay is not about cybernetics, we will only give a whirlwind tour of highlights from the cybernetic approach to AI.\nIn 1948, Ross Ashby built a “homeostat machine”, consisting of four interacting electromechanical controllers. Each controller had some needles that can move in arcs. If one perturbs it, so that the needles move out of their “comfort zones”, the needles would complete an electric circuit, and the controller would start going through every possible setting one by one, until the needles return to their comfort zones.18 The other thing for which Ashby is famous is the “law of requisite variety”, which is equivalent to the theorem that to solve \\(f(x) = y\\), generically, the \\(x\\) must have at least as many dimensions as the \\(y\\).\n18 Perhaps Marvin Minsky’s useless machine was a gentle parody of the homeostat machine. As the homeostat was built in 1948 and the useless machine in 1952, the timing checks out.\n\n\n(Slocum and Yablonina 2024, fig. 2)\n\n\n\n\n\n(Slocum and Yablonina 2024, fig. 3)\n\n\nStafford Beer started his professional life as a business consultant, and devised methods to increase steel plant production efficiency. He understood production management as a problem analogous to that of biological homeostasis – a problem solved by the nervous system – and so he managed production by imitating the nervous system19. He also investigated a wide variety of strange machines, including one that used an entire pond ecosystem as a computer for black-box homeostatic control (Beer 1962):\n19 Stafford Beer might have meant this literally, according to (Pickering 2004):\n\n… it is clear from subsequent developments that the homeostatic system Beer really had in mind was something like the human spinal cord and brain. He never mentioned this in his work on biological computers, but the image that sticks in my mind is that the brain of the cybernetic factory should really have been an unconscious human body, floating in a vat of nutrients and with electronic readouts tapping its higher and lower reflexes …\n\n\nWhy not use an entire ecological system, such as a pond? The theory on which all this thinking is based does not require a knowledge of the black box employed. Accordingly, over the past year, I have been conducting experiments with a large tank or pond. The contents of the tank were randomly sampled from ponds in Derbyshire and Surrey. Currently there are a few of the usual creatures visible to the naked eye (Hydra, Cyclops, Daphnia, and a leech); microscopically there is the expected multitude of micro-organisms. In this tank are suspended four lights, the intensities of which can be varied to fine limits. At other points are suspended photocells with amplifying circuits which give them very high sensitivity. … There is an ecological guarantee that this system stabilizes itself, and is indeed ultra-stable. But, of course, the problem of making it act as a control system is very difficult indeed. I regard the machine as a tending-to-be-stable-but-not-quite homeostat …\n\nIn 1971, he was invited to become the principal architect of Project Cybersyn, which was a nervous system for the Chilean socialist economy, employing “algedonic control” (“algedonic” is Greek for “pain-pleasure”). This project, like president Allende, was shot in the head by the 1973 coup that established a free market economy in Chile. After this, he lived as a hermit, though he still published a few books and did occasional business consulting.(Morozov 2014)\nIn the 1950s, Gordon Pask constructed electrochemical “sensory organs”. He prepared a dish of acidic metal salt solution (such as \\(\\text{FeSO}_4\\)) and then immersed electrodes into it. Metal tends to dissolve in the acidic solution, but applying a voltage through the electrodes pulls metal out of the solution by electrolysis. This allows him to “reward” whatever metallic structure in the dish by applying a voltage. He reported success at growing some simple sensory organs in the dish (Gordon 1959; Cariani 1993):\n\nWe have made an ear and we have made a magnetic receptor. The ear can discriminate two frequencies, one of the order of fifty cycles per second and the other on the order of one hundred cycles per second. The ‘training’ procedure takes approximately half a day and once having got the ability to recognize sound at all, the ability to recognize and discriminate two sounds comes more rapidly. … The ear, incidentally, looks rather like an ear. It is a gap in the thread structure in which you have fibrils which resonate at the excitation frequency. (Gordon 1959)\n\nThe details of the electrochemical ear are lost, and this line of research had no followups.\nA faint echo of Pask’s electrochemical ear was heard in late 1990s, when Adrian Thompson used evolutionary algorithm to evolve circuits on field-programmable gate arrays to tell apart input signals of frequencies \\(1 \\mathrm{~kHz}\\) and \\(10 \\mathrm{~kHz}\\). Curiously, some circuits succeeded at the task despite using no master clock signal. He traced this down to the detailed physical properties that digital circuit design was precisely meant to abstract away from. The circuit functioned precisely because the electronic elements were not digital, but analog.20 The circuits’ performance degraded when outside the temperature range in which they evolved in (Thompson and Layzell 1999; Thompson, Layzell, and Zebulum 1999).\n20 It is as if a linear neural network managed to compute a nonlinear function precisely because floating point operations are not perfect.(Foerster 2017)\n… at \\(43.0^{\\circ} \\mathrm{C}\\) the output is not steady at \\(+5 \\mathrm{~V}\\) for \\(\\mathrm{F} 1\\), but is pulsing to \\(0 \\mathrm{~V}\\) for a small fraction of the time. Conversely, at \\(23.5^{\\circ} \\mathrm{C}\\) the output is not a steady \\(0 \\mathrm{~V}\\) for \\(\\mathrm{F} 2\\), but is pulsing to \\(+5 \\mathrm{~V}\\) for a small fraction of the time. This is not surprising: the only time reference that the system has is the natural dynamical behaviour of the components, and properties such as resistance, capacitance and propagation delays are temperature dependent. The circuit operates perfectly over the \\(10^{\\circ} \\mathrm{C}\\) range of temperatures that the population was exposed to during evolution, and no more could reasonably be expected of it. (Thompson 1996)\n\nContinuing the tradition of one-hit wonders, there was no followup work to this.21\n21 I have tried tracking down what Adrian Thompson has been doing since his late 1990s work. It turned out that the hardware evolution was his PhD work (Thompson 1998). He has almost completely dropped off the face of academia. His website at University of Sussex did not see another update since 2002 and is currently dead. His minimalistic Google Site was created around 2014, and currently only survives on the Internet Archive. There was also a single gif of the circuit in operation, which I decided to download and save for posterity.\n\nSymbolic AI\nThe founding metaphor of the symbolic system camp was that intelligence is symbolic manipulation using preprogrammed symbolic rules: logical inference, heuristic tree search, list processing, syntactic trees, and such. The symbolic camp was not strongly centered, though it had key players like Alan Turing, John McCarthy, Herbert Simon, and Marvin Minsky.\nThe project of symbolic AI is a curious episode in AI history. Despite its early successes such as SHRDLU, LISP, and ELIZA, despite the support of most of AI researchers during 1960–2000, and despite the support of reigning cognitivists in the adjacent field of psychology, it never achieved something as successful as neural networks.\nA brief sketch of the greatest project in symbolic AI might give you a feel for the difficulty involved. Have you ever heard of the Cyc doing anything useful in its 39 years of life? Compare that with something even as small as BERT.\nIn 1984, Douglas Lenat began the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. The Cyc project used a LISP-like symbolic logic language to encode common sense. Even from the vantage point of 1985, it was clear to all that there was a lot of common sense to code in 22, although few could have predicted that Lenat would persevere at it for over 30 years. In 2016, Lenat finally declared the Cyc project “done” and set about commercializing it.\n22 They themselves probably underestimated the difficulty. In 1990, they confidently titled a paper “Cyc: A midterm report” (Lenat and Guha 1990), suggesting that they expected to be done around 1995.\nHaving spent the past 31 years memorizing an astonishing collection of general knowledge, the artificial-intelligence engine created by Doug Lenat is finally ready to go to work. Lenat’s creation is Cyc, a knowledge base of semantic information designed to give computers some understanding of how things work in the real world. … “Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, the company is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri but less predisposed to foolish misunderstandings. Michael Stewart, a longtime collaborator of Lenat’s and the CEO of Lucid, says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies. (Knight 2016)\n\nThat was essentially the last we heard from Cyc.\nWhy has the symbolic AI project failed to scale? It is hard to say, and a definitive answer would not be forthcoming until we have a human level AI as an existence proof of the necessary requirements for scalable AI. However, I can speculate. When looking at this figure from 1985, one is simultaneously filled with respect and sadness, for they were facing impossible odds, and yet they charged right into it.\n\n\n\n(Lenat, Prakash, and Shepherd 1985, fig. 1)\n\n\nTheir “midterm report” only accentuates this sense of tragedy, of seeing them fighting impossible odds, and losing. They saw with clarity that there is no shortcut to intelligence, no “Maxwell’s equations of thought”.\n\nThe majority of work in knowledge representation has dealt with the technicalities of relating predicate calculus to other formalisms and with the details of various schemes for default reasoning. There has almost been an aversion to addressing the problems that arise in actually representing large bodies of knowledge with content. However, deep, important issues must be addressed if we are to ever have a large intelligent knowledge-based program: What ontological categories would make up an adequate set for carving up the universe? How are they related? What are the important facts and heuristics most humans today know about solid objects? And so on. In short, we must bite the bullet.\nWe don’t believe there is any shortcut to being intelligent, any yet-to-be-discovered Maxwell’s equations of thought, any AI Risc architecture that will yield vast amounts of problem-solving power. Although issues such as architecture are important, no powerful formalism can obviate the need for a lot of knowledge.\nBy knowledge, we don’t just mean dry, almanac-like or highly domain-specific facts. Rather, most of what we need to know to get by in the real world is prescientific (knowledge that is too commonsensical to be included in reference books; for example, animals live for a single solid interval of time, nothing can be in two places at once, animals don’t like pain), dynamic (scripts and rules of thumb for solving problems) and metaknowledge (how to fill in gaps in the knowledge base, how to keep it organized, how to monitor and switch among problem-solving methods, and so on). Perhaps the hardest truth to face, one that AI has been trying to wriggle out of for 34 years, is that there is probably no elegant, effortless way to obtain this immense knowledge base. Rather, the bulk of the effort must (at least initially) be manual entry of assertion after assertion. (Lenat and Guha 1990)\n\nI was struck by the same sense of ontological vertigo when looking back at Simon and Newell’s Human Problem Solving, a compendium of their work on decomposing human problem solving into symbolic processes:\n\n\n\n(Newell and Simon 1972, 533)\n\n\n\n\n\n(Newell and Simon 1972, 534)\n\n\nThis sense of vertigo is perhaps best described by Borges in The analytical language of John Wilkins (Borges 2000, 229–32):\n\n… we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.\nThese ambiguities, redundancies, and deficiencies recall those attributed by Dr. Franz Kuhn to a certain Chinese encyclopedia called the Heavenly Emporium of Benevolent Knowledge. In its distant pages it is written that animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained; (d) suckling pigs; (e) mermaids; (f) fabulous ones; (g) stray dogs; (h) those that are included in this classification; (i) those that tremble as if they were mad; (j) in­ numerable ones; (k) those drawn with a very fine camel’s-hair brush; (1) etcetera; (m) those that have just broken the flower vase; (n) those that at a distance resemble flies. The Bibliographical Institute of Brussels also exercises chaos: it has parceled the universe into 1,000 subdivisions, of which number 262 corresponds to the Pope, number 282 to the Roman Catholic Church, number 263 to the Lord’s Day, number 268 to Sunday schools, number 298 to Mormonism, and number 294 to Brahmanism, Buddhism, Shintoism, and Taoism. Nor does it disdain the employment of heterogeneous subdivisions, for example, number 179: “Cruelty to animals. Protection of animals. Dueling and suicide from a moral point of view. Various vices and defects. Various virtues and qualities.”"
  },
  {
    "objectID": "essays/posts/perceptron-controversy/index.html#appendix-the-chomskyans",
    "href": "essays/posts/perceptron-controversy/index.html#appendix-the-chomskyans",
    "title": "The Perceptron Controversy",
    "section": "Appendix: the Chomskyans",
    "text": "Appendix: the Chomskyans\nIn the field of psychology, the neural network vs Minsky divide corresponds quite well with the behaviorism vs cognitivism divide.\nAmong the cognitivists, there is no consensus view about the exact role that neural networks must play. Some, like Fodor and Marr, argue that intelligence is just a hardware-independent program, and brains just happen to use neural networks to run the program due to accidents of evolution and biophysics. Others, like Gary Marcus, grant neural networks a more substantial role in constraining the possible programs – that is, each neural network architecture is designed , and each architecture can only run a certain kind of program efficiently – but they still insist that neural networks must have very particular architectures.\nSome might allow simple learning algorithms with simple environmental stimulus (the universal grammar approach of Chomsky), while others might allow complex learning algorithms with complex environmental stimulus (the society of mind approach of Minsky), but what unifies the cognitivists is that they are against simple learning algorithms applied to complex environmental stimulus. They called this enemy by many names, such as “radical behaviorism”, “Skinnerism”, “perceptrons”, “radical connectionism”, and now “deep learning”.\n\nNoam Chomsky\nThe cognitivist revolution was led by Noam Chomsky against behaviorism during the 1950s, ending with the victory of cognitivism in “higher psychology”, such as linguistics, though behaviorism survived respectably to this day in other aspects of psychology, such as addiction studies and animal behavior.\nIn a curious parallel, just as neural networks for AI became popular again in the late 1980s, statistical methods for NLP returned during the same period. The watershed moment was the series of IBM alignment models published in 1993 (Brown et al. 1993).\nIn the 1950s, language production was modelled by theoretical machines: finite state automata, stack machines, Markov transition models, and variants thereof. We must understand Chomsky’s two contributions to linguistics. On the first part, he constructed a hierarchy of increasingly powerful kinds of language. This hierarchy was in one-to-one correspondence with the hierarchy of theoretical machines, from finite state automata (type-3, or regular) all the way to Turing machines (type-0, or recursively enumerable). On the second part, he rejected statistical language learning and argued for inborn universal grammar.\nChomsky argued, and subsequent linguists have found, that the syntax of all human languages is at the type-2 level, or a context-free grammar. None are regular and almost none are context-dependent. Regular languages are modeled by finite state machines and cannot model arbitrarily deep recursion, whereas context-free languages allow for arbitrarily deep recursion such as center embedding. This fact would come into play later.\nWith the dominance of the Chomsky approach, finite state machines were abandoned, as they are not capable of parsing context-free grammar. You might have heard of the controversy over Pirahã. It is mainly fought over the problem of recursion: does the Pirahã language have recursion or not?23\n23 Tracing the battle lines, I predicted that Pinker would argue that it must have recursion… and I turned out to be wrong. Pinker argued against Chomsky in this case.\n\n“There’s a lot of strange stuff going on in the Chomskyan program. He’s a guru, he makes pronouncements that his disciples accept on faith and that he doesn’t feel compelled to defend in the conventional scientific manner. Some of them become accepted within his circle as God’s truth without really being properly evaluated, and, surprisingly for someone who talks about universal grammar, he hasn’t actually done the spadework of seeing how it works in some weird little language that they speak in New Guinea.”\nPinker says that his own doubts about the “Chomskyan program” increased in 2002, when Marc Hauser, Chomsky, and Tecumseh Fitch published their paper on recursion in Science. The authors wrote that the distinctive feature of the human faculty of language, narrowly defined, is recursion. Dogs, starlings, whales, porpoises, and chimpanzees all use vocally generated sounds to communicate with other members of their species, but none do so recursively, and thus none can produce complex utterances of infinitely varied meaning. “Recursion had always been an important part of Chomsky’s theory,” Pinker said. “But in Chomsky Mark II, or Mark III, or Mark VII, he all of a sudden said that the only thing unique to language is recursion. It’s not just that it’s the universal that has to be there; it’s the magic ingredient that makes language possible.” (Colapinto 2007)\n\nA key principle Chomsky used was the “poverty of stimulus” argument, which he used to argue that humans must have a universal grammar built in at birth, because children cannot possibly learn to speak when they are just a few years old – they cannot possibly have heard and seen enough. For one thing, true recursion can never be learned empirically, because true recursion can only be conclusively proven by observing an infinite number of sentences.\nConsider the simple example of the balanced brackets language. A language learner observes sample sentences from the language and tries to infer the language. Suppose the learner sees a sequence (), (()), ((())), (((()))). What can they conclude? That it is the balanced brackets language? So we ask them to construct another sentence, and they confidently write ((((())))), but then we inform them that they have been tricked! The language is the balanced brackets language – except that the brackets only go 4 levels deep. In general, only by seeing all levels of recursion can the balanced brackets language be conclusively learned.\nApplied to linguistics, Chomsky claimed that statistical learning cannot learn syntax, and all attempts have been “colossal failures”.\n\nJust to illustrate, I’ll take one example that was presented back in the 1950s and has become a sort of a classic case because it’s so trivial. ‘Can eagles that fly swim?’ Okay, simple sentence. Everyone understands it. Any young child understands it. There is a question about it. We know that we associate the word ‘can’ with ‘swim,’ not with ‘fly.’ We’re asking ‘Can they swim?’ We’re not asking ‘Can they fly?’ Well, why is that? A natural answer ought to be that you associate ‘can’ with ‘fly.’ After all, ‘fly’ is the word that’s closest to ‘can,’ so why don’t you just take the closest word and interpret it that way? … Well, that property is universal. It holds up in every language. Languages may do it differently, but they’re going to have the same property. It holds in every construction anyone knows, and it’s just a universal property of language.\nWell, this particular example has taken on a life of its own. For one thing, it’s a poverty of stimulus problem, like Descartes’ triangle. There’s been a huge effort to show that it’s not a problem, that if you just do a complex statistical analysis of complex data, you’ll find that that’s what the child will determine from the data. The approaches are odd in several respects. First, every one is not only a failure but a colossal failure. I’m not going to talk about that. I actually have a recent paper about it with a computer scientist at MIT, Bob Berwick, where we run through a lot of the current proposals but it’s easy to show that they’re all just wildly wrong. But they keep coming. Almost every issue of the main cognitive science journals has another proposal of this sort, so that’s one odd fact. There are many efforts to show that there is nothing puzzling about it, they’re all colossal failures. (Chomsky 2010)\n\nAt last, a testable hypothesis! I asked GPT-4 to Draw a syntax tree for \"Can eagles that fly swim?\", and received this:24\n24 The code it gave failed on the first try, due to an environment variable issue on the Linux virtual machine it ran on. We fixed it after two more plies of conversation.\n\n\n\n\n\nCode\n\n\n\n\n\nimport nltk\nfrom nltk import Tree\nfrom IPython.display import display\nnltk.download('punkt')\nimport svgling\nfrom IPython.display import SVG, display\n\ntree = Tree.fromstring(\"\"\"\n    (SQ\n        (MD Can)\n        (NP\n            (NNS eagles)\n            (SBAR\n                (WHNP (WDT that))\n                (S\n                    (VP (VBP fly))\n                )\n            )\n        )\n        (VP (VB swim))\n    )\n\"\"\")\n\ntree_svg = svgling.draw_tree(tree)\ntree_svg.get_svg().save()\ndisplay(SVG(tree_svg.get_svg()))\n\n\n\n\n\n\nThe syntax tree for “Can eagles that fly swim?”, drawn by GPT-4.\n\n\nChomsky has consistently rejected statistical language learning from the beginning to the end.\n\nBut it must be recognized that the notion of “probability of a sentence” is an entirely useless one, under any known interpretation of this term.\n(Chomsky 1969)\n\n\nIt’s true there’s been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success … which I think is novel in the history of science. It interprets success as approximating unanalyzed data.\nChomsky at the MIT150: Brains, Minds and Machines Symposium (2011), quoted in (Norvig 2017)\n\n\nWell, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They’ve achieved zero… GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It’ll use even more energy and achieve exactly nothing, for the same reasons. So there’s nothing to discuss.\nMachine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding (2022)\n\nPeter Norvig gave a detailed analysis and a rebuttal in (Norvig 2017).\n\n\nThe Chomskyans\nGold’s theorem about language learning in the limit is occasionally cited in the same context as a justification for the “poverty of stimulus” argument. It appears Chomsky did not regard it as a relevant argument (Johnson 2004), and I agree with Chomsky in this respect, as Gold’s theorem is extremely generic.\nAfter the second rise of neural networks, there was a bitter controversy that raged in the 1990s but is now essentially forgotten: the past tense debate. On one side were the connectionists, and on the other were the cognitivists, including Steven Pinker and Gary Marcus (Pinker and Ullman 2002). Tellingly, both Steven Pinker and Gary Marcus sided with the cognitivists. Steven Pinker is best known for his other books such as The Blank Slate, which applies Chomskyan linguistics to general psychology.\nHuman language exhibits a distinctly fractal-like structure: for every rule, there are exceptions, and for every exception, there are exceptional exceptions, and so on. This is called “quasi-regularity”. Sejnowski in an interview described how quasi-regularity shows up in phonology, and offered a perfect demonstration project for the power of neural networks against the Chomskyans:\n\nI went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.” (Rosenfeld and Anderson 2000, 324–25)\n\n(Sejnowski 2018, 75–78) recounts an anecdote about Jerry Fodor, another prominent cognitivist. While Fodor is no longer a hot name in AI nowadays, this anecdote illustrates the Chomsky way of thinking.\n\nIn 1988, I served on a committee for the McDonnell and Pew Foundations that interviewed prominent cognitive scientists and neuroscientists to get their recommendations on how to jump-start a new field called “cognitive neuroscience”. … [Fodor] started by throwing down the gauntlet, “Cognitive neuroscience is not a science and it never will be.” … Fodor explained why the mind had to be thought of as a modular symbol-processing system running an intelligent computer program. [Patricia Churchland] asked him whether his theory also applied to cats. “Yes,” said Fodor, “cats are running the cat program.” But when Mortimer Mishkin, an NIH neuroscientist studying vision and memory, asked him to tell us about discoveries made in his own lab, Fodor mumbled something I couldn’t follow about using event-related potentials in a language experiment. Mercifully, at that moment, a fire drill was called and we all filed outside. Standing in the courtyard, I overheard Mishkin say to Fodor: “Those are pretty small potatoes.” When the drill was over, Fodor had disappeared.\n\nMinsky was himself dismissive of the Chomskyans:\n\n… fairly soon ideas that have been brewing in AI since the 1960’s, on making computers understand significant fragments of natural language, will enter and, I’m sure, soon dominate the main stream of Linguistics. (In the era of these memos, it was the students in AI, almost alone, who carried on the quest for meaningful theories of linguistic processes, when most all other language work was stuck in shallow, syntax-oriented, formalisms.)\n(Minsky 1983)\n\nSimilarly, Gary Marcus has consistently criticized neural network language models since at least 1992 (G. F. Marcus et al. 1992). His theory of intelligence is fundamentally Chomskyan: neural networks can exhibit intelligence but only if they implement rules for symbolic manipulation.25 Moreover, many symbolic rules must be present at birth, by the poverty of stimulus.\n25 See (G. F. Marcus 2003) for a book-length treatment.For example, here is him saying in 1993:\n\nWhether children require “negative evidence” (i.e., information about which strings of words are not grammatical sentences) to eliminate their ungrammatical utterances is a central question in language acquisition because, lacking negative evidence, a child would require internal mechanisms to unlearn grammatical errors. … There is no evidence that noisy feedback is required for language learning, or even that noisy feedback exists. Thus internal mechanisms are necessary to account for the unlearning of ungrammatical utterances.\n(G. F. Marcus 1993)\n\nAnd here is him in 2012, quoting the old past tense debate to urge caution against the hype of AlexNet:\n\nThey learned slowly and inefficiently, and as Steven Pinker and I showed, couldn’t master even some of the basic things that children do, like learning the past tense of regular verbs. By the late nineteen-nineties, neural networks had again begun to fall out of favor. … Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships, and are likely to face challenges in acquiring abstract ideas like “sibling” or “identical to”. They have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. … Norvig is clearly very interested in seeing what Hinton could come up with. But even Norvig didn’t see how you could build a machine that could understand stories using deep learning alone.\n(G. Marcus 2012)\n\nAnd here is him writing in 2018 dismissing deep learning as not working on the problem of general intelligence, just working on applications:\n\nOnce upon a time, before the fashionable rise of machine learning and “big data”, A.I. researchers tried to understand how complex knowledge could be encoded and processed in computers. This project, known as knowledge engineering… A.I. researchers need to return to that project sooner rather than later, ideally enlisting the help of cognitive psychologists who study the question of how human cognition manages to be endlessly flexible. Today’s dominant approach to A.I. has not worked out. Yes, some remarkable applications have been built from it, including Google Translate and Google Duplex. But the limitations of these applications as a form of intelligence should be a wake-up call.\n(G. Marcus and Davis 2018)\n\nAs it happens, 2018 was the year of Transformer revolution in natural language modeling, with BERT and GPT-1. Never one to give up, he continued with the same criticisms as easily as substituting “Transformers” for “recurrent networks”. Given the track record, he is conveniently predictable,26 and we can expect nothing less than his recent criticisms of deep learning in general (G. Marcus 2018) and large language models in particular, repeatedly.\n\n\n\n\n\n26 It would be funny if someone could train a language model to pass the “Gary Marcus test”: impersonate Gary Marcus in a Turing test setup. If such a model were to pass, Marcus would either have to admit that the language model makes sense or accept that what he says is indistinguishable from nonsense.\nThe same could work for Noam Chomsky – a statistical language model who can editorialize against every one of GPT-5, Gemini 2, GPT-6, etc, as soon as they come out. A performative contradiction."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html",
    "href": "essays/posts/perplexity-turing-test/index.html",
    "title": "Predicting AGI by the Turing Test",
    "section": "",
    "text": "This essay explains the Direct Approach proposed by (Barnett and Besiroglu 2023a).1 I encourage you to play with the Direct Approach Interactive Model to explore an interactive simulation using the approach.\n1 The thing is released in a scattered way, typical for an internet-native publication. There is the report (Barnett and Besiroglu 2023a), in the form of a paper – clearly meant to be cited, despite being hard to read. There is the website (Barnett and Besiroglu 2023b), in the form of a blog post – clearly meant to be read, despite not being upper-class enough to be cited in journal papers. Finally there is the interactive model which looks like an optional add-on to the blog post.\nThe Direct Approach framework bounds the compute requirements for transformative AI by extrapolating neural scaling laws. We combine those estimates with simple models of future progress in algorithms, investment, and compute costs to produce a user-adjustable forecast over the date at which TAI will be achieved. (Barnett and Besiroglu 2023b)\n\nFrom the POV of the judge, a Turing test is a sequential test for two statistical hypotheses – “is human” and “is machine”. Under reasonable assumptions, halving the (reducible part of) log-perplexity loss of the language model would double the time it can survive in a Turing test.\nWe can think of the peer-review of scientific papers as a Turing test, and say that AGI has arrived when we have AI scientists that can pass the papers peer-review. This allows us to calculate the log-perplexity loss of the first AGI. If we assume it is just a scaled-up GPT, then assuming the Chinchilla scaling law, it would cost about 200 years of global GDP. This makes it virtually certain that the first AGI will not be a scaled-up GPT."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#abstract",
    "href": "essays/posts/perplexity-turing-test/index.html#abstract",
    "title": "Predicting AGI by the Turing Test",
    "section": "",
    "text": "This essay explains the Direct Approach proposed by (Barnett and Besiroglu 2023a).1 I encourage you to play with the Direct Approach Interactive Model to explore an interactive simulation using the approach.\n1 The thing is released in a scattered way, typical for an internet-native publication. There is the report (Barnett and Besiroglu 2023a), in the form of a paper – clearly meant to be cited, despite being hard to read. There is the website (Barnett and Besiroglu 2023b), in the form of a blog post – clearly meant to be read, despite not being upper-class enough to be cited in journal papers. Finally there is the interactive model which looks like an optional add-on to the blog post.\nThe Direct Approach framework bounds the compute requirements for transformative AI by extrapolating neural scaling laws. We combine those estimates with simple models of future progress in algorithms, investment, and compute costs to produce a user-adjustable forecast over the date at which TAI will be achieved. (Barnett and Besiroglu 2023b)\n\nFrom the POV of the judge, a Turing test is a sequential test for two statistical hypotheses – “is human” and “is machine”. Under reasonable assumptions, halving the (reducible part of) log-perplexity loss of the language model would double the time it can survive in a Turing test.\nWe can think of the peer-review of scientific papers as a Turing test, and say that AGI has arrived when we have AI scientists that can pass the papers peer-review. This allows us to calculate the log-perplexity loss of the first AGI. If we assume it is just a scaled-up GPT, then assuming the Chinchilla scaling law, it would cost about 200 years of global GDP. This makes it virtually certain that the first AGI will not be a scaled-up GPT."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-test",
    "href": "essays/posts/perplexity-turing-test/index.html#turing-test-as-statistical-hypothesis-test",
    "title": "Predicting AGI by the Turing Test",
    "section": "Turing test as statistical hypothesis test",
    "text": "Turing test as statistical hypothesis test\n\nTuring test\nIn the Turing test, there are three players: one judge and two players. The first player is a human, and the second is a machine. The judge asks each player text questions and receives text answers. The judge must decide who is the human.\nWe consider a simplified Turing test. In this test, the judge does not ask, and simply receives one stream of text \\(X_{1:\\infty}\\). The judge must decide whether the stream is produced by the human or the machine, and do so quickly.\nCast in the language of statistical hypothesis testing, we have two hypotheses:\n\n\\(H_0\\): “the stream is produced by the human”;\n\\(H_1\\): “the stream is produced by the machine”.\n\nThe judge would read from the stream \\(X_{1:\\infty}\\), o-n-e- -t-o-k-e-n at a time, and at each token, decide whether to take another one, or announce its judgment: \\(H_0\\) or \\(H_1\\).\nAs the organizers of the Turing test, we would start the test by flipping a fair coin to decide whether to use the human or the machine. Therefore, \\(Pr(H_0) = Pr(H_1)\\), and by Bayes, the posterior log-probability ratio is\n\\[\n\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} = \\ln\\frac{Pr(H_0|X_{1:n})}{Pr(H_1|X_{1:n})}\n\\]\nThis allows us to use the sequential probability ratio test (SPRT). The judge would decide on two decision boundaries, and calculate \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) at each token. It would stop and announce the decision as soon as the quantity exceeds one of the boundaries.\nFor example, suppose the judge wants to decide when the odds ratio is 10 to 1, then it would set the decision boundaries to be \\([-\\ln 10, + \\ln 10]\\). If \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\) goes above \\(+\\ln 10\\) when \\(n = 60\\), then the judge would announce “\\(H_0\\)” at that point.\nThe \\(\\ln 10\\) is a good rule of thumb, which we will use for the remainder of the essay.\n\n\nSequential hypothesis testing\nConsider the following simple equation:\n\\[\n\\underbrace{\\frac 1n \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[ \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{$\\frac 1n D_{KL}(Pr(\\cdot | H_0)\\| Pr(\\cdot | H_1))$}} = \\underbrace{\\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]}_{\\text{negative log-likelihood loss per token}} - \\underbrace{\\frac 1n  \\mathbb{E}_{X \\sim Pr(\\cdot | H_0)}\\left[\\frac{1}{\\ln Pr(X_{1:n}|H_0)}\\right]}_{\\text{entropy rate of the human itself}}\n\\tag{1}\\]\nThe first term is the KL-divergence per token between the machine and the human. Roughly speaking, it is how different they are, per token emitted. It is an information-theoretic quantity.\nThe second term is negative log-likelihood loss per token. This is what language models are trained to minimize. We write it as \\(L\\).\nThe third term is the entropy rate of the human. It is how random the human is. We write it as \\(L_\\infty\\), because it is the theoretical minimal loss that the language model can reach.\nIf the machine is a perfect replica of the human, then the second term is zero, and the first term equals the third term.\nAssuming that the human is an ergodic speakers of English,2 we can sample an infinite stream \\(X_{1:\\infty}\\) from the human, then call up the Shannon–McMillan–Breiman theorem and find that\n2 In short, an ergodic speaker is someone who has only one speech. If you hear it speak once for a very long time, then hear it speak again for a very long time, then you can take the first and shift it around, so that it looks like the second over a very long sub-segment. Ergodic speakers allow you to take the average over a single very long speech, and be assured that it is close to the average over all possible speeches.\nIn long, see the appendix on ergodic theory.\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)} \\to L - L_\\infty\n\\]\nOn the other hand, if the machine is also an ergodic speaker of English, then we can sample an infinite stream \\(X_{1:\\infty}\\) from the machine, then call up the SMB theorem and find that\n\\[\n\\frac 1n \\ln\\frac{Pr(X_{1:n}|H_1)}{Pr(X_{1:n}|H_0)} \\to L' - L_\\infty'\n\\]\nwhere unfortunately, we have the odd \\(L'\\) and \\(L_\\infty'\\), defined by\n\\[\nL' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_0)}\\right], \\quad L_\\infty' := \\lim_n \\frac 1n\n\\mathbb{E}_{X \\sim Pr(\\cdot | H_1)}\\left[\\ln\\frac{1}{Pr(X_{1:n}|H_1)}\\right]\n\\]\nWe can interpret them as the loss of the human at imitating the machine, and the entropy rate of the machine itself. When the machine is close enough to the human, we can take the approximation \\(L' \\approx L,  L_\\infty' \\approx L_\\infty\\).\nNow, define the log-ratio at step \\(n\\) to be \\(r_n := \\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\). During a Turing test, the judge calculates\n\\[\n\\begin{aligned}\nr_0 &= 1 \\\\\nr_1 &= r_0 + \\frac{Pr(X_{1:1}|H_0)}{Pr(X_{1:1}|H_1)} \\\\\nr_2 &= r_1 + \\frac{Pr(X_{1:2}|X_{1:1}, H_0)}{Pr(X_{1:2}|X_{1:1}, H_1)} \\\\\n&\\cdots\n\\end{aligned}\n\\]\nSo, imagine that such a perfect judge is going through a Turing test, upon receiving “my cat is technically”, and we are listening on its thoughts:\n\n“If it were a human, then it would start with ‘my’ with probability \\(0.01\\). If it were a machine, then \\(0.05\\). Therefore, the odds ratio is 2 to 1.”\n“If it were a human, then it would follow ‘my’ with ‘cat’ with probability \\(0.01\\). If it were a machine, then \\(0.033\\). Therefore, the odds ratio is 3 to 1.”\n“If it were a human, then it would follow ‘is’ with ‘my cat’ with probability… I do not know. However, I do know that the odds ratio is 2 to 1. Now that the total odds ratio is 12 to 1, I can decide: \\(H_0\\).”\n\nWe see that the judge does not have to know the probabilities \\(Pr(X_{1:n}|H_0)\\) and \\(Pr(X_{1:n}|H_1)\\), only their ratio. This might be a minor point, but this idea of likelihood ratio is quite important. It is like “I don’t know how often you say ‘cat’ but I know that you say it twice as often than I do!”.\nLet \\(T^*\\) be the time it takes for the judge to decide.\n\\[T^* \\approx \\frac{\\ln 10}{L - L_\\infty}\\]\nIntuitively, each token on average moves the log-probability-ratio away from 0 by another \\((L-L_\\infty)\\). Decision is triggered when it finally moves out of the interval \\([-\\ln 10, +\\ln 10]\\).\nWe are not able to simply look at a few tokens, draw a straight line, and call it a day, because the trajectory of log-probability-ratio is much closer to a random walk with drift. Subjectively, if you were a judge and watching the log-probability-ratio moving, you’d see ups and downs, keeping you in suspense, until it finally crosses the decision boundaries.\n\n\nSlowdown factor\nTo perform the SPRT as described, the judge must know intimately the difference between a human and a machine. Can the judge do that? Can anyone know, with certainty, that I would start my speech with “Forty cats …” with a probability that is exactly 32.42 times that of GPT-3?\nAs a crude approximation, we can model real-world judges as slowed-down version of the perfect judge. We can imagine that at each step, instead of updating the log-ratio by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwe update it by\n\\[\n\\ln r_{n+1} \\leftarrow \\ln r_n + \\frac 1s \\ln \\frac{Pr(X_{n+1}|H_0, X_{1:n})}{Pr(X_{n+1}|H_1, X_{1:n})}\n\\]\nwhere \\(s &gt; 1\\) is the slowdown factor. This implies that if it takes \\(\\sim T\\) tokens for the perfect judge to reach a likelihood ratio of \\(r\\), it would take \\(\\sim sT\\) tokens for a human judge.\n\n\nMeasuring the slowdown factor\nThe slowdown factor \\(s\\) is unknown.\n\nInformed by an internal poll, we enforce a lognormal distribution with a median of 53.1, a 15th percentile estimate of 9.84, and an 85th percentile of 290. (Atkinson 2023)\n\nThe original paper (Barnett and Besiroglu 2023a) contains no estimate of \\(s\\). They did propose to measure it experimentally by running the Turing test with a human judge and two language models. One model \\(H_0\\) “perfectly imitates humans” by simply sampling a random text segment from a corpus, and the other model \\(H_1\\) is a trained language model, finetuned to imitate the same corpus. They claimed that for any piece of text \\(X_{1:n}\\), they can calculate the log-ratio \\(\\ln\\frac{Pr(X_{1:n}|H_0)}{Pr(X_{1:n}|H_1)}\\), but I found it difficult: Suppose \\(X_{1:n} = \\text{ technically fork}\\), which is unlikely but possible, yet the phrase never appears in the corpus, what should be \\(Pr(X_{1:n}|H_0)\\)? We can use one of the many smoothing tricks (Jurafsky and Martin 2023, chap. 3), but this gets complicated.\n\nMy ideas\nWhat I think would work well is if both \\(H_0\\)and \\(H_1\\) are language models, perhaps even the same model with different sampling temperatures, then the human judge only has to distinguish the two models.\nPerhaps we can make this into a gambling game. The human subject would be presented with two long outputs from two hidden Markov models. Then the subject becomes the judge of a Turing test: “Are you seeing the output from machine 0 or machine 1?”. At each step, the subject can either pay a few cents of fake money to see another character, or stop and make a bet with the entire bankroll: “I bet 70% of my bankroll on machine 0 and the rest on machine 1!”. Both bets have payoff odds of \\(2:1\\). I believe that if the cost of seeing another character is just right, the subject would be nudged to make a decision at exactly \\(10:1\\) posterior odds ratio on the two hypotheses “machine 0” and “machine 1”.\nA diffusion guessing game: The user uploads a lot of MIDI musics. The program plays back note by note, but adds increasingly severe distortions (add a gaussian or Poisson noise to each note). The user guesses which music it is. The more noise there is, the longer it should take the user to guess. Compare the length with the Bayesian optimal predictor.\n\n\nHuman-or-not\nThere was one large-scale attempt at the Turing test in early 2023, in a game called “Human or Not?” (Jannai et al. 2023). Human participants took 2-minute conversations, and at the end, had to decide whether they were talking to a human or a bot.3\n3 There was no mention of whether the bots had to decide the same question.\nThe conversations have a “ping-pong” structure that prevents players from sending two consecutive messages without a response, in order to ensure a balanced and dynamic exchange. Each message, limited to a maximum of 100 characters, has to be composed and sent within a 20-second window, and the chat ends after 2 minutes, usually consisting of 4-5 messages from each side. This ensures that players don’t have to wait for too long, so they can remain engaged with the game and a constant suspense is kept. Once the conversation is over, players are prompted to guess whether their conversational partner was a fellow human or an AI bot. (Jannai et al. 2023)\n\nI counted that during a typical message, each side sends \\([20, 40]\\) English words in total, or \\([30, 50]\\) tokens. In \\([60\\%, 70\\%]\\) of trials, the human participant judged correctly. This suggests that the log-ratio achieved after \\([30, 50]\\) tokens is around the range of \\([\\pm \\ln 6/4, \\pm \\ln 7/3]\\). In other words, the average log-ratio per token is\n\\[\n\\frac{[\\ln 6/4, \\ln 7/3]}{[30, 50]} \\in [0.01, 0.03] \\;\\rm{ nat/token}\n\\]\nThey used several different AI, ranging between Jurassic-2, GPT-4, and Cohere. None of them have published their training compute or loss curves. The only good estimate is for GPT-4, which has training cost \\(C = 2\\times 10^{25}\\rm{FLOP}\\).\nAssuming that Chinchilla scaling holds, average log-ratio per token that an ideal judge should achieve is \\(L - L_\\infty = \\frac{1070}{C^{0.154}} = 0.14 \\;\\rm{ nat/token}\\). Therefore,\n\\[s \\in [5, 14]\\]\nI did not expect the estimate to be nearly symmetric around \\(10\\)."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "href": "essays/posts/perplexity-turing-test/index.html#entropy-of-natural-languages",
    "title": "Predicting AGI by the Turing Test",
    "section": "Entropy of natural languages",
    "text": "Entropy of natural languages\nIn Equation 1, we argued that \\(L_\\infty\\) should be interpreted as the entropy rate of the source, usually human-generated English. Unfortunately, unlike that of coin flips or Markov chains, the entropy rate of English cannot be calculated, only estimated. Fortunately, it can be estimated in several ways, and we can check their agreement.\nSince tokenizers are temporary, but English is permanent, we convert all units to \\(\\;\\rm{bit/character}\\) for easy comparison.\n\nChinchilla scaling\nIn the Chinchilla scaling law paper, the authors trained many language models with various sizes from a single architecture family, and fitted a statistical law to the data, giving \\(L_\\infty = 1.69 \\;\\rm{ nat/token}\\) (without error bars, unfortunately) (Hoffmann et al. 2022, 25).\nTo find the effective \\(\\;\\rm{bit/character}\\) for the Chinchilla scaling law, we need to convert \\(\\rm{nat}\\) to \\(\\rm{bit}\\), and \\(\\rm{token}\\) to \\(\\rm{character}\\). The first is easy: \\(1 \\;\\mathrm{bit} = \\ln(2)\\;\\mathrm{nat}\\). The second can be estimated by running a tokenizer over a large natural English corpus. I have estimated this by running the GPT-2 tokenizer on the WikiText-2 corpus, and found that on average,\n\\[\n1 \\;\\rm{token} = 4.5 \\;\\rm{character} = 0.85 \\;\\rm{word}\n\\]\nThus, \\(L_\\infty \\approx \\frac{1.69}{4.5\\times \\ln 2} = 0.54 \\;\\rm{bit/character}\\).\n\n\nTuring-completeness\nUnfortunately, to lower-bound entropy rates, we typically have to make strong assumptions, because in general, lower-bounding entropy is not computable. For example, the entropy of \\(982148086513282306647093844\\dots\\) appears to be \\(\\ln 10\\), but it is in fact zero, because those are the digits of \\(10^{100} \\pi\\) starting at the radix point. In general, this means we have all the hairy problems of measuring the Kolmogorov complexity.\nThere is also another problem: Probabilistically speaking, there is no such thing as the entropy rate of a single sequence. We can convert a single sequence into a stochastic process, by, for example, sampling a random positive integer \\(n\\), then start the sequence at the \\(n\\)-th place. The problem is that we really have two versions of the same word “entropy”, one is by minimal description length, and another is by probability. The two versions are connected by Shannon coding theorem, but they are not the same.\nThis is similar to the Halting problem. It’s possible in general to prove that a machine halts: just run it. It’s not possible in general to prove that it does not halt. Similarly, there is a program \\(P\\), such that given any input \\(x\\), it will output a sequence of numbers \\(P(x)_1, P(x)_2, \\dots\\) that can converge to the Kolmogorov complexity from above: \\(P(x)_1 &gt; P(x)_2 &gt; \\cdots, \\lim_n P(x)_n = K(x)\\). The program just runs every possible Turing machine in parallel.\nThere are two catches.\nOne, it is impossible to know in general whether \\(P(x)_n\\) is the last time it will output, or whether if we wait a few more eternities, we will see another output \\(P(x)_{n+1}\\).\nTwo, it is also impossible to have a program \\(P'\\) that approaches it from below: \\(P(x)_1 &lt; P(x)_2 &lt; \\cdots, \\lim_n P(x)_n = K(x)\\). Otherwise, we would have a machine that can calculate the Busy Beaver function.\n(Feutrill and Roughan 2021) reviews the entropy rate formulas for several commonly used models. An ergodic Markov chain with stationary distribution \\(\\pi_i\\) and transition probabilities \\(p_{ij}\\) has entropy rate \\(-\\sum_{ij}\\pi_i p_{ij}\\ln p_{ij}\\). For the hidden Markov model, there is no known closed-form formula in the transition probabilities, though there are upper and lower bounds (T. M. Cover and Thomas 2006, Theorem 4.5.1).\n\n\nGuessing game\nThe earliest attempt to measure the entropy rate of English is by Shannon himself (Shannon 1951): \\([0.6, 1.3] \\;\\rm{bit/character}\\). He obtained the estimate by presenting human subjects \\(n-1\\) characters from a text, and ask them to guess the next character repeatedly, until they got it right. In this case, the optimal strategy is to construct the \\(n\\)-gram table, and pick the argmax character for the given \\((n-1)\\)-gram, then the arg-next-max, and so on.\nLet \\(N\\) be the total number of characters allowed – Shannon’s experiment used \\(N = 27\\), with 26 lowercase letters and one white space. Let \\(p_k\\) be the frequency that the subject makes exactly \\(k\\) guesses – including the correct guess, so that \\(\\sum_{k=1}^N p_k = 1\\). By convention, \\(p_{N+1} := 0\\). Shannon derived both an upper and a lower bound for the entropy per character:\n\\[\n\\sum_{k=1}^N k(p_k - p_{k+1}) \\ln k \\leq H \\leq - \\sum_{k=1}^N p_k \\ln p_k\n\\]\nThe upper bound is proved by Shannon’s source coding theorem. Taking a human subject, copy it, then they can be used as an encoder-decoder pair.4 The lower bound is not only tricky to prove, but also wrong in general. It is only correct when the human subject is the optimal \\(N\\)-gram predictor, and when the language is exactly generated by an \\(N\\)-gram model.5\n4 It still works even if the humans are pseudorandom. We just have to whisper the same RNG seed into both humans’ ears, and then they would behave in the same pseudorandom way.5 The simplest counterexample: Suppose the source is binary, and satisfies \\(X_{n+1} = X_{n} + 1 \\mod 2\\), so it has zero entropy. Nevertheless, the human intentionally guesses wrong the first time. Therefore, we have \\(p_2 = 1\\), and we have violated the lower bound by \\(2\\ln 2 &gt; 0\\).\nThis source can be made ergodic by adding an \\(\\epsilon\\) amount of coin-flip noise: \\(X_{n+1} = X_{n} + 1 \\mod 2\\) with probability \\(1-\\epsilon\\). This would still give us \\(2\\ln 2 + O(\\epsilon) &gt; O(\\epsilon \\ln \\epsilon)\\).(Burton and Licklider 1955) uses the same method as Shannon, but with longer contexts and texts from more books (Shannon sampled all passages from just one book). They found that English has “redundancy” in \\([0.6, 0.8]\\), meaning that English has entropy rate \\(\\ln 27 \\times (1- [0.6, 0.8]) = [0.7, 1.3] \\;\\rm{bit/character}\\).\nOver the years, others devised other methods to estimate this entropy. For example, (T. Cover and King 1978) used a gambling game estimation, in the style of the Kelly criterion. Subjects were required to divide their entire bankroll into 27 differently-sized bets over 27 possibilities (26 letters and 1 whitespace). The right bet pays back 27-fold, and the other bets are lost. Let \\(S_n\\) be the size of bankroll after \\(n\\) rounds of betting, then\n\\[\nH \\leq \\ln 27 - \\limsup_n \\frac 1n \\ln S_n\n\\]\nThey found that \\(H \\leq 1.3 \\;\\rm{bit/character}\\).\nThe guesser does not have to be a human. It can very well be a language model. (Brown et al. 1992) made a simple trigram model over the Brown corpus (600 million words), and found that it gives \\(H \\leq 1.75 \\;\\rm{bit/character}\\). (Behr Jr et al. 2002) used a model that combines multiple n-gram models, giving \\(H \\leq 1.46 \\;\\rm{bit/character}\\).\nA more recent attempt in 2022 is crucial, as it seems to show humans are already surpassed by the best language models like GPT-3 in terms of reaching low perplexity, but I haven’t yet studied it in detail.\n\n\nLossless compression\nAnother way to estimate is by lossless compression of a large corpus, since the entropy rate is the lower bound on compression rate. In more detail, if you have a source of information emitting symbols, and its symbol stream has an entropy rate of \\(x \\;\\mathrm{bit/symbol}\\), then it takes at least \\(\\sim xl\\) bits to encode a long segment with \\(l\\) symbols. Furthermore, this lower bound is approachable using the entropy encoding.\nThe Hutter prize is a competition for compressing a \\(10^9\\)-byte corpus from the English Wikipedia (enwik9). For the size of the finished product, both the algorithm and the compressed data must be counted. In particular, if a neural network is used, then the size of the neural network weights must be counted as well.\nThe enwik9 dataset is in XML format, and thus contains a lot of non-English content like &lt;timestamp&gt;2005-12-27T18:46:47Z&lt;/timestamp&gt;. It has \\(10^9\\) bytes. It is tricky to decide how to clean it up to remove all the XML formatting. As a simple estimate, we simply counted its characters:\n\\[\n997,520,891 \\text{ characters} = 1,000,000,000 \\text{ bytes}\n\\]\nTherefore, the entropy rate is\n\\[\n\\frac{8\\times 10^8 / 997,520,891}{\\text{compression ratio}} = \\frac{8.02}{\\text{compression ratio}}\\;\\rm{bit/character}\n\\tag{2}\\]\nThe standard zip algorithm can compress it down to about 300 Mb in size, a compression ratio of \\(\\sim 3\\times\\). Over the years, the progress has been slow but somewhat steady. The current winning entry (Kaido Orav, 2024) has a compression ratio of \\(8.88\\times\\). If we extrapolate the prize-winning entries over the years, it seems that the best possible compression ratio is \\(\\sim 10\\times\\).\nSimilar to the Hutter prize, the Large Text Compression Benchmark also asks for compressing the enwik9 dataset. However, there is no limit to the algorithm runtime or size, so the compression ratio for this benchmark is always higher. Currently (2024-01-19), the maximal compression rate reached is \\(9.35\\times\\) with nncp v3.2, which uses a small Transformer model.\n(Grassberger 2002) used a substitutional compression algorithm with increasingly large codebooks. When the codebook had 6000 codes, the algorithm gave \\(h \\leq 1.82 \\;\\rm{bit/character}\\). By extrapolating the {codebook size}-{entropy rate} curve to an infinitely large codebook, they estimated that English has entropy rate \\(0.7 \\pm 0.2 \\;\\rm{bit/character}\\).\n\n\nSummary\n\n\n\nestimate\nmethod\nraw number\neffective entropy rate (bit/char)\n\n\n\n\n(Grassberger 2002)\ncompression, extrapolation\n\\(0.7 \\pm 0.2 \\;\\rm{bit/character}\\)\n\\(\\sim[0.5, 0.9]\\)\n\n\nHutter prize (Kaido Orav, 2024)\ncompression\ncompression ratio \\(\\geq 8.88\\)\n\\(\\leq 0.90\\)\n\n\nHutter prize extrapolated\ncompression, extrapolation\ncompression ratio \\(\\sim 10\\)\n\\(\\sim 0.80\\)\n\n\nLarge Text Compression Benchmark (nncp v3.2, 2023)\ncompression\ncompression ratio \\(\\geq 9.35\\)\n\\(\\leq 0.86\\)\n\n\n(Shannon 1951)\nguessing game\n\\(\\in [0.6, 1.3] \\;\\rm{bit/character}\\)\n\\(\\in [0.6, 1.3]\\)\n\n\n(Burton and Licklider 1955)\nguessing game\n\\(\\in [0.6, 1.3] \\;\\rm{bit/character}\\)\n\\(\\in [0.7, 1.3]\\)\n\n\n(T. Cover and King 1978)\nguessing game\n\\(\\leq 1.3 \\;\\rm{bit/character}\\)\n\\(\\leq 1.3\\)\n\n\n(Brown et al. 1992)\n3-gram language model\n\\(\\leq 1.75 \\;\\rm{bit/character}\\)\n\\(\\leq 1.75\\)\n\n\n(Behr Jr et al. 2002)\nn-gram language model\n\\(\\leq 1.46 \\;\\rm{bit/character}\\)\n\\(\\leq 1.46\\)\n\n\n(Kaplan et al. 2020)\nTransformer language model, extrapolation\n\\(\\sim 1.7 \\;\\rm{nat/token}\\)\n\\(\\sim 0.57\\)\n\n\n(Hoffmann et al. 2022)\nTransformer language model, extrapolation\n\\(L_\\infty = 1.69 \\;\\rm{nat/token}\\)\n\\(\\sim 0.54\\)6\n\n\n\n6 Because the tokenizers differ, the same nat/token translates to different bit/char.Notably, the above table has mostly upper bounds, and only one dubious lower bound (by Shannon) from 1951. Perhaps lower bounds can be established by using randomness extractors on a large corpus, and checking that the output from the extractor passes pseudorandomness tests.\nMost of the data seems to be centered around 0.8 bpc. The one outlier is the Chinchilla scaling law estimate: 0.54 bpc. I have found that a lot hinges on the exact tokenizer-dataset fit, which is why tokenization is so annoying and I wish people would try to do away with it, or at least report bit-per-character in addition to nat-per-token."
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#sec-forecasting-agi",
    "href": "essays/posts/perplexity-turing-test/index.html#sec-forecasting-agi",
    "title": "Predicting AGI by the Turing Test",
    "section": "Forecasting AGI",
    "text": "Forecasting AGI\nAccording to the Chinchilla scaling law (Hoffmann et al. 2022), if we have a fixed amount of computing budget \\(C\\), by choosing the model and dataset size correctly, the minimal reducible loss achievable is\n\\[\nL - L_\\infty = \\frac{1070}{(C/\\;\\rm{FLOP})^{0.154}} \\;\\rm{nat/token}\n\\tag{3}\\]\nAssuming a slowdown factor \\(s\\), that the judge decides when the odds ratio is \\(r:1\\), and the Chinchilla scaling law, we have a direct method to predict how long a language model can survive in a Turing test, according to the cost of training compute \\(C\\):\n\\[T^* \\sim \\frac{s\\ln r}{1070}(C/\\;\\rm{FLOP})^{0.154} \\;\\rm{token}\\]\nThis gives, as a rule of thumb, \\(100\\times\\) compute means \\(2 \\times\\) length of survival in a Turing test.\nFor example, assuming a slowdown factor of \\(s=10\\), and that the judge decides when the odds ratio is \\(10:1\\), for a language model to survive for 1000 tokens, it needs\n\\[L - L_\\infty \\leq 10 \\times \\ln 10 / 1000 = 0.023 \\;\\rm{nat/token}\\]\nIf GPT-4 costs \\(2\\times 10^{25} \\;\\rm{FLOP}\\) in compute, and \\(1 \\;\\rm{word} \\approx 1.2 \\;\\rm{token}\\), then\n\\[T^* \\approx 170 \\text{ tokens} \\approx 150 \\text{ words}\\]\nmeaning it has a good chance of passing the Turing test if limited to only 150 words. For context, the Attention is All You Need paper has an abstract that’s 200 tokens long.\nA typical scientific paper is about 4000 words long, which is \\(27\\times\\) that of 150 words, so it would need \\(27^{1/0.153} = (2\\times 10^9)\\times\\) that of compute. Assuming that GPT-4 cost 10 million USD to train, this hypothetical AI would cost \\(2\\times 10^{16}\\) USD, or 200 years of global GDP2023.\nThis implies that the first AGI will not be a scaled-up GPT – autoregressive transformer generatively pretrained on a lightly filtered text dataset. It has to include something else, perhaps multimodal data, high-quality data, better architecture, etc. Even if we were to attempt to merely scale it up, turning earth into a GPT-factory,7 with even 50% of global GDP devoted,8 and with 2% growth rate forever, it would still take 110 years,9 arriving at year 2133. Whole brain emulation would likely take less time.10\n7 Consider this anecdote from Edward Teller:\n\nThe possibilities of developing an atomic weapon and the desirability of doing it secretly were discussed at a Princeton University conference in which I participated in March 1939… Bohr said this rare variety could not be separated from common uranium except by turning the country into a gigantic factory. Bohr was worried that this could be done and that an atomic bomb could be developed–but he hoped that neither could be accomplished. Years later, when Bohr came to Los Alamos, I was prepared to say, “You see…” But before I could open my mouth, he said: “You see, I told you it couldn’t be done without turning the whole country into a factory. You have done just that.” (Teller and Brown 1975)\n\n8 Only in a life-or-death situation does 50% of GDP get devoted to one purpose. For example, that is about the level of GDP devoted to war production during WWII in the major combatant countries. The USA spent 4 trillion USD2011 over 6 years out of an annual GDP of 1.3 trillion USD2011.9 Solve for \\(x\\) in \\(200 = \\sum_{k=0}^x 0.5 \\times 1.02^k\\).10"
  },
  {
    "objectID": "essays/posts/perplexity-turing-test/index.html#sec-ergodic-theory",
    "href": "essays/posts/perplexity-turing-test/index.html#sec-ergodic-theory",
    "title": "Predicting AGI by the Turing Test",
    "section": "Appendix: Ergodic theory",
    "text": "Appendix: Ergodic theory\nSince we used ergodic theory during the essay, we should quickly explain what it is about. This section is foundational, but the full complexity is not necessary.\n\nMeasure-theoretic POV\nI know, you know too, nobody really likes measure theory any more than pianists like practicing scales hundreds of times. Still, it is at the right level of abstraction for many theories, including probability.\nWe omit all mentions of “almost-everywhere”, “except on a set of measure zero”, and similar annoying phrases. As long as you never make a union of uncountable many subsets, you will not be hurt by this omission.\nA probability space is a measurable space with a measure of \\(1\\). We write it as \\((\\Omega, \\mathcal B, Pr)\\), where \\(\\mathcal B\\) is the sigma-algebra of measurable sets, and \\(Pr\\) is the probability measure. We also write \\(\\mu\\) for the measure.11\n11 Pronounced “mu” – it is a pun because both “mu” and “measure” starts with “m”.We consider a single measurable function \\(T : \\Omega \\to \\Omega\\), and call it the shift map.\nWe demand that \\(T\\) must preserve measure. That is, \\(\\forall S \\in \\mathcal B\\), we have \\(Pr(T^{-1}(S)) = Pr(S)\\).\nA subset is measurable iff it is an element of \\(\\mathcal B\\). A measurable set is also called an event.\nA subset \\(S \\in \\mathcal B\\) is \\(T\\)-invariant iff \\(T^{-1}(S) = S\\) almost everywhere.12 Let \\(\\mathcal I\\) be the set of all \\(T\\)-invariant subsets:\n12 That is, except on a subset of measure zero: \\(Pr(T^{-1}(S) - S) = 0\\) and \\(Pr(S - T^{-1}(S)) = 0\\). This is the last time we will measure this.\\[\n\\mathcal I := \\{S \\in \\mathcal B : T^{-1}(S) = S\\}\n\\]\nNow, obviously any set of measure zero or one are \\(T\\)-invariant. We say that those are trivially \\(T\\)-invariant. We say that \\(T\\) is ergodic iff \\(\\mathcal I\\) has only such trivial subsets. In other words, \\(T\\) is ergodic iff it cannot be factored into two nontrivial chunks:\n\\[\nS, S' \\text{ partitions } \\Omega,\\quad \\text{such that } T^{-1}(S) = S ,\\; T^{-1}(S') = S',\\; Pr(S) &gt; 0 ,\\; Pr(S') &gt; 0\n\\]\nWe usually ask \\(T\\) to also be ergodic, though sometimes we don’t need that.\nErgodic maps have many very good properties. We will use the following one. For the theorem, you can picture it as the real space \\(\\mathbb{R}^n\\) with the gaussian probability distribution, but in fact, it applies for just about everything we would care about, such as the space of English texts, queuing jobs, random walks, etc.13\n13 Except pathological examples constructed by logicians who have nothing better to do than to care about the continuum hypothesis, large cardinals, and the arithmetic hierarchy. Those who desire the rigor-mortis of logic, let them have it.\nTheorem 1 (Dense orbits) If the state space is a topological space with a countable basis, and any nonempty open set has positive measure, then almost any \\(X\\in\\Omega\\) has a dense orbit.\n\n\nProof. Let \\(U\\) be a nonempty open set.\n\\(\\Omega - \\cup_{i \\geq 0} T^{-i}U\\) is \\(T\\)-invariant, and since it excludes \\(U\\), it does not have the full measure. Since \\(T\\) is ergodic, the set actually has zero measure.\nNow, \\(\\cup(\\Omega - \\cup T^{-i}U)\\) is a union of countably many zero-measure sets, so it still has zero measure. By expanding the definition, this is the set of all points with non-dense orbit.\n\nFinally, there is a common theme in ergodic theory. There are rigorous versions of it, but instead of going for rigor, the spirit is more important:\n\nTheorem 2 (ergodic decomposition) Any interesting map is a partition/sum/integral of ergodic maps.\n\nFor example, the shear map on the unit square \\([0, 1]^2\\) defined by\n\\[\n(x, y) \\mapsto (x, x+y \\mod 1)\n\\]\ncan be thought of as an integral over rotations: For each \\(x \\in [0, 1]\\), we have \\(T_x : y \\mapsto x+y\\mod 1\\). For almost all \\(x\\in [0, 1]\\), we have \\(T_x\\) an irrational rotation, thus ergodic.\n\n\nSequence POV\nWe must interpret the language of measure theory, which is dead like chalk dust, back into the language of sequence predictions, which is alive like reinforced concrete.\nEach point in the state space \\(X\\in \\Omega\\) is a text: a stream of tokens infinite both forwards and backwards. The state space \\(\\Omega\\) is the all possible texts \\((X_n)_n\\). We assume that all tokens come from the same finite-size alphabet, for example, the 128 ASCII symbols.\nThe shift map on the state space \\(T : \\Omega \\to \\Omega\\) is defined by moving the origin to the right by one:\n\\[\nT(\\dots, X_{-1}, X_0, X_1, \\dots) := (\\dots, X_0, X_1, X_2, \\dots)\n\\]\nThe shift map is measure-preserving, meaning that the process is stationary: We could have started reading at any point, and we would still expect to see the same kind of probability distribution. It would not be like “Sorry, the word ‘cat’ appears with zero probability when \\(n \\geq 1000\\).”. It would be like “No matter where we start reading, we should expect to the first three tokens to be ‘cat’ with probability \\(10^{-4}\\).”.\nRepeatedly applying the shift map \\(T\\) is just reading through the stream, one token at a time:\n\\[\n\\text{...Lorem ipsum ...} \\mapsto \\text{...orem ipsum d...} \\mapsto \\text{...rem ipsum do...} \\mapsto \\cdots\n\\]\nA periodic point of \\(T\\) is a text that repeats itself like a broken record. For example, \\(X := \\text{... and and and ...}\\) satisfies \\(T^4X = X\\).\nA \\(T\\)-invariant set \\(S\\subset \\Omega\\) is a set of texts, such that if we take any text \\(X\\) from \\(S\\), and jump either forwards or backwards for an arbitrary amount, we get another set in \\(S\\). In other words, \\(S\\) is a set of token streams where there is no origin: you can start reading from any token.\nA probability distribution over \\(\\Omega\\) describes the probability of observing various kinds of text streams.\nIf we can partition \\(\\Omega\\) into two subsets \\(P, Q\\), with probabilities \\(\\epsilon &gt; 0, 1-\\epsilon &gt; 0\\), then it means that any text from \\(P\\) is different from any text from \\(Q\\), after any shift. It is as if there are two languages, and each text can be exclusively written in one language only.\nWe wish to consider only texts created by some imaginary “universal English speaker”. In particular, we do not want it to get stuck in one sub-language of English, then never escape from it. That is, we assume the universal speaker is ergodic.\nNow imagine that we randomly sample two pieces of text generated by the universal speaker, and we shift the first text around to match it against the second. By Theorem 1, the orbit of the first text is dense in the space of all possible English texts spoken by the universal speaker. We can gamify this situation thus:\n\nProver: “I take one piece of text \\(x\\), then another piece \\(x'\\).”.\nChallenger: “I challenge you to find a stretch of text from \\(x\\) that matches the \\(-1000:1000\\) stretch in \\(x'\\).”.\nProver asks a team of immortal monkeys to do the task. A million years later: “At \\(49134819\\).”.\nChallenger verifies that \\(T^{49134819}(x)_{-1000:1000} = (x')_{-1000:1000}\\).\n\n\n\nShannon–McMillan–Breiman\nIf someone has created an infinite sequence of coin flips \\(X_{-\\infty:+\\infty}\\), then revealed it to us one by one, then each reveal would give us \\(1 \\;\\rm{bit} = \\ln 2 \\;\\rm{nat}\\). The long-term average obtained per reveal is still \\(\\ln 2 \\;\\rm{nat}\\), a rather boring situation.\nHow do we measure the entropy of an English speaker? It speaks token by token, and we have to measure the average information we obtain per token. The problem is that there are two senses of “average”. It could be the time-average: we listen to the speaker speak for a very long time, and calculate the entropy in the speech. It could be the ensemble-average: we listen to the speaker speak for a very long time, then do it again, then again, etc, then average together the time-averages.\nIf the speaker is ergodic, then the speaker essentially has just one speech, and any two samples of its speech are just translations of each other.14 Consequently, it is intuitively clear that with probability 1, the time-average of the entropy of one speech equals the ensemble-average of the entropy of all speeches. Intuitively, with probability 1,\n14 Statistical mechanists might recognize this as saying that the language is self-averaging. That is, sampling many speeches from the language is essentially the same as sampling one very long speech that we then cut into many sub-speeches.\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}) \\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n})\\right]\n\\]\nFor non-ergodic speakers. We simply decompose the speaker into an ensemble of ergodic speakers, then apply the SMB theorem to each one. It is like the strong law of large numbers. Intuitively, with probability 1,\n\\[\n\\frac{1}{n} \\ln Pr(X_{1:n}| X \\text{ is type }i)\\to \\mathbb{E}\\left[\\frac{1}{n} \\ln Pr(X_{1:n}) | X \\text{ is type }i\\right]\n\\]\nThis is the Shannon–McMillan–Breiman theorem.\nIn textbooks and Wikipedia, the SMB theorem is stated rigorously, but you have already understood the idea of SMB, and the rigorous versions are simply paraphrases of the idea."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html",
    "href": "essays/posts/renormalization-how-to/index.html",
    "title": "How to do Renormalization",
    "section": "",
    "text": "Renormalization (RN) theory is a powerful tool for understanding the behavior of physical systems at different length scales. It allows us to explain why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models.\nRN theory is based on the idea of self-similarity – that a system looks the same at different length scales. For example, a coastline looks the same whether you are looking at it from a satellite or from a boat. Similarly, a magnet looks the same whether you are looking at it with a microscope or with your naked eye.\nThe basic idea of RN is to start with a microscopic description of a system and then to coarse-grain it. That is, to average over the details of the system at a smaller length scale to obtain a description of the system at a larger length scale. This process can be repeated, leading to a sequence of descriptions of the system at increasingly larger length scales.\nThe key insight of RN theory is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. This is known as the universality hypothesis. Universality explains why systems that are very different at the microscopic level can have the same behavior at the macroscopic level. For example, water, oil, and electricity all exhibit the same percolation behavior near the percolation threshold, even though they are very different at the microscopic level.\nUniversality also justifies the use of toy models to study physical systems. Toy models are simplified models that capture the essential features of a system without including all of the details. For example, the Ising model is a toy model of a magnet that captures the essential features of ferromagnetism without including all of the details of the interactions between the atoms.\nIn this essay, we will explore the basic ideas of RN theory and see how it can be used to understand a variety of physical systems."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#introduction",
    "href": "essays/posts/renormalization-how-to/index.html#introduction",
    "title": "How to do Renormalization",
    "section": "",
    "text": "Renormalization (RN) theory is a powerful tool for understanding the behavior of physical systems at different length scales. It allows us to explain why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models.\nRN theory is based on the idea of self-similarity – that a system looks the same at different length scales. For example, a coastline looks the same whether you are looking at it from a satellite or from a boat. Similarly, a magnet looks the same whether you are looking at it with a microscope or with your naked eye.\nThe basic idea of RN is to start with a microscopic description of a system and then to coarse-grain it. That is, to average over the details of the system at a smaller length scale to obtain a description of the system at a larger length scale. This process can be repeated, leading to a sequence of descriptions of the system at increasingly larger length scales.\nThe key insight of RN theory is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. This is known as the universality hypothesis. Universality explains why systems that are very different at the microscopic level can have the same behavior at the macroscopic level. For example, water, oil, and electricity all exhibit the same percolation behavior near the percolation threshold, even though they are very different at the microscopic level.\nUniversality also justifies the use of toy models to study physical systems. Toy models are simplified models that capture the essential features of a system without including all of the details. For example, the Ising model is a toy model of a magnet that captures the essential features of ferromagnetism without including all of the details of the interactions between the atoms.\nIn this essay, we will explore the basic ideas of RN theory and see how it can be used to understand a variety of physical systems."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#the-logistic-map-rn-on-mathbbr",
    "href": "essays/posts/renormalization-how-to/index.html#the-logistic-map-rn-on-mathbbr",
    "title": "How to do Renormalization",
    "section": "The logistic map: RN on \\(\\mathbb{R}\\)",
    "text": "The logistic map: RN on \\(\\mathbb{R}\\)\nLet’s first study the logistic map, the simplest nontrivial example of renormalization that I know of. This section is based on Wikipedia.\n\nThe logistic map\nConsider a function \\(f_r(x)=r x(1-x)\\), and we want to study what happens when we iterate the map many times. The map might fall into a fixed point, a fixed cycle, or chaos. We can see all those cases in its bifurcation diagram.\n\n\n\nThe bifurcation diagram of the logistic map. Source\n\n\nWhen the map falls into a stable fixed cycle of length \\(n\\), we would find that the graph of \\(f_r^n\\) and the graph of \\(x \\mapsto x\\) intersect at \\(n\\) points, and the slope of the graph of \\(f_r^n\\) is bounded in \\((-1, +1)\\) at those intersections.\nFor example, when \\(r=3.0\\), we find that there is only a single intersection, at which point the slope is exactly \\(+1\\), indicating that it is a stable single fixed point, but is about to undergo a bifurcation.\n\n\n\nThe relationship between \\(x_{n+2}\\) and \\(x_{n}\\) as \\(r\\) increases from \\(2.7\\) to \\(3.3\\). Before the period doubling bifurcation occurs. The orbit converges to a single fixed point where the graph of \\(f_r^2\\) intersects the diagonal line. As \\(r\\) reaches \\(3\\), the intersection pitchforks into three. The middle intersection becomes unstable, but the two neighboring intersections remain stable.\n\n\nAs \\(r\\) increases to beyond \\(r=3.0\\), the intersection point splits to two, which is a period doubling. For example, when \\(r=3.4\\), there are three intersection points, with the middle one unstable, and the two others stable.\n\n\n\nWhen \\(r=3.4\\), there are three intersection points, with the middle one unstable, and the two others stable. Source\n\n\nAs \\(r\\) approaches \\(r=3.45\\), another period-doubling occurs in the same way. The period-doublings occur more and more frequently, until at a certain \\(r \\approx 3.56994567\\), the period doublings become infinite, and the map becomes chaotic. This is the period-doubling route to chaos.\n\n\n\nWhen \\(r \\approx 3.56994567\\), there are infinitely many intersections, and we have arrived at chaos via the period-doubling route. Source\n\n\nSomething remarkable happens when we superimpose the graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) when \\(r\\) is at the critical point \\(3.5699\\dots\\). We see that each iteration of the graph seems to resemble itself, except that it is scaled and rotated by 180 degrees. We can naturally guess that \\(f_r^{\\infty}\\) converges to a certain function that is infinitely jagged, such that it exactly resembles itself when scaled and rotated; that is, it is a fractal.\n\n\n\nThe graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) when \\(r\\) is at the critical point.\n\n\nAs \\(r\\) approaches the critical value, we can see how the graph of \\(f_r^\\infty\\) takes on more and more details, and at the critical point, becomes a perfect fractal.\n\n\n\n\n\nThe graphs of \\(f_r, f_r^2, f_r^4, \\dots\\) as \\(r\\) approaches the critical point from below.\n\n\n\n\nUniversality\nLooking at the bifurcation diagram, we can see a region, starting just after \\(r = 3.8\\), where there is a clear “window” with period \\(3\\) bursting out of a sea of chaos. The window then bifurcates repeatedly, to stable cycles of periods \\(6, 12, 24, \\dots\\) until it all collapses back into the chaos again at around \\(r \\approx 3.8494344\\). Though this is a different place, the bifurcation diagram looks suspiciously similar to the previous case.\nNot only that, if we look at the movie of \\(f_r^\\infty\\) as \\(r\\) approaches this critical point, we again see the same jagged shape.\n\n\n\nAre we seeing some kind of universal feature of period-doubling routes to chaos? Is this a general pattern independent of the details of how exactly the logistic map is defined? What if we change to another dynamical system completely different?\nFor example, we can consider the gauss map \\(x_{n+1} = \\exp(-\\alpha x^2_n)+\\beta\\). For a fixed \\(\\alpha\\), we can plot the bifurcation graph as we vary \\(\\beta\\). Though it looks different, the two bifurcation graphs have a clear resemblance. This is an instance of universality, for which we will see again and again later. If \\(f_r\\) is a family of curves with parabolic tops1, then it will bifurcate just like the logistic curve.\n1 Rigorously, we can describe it as follows. If \\(F: \\mathbb{R}^2 \\to \\mathbb{R}\\) is smooth, and for all \\(r \\in \\mathbb{R}\\), the function \\(F(r, \\cdot): \\mathbb{R}\\to \\mathbb{R}\\) has a single global maximum, at which point \\(\\partial_x^2 F(r, x) &lt; 0\\), then its bifurcation diagram looks the same as that of the logistic map, and it will have the same two scaling exponents \\(\\alpha, \\delta\\), to be calculated below.\n\n\nThe bifurcation graphs of gauss map with \\(\\alpha = 5\\) and the logistic map.\n\n\nMore to the issue at hand: why do the two graphs look similar?\n\n\n\nThe bifurcation graph is self-similar. Source\n\n\n\n\nThe self-similarity equation\nRecall that we said the limit of \\(f^\\infty_r\\) should be self-similar, in the sense that if we iterate it twice, then rotate and scale it by a factor, we get back the same function. That is, it should be a solution to the self-similarity equation.\n\\[\nf(x) = -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\n\\]\nIn words, if we scale up the graph for \\(f^2\\) by \\(\\alpha &gt; 0\\), and then rotate by 180 degrees, we get back the graph for \\(f\\). Why should this be the case?\nWell, look back at the previous diagram of the first bifurcation:\n\n\n\nThe relationship between \\(x_{n+2}\\) and \\(x_{n}\\) as \\(r\\) increases from \\(2.7\\) to \\(3.3\\).\n\n\nThe graph of \\(f\\) is a parabola, and the graph of \\(f^2\\) began by looking like a parabola, but its top eventually collapses like an overbaked pie, down and down, until it punches right through the \\(y = x\\) diagonal line again and … wait, wait, it looks like a little parabola again, except this time, it’s rotated by 180 degrees! And so we can run the same argument on \\(f^2\\), and conclude that the graph of \\((f^2)^2 = f^4\\) would collapse upwards, punching right through the \\(y=x\\) diagonal line again, and then history repeats with \\(f^8, f^{16}, \\dots\\), repeating the same story again & again, but smaller & smaller.2\n2 As a wise meteorologist has said:\n\nBig whorls have little whorls, Which feed on their velocity; And little whorls have lesser whorls, And so on to viscosity.\n\nAnd indeed, chaos theory, fractals, and RN all made early appearances in the study of turbulence.\n\n\nThe center of \\(f\\) rises, until the center of \\(f^2\\) collapses, causing a bifurcation at \\(r_1\\). As \\(r\\) increases beyond that point, the center of \\(f^2\\) looks like a parabola again, looking like \\(f\\) in miniature. Thus, as \\(r\\) increases further, \\(f^4\\) will eventually undergo a bifurcation that is similar to the previous bifurcation, except in miniature. (Strogatz 2015, 388)\n\n\nBy eye-balling the curve, we see that \\(f\\) should be an even function, and that in a neighborhood of its centerpoint, . Also, since the \\(f^2\\) can be graphically calculated by doing the cobweb diagram with the graph of \\(f\\), it does not matter if we first scale up the graph of \\(f\\) by a factor of \\(r\\) to \\(F\\), then double it to \\(F^2\\), or if we first double it to \\(f^2\\), then scale its graph. We would get back the same thing. Thus, without loss of generality, we can scale \\(f\\) such that \\(f(0) = 1\\).\nSo, our task is to solve the following equation:\n\\[\n\\begin{cases}\nf(x) = -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\\\\\nf(x) = 1 - a_2 x^2 + a_4 x^4 + \\dots\n\\end{cases}\n\\]\nWe can solve the equation numerically as the fixed point. We would start with \\(f(x) = 1-x^2\\), then guess a good \\(\\alpha\\) and repeatedly apply \\(f \\mapsto -\\alpha f\\left(f\\left(\\frac{x}{-\\alpha} \\right)\\right)\\). If we picked \\(\\alpha\\) correctly, we would have gotten the right result, as shown:\n\n\n\nAt the point of chaos \\(r^*=3.5699 \\dots\\), as we repeat the functional equation iteration \\(f(x) \\mapsto-\\alpha f(f(-x / \\alpha))\\) with \\(\\alpha=2.5029 \\ldots\\), we find that the map does converge to a limit. Source\n\n\nIf \\(\\alpha\\) is not correct, the iterates would not converge; instead, they would have a zooming effect that looks cool.\n\n\n\n\n\n\n\n\n\nSolving the equation at order 2\n\n\n\n\n\nAt order 2, we approximate by \\(f(x) \\approx 1 - a_2 x^2\\) and ignore all higher-order terms. This gives us two equations for two unknowns:\n\\[\n\\begin{cases}\n1-a_2 = \\frac{1}{-\\alpha} \\\\\n\\frac{2a_2^2}{\\alpha} = a_2\n\\end{cases}\n\\]\nIt has two solutions. One solution has \\(\\alpha &lt; 0\\), which we know is unphysical. The other one is\n\\[\n\\begin{cases}\n\\alpha = 1 + \\sqrt{3} \\approx 2.732 \\\\\na_2 = \\frac{1 + \\sqrt{3}}{2} \\approx 1.366\n\\end{cases}\n\\]\n\n\n\nWhat happens if we are not exactly at the fixed point but start slightly off? Let’s say we start with a function \\(f_0(x) = 1 - a_{2,0}x^2\\), where \\(a_{2,0} = a_2^* + \\Delta\\), where \\(a_2^*\\) is the fixed point, and \\(\\Delta\\) is small but nonzero. Here, we should think of the space of possible functions. Each point in this space is a possible scaling limit, but if we start a bit too small, we fall into boredom, and if we start a bit too high, we fall into chaos. Start just right, and we harvest a beautiful fractal.\nAfter one iteration, we have \\(f_1(x) = -\\alpha_0 f_0(f_0(x/(-\\alpha_0)))\\), where \\(\\alpha_0\\) was fixed by \\(f_1(0) = 1\\). This gives us\n\\[\n\\begin{cases}\n\\alpha_0 = \\frac{1}{-1+a_{2, 0}} \\\\\n\\frac{2a_{2, 0}^2}{\\alpha_0} = a_{2, 1}\n\\end{cases}\n\\]\nThat is, we have the renormalization flow equation\n\\[\n2a_{2, 0}^2(a_{2, 0}-1)= a_{2, 1}\n\\]\nWe can plot the space of all possible \\(f(x)\\) as a line, like\n\\[1-0x^2, 1-0.5 x^2, 1-x^2, 1-1.5x^2, \\dots\\]\n\n\n\nRN flow diagram of the self-similarity map at order 2.\n\n\nThis is a 1-dimensional slice of the space of all possible \\(f\\) (the space of theories). Then, the effect of repeatedly applying the self-similarity map is to iterate the map \\(a_2 \\mapsto 2a_{2}^2(a_{2}-1)\\). If we are precisely at the fixed-point \\(a_2^*\\), then we are not going anywhere, but if we are not exactly there, then since the slope of \\(a_2 \\mapsto 2a_{2}^2(a_{2}-1)\\) is \\(\\delta \\approx 5.73\\) at that point, we would get farther and farther away:\n\\[\nf_0 = 1-(a_2^* + \\Delta)x^2, \\quad f_1 = 1-(a_2^* + \\delta\\Delta)x^2, \\quad f_1 = 1-(a_2^* + \\delta^2\\Delta)x^2, \\quad \\dots\n\\]\nand after \\(\\log_\\delta(\\frac{0.1}{\\Delta})\\), we would be at roughly \\(1-(a_2^* \\pm 0.1)x^2\\), which is when we can finally notice that we are obviously no longer in the neighborhood of the fixed point anymore. If we start at \\(a_2^* + \\Delta/\\delta\\), then we can sustain the illusion for one more iteration. Similarly, if we start at \\(a_2^* + \\Delta/\\delta^n\\), then we can sustain the illusion for \\(n\\) more iterations.\nNow, thinking back to what the logistic map says, we understand what we have discovered: The graph of \\(f_{r^* - \\Delta}\\) is similar to the graph of \\(f_{r^* - \\Delta/\\delta}^2\\) scaled by \\(-\\alpha\\). If we let \\(r_1, r_2, r_3, \\dots\\) be the points at which the logistic map splits into a stable cycle of period \\(2^1, 2^2, 2^3, \\dots\\), then we have \\(r_{n} \\approx r^* - \\Delta/\\delta^{n}\\), and so we have:\n\\[\n\\frac{r^* - r_n}{r^* - r_{n+1}} \\to \\delta\n\\]\nThis is usually spoken in this way: the intervals between two bifurcations shrinks at a rate of \\(\\delta\\).\n\n\n\nThe bifurcation diagram for the logistic map. As the bifurcations approach the point of chaos, the interval between two bifurcations gets shorter and shorter, at a rate of \\(\\delta\\) per bifurcation. Source\n\n\n\\(\\delta\\) is called Feigenbaum’s first constant, and \\(\\alpha\\) is Feigenbaum’s second constant.\n\n\n\n\n\n\nSolving the equation at order 4\n\n\n\n\n\nSimilarly, we can solve the equation at order 4 by plugging in \\(f(x) \\approx 1 - a_2 x^2 + a_4 x^4\\), obtaining 3 equations for 3 variables:\n\\[\n\\begin{cases}\n1-a_2+a_4 = \\frac{1}{-\\alpha} \\\\\n\\frac{2a_2^2 - 4a_2a_4}{\\alpha} = a_2 \\\\\n\\frac{a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2)}{-\\alpha^2} = a_4\n\\end{cases}\n\\]\nTo solve this numerically, first guess a solution from the previous one, \\(\\alpha \\approx 2.732, a_2 \\approx 1.366\\), then plug into the first equation to get \\(a_4 \\approx 0\\). Then, standard numerical root-finding gives\n\\[\n\\begin{cases}\n\\alpha \\approx 2.534 \\\\\na_2 \\approx 1.522 \\\\\na_4 \\approx 0.128\n\\end{cases}\n\\]\n\n\n\nWe can also make the same argument using a flow in theory-space, except now we are doing it over a 2-dimensional slice of it. The flow map is\n\\[\nF(a_2, a_4) = \\left(\n   (2a_2^2 - 4a_2a_4)(-1+a_2 - a_4), -(a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2))(-1+a_2 - a_4)^3\n\\right)\n\\]\nAt the fixed-point \\((a_2, a_4) = (1.522, 0.128)\\), the Jacobian matrix is \\[\n\\nabla F = \\begin{bmatrix}\n6.0506 & -6.2524 \\\\\n1.2621 & -1.6909\n\\end{bmatrix}\n\\]\nThis matrix has eigenvalues of \\(4.843, -0.483\\), so it is a saddle point, with \\(\\delta = 4.843\\). The flow and the eigenvectors \\((0.982, 0.190), (0.691, 0.723)\\) are plotted below.\n\n\n\nRN flow diagram of the self-similarity map at order 4. The two eigenvector directions at the fixed point are plotted as dashed lines.\n\n\nIn summary:\n\nThe solution to the self-similarity equation, at increasingly high orders of approximation.\n\n\n\nOrder 2\n4\n\\(\\infty\\)\n\n\n\n\n\\(a_2\\)\n1.366\n1.522\n1.530\n\n\n\\(a_4\\)\n\n0.128\n0.105\n\n\n\\(\\alpha\\)\n2.732\n2.534\n2.503\n\n\n\\(\\delta\\)\n5.73\n4.843\n4.669\n\n\n\n\n\nLessons\nEven in this tiny problem, we can already draw several lessons, which will appear again and again in RN:\n\nWe assume a function is self-similar, and calculate from there.\nSelf-similarity is a transform on a function (or “theory”).\nWe often need to use a “fudge factor” like \\(\\alpha\\) to make sure that the transformed function does not collapse to zero, for trivial reasons.\nIf we repeatedly apply the self-similarity transform on a function, we would obtain a scaling limit, a perfectly self-similar object – a fractal.\nIn the space of all possible theories, the self-similarity transform creates a flow-field in the theory-space. The interesting fixed-points of the flow-field are its saddle points.\nThe largest eigenvalue of the saddle point describes what happens when you are close to the saddle point, but not quite there.\nBravely calculate using the cheapest approximation you can think of. It often gets you within 50% of the right answer.\nBut if you want accuracy, you can always use a computer and calculate many orders higher."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-grid",
    "href": "essays/posts/renormalization-how-to/index.html#ising-model-and-friends-rn-on-a-grid",
    "title": "How to do Renormalization",
    "section": "Ising model and friends: RN on a grid",
    "text": "Ising model and friends: RN on a grid\nThis section studies RN on a grid, using what is called real space RN, in contrast to momentum space RN. Real space RN is a garden of tricks, useful and intuitive, but not as powerful as momentum space RN. If you find the kind of mathematical trickery in this section fun, look at (Kadanoff 1999b, chap. 14; Burkhardt and Leeuwen 1982) for more.\n\nThis paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to run rapidly into insuperable difficulties and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.\n(Fisher 1998)\n\nI recommend that you open these and play as you follow along:\n\nComplexity Explorables | I sing well-tempered\nIsing model by Evgeny Demidov.\n\n\nPercolation\nPercolation is about randomly punching holes in a material until it falls apart. In the simplest setting, the material is a square lattice \\(\\mathbb{Z}^2\\), and each site (vertex) is either open or closed.3 Open means there is a hole there, and closed means the site is intact. Sites are open or closed randomly and independently with probability \\(p\\). We are interested in whether there is an infinite connected cluster of open sites.\n3 Percolation on \\(\\mathbb{Z}\\) is trivial, even more trivial than Ising model on \\(\\mathbb{Z}\\). The trouble is the same: you can only go from one point to another point by one route, and if at any point on the route, you are stopped, then that’s the end – you can’t get there by any other way. Thus, long-range interactions decay exponentially with distance, which means no power law, no phase transition, no critical point. See a later section.This is a model for a porous material: for example, water seeping through the ground. If we have a layer of rock, then groundwater can seep through if there is a single connected path from top to bottom. A layer of rock can be thought of as a grid, with little cracks between grid-points. According to percolation theory, at a “critical probability”, suddenly we have arbitrarily large connected clusters of cracks, and so water can seep arbitrarily far in the rock – it is all or nothing.\nThat is, there is a sharp transition: if \\(p\\) is small, then there is no infinite cluster of open sites, and the water cannot go through; but if \\(p\\) is large, then there is an infinite cluster of open sites, and water can go through. The critical value \\(p_c\\) is about \\(0.5927...\\). See (Grimmett 1999) for more details about percolation.\nTo use Kadanoff blocking for percolation, the first step is to coarse grain the lattice. We group the sites into blocks of \\(3 \\times 3\\) and call a block open if there is a path of open sites connecting the left and right sides of the block. Otherwise, the block is closed.\nThe next step is to define a new percolation model on the coarse-grained lattice, but this is a little trickier than in the Ising model, because there is no obvious way to map the parameters of the original model to the parameters of the new model. We need to find a new probability \\(p'\\) such that the new model on the coarse-grained lattice has the same behavior as the original model on the fine lattice. In particular, we want the probability of having an infinite cluster of open sites to be the same in both models.\nIt turns out that there is no exact way to do this, but we can make an approximation. One way to do this is to use Monte Carlo simulations to estimate the probability of having an infinite cluster for different values of \\(p'\\).\n\n\nKadanoff blocking\nThis section based on (Simkin and Roychowdhury 2011, sec. 10) and (Stinchcombe 1991).\n\n\n\nWhen an Ising model is at the critical temperature \\(T_c\\), coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. (Sethna 2007, fig. 13).\n\n\nThe idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”\nGiven a hexagonal grid, you make \\(p\\) of them black and the rest white. What is the critical \\(p\\) where you get a percolation (infinitely big black island)? To do this by blocking, we regard a triangle as a large spin, and “merge” the three spins on a triangle to one single spin. If two or three spins are black, then the whole spin is also black, otherwise, the whole spin is white.\nThen, after one blocking operation, the lattice length increases from \\(l\\) to \\(\\sqrt 3 l\\) , and the renormalized occupation probability changes from \\(p\\) to \\(p^3 + 3p^2(1-p) = p^2(3-2p)\\). That is, we have the RN flow equation\n\\[p' = p^3 + 3p^2(1-p) = p^2(3-2p)\\]\n\n\n\nKadanoff blocking on a triangular lattice. (Simkin and Roychowdhury 2011, fig. 1)\n\n\nThe equilibrium point is \\(p=1/2\\). This is the percolation probability. Let the reduced probability be \\(\\bar p = p-1/2\\). We find that one iteration of the RN flow makes \\(\\bar p_{n+1} = \\frac 32 \\bar p - 2 \\bar p^3\\) which is \\(\\approx \\frac 32 \\bar p_n\\) for small values of \\(\\bar p\\).\n\n\n\nRN flow for Kadanoff blocking on a triangular lattice.\n\n\nSuppose we start with \\(\\bar p_0\\) , and we perform \\(N\\) repeats of RN to reach some constant \\(\\Delta \\bar p\\) (for example, 0.001), then\n\\[N = \\frac{\\ln \\Delta \\bar p - \\ln \\bar p_0}{\\ln \\frac 32}\\]\nduring which time, the lattice length has increased by\n\\[3^{\\frac 12 N} \\propto \\bar p_0^{-\\frac{\\ln 3}{2\\ln \\frac 32}}\\]\nSince at constant \\(\\Delta \\bar p\\), the lattice has a certain characteristic look-and-feel with a certain characteristic size for its clusters, we find that the characteristic length of its clusters is \\(\\propto (p-1/2)^{-\\nu}\\), where4\n4 It amuses me to no end that the word characteristic is something chemists use a lot. Physicists do it too, sure, with their “characteristic length”, “characteristic height”, “characteristic temperature”, and such, but it is abstract. You rarely need to actually check a cake’s characteristic length against a standard cake. However, when you are doing chemistry, and you need to check a chemical’s characteristic smell, then you are out of luck.\n\nI’m saddened to report that the chemical literature contains descriptions of dimethylcadmium’s smell. Whoever provided these reports was surely exposed to far more of the vapor than common sense would allow … its odor is variously described as “foul”, “unpleasant”, “metallic”, “disagreeable”, and (wait for it) “characteristic”, which is an adjective that shows up often in the literature with regard to smells, and almost always makes a person want to punch whoever thought it was useful. … if you’re working with organocadmium derivatives and smell something nasty, but nasty in a new, exciting way that you’ve never quite smelled before, then you can probably assume the worst. (Lowe 2013)\n\n\\[\\nu = \\frac{\\ln 3}{2\\ln \\frac 32} \\approx 1.355\\]\nThe actual exponent is believed to be \\(\\nu = 4/3\\), so we are only 1.6% off. Very good for such a cheap calculation!\n\n\nThe Ising model\nFerromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?\nIn 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until (Peierls 1936) showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See (Domb 1985) for some more historical details.\nThe Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature \\(T\\) makes the atoms jiggle and be random.\nThere are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: \\(+1\\) or \\(-1\\). Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is\n\\[\nH(s) = -J \\sum_{i, j} s_i s_{j}\n\\]\nwhere \\(J&gt;0\\) is the strength of interaction between neighboring atoms, \\(s_i\\) represents the spin at site \\(i\\), and the summation is over neighboring pairs of \\(i, j\\). For example, if we have Ising model on the integer line \\(\\mathbb{Z}\\), then \\(H = -J \\sum_{i} s_i s_{i+1}\\). Similarly, if we have an Ising model on the square grid \\(\\mathbb{Z}^2\\), then \\(H = -J \\sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\\).\nWe are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity \\(Z\\), called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating \\(Z\\) at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, \\(Z\\) is defined as\n\\[\nZ := \\sum_{s} e^{-\\beta H(s)}\n\\]\nwhere \\(\\beta = 1/T\\) is the inverse of the temperature \\(T\\). The summation is over all possible configurations of the system. For example, if the Ising model is over \\(\\{0, 1, 2, \\dots, 99\\}\\), then the summation is over \\(\\{-1, +1\\}^{100}\\).\nThe temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the \\(J\\) like this:\n\\[Z = \\sum_{s} e^{-\\beta H(s)}, \\quad H(s) = -J_0 \\sum_{i, j} s_i s_{j}, \\quad J_0 = J/T\\]\nAfter we finish the calculation, we should write the temperature explicitly again in order to interpret what we have found.\n\n\nKadanoff decimation, take 0\nLike ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.\nLet us consider the Ising model on the integer line \\(\\mathbb{Z}\\) with periodic boundary conditions. That is, we have \\(2n\\) spins \\(s_0, s_1, \\dots, s_{2n-1}\\) arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:\n\\[\n\\begin{aligned}\nZ &= \\sum_{s_0, s_1, \\dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &= \\sum_{s_0, s_1, \\dots, s_{2n-2}} \\sum_{s_1, s_3, \\dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \\dots + s_{2n-1}s_0)} \\\\\n  &\\overset{\\mathrm{hopefully}}{=} \\sum_{s_0, s_2, \\dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)} \\\\\n\\end{aligned}\n\\]\nwhere we have hopefully written the \\(J'\\). It is our ardent hope that there exists some \\(J'\\), somewhere in the universe, that will make the equation come true.5\n5 Physicists call it “ansatz”, but I prefer to say it like this:\n\nThe author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.\n— Yu Tsun, professor of English at Deutsch-Chinesische Hochschule.\n\nBecause each even spin can only reach its nearest-neighboring even spins via the odd spin between them, the partition function splits cleanly:\n\\[\nZ = \\sum_{s_0, s_2, \\dots} \\left(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\\right) \\left(\\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\\right) \\cdots\n\\]\nThus, our wish would be fulfilled if \\(\\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\\) for all 4 possible choices of \\(s_0, s_2\\). Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:\n\n\n\n\n+\n-\n\n\n\n\n+\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\\(e^{-J'} = 2\\)\n\n\n-\n\\(e^{-J'} = 2\\)\n\\(e^{J'} = 2 \\cosh(2J)\\)\n\n\n\nImmediately we see a problem: if both \\(e^{J'} = 2 \\cosh(2J)\\) and \\(e^{-J'} = 2\\) hold, then \\(1 = 4 \\cosh(2J)\\). This means that we have to introduce a “fudge factor” again. Does that remind you of the \\(\\alpha\\) from the logistic map calculation? It should. Add in the fudge factor \\(k\\) with:\n\\[\nZ = k^{n} \\sum_{s_0, s_2, \\dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \\dots + s_{2n-2}s_0)}, \\quad \\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}\n\\]\nNow the solution is: \\(J' = \\frac 12 \\ln \\cosh(2J), k = 2\\sqrt{\\cosh(2J)}\\). Again, we see that the RN flow equation\n\\[\nJ \\mapsto \\frac 12 \\ln \\cosh(2J)\n\\]\nBut this time, the RN flow has only a single stable fixed point at \\(J = 0\\). Not only that, since \\(\\frac 12 \\ln \\cosh(2J) \\approx J^2\\) if \\(J\\) is small, if we decimate for \\(n\\) times, we would end up with \\(J' \\sim J^{2^n}\\). What does this mean?\n\n\n\nThe RN flow for the 1D Ising model has only one fixed point at zero.\n\n\nSuppose we start with a magnet with interaction strength \\(J\\), then after we zoom out for \\(n\\) times, where \\(n\\) is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites \\(0, 2^n, 2\\times 2^n, 3 \\times 2^n, \\dots\\). Our RN flow calculation states that the strength of interaction between \\(s_0\\) and \\(s_{2^n}\\) are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for \\(s_0\\) to interact with \\(s_{2^n}\\) is if they laboriously go, bond-by-bond, across all the \\(2^n\\) bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.\nIn particular, it shows that no matter how strong \\(J\\) is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This disappointment led Ising to abandon this model of magnetism back in 1925.\n\n\n\n\n\n\nNote\n\n\n\nIf you are ambitious, you can try doing the same RN analysis for a “ladder” made of two \\(\\mathbb{Z}\\) put side-by-side. If you have “a lot of time” like Onsager, you would be on your way to solving the Ising model in 2 dimensions. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See page 12 for a sketch of solution.\n\n\n\n\nKadanoff decimation, take 1 (abortive)\nNow that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:\n\\[\nZ = \\sum_s e^{-H}, \\quad H = -J \\sum_{i, j \\text{ are neighbors}} s_i s_j\n\\]\nwhere again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.\n\n\n\nFigure source\n\n\nHowever, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from \\((-1, 0)\\) to a nearest-neighbor \\((0, +1)\\) via the to-be-decimated atom \\((0, 0)\\), but also go to the next-nearest neighbor \\((+1, 0)\\). Not only that, the square of 4 spins neighboring \\((0, 0)\\) are connected via \\((0, 0)\\), so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:\n\\[\n-H' = J_{nn} \\sum_{i, j \\text{ are nn}} s_i s_j + J_{nnn} \\sum_{i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\n\\]\n\n\n\n\n\n\nExact solution\n\n\n\n\n\nLet the grid have \\(N\\) sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when \\(N\\) is large. The partition function of the system is\n\\[\nZ = \\sum_{s \\in \\{-1, +1\\}^\\text{grid}} \\exp\\left(J \\sum_{ i, j \\text{ are nn}} s_i s_j\\right)\n\\]\nNow, decimate half of the grid, and leave behind the other half. As derived in (Maris and Kadanoff 1978), we have\n\\[\nZ = f(J)^{N/2} \\sum_{s \\in \\{-1, +1\\}^\\text{decimated grid}} \\exp\\left(J_{nn} \\sum_{ i, j \\text{ are nn}} s_i s_j + J_{nn} \\sum_{ i, j \\text{ are nnn}} s_i s_j + J_{\\square} \\sum_{ i, j, k, l \\text{ are }\\square} s_i s_j s_k s_l\\right)\n\\]\nwhere \\[\n\\begin{aligned}\nf(J) &= 2 \\cosh ^{1 / 2}(2 J) \\cosh ^{1 / 8}(4 J) \\\\\nJ_{nn} &= (1 / 4) \\ln \\cosh (4 J) \\\\\nJ_{nnn} &= (1 / 8) \\ln \\cosh (4 J) \\\\\nJ_{\\square} &= (1 / 8) \\ln \\cosh (4 J)-(1 / 2) \\ln \\cosh (2 J)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhy don’t we have the triangle terms like \\(J_\\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\\)?\nSince the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.\n\n\nWell, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet infinitely many interaction terms! How so?\nLook at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, \\((-1, 0)\\) and \\((0, +1)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. No problem, that’s the nn term, written as \\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\\). Similarly, \\((-1, 0)\\) and \\((+1, 0)\\) are mediated by \\((0, 0)\\), so they must have an interaction term. That’s the nnn term, written as \\(J_{nnn}s_{(-1, 0)}s_{(+1, 0)}\\). Do you see it now?\nWe can connect \\((-1, 0)\\) and \\((3, 0)\\) via \\((0, 0), (1, -1), (2, 0)\\). Not only that, we can connect them by infinitely many possible routes: \\((0, 0), (1, -1), (2, 0), (3, -1)\\), and \\((0, 0), (1, -1), (2, 0), (3, 1)\\), etc. And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.\nWe can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.\n\nThese calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager’s values to with­ in about 0.2%.\n(Wilson 1979)\n\n\n\nMigdal bond-moving trick\nBefore we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.\n\n\n\nThe Migdal bond-moving trick. (Kadanoff 1999b, fig. 14.2).\n\n\nWe perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as \\(J_x, J_y\\). The RN flow of this step is\n\\[\n(J_x, J_y) \\mapsto \\left(2 J_x, \\frac 12 \\ln \\cosh(2 J_x)\\right)\n\\]\nAfter doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:\n\\[\n(J_x, J_y) \\mapsto (f_1(J_x), f_2(J_y))\n\\]\nwhere \\(f_1(x) = \\ln\\cosh(2x)\\) and \\(f_2(x) = \\frac 12 \\ln\\cosh(4x)\\). This RN flow has saddle point \\((0.609, 0.305)\\). If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead \\((J_x, J_y) \\mapsto (f_2(J_x), f_1(J_y))\\), with saddle point \\((0.305, 0.609)\\). Well, the true saddle point of the whole system should have equal \\(J_x, J_y\\), since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: \\((0.609 + 0.305)/2 = 0.457\\).\nAccording to Onsager’s exact solution, the true critical point is \\(J_c = \\frac{\\ln(1+\\sqrt 2)}{2} = 0.4407\\). So by this simple trick, we have already gotten within 3.7% of the true answer.\nA small improvement is to find the “double fixed point”. That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to\n\\[x = f_1(f_2(x)); \\quad x = f_2(f_1(x))\\]\nNow, these have different solutions, so we do the obvious thing and take their midpoint. This gives \\(0.4417\\). And with that simple idea, I got within 0.23% of the true answer.\nWe can also calculate the scaling exponent, like how we found \\(\\delta\\) for the logistic map. The average gradient at the fixed point is \\(\\frac{(f_1\\circ f_2)' + (f_2\\circ f_1)'}{2}\\) which is \\(2.7633\\), corresponding to a length scaling exponent of \\(\\nu = \\frac{\\ln 4}{\\ln 2.7633} = 1.364\\), which is well off the real value of 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Two RN flows on the \\((J_x, J_y)\\) plane found by Migdal bond-moving, and the saddle points thereof.\n\n\n\n\n\nSide note: Onsager’s solution of Ising model in 2D\nSo far, we have been doing it for the Ising model on \\(\\mathbb{Z}\\). But it’s clear that we can also do it for two \\(\\mathbb{Z}\\)s put side by side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a \\(2\\times 2\\) matrix, we have a \\(4 \\times 4\\) matrix.\nFor that matter, we can do it for any finite number of those \\(\\mathbb{Z}\\) put together. We can then imagine doing that for such ladders with widths \\(2, 3, 4, 5, 6, \\dots\\), then discover a pattern, and take the limit. If this works, we would solve the Ising model on \\(\\mathbb{Z}^2\\).\nArduous as it sounds, this is exactly how Lars Onsager arrived at his solution for the Ising model on \\(\\mathbb{Z}^2\\). He calculated up to ladders with width 6, diagonalizing matrices of size \\(64\\times 64\\) in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:\n\nIn 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….\nIn March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a \\(2 \\times \\infty\\), then a \\(3 \\times \\infty\\), then a \\(4 \\times \\infty\\) lattice. He then went on to a \\(5 \\times \\infty\\) lattice, for which the transfer matrix is \\(32 \\times 32\\) in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the \\(6 \\times \\infty\\) case, and eventually diagonalized the \\(64 \\times 64\\) matrix, finding that all the eigenvalues were of the form \\(e^{\\pm \\gamma_1 \\pm \\gamma_2 \\pm \\gamma_3 \\pm \\gamma_4 \\pm \\gamma_5 \\pm \\gamma_6}\\). That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper.\n(Yang 2005, 11–13)\n\nI also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:\n\na long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model]\n(Yang 2005, 12)\n\n\n\nBonus: Edgeworth series\nIn a typical intermediate probability course taught to graduate students, the central limit theorem is taught in two lines: For any probability distribution with finite variance, check how its characteristic function varies under sum-then-average operations, and check that it converges to the characteristic function of the gaussian distribution.\nThis simple argument is the seeds to functional renormalization theory. This section is based on (Sethna 2021, exercise 12.11).\nIf we have a sequence of random variables \\(X_1, X_2, \\dots\\), then we can do the double-then-average trick repeatedly:\n\\[\nX_1, \\frac{X_1 + X_2}{\\sqrt 2}, \\frac{\\frac{X_1 + X_2}{\\sqrt 2} + \\frac{X_3 + X_4}{\\sqrt 2}}{\\sqrt 2}, \\dots\n\\]\nNow, consider Fourier transforms to the probability density function\n\\[\n\\tilde\\rho(k) := \\int_\\mathbb{R}e^{-ikx} \\rho(x) dx\n\\]\nIf \\(X, X'\\) are sampled from the pdf \\(\\rho\\), then \\(\\frac{X+X'}{\\sqrt 2}\\) has the pdf \\(\\rho'\\) such that\n\\[\\tilde{\\rho'}(k) = \\tilde\\rho(k/\\sqrt 2)^2\\]\nThus, we define an operator by \\(R[\\tilde f](k) = \\tilde f(k/\\sqrt 2)^2\\), and the problem of central limit theorem is finding the fixed points of \\(R\\) and their local stability.\n\n\n\n\n\n\nderivation\n\n\n\n\n\nThe pdf of \\(\\mathcal N(\\mu, \\sigma^2)\\) transforms to \\(e^{-ik\\mu - \\frac 12 \\sigma^2 k^2}\\), so \\(\\mathcal N(0, \\sigma^2)\\) is a fixed point of \\(R\\).\nLet \\(\\rho^*\\) be the pdf for \\(\\mathcal N(0, 1)\\), then the question is to find the local linear expansion of \\(R\\) near \\(\\tilde \\rho^*\\). That is, we want to find the eigenvectors of \\(R\\):\n\\[\nR[\\tilde \\rho^* + \\epsilon f] = \\tilde \\rho^* + \\lambda\\epsilon f\n\\]\nNow, \\(\\tilde f_n := (ik)^n \\tilde \\rho^*(k)\\) has eigenvalue \\(1/2^{\\frac{n-2}{2}}\\). Since these \\(\\tilde f_0, \\tilde f_1, \\dots\\) allow for Taylor expansion, these exhaust all possible eigenvectors of \\(R\\) near \\(\\tilde \\rho^*\\).\nConsider a generic function \\((1+a_0) \\tilde f_0 + a_1 \\tilde f_1 + \\cdots\\). It corresponds to a pdf\n\\[\n\\rho = (1+a_0) f_0 + a_1 f_1 + \\cdots\n\\]\n\\[\nf_n(x) = \\frac{1}{2\\pi} \\int_\\mathbb{R}e^{ikx}\\tilde f_n(k) dk = \\partial_x^n \\rho^*(x) = \\rho^*(x) He_n(-x/\\sigma)/\\sigma^n\n\\]\n\\[\nHe_0 = 1, He_1 = x, He_2 = x^2 - 1, He_3 = x^3 - 3x, \\dots\n\\]\nThose are the probabilist’s Hermite polynomial.\nThe only dangers to convergence are \\(f_n\\) where \\(n = 0, 1, 2\\), since the other terms have eigenvalue less than 1. The effect of adding a small amount of \\(f_0, f_1, f_2\\) to \\(\\rho^*\\) is to change it from \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}}\\) to\n\\[\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2} + \\ln a_0 - a_1 \\frac{x}{\\sigma^2} + a_2 \\frac{x^2 - \\sigma^2 }{\\sigma^4}}\n\\]\nSo we see that the effect of \\(a_0\\) is to normalize the probability mass to 1, the effect of \\(a_1\\) is to shift the mean, and \\(a_2\\) is to change the variance.\n\n\n\nTherefore, if we always subtract the mean and dividing by the variance, then the first three perturbation terms \\(f_0, f_1, f_2\\) are zeroed out, leaving the other terms decaying to zero.\nNormalize \\(X\\) to have mean 0 and variance 1, then we find that the pdf of \\(2^{-n}\\sum_{i=1}^{2^n}X_i\\) is\n\\[\n\\rho_{2^n} = \\rho^*(1 + 2^{-n/2}a_3 He_3(-x) + 2^{-n}a_4 He_4(-x) + \\cdots)\n\\]\nwhere\n\\[e^{\\frac 12 k^2}\\tilde \\rho(k) = 1 + a_3 (ik)^3+ a_4 (ik)^4 + \\cdots\\]\n\n\n\n\n\n\nexample: Bernoulli variable\n\n\n\n\n\nThe Bernoulli variable \\(\\frac 12(\\delta_{-1} + \\delta_{+1})\\) has\n\\[e^{\\frac 12 k^2}\\tilde \\rho(k) = e^{\\frac 12 k^2}\\cos k = 1 - \\frac{k^4}{12} - \\frac{k^6}{45} + \\cdots\\]\ngiving\n\\[\na_4 = -\\frac{1}{12}, a_6 = \\frac{1}{45}, \\dots\n\\]\n\\[\n\\rho_{2^n}(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac 12 x^2} \\left[1 -\\frac{1}{12} 2^{-n} He_4(-x) + \\frac{1}{45} 2^{-2n} He_6(-x) + \\cdots\\right]\n\\]\nThe exact result is\n\\[\n\\rho_{2^n} = \\sum_{i=0}^{2^n} \\frac{1}{2^{2^n}} \\binom{2^n}{i}  \\delta_{\\frac{-2^n + 2i}{2^{n/2}}}\n\\]\nwhich reduces to the previous result by Stirling approximation.\n\n\n\n\n\nBonus: Generalized central limit theorem\nThis section is based on (Amir 2020).\nConsider three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto \\(\\mu=0.7\\). We notice two facts:\n\nThe random walks are self-similar. A small section has the same look-and-feel as a large section of it.\nDifferent random walks have very different characters. The Gaussian walk appears smoother, while the Cauchy and Pareto walks display more dramatic jumps and bursts, reflecting the heavier tails of their respective distributions.\n\n\n\n\nThree random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto \\(\\mu=0.7\\)\n\n\nWhen we see self-similarity, we think RN. Can we use RN to study random walks? Yes.\nLet’s take a fresh look at the central limit theorem. It says that if \\(X_1, X_2, \\dots\\) are IID samples from a distribution with finite mean \\(E[X]\\) and variance \\(V[X]\\), then \\(\\frac{(X_1 + \\dots + X_n) - n E[X]}{\\sqrt{n V[X]}}\\) converges to the standard normal distribution. If we think about it from the RN point of view, we can decompose each \\(X\\) into a sum of two random variables: \\(X_i = A_i + Z_i\\), where \\(Z_i\\) is a normal distribution with the same mean and variance, and \\(A_i\\) is the “noise” part of it. Each \\(A_i\\) might be overpowering, but when we repeatedly coarse-grain by taking a bunch of \\(X_i\\), and adding them up (a lossy operation!), we would eventually destroy all traces of what cannot survive coarse-graining, and leaving behind a fixed-point of coarse-graining.\nWe define the following letters:\n\n\\(X_1, X_2, \\dots\\) are IID random variables, with characteristic function \\(\\phi_X(t)= E[e^{itX}]\\).\n\\(S_n = X_1 + \\dots + X_n\\).\n\\(a_n, b_n\\) are two sequences of real numbers, such that \\(\\frac{S_n - b_n}{a_n}\\) converges in distribution to a nontrivial random variable \\(Z\\) with characteristic function \\(\\phi(t)\\).\n\n\n\n\n\n\n\nDeriving the field equation by RN\n\n\n\n\n\nSince \\(\\frac{X_1 + \\dots + X_n - b_n}{a_n}\\) converges in distribution to a nontrivial random variable, the sequence \\(a_n\\) must diverge to infinity. For, if the sequence \\(a_n\\) is bounded, then for large enough \\(n\\) , the sum \\(X_1 + \\dots + X_n\\) would spread wider and wider, and dividing it by \\(a_n\\) cannot keep it together.\nLet \\(Z\\) be a random variable with characteristic function \\(\\phi\\). By assumption, \\((S_n- b_n)/a_n\\) is approximately distributed like \\(Z\\) , that is, \\(S_n\\) is approximately distributed as \\(a_nZ + b_n\\). Thus,\n\\[\\phi_{S_n}(t) \\approx e^{ib_n t}\\phi(a_nt )\\]\nGiven \\(1 \\ll n \\ll N\\) , we can compute \\(\\phi_{S_N}\\) in two ways: adding it up as \\(N\\) copies of \\(X\\) , or adding it up as \\(N/n\\) copies of \\(S_n\\). Both should give us the same result. That is: \\[\\phi_{S_N} = \\phi_X^N = \\phi_{S_n}^{N/n}\\]\nHowever, since \\(n\\) is very large, we have the approximations \\(\\phi_{S_n}(t) \\approx e^{ib_n t}\\phi(a_nt )\\). Thus, we have\n\\[\n\\ln \\phi_{S_N}(t) \\approx \\frac{N}{n}(ib_n t + \\ln\\phi(a_n t))\n\\]\nNote how we have an exponent of the form \\(Nf(n)\\) , where \\(N\\) is a very large number, and \\(n\\) is a number that is small compared to it. This is a common pattern in RN calculation.\nSince \\(n\\) is small compared to \\(N\\) , but large compared to \\(1\\) , we can pretend that it’s a continuous variable, and take derivative of it. Since the left side is independent of \\(n\\) , the derivative should be zero:\n\\[\n\\partial_n \\frac{N}{n}(ib_n t + \\ln\\phi(a_n t)) = 0\n\\]\nSimplifying it, and substituting \\(t\\) for \\(a_n t\\) , we get the field equation\n\\[\\frac{\\phi'(t)}{\\phi(t)}t - \\ln \\phi(t) \\frac{a_n}{n \\partial_n  a_n} + it\\partial_n (b_n/n) \\frac{n}{\\partial_n a_n} = 0\\]\n\n\n\nThus, we have obtained the field equation:\n\\[\\frac{\\phi'(t)}{\\phi(t)}t -\\frac{a_n}{n \\partial_n  a_n} \\ln \\phi(t)  + \\frac{n\\partial_n (b_n/n)}{\\partial_n a_n} it = 0\\]\nwhich we can solve by standard mathematical analysis without any more use of RN, so we don’t do those. You can read (Amir 2020) if you are interested.\nHowever, there is a problem: If we have a “field” equation, what is the “field”? Well, here is one way to think of it.\nImagine a line of atoms, at locations \\(1, 2, 3, \\dots\\). Each atom has a height \\(X_1, X_2, X_3, \\dots\\). Now, we can coarse-grain the system by a factor of \\(4\\), by defining\n\\[Y_1 = \\frac{X_1 + \\dots + X_4 - b_4}{a_4}, \\quad Y_2 = \\frac{X_5 + \\dots + X_8 - b_4}{a_4}, \\quad \\dots\\]\nfrom which we can perform another coarse-graining by a factor of \\(100\\), ending up with a coarse-grain by a factor of \\(400\\). Now, if the system has a nontrivial scaling limit, then this should give us the same result as doing a coarse-graining by \\(5\\), then by \\(80\\), or first \\(6\\) then \\(67\\). This is the RN argument we used here.\nNow, since \\(S_n \\approx a_n Z + b_n\\), we see that \\(b_n\\) can be thought of as the coarse-grained height of the height field, and \\(a_n\\) as the coarse-grained jaggedness of the height-field. Then, the field equation describes how the two numbers vary according to \\(n\\).\n\n\n\n\n\n\nExercise: extreme value distribution\n\n\n\n\n\nThe maximum of random variables often has a nontrivial scaling limit as well. That is, there exists some sequence \\(a_n, b_n\\) such that \\(\\frac{\\max(X_1, \\dots, X_n) - b_n}{a_n}\\) converges to a nontrivial distribution with cumulative distribution function (CDF) \\(F\\).\nLet \\(F_X\\) be the CDF of \\(X\\); then we have \\(F_{\\max(X_1, \\dots, X_N)}(t) = F_{\\max(X_1)}(t)^{N}\\). Now, derive the field equation by an RN argument.\nAnswer: \\(\\partial_n \\frac 1n \\ln F(\\frac{t-b_n}{a_n}) = 0\\)."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-momentum-space",
    "href": "essays/posts/renormalization-how-to/index.html#wilsons-nobel-prize-rn-in-momentum-space",
    "title": "How to do Renormalization",
    "section": "Wilson’s Nobel Prize: RN in momentum space",
    "text": "Wilson’s Nobel Prize: RN in momentum space\n\n\n\nEvolution of Firefox icon under RN flow in frequency space. The high-spatial-frequency details are summed away, leaving low-frequency broad brushstrokes.\n\n\nRN in momentum space, or RN in frequency space, is the most widely used method of RN nowadays. It is called “momentum” because it was first done by quantum field theorists to study subatomic particles, and in quantum mechanics, the spatial frequency of a particle-wave is proportional its momentum: \\(p = \\hbar k\\), for which de Broglie was awarded a Nobel Prize in 1929.\nIn the 1970s, Kenneth Wilson invented RN in momentum space and used it to solve many problems. He was awarded the 1982 Nobel Prize in Physics for this work.\nUnfortunately, unlike RN in real space, RN in momentum space is extremely verbose, requiring pages and pages of symbols. Instead of subjecting you to the horrible experience, I will sketch out the big ideas only.\n\nThere remained the possibility that there might be smaller but still infinite quantities left over. No one had the patience needed to calculate whether these theories were actually completely finite. It was reckoned it would take a good student two hundred years, and how would you know he hadn’t made a mistake on the second page? Still, up to 1985, most people believed that most supersymmetric supergravity theories would be free of infinities.\n(Hawking 2001, 52)\n\n\nTo know for sure whether a Feynman diagram with three virtual graviton loops produces infinite quantities, we would need to evaluate \\(10^{20}\\) terms. By five loops, a diagram spawns \\(10^{30}\\) terms … The unitarity method has completely changed the situation … What would have taken the Feynman technique \\(10^{20}\\) terms, we can now do with dozens. … we found that the 1980s era speculations were wrong. Quantities that seemed destined to be infinite are in fact finite. Supergravity is not as nonsensical as physicists thought. In concrete terms, it means that quantum fluctuations of space and time are much more innocuous in supergravity than previously imagined. If you ply us with fine wine, you might catch us speculating that some version of it might be the long sought quantum theory of gravity.\n(Bern, Dixon, and Kosower 2012)\n\n\nField theory: continuous Ising model\nArguably the first field theory was hydrodynamics. Working in the era just after Newton, Euler and Lagrange understood water as a block of infinitely many tiny mass-points. Because each point is so tiny, they do not study the velocity of individual particles of water, but study the velocity field of the entire block of water.\nThe Ising model, with its grid of spins, provides a clear example of this continuum limit. As the grid of spins grow large, the individuals blur together into a field, similar to deriving hydrodynamics from particle dynamics. Let’s take a concrete example, of Ising model on the square grid \\(\\mathbb{Z}^2\\).\nInitially, the system’s energy and partition function are represented as a sum over individual spins:\n\\[\nZ = \\sum_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)}, \\quad H(s) = -J \\sum_{i, j \\text{ are nearest neighbors}} s_i s_j\n\\]\nRecall how, after two Kadanoff decimations, all forms of spin-spin interactions are unlocked, and so we arrive at an energy function in the most general form:\n\\[\nZ = \\sum_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)}, \\quad H(s) = -\\sum_{\\text{configuration }C} \\sum_{i_1, i_2, \\dots \\text{ are configured like }C} J_C s_{i_1}s_{i_2}\\cdots\n\\]\nWe can convert the summation into an integral, and suggestively write it as \\(Z = \\int_{s: \\mathbb{Z}^2 \\to \\{-1, +1\\}} e^{-H(s)} ds\\).\nIn the continuous limit, the discrete field of spins \\(s: \\mathbb{Z}^2 \\to \\{-1, +1\\}\\) blurs into a continuous field of spins \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\). The energy \\(H(s)\\) becomes \\(S[\\phi]\\), a functional6 an \\(\\phi(x)\\) and its gradients. This gives us\n6 A “functional” is nothing but a special kind of function. Specifically, it is a function of type \\(\\text{function} \\to \\text{number}\\). We use square brackets in \\(S[\\phi]\\), not round brackets like \\(S(\\phi)\\), because it is conventional for functionals to use square brackets, not round brackets. It is written as \\(S[\\phi]\\) rather than \\(H[\\phi]\\), and called “action”, because of some old historical usage in variational calculus (as in “the principle of least action”).\\[\nZ = \\int_{\\mathbb{R}^2 \\to \\mathbb{R}} D[\\phi] \\; e^{-S[\\phi]}\n\\]\n\n\nField theory, in general\nConsider a mattress as an analogy. Let the state The state of the mattress is determined by the height and velocity of each point. That is, the energy function of the mattress comprises two quadratic terms: height of each point; height differences between nearby points. We assume the mattress points are massless, so that we have no kinetic energy. We can write the energy of the mattress schematically as\n\\[\n\\text{energy} = H(\\phi) = \\frac 12 K_0 \\sum_i \\phi_i^2 + \\frac 12 K_1 \\sum_{i, j \\text{ are neighbors}} (\\phi_i - \\phi_j)^2\n\\]\n\n\n\nA mattress in space. The energy of the mattress is determined by the height of each point, and the height differences between neighboring points.\n\n\nIf the mattress is held in an atmosphere of temperature \\(T\\), then its state becomes uncertain, following a Boltzmann distribution, just like how a pollen’s position in hot water becomes uncertain, due to Brownian motion.7 The probability that you would find the mattress in state \\(\\phi\\) is then \\(e^{-H(\\phi)/T}/Z\\), where \\(Z\\) is the partition function again:\n7 When statistical field theory gets too scary, I call it “hot water theory” to make it sound nicer.\\[\nZ = \\int d\\phi e^{- H(\\phi)/T}\n\\]\nIn the continuum limit, this analogy leads us to statistical field theory.\nCalculating useful quantities within statistical field theory often involves complex mathematical techniques. This complexity arises from the need to compute the partition function, which involves integrating over all possible states of the system. In the case of the mattress analogy, the partition function involves an integral over all possible height fields, leading to the use of path integrals, written like \\(\\int_{\\phi: \\mathbb{R}^2 \\to \\mathbb{R}} D[\\phi] e^{-S[\\phi]/T}\\).\nIntegrating over \\(\\mathbb{R}^{10^{23}}\\) would be bad enough, let along integrating over \\(\\mathbb{R}^{\\mathbb{R}^{2}}\\), and yet the miracle is that this can be done, and can be done in a way that matches experiments.\nStatistical field theory employs various tricks to calculate the partition function or its limits. A common approach involves alternating between discrete and continuous representations of the field for calculations.\nInterestingly, statistical field theory is almost isomorphic with quantum field theory. The general idea is that if you take the dimension of time in a quantum field theory over space of dimension \\(n\\), and do a substitution \\(t \\mapsto it\\), you somehow end up with a statistical field theory over space of dimension \\(n+1\\). This is the Wick rotation.\nFor example, the 1D Ising model is analogous to a particle in a double well. Whereas the single particle might switch from left to right after time \\(\\Delta T\\), the Ising model chain might switch from \\(+1\\) to \\(-1\\) after space \\(\\Delta L\\). Because of quantum tunneling, it is impossible to confine a particle in one side of the well – it will always jump to the other side, and the jumping probability is on the order of \\(1 - e^{-kt}\\) for some constant \\(k\\). This corresponds to the fact that there is no way to “freeze” an Ising model in one dimension. Even at low temperatures, the system cannot be entirely frozen. Long stretches of “up” spins can suddenly flip to “down” spins, no matter how cold the chain gets. Similarly, in the quantum analogy, the particle can tunnel between the two wells, no matter how high the barrier between them gets.\nGenerally, 1D statistical fields lack phase transitions, just like how a single particle with finitely many states would always quantum-tunnel between states, no matter how cold it gets. Conversely, since the 2D Ising model can be frozen, we naturally suspect that the quantum field theory on one dimension should have some kind of phase transition.8\n8 I don’t know what it is, but perhaps Bose–Einstein condensation? I mean, I just flipped through (Herbut 2007) and the book suggests that this is it. Don’t quote me on this.You get another perspective by noting that magnetism requires spontaneous symmetry breaking: you have more spins pointing up than down, even though the underlying energy function does not distinguish up from down. No symmetry breaking, no phase transition. And since the quantum states of a single particle can always tunnel between each other, it cannot fall into a symmetry-breaking state. However, since magnets can exist in 2D, we see that spontaneous symmetry breaking can occur for a 1D line of quantum particles evolving through time.\n\n\nRN flow in the space of field theories\nThink back to the Ising model on \\(\\mathbb{R}^2\\). What is its action \\(S[\\phi]\\)? If we were mathematically omnipotent, then we can simply perform RN flow on the discrete Ising model, and just find its fixed point, which should hopefully tell us what \\(S\\) is. But we can’t even perform a single RN flow. What to do?\nWell, by universality, we can start with some very different discrete Ising model and end up with the same continuum limit after renormalizing enough times. Why can’t we start with some very different continuous Ising model, discretize it to a discrete Ising model, then renormalize it again until we are back to a continuous Ising model? And if we can do that, why can’t we renormalize directly in the space of all continuous Ising models? We can start with whatever Ising field theory we can write down, and then just repeatedly renormalize it. By universality, we will end up in an interesting place, no matter where we started.\nBut before we do that, we have to construct the space of possible Ising field theories. As Wilson would say, it is all about the symmetries.\nThere are two kinds of symmetries: the symmetry without, and the symmetry within. For the Ising field \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\), we have Euclidean symmetry for \\(\\mathbb{R}^2\\), and up-down symmetry for \\(\\mathbb{R}\\). To ensure Euclidean geometry, the action \\(S[\\phi]\\) should not explicitly depend on the position. That is, if we take some \\(\\phi\\), and translate it by \\(\\delta\\), then we must have\n\\[\nS[\\phi] = S[x \\mapsto \\phi(x + \\delta)]\n\\]\nSimilarly for reflections and rotations of the field. This shows that \\(S\\) must involve only terms like \\(\\phi, \\nabla \\phi, \\nabla^2 \\phi, \\nabla \\phi \\cdot \\nabla \\phi\\), etc.\nTo account for the symmetry within – the up-down symmetry – we must have \\(S[\\phi] = S[-\\phi]\\). This shows that \\(S\\) must have only even-ordered terms\nUnder these assumptions, you can convince yourself that the most generic form of \\(S\\) is\n\\[\nS[\\phi] = \\int d x\\left[\\frac{1}{2} \\nabla \\phi \\cdot \\nabla \\phi+\\frac{1}{2} \\mu^2 \\phi^2+g \\phi^4+\\cdots\\right]\n\\]\nwhere we removed an irrelevant constant term independent of \\(\\phi\\), and picked the scale of length so that the coefficient for \\(\\frac{1}{2} \\nabla \\phi \\cdot \\nabla \\phi\\) is one.\nThis is the space of all possible Ising field theories. Each Ising field theory is completely specified if we specify the real numbers \\(\\mu, g, \\dots\\). Doing RN would then consist of taking one Ising field theory specified by some \\(\\mu, g, \\dots\\), then renormalize it to some other Ising field theory specified by \\(\\mu', g', \\dots\\). We can then find the fixed points in the theory-space, and say, “These are the most interesting theories. Let’s calculate their properties, scaling exponents, and look-and-feel.”\n\n\nRN in momentum space\nConsider an Ising field \\(\\phi: \\mathbb{R}^2 \\to \\mathbb{R}\\). It is of course possible to directly renormalize the field in real space: we blur it a bit, then zoom out. However, this turns out to be very hard to calculate with. Instead, it is much easier to renormalize the field in frequency space.\nTo “blur and zoom out” in real space – what does it look like if we take a Fourier transform? It would look like we are removing some high-frequency vibrations, then expand the vibrations so that low-frequency vibrations become high-frequency vibrations.\nNow we are ready to meet RN in momentum space.\nLet \\(\\tilde \\phi\\) denote the Fourier transform of a field. Now, solve9 for \\(\\tilde S\\), so that \\(S[\\phi] = \\tilde S[\\tilde\\phi]\\) for all \\(\\phi\\) in theory-space.\n9 This is typically called “Fourier-transforming the operator”. For example, the Fourier transform of the gradient operator \\(\\nabla\\) is \\(ik\\), where \\(k\\) is the wave vector.\\[\nZ = \\int D[\\phi] e^{-S[\\phi]} = \\int D[\\tilde\\phi] e^{-\\tilde S[\\tilde\\phi]}\n\\]\nNext, we restrict the domain of integration from all frequencies, to only frequencies below an upper limit \\(\\Lambda\\). This is called “frequency cut-off”10:\n10 This frequency cut-off is often called “ultraviolet cutoff”, because we throw away all frequencies that are too high, and for light, ultraviolet light is high-frequency. That’s it. It started as a hack meant to remove the annoying infinities that crop up everywhere in quantum field theory. You might have heard mysterious whispers of how renormalization “cuts off infinities” and makes them “normal again”. In fact, this is where the horrendous name “re-normalization” came from. If I had any choice in the matter, I would have called it “re-scaling”. At least that would be more descriptive.\nFor that matter, “renormalization group theory” is also a terrible name. To an applied physicist, the name “group theory” is abstract, inspiring fear and uncertainty. To a mathematician, the name “group theory” is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining.\nIn modern QFT, this compromise has been converted into a triumph. Instead of a hack to remove infinities, it is now interpreted as a necessary fact about the world. To see high-frequency features in space, we have to be able to probe it. To probe it, we need to fire some high-frequency particles at a space. High-frequency particles have high energy, and the higher the energy gets, the more complicated the interaction gets, and in this way, the quantum field theory actually changes depending on our choice of frequency-cutoff.\n\nWilson’s analysis takes just the opposite point of view, that any quantum field theory is defined fundamentally with a cutoff A that has some physical significance. In statistical mechanical applications, this momentum scale is the inverse atomic spacing. In QED and other quantum field theories appropriate to elementary particle physics, the cutoff would have to be associated with some fundamental graininess of spacetime, perhaps a result of quantum fluctuations in gravity. … whatever this scale is, it lies far beyond the reach of present-day experiments. The argument we have just given shows that this circumstance explains the renormalizability of QED and other quantum field theories of particle interactions. Whatever the Lagrangian of QED was at its fundamental scale, as long as its couplings are sufficiently weak, it must be described at the energies of our experiments by a renormalizable effective Lagrangian.\n(Peskin and Schroeder 1995, 402–3)\n\n\\[\nZ \\approx \\int_{\\tilde \\phi(k) \\text{ is nonzero only for }\\|k \\| \\leq \\Lambda} D[\\tilde\\phi] e^{-\\tilde S[\\tilde\\phi]}\n\\]\nNext, we pick some small number \\(\\epsilon\\), and integrate away all frequencies on a shell:\n\\[\n\\begin{aligned}\nZ &\\approx \\int_{\\tilde \\phi^- \\text{ is nonzero only for }\\|k \\| \\leq (1-\\epsilon)\\Lambda} D[\\tilde \\phi^-] \\left(\\int_{\\tilde\\phi^+ \\text{ is nonzero only for }(1-\\epsilon)\\Lambda \\leq \\|k \\| \\leq \\Lambda} D[\\tilde\\phi^+] e^{-\\tilde S[\\tilde\\phi^-, \\tilde\\phi^+]}\\right) \\\\\n&\\overset{\\mathrm{hopefully}}{=} \\int_{\\tilde \\phi^- \\text{ is nonzero only for }\\|k \\| \\leq (1-\\epsilon)\\Lambda} D[\\tilde \\phi^-] e^{-\\tilde S'[\\tilde \\phi^-]}\n\\end{aligned}\n\\]\nThis gives us some renormalized action \\(\\tilde S'\\) in frequency space. Do an inverse Fourier transform to obtain \\(S'\\), then scale space down by \\((1-\\epsilon)\\), to obtain the fully renormalized action \\(S''\\). The RN flow is then defined by\n\\[\nS \\mapsto S''\n\\]\nSince \\(\\epsilon\\) is small, we can make it infinitesimal, to obtain the RN flow (a real flow this time!)\n\\[\nS \\mapsto S + \\epsilon F[S]\n\\]\nwhere \\(F\\) denotes the RN flow field in theory-space. It is a functional of \\(S\\), since the flow field differs for each theory in theory-space.\nThis is as far as we are going to discuss the RN in momentum space. Any more and I would be writing a textbook on QFT, and you are better served by a proper textbook like (Zee 2023, 2010), etc.\n\n\nBonus: How to publish in quantum field theory\n\nWork through a textbook and learn RN in momentum space.\nLearn group theory and group representation theory.\nWrite down many groups with some nice geometry, like \\(SO(3)\\).\nConstruct a group out of those. For example, \\(G = SO(4) \\rtimes SU(3) \\times SU(2) \\times U(1)\\). The group \\(G\\) should have around 10–20 dimensions, but if you are a string theory enthusiast, then 500 dimensions is perfectly fine.\nConstruct another group \\(H\\).\nPick a nice space \\(X\\). For example, \\(X = \\mathbb{R}^4 \\times \\mathbb{C}^6\\). It must be a space that \\(H\\) can act upon.\nPick another space \\(Y\\). It must be a space that \\(G\\) can act upon.\nConstruct the most generic possible functional of type \\(S: (X \\to Y) \\to \\mathbb{C}\\) that is still compatible with the two symmetry groups \\(G, H\\).\nSpend the next month doing RN calculations about \\(Z := \\int_{\\phi: X \\to Y} D[\\phi] \\; e^{-S[\\phi]}\\), probably with Feynman diagrams scribbled everywhere.\nType it up in LaTeX.\nSuffer through peer review, or just put it up on arXiv."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#a-bag-of-intuitions",
    "href": "essays/posts/renormalization-how-to/index.html#a-bag-of-intuitions",
    "title": "How to do Renormalization",
    "section": "A bag of intuitions",
    "text": "A bag of intuitions\nAfter the long, hard slog at mathematics, we can take a break, take stock of what we have learned, and make some philosophical reflections. I guarantee that you will find at least one sentence here that you can proclaim with style at a party.\n\nPower laws are born of two exponential parents\nWhy is it that a critical point is typically surrounded by a power law? One intuition is that at a critical point, two exponentials are matched exactly – an exponentially decaying interaction strength and an exponentially increasing number of interaction paths – and a power law is born in their collision.\nConsider the Ising model on the plane. Fix an origin \\(0\\) , and we ask, how strong is the correlation between the origin \\(0\\) and a point that is at distance \\((n, n)\\) away from the origin, where \\(n\\) is large?\nWell, the two points are correlated because they are connected by chains of spins. The more chains there are, the stronger the correlation, but the longer each chain is, the weaker the correlation. Since the correlation along each chain is exponentially weak, we can crudely pretend that all the correlations can be added.11 As a good approximation, we consider only the shortest chains, which are of length \\(2n\\). By Stirling approximation, there are \\({2n \\choose n} \\sim \\frac{4^{n}}{\\sqrt{n\\pi }}\\) such chains. We can think of spin at origin as \\(x_{(0,0)} + z_1 + z_2 + \\cdots\\), and the spin at \\((n, n)\\) as \\(x_{(n,n)} + z_1 + z_2 + \\cdots\\), where \\(z_1, z_2,...\\) are random variables that are responsible for creating the correlations between the two spins along each chain.\n11 Every biologist knows intuitively that weak correlations are additive. This is why, for instance, we can predict height accurately by a simple linear sum of the genes correlated with height, ignoring pairwise, triple-wise, and higher-order interactions. Indeed, there is a common pattern in biology: If you have a developmental process of type \\(\\text{many genes}\\to \\mathbb{R}\\), where \\(\\mathbb{R}\\) stands for some kind of real-valued trait, like probability of diabetes, then the developmental process is pretty much just a linear map. Epistasis, defanged. Heritability, vindicated.\nWhy? One possibility is that “even nature does not laugh at the difficulties of integration”. That is, even natural evolution will be hopelessly confused by a fully nonlinear developmental process, so such a process is unadaptive, un-evolvable, a tightly woven ball of spaghetti code where a single base-pair change collapses the whole thing. No, evolution favors those that are evolvable, which usually means a linear function.Now, each chain contributes a weak correlation that decays exponentially with distance. We can assume the chains do not interact. Along each chain, we have a 1D Ising model. The covariance between two neighboring spins is\n\\[Cov(s_0, s_1) = E[s_0s_1] - \\underbrace{E[s_0]E[s_1]}_{\\text{=0}} = Pr(s_0 = s_1 ) - Pr(s_0 \\neq s_1 ) = \\tanh(\\beta J)\\]\nNow, we need a trick.12 If you think a bit, you would see that whether \\(s_0 = s_1\\) is independent of whether \\(s_1 = s_2\\). Thus,\n12 If you don’t like the trick, then you can use the transfer matrix method. We have no use for the transfer matrix, so we don’t do it.\\[Cov(s_0, s_2) = E[s_0 s_2] = E[(s_0 s_1) (s_1 s_2)] = Cov(s_0, s_1) Cov(s_1, s_2) = \\tanh(\\beta J)^2\\]\nAnd since the chain has length \\(2n\\), the correlation contributed by the chain is \\(\\tanh^{2n}(\\beta J)\\). The total correlation is\n\\[\\sim \\frac{4^{n}}{\\sqrt{n\\pi }} \\tanh^{2n}(\\beta J)\\]\nThe two terms are exactly balanced when \\(\\beta J = \\tanh^{-1}(1/2) = 0.549\\dots\\). In fact, the exact result is \\(\\beta J = 0.44\\dots\\) , so our crude estimate is only 25% too high.\nNow, right at the critical point, the correlation is \\(\\sim (n\\pi)^{-1/2}\\) , so we see that when the exponential decay in correlation is exactly matched with the exponential growth in possible paths, the remaining polynomial decay comes to the fore. Notice that we have also estimated one of the Ising critical exponents: \\(\\nu = 1/2\\). The actual answer is \\(1\\).\nSimilarly, with \\[\\binom{kn}{n, \\dots n}\\sim \\frac{k^{kn}}{n^{\\frac{k-1}2}}\\frac{k^{1/2}}{(2\\pi)^{\\frac{k-1}2}}\\]\nwe can estimate that the Ising model in \\(\\mathbb{Z}^k\\) has a critical \\(\\beta J \\approx \\tanh^{-1}(1/k)\\) and critical exponent \\(\\nu = \\frac{k-1}2\\). It turns out that for all dimensions \\(\\geq 4\\), we have the exact result of \\(\\nu = 1/2\\). This can be derived by “mean field theory”, which we are not going to discuss.\n\nStevens’ power law\nAs a side note, this “two exponents lead to a power law” is known in psychophysics as Stevens’ power law (Stevens 1970).\nConsider the case where the brain needs to respond to the presence of a stimulus (e.g., a sound, a smell, etc.) with intensity \\(I\\). The response intensity (such as in the height of jumping, or a verbal report, or wincing of the face) is \\(R\\). Stevens found that for many kinds of stimulus, \\(R \\propto I^k\\) for some exponent \\(k\\) that depends on the type of stimulus and response.\nStevens conjectured that the number of neurons firing \\(N\\) is proportional to the log of intensity of stimulus \\(I\\), and that \\(N\\) is also proportional to the log of intensity of response \\(R\\). Thus, we have\n\\[\nk_I \\ln I = N = k_R \\ln R\n\\]\nfor two constants \\(k_I, k_R\\), which implies that \\(R = I^{k_I/k_R}\\), a power law. In this way, a small number of neurons can allow us to perceive and react to a wide range of stimuli intensities – for example, the physical brightness between noon and a starlit moonless night is more than \\(10^{8}\\), and yet the optical nerve, with only \\(10^{6}\\) neurons (Evangelou and Alrawashdeh 2016), can comfortably accommodate them both.\n\nAt the Ciba Symposium in 1966, there was a general discussion on the topic “Linearity of transmission along the perceptual pathway”. In that discussion, and elsewhere at the symposium, Sir John Eccles turned forceful attention to the question of whether the sense organ could adequately account for the nonlinearity in the coupling between stimulus and sensation, leaving the central nervous system with the task of performing only linear transformations. He observed that “there is no great impediment to the idea that… the transfer functions across the synaptic mechanism are approximately linear.” To which Professor Mountcastle added, “The interesting point for me here is the great importance that we must now place upon the transducer process itself, at the periphery.”\n(Stevens 1970)\n\n\n\n\nRN is a journey in the space of possible theories\nSo far, we have seen again and again the common refrain of “the space of theories” and “moving to another theory”. It is time to make this clear. The space of possible theories is defined by the symmetry of the physical system, and the Renormalization Group (RG) flow defines a journey in this space.\n\n\n\n(Fisher 1998, fig. 4)\n\n\nConsider a generic RG flow in a generic space of theories. diagram for a system with two coupling constants \\(K\\) and \\(y\\). Each point in the diagram represents a theory, and the arrows indicate the direction of the RG flow. The fixed points, where the arrows converge, correspond to theories that are scale-invariant.\n\n\n\n(Fisher 1998, fig. 5)\n\n\nThe above figure shows the RG flow for the Ising model, a simple model of ferromagnetism. The fixed points correspond to the paramagnetic phase (\\(K = 0\\)) and the ferromagnetic phase (\\(K = K_c\\)). The critical point, where the two phases meet, is at \\(K = K_c\\).\nThe RG flow provides a way to understand the behavior of a system at different length scales. As we zoom out, the system flows towards a fixed point. The fixed point describes the long-distance behavior of the system.\nFor example, in the Ising model, as we zoom out, the system flows towards either the paramagnetic or the ferromagnetic fixed point, depending on the initial value of \\(K\\). If \\(K &lt; K_c\\), the system flows towards the paramagnetic fixed point, and the spins become disordered at long distances. If \\(K &gt; K_c\\), the system flows towards the ferromagnetic fixed point, and the spins become ordered at long distances.\nWhat defines the space of possible theories? The symmetry of the physical system.\n\n\nSymmetries determine the shape of theory-space\nThe Ising model on \\(\\mathbb{Z}^2\\) is not a single theory, but an entire infinite-dimensional space of possible theories. Each Ising model can be specified by all coupling strengths for all possible spin configurations – nearest neighbors, next-nearest neighbors, four in a square, etc. However, they must follow the symmetries. We can’t have three-in-a-triangle, because switching up and down gives us the same energy. We can’t have \\(Js_{(0, 0)}s_{(1, 0)}\\) different from \\(J's_{(1, 0)}s_{(2, 0)}\\), because translating the whole plane by \\((1, 0)\\) gives us the same energy.\nThis is true in general: Symmetries determine the shape of theory-space.\nConversely, if two physical systems are constrained under the same symmetries, then their behavior are the same near the critical point, because their renormalization flows are the same.\nThe following table shows many theory-spaces with their corresponding symmetries.\n\nTable of theory-spaces with their corresponding symmetries. Reproduced from (Wilson 1979).\n\n\n\n\n\n\n\n\n\n\\(d\\), the underlying space’s dimensions\n\\(n\\), the internal degrees of freedom\nTheoretical Model\nPhysical System\nOrder Parameter\n\n\n\n\n2\n1\nTwo dimensions\nAdsorbed films\nSurface density\n\n\n\n2\nXY model in two dimensions\nHelium-4 films\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in two dimensions\n\nMagnetization\n\n\n&gt;2\n∞\n“Spherical” model\nNone\n\n\n\n3\n0\nSelf-avoiding random walk\nConformation of long-chain polymers\nDensity of chain ends\n\n\n\n1\nIsing model in three dimensions\nUniaxial ferromagnet\nMagnetization\n\n\n\n\n\nFluid near a critical point\nDensity difference between phases\n\n\n\n\n\nMixture of liquids near consolute point\nConcentration difference\n\n\n\n\n\nAlloy near order-disorder transition\nConcentration difference\n\n\n\n2\nXY model in three dimensions\nPlanar ferromagnet\nMagnetization\n\n\n\n\n\nHelium 4 near superfluid transition\nAmplitude of superfluid phase\n\n\n\n3\nHeisenberg model in three dimensions\nIsotropic ferromagnet\nMagnetization\n\n\n≤4\n-2\n\nNone\n\n\n\n\n32\nQuantum chromodynamics\nQuarks bound in protons, neutrons, etc.\n\n\n\n\nAs a particular example, with \\(d=3, n=1\\), we see that water-vapor mixture has the same critical behavior as copper-zinc alloy, or Ising model on three dimensions. We can think of vapor as just up-spin, and liquid as just down-spin. Next time you find yourself meditating upon a magnet, try reimagining it as a kettle of water, about to boil over.\n\nExamples of \\(d=3, n=1\\) systems.\n\n\nphysical system\nsite types\n\n\n\n\n\nuniaxial magnet\nup / down\n\n\n\nfluid\nhas atom / no atom\n\n\n\nbrass crystal\nzinc / copper\n\n\n\nsimple lattice field theory\nhas particle / no particle\n\n\n\n\n\n\nDroplets inside droplets\n\nBig whirls have little whirls that feed on their velocity,\nand little whirls have lesser whirls and so on to viscosity.\n\nI especially recommend playing with the interactive Ising model by Evgeny Demidov while reading this section.\nWe can interpret an Ising model over \\(\\mathbb{Z}^3\\) as a mixture of liquid and gaseous water. Each site can either be in a liquid state or in a vapor state. We start below the critical temperature, so that the bonding strength \\(J\\) is just slightly larger than the critical \\(J_c\\). The system must make a free choice to make between being a liquid ocean with tiny islands of liquid or a vapor ocean with tiny islands of liquid. Both choices are equally good in our toy model. Since liquid wants to be in contact with liquid and vapor with vapor, the system must decide. For the sake of intuition, we say that we have an ocean of liquid with tiny droplets of vapor inside.\nNow we increase the temperature towards the critical temperature. As we get hotter, thermal fluctuations become larger. In the ocean of liquid, a thermal fluctuation produces a droplet of vapor. This droplet secretes other material of the same density, and it grows larger and larger.\n\n\n\nVapor droplets inside an ocean of liquid. (Kadanoff 1999a, 298)\n\n\nHowever, as criticality is approached, the energetic cost in making a fluctuation approaches zero. The energetic penalty in creating droplets gets smaller and smaller. So, the droplets can grow large, until it becomes infinite right at the critical point.\nFurthermore, each droplet itself is a nearly-critical system, so fluctuations appear within the droplets, in a delicate fractal.\n\n\n\nDroplets inside droplets inside droplets… (Kadanoff 1999a, 299)\n\n\n\n\nMore is different\nLike Wilson, Philip Anderson is a physicist who has won a Nobel Prize for his work on surprising things that happen when many particles interact. His Nobel Prize was awarded for his work on magnetism and “Anderson localization”. Consider a pure piece of metal. An electron wave can vibrate freely across it from side to side, like a wave on a perfectly uniform infinite ocean. Now if we dope the metal with impurities, like planting wave-breakers in the ocean, then as the amount of impurity increases, an electron wave would suddenly become trapped, and the metal would become an insulator.\nIn a famous paper (Anderson 1972), he interprets many-body physics like a philosopher (like what I’ve been doing in this whole section). He called the “constructionist hypothesis” the view that science divides neatly into fundamental laws and applications of those. He countered this view by proposing that scale and complexity create distinct stages in nature. Each stage necessitates new laws, concepts, and generalizations, and requires just as much ingenuity as the other stages. For example, psychology is not merely applied biology, biology is not simply applied chemistry, and condensed matter physics is not only “device engineering”.\nAnderson proposed that new stages appear because of “broken symmetries”. Consider the sugar molecule. Though the sugar molecule is governed by quantum mechanics, which does not distinguish left from right, in our world we mostly only see sugar in one chirality. Sometime in the distant past, the symmetry was broken, and we are living in the consequences of its history.\nMore concretely, consider cars driving on a road. Why do we drive on the right instead of the left? Left or right, it’s better (“lower energy”) if everyone agrees. If people disagree, then it’s chaos (“higher energy”), so in the past, a coin was flipped, and we are here. In a parallel universe, we are driving on the left instead of the right. This is clear in the many-worlds interpretation of quantum mechanics:\n\\[\n\\ket{\\Psi_{\\text{world}}} = \\frac{1}{\\sqrt{2}} \\ket{\\Psi_{\\text{world where we drive on the left}}} + \\frac{1}{\\sqrt{2}} \\ket{\\Psi_{\\text{world where we drive on the right}}}\n\\]\nThe state of the entire multiverse \\(\\ket{\\Psi_{\\text{world}}}\\) is symmetric, but any observer has to fall into one of the sub-states, where the symmetry no longer holds.\nHowever, Anderson pointedly did not explain what allows the stages to have laws largely independent of each other. Suppose that we have explained how some symmetries of the quantum-mechanical level break on the biochemistry level; we still have many questions. Why does the biochemistry level still have simple laws? And why these biochemical laws, but not others? Why are quantum mechanical laws and the everyday physics of frogs and cats so different, without the laws “bleeding into each other”?\n\n\nMesophysics: Why are things interesting between the large and the small?\nThe space of all theories is big, but most of it is rather uninteresting. Consider the humble Ising model on \\(\\mathbb{Z}^2\\). Too much interaction and you get a block of spins all pointing in one direction. Too little interaction and you get a gas of spins pointing noisily in all directions. Only right at the critical point do we get interesting behavior in all possible scales. Not only that, the critical point is very delicate.\nIf you take an Ising model at \\(J\\) that is just a bit above \\(J_c\\) and zoom out, then by the RN flow equation, the effective \\(J\\) would keep increasing, and it becomes more and more uniform until it’s a perfect shade of black/white at \\(J = +\\infty\\). Conversely, if you start with \\(J\\) slightly below \\(J_c\\) and keep zooming out, \\(J\\) would approach \\(0\\) and everything would become a uniform shade of gray, with the spins pointing up and down with no regard for any other spin. Balanced on a knife’s edge is \\(J = J_c\\), where there is interesting behavior at all levels of zooming.\n\n\n\n\n\nZooming in and out of the Ising model. Video by Douglas Ashton, taken from The Renormalisation Group | dougmet-dot-net.\n\n\nBut what keeps \\(J = J_c\\)?\nWhy is it that we are surprised by the quantum mechanics in the microscopic world? Because daily life in the mesoscopic world does not betray its origin from the microscopic world. The details has been renormalized away. But if that’s the case, how come our world is neither a homogeneous block of spins all pointing up nor a hot mess of spins unrelated to every other spin? Why is the mesoscopic world interesting?\nLook around you. The world is interesting, with power laws, fractal patterns, and details at all scales. You never see a pencil standing on its end without a hand keeping it there. What keeps the mesoscopic world in its critical place?\nOne answer is that most of the interestingness did not come from criticality. However, if there exists some criticality in the mesoscopic world, and there does not seem to be an intelligent agent keeping the criticality there, then we have a mystery. This is the question that launched a thousand papers, including the famed self-organized criticality paper (Bak, Tang, and Wiesenfeld 1987), itself launching a thousand papers. The idea is typically illustrated by the forest fire model.\nConsider the standard percolation model on a square grid. Each point might be occupied (a “tree” grows there) or unoccupied (empty plot of land). Randomly, lightning falls, and if it hits a tree, the tree catches on fire and the fire spreads to any neighboring trees. The process ends when there are no more burning trees.\nAs the proportion \\(p\\) of a site being occupied (“tree density”) changes, we see a phase transition. For low \\(p &lt; p_c\\), there is no percolation, and so the fire quickly dies out. For high \\(p &gt; p_c\\), there is percolation, and so the fire spreads across the entire grid. The process automatically balances the system at the critical value \\(p_c\\), the system is poised between these two regimes, and the burnt-out patches can be of any size, following a power-law distribution. In this way, the delicate critical point in the standard percolation model has been transformed to a robust critical point.\n\nNature shows an amazing variety of length scales: There is the Hubble radius of the universe, \\(10^{10}\\) light years or so and the radius of our own solar system, \\(10^{11}\\) meters roughly, and us-two meters perhaps, and an atom \\(-10^{-10}\\) meters in radius, and a proton \\(10^{-16}\\) meters, and the characteristic length of quantum gravity-which involves another factor of about \\(10^{20}\\).\nHow these vastly different lengths arise is a very interesting and fundamental question…. However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks [at] any event which takes place between the scale of the lattice constant [the spacing between molecules or spins] and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths. (Kadanoff 1999b, 251)\n\nIn a footnote, Kadanoff suggested that self-organized criticality might explain forms of stable criticality we see around us. Kadanoff wrote the words in 1999, near the end of the 1980s–90s chaos theory boom. Since then, the self-organized criticality theory has fallen by the wayside, like fractal compression, mirrorshades, and large-folio printed pages of fractal art. The modern evaluation is that while it was oversold by Per Bak, and certainly could not explain all critical phenomena (Bak 1996), it can explain some of them (Watkins et al. 2016, sec. 8).\n\n\nUniversality: The details don’t matter\nOr: Why elephants don’t know quantum mechanics, but don’t need to know it either.\nIn the early 20th century, material scientists noticed the remarkable phenomenon of “corresponding states”. As first reported by (Guggenheim 1945), scientists measured the density \\(\\rho\\) of many substances near their liquid-vapor critical point. They fixed pressure and increased temperature \\(T\\) around the critical temperature \\(T_c\\). As they plotted the relation between \\(T\\) and \\(\\rho\\), rescaled by critical temperature \\(T_c\\) and density at critical point \\(\\rho_c\\), remarkably, all the substances fell onto a single curve.\n\n\n\nHigh-resolution reprint of (Guggenheim 1945, fig. 2) in (Herbut 2007, 13).\n\n\nDespite the diversity of intermolecular forces, the phase transition behavior of a wide variety of gasses follows a universal pattern.\nWe see this pattern over and over again in physics. To give another example, imagine water flowing through a porous rock, oil through sand, or electricity through a random network of resistors. These systems, seemingly completely unrelated, share the same underlying mathematical structure and exhibit universal behavior near the percolation threshold. This universality arises because the details of the microscopic interactions become irrelevant at larger scales, and the system’s behavior is governed by the collective properties of its components.\nIt is often noted that quantum mechanics is unintuitive, because the mesoscopic physics is so different from the microscopic physics, so we had evolved to intuit the mesoscopic world, and not the microscopic world. But why is it possible to ignore quantum mechanics? Elephants don’t need to know, or don’t care about, the Standard Model of particle physics.13 They don’t know and don’t need to know, because there is no natural selective pressure favoring elephants that have intuitions about quantum mechanics. When they walk, they push dirt around. When water flows, it pushes water around.\n13 Nor do they play chess. (Brooks 1990)For example, a cup of water is a composite of H2O molecules, so it should be studied by quantum mechanics. However, zooming out and out, we would find that the system falls into one of several possible fixed points (liquid, gas, solid) or critical points (boiling point, freezing point, triple point). In each case, the quantum-mechanical messiness has been renormalized away.\nIn general, the lesson is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. The benefit of RN is that it saves us from the effort of understanding the microscopic details. On the flip-side, the details do matter if we are far from the critical or fixed point.\nThis is an overarching theme in renormalization theory, which might be called the universality hypothesis:\n\nAll phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed. (Kadanoff 1999a, 273)\n\nFurthermore, universality justifies our toy models as “serious play”. When I was first learning statistical mechanics, I was terribly confused by it. “They can’t be serious – do they think I’m stupid? How could the Ising model possibly be relevant to real magnets?” But the universality hypothesis justifies Ising models as serious toy models. Even if they are completely different from real magnets when far from the critical point, as we approach the critical point, their behavior becomes exactly equal at the limit.\nRN theory explains why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up at the same destination as the toy model. (Batterman 2019)\n\nWe may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. … [A] good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details. (Fisher 1983, 47)\n\n\nKoan: Sociophysics\n\n“The details don’t matter.” said them triumphantly as they declared their independence from biophysics.\n“‘The details don’t matter.’” said them mockingly as they declared their insurrection against sociophysics.\n\nYuxi’s Comment:14 The traditional approach of historians, going back to the days of “kings and battles”, is to run to personality theory and the individual acts, when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups, and then attempts to explain events, actions, and so on. However, for truly complicated systems that these days are studied under the name of “sociophysics”, this is a hopeless task; furthermore, the questions it answers are not even the right ones. The modern theorist would rather explain how the stable features of the problem are invariant under different assumptions of what individual people do, and arise from features of their interactions. Indeed, if one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!\n14 Inspired by Tolstoy’s War and Peace.\n\nThe movement of humanity, arising as it does from innumerable arbitrary human wills, is continuous. To understand the laws of this continuous movement is the aim of history. But to arrive at these laws, resulting from the sum of all those human wills, man’s mind postulates arbitrary and disconnected units. The first method of history is to take an arbitrarily selected series of continuous events and examine it apart from others, though there is and can be no beginning to any event, for one event always flows uninterruptedly from another.\nThe second method is to consider the actions of some one man – a king or a commander – as equivalent to the sum of many individual wills; whereas the sum of individual wills is never expressed by the activity of a single historic personage.\nHistorical science in its endeavor to draw nearer to truth continually takes smaller and smaller units for examination. But however small the units it takes, we feel that to take any unit disconnected from others, or to assume a beginning of any phenomenon, or to say that the will of many men is expressed by the actions of any one historic personage, is in itself false.\n\nMumon’s Comment: Historians search for the king’s motive and the general’s ambition. They build castles of personality, moats of actions, yet understand nothing of the war. The great black spider traps its preys with a net so dense that nothing is lost after it has digested all of them."
  },
  {
    "objectID": "essays/posts/renormalization-how-to/index.html#appendix",
    "href": "essays/posts/renormalization-how-to/index.html#appendix",
    "title": "How to do Renormalization",
    "section": "Appendix",
    "text": "Appendix\n\nIf only I understood what this is saying, then I would have written it in. (Mehta and Schwab 2014)"
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#my-summary",
    "href": "essays/posts/serial-experiments-lain/index.html#my-summary",
    "title": "Serial Experiments Lain",
    "section": "My summary",
    "text": "My summary\nIt seems Lain is the collective subconscious, implemented as a neural network, with every human brain as a neuron, and “wired together” by some kind of unexplained electromagnetic coupling to the earth’s ionosphere.\nAs a suggestive evidence for this, the frequencies of the Schumann resonance (8 Hz, 14 Hz, 20 Hz, etc) happen to be close to the main brainwave frequencies. See the appendix Section 4.1 for some additional details about it. It is not actually relevant for the story.\nSome people drew up a plan to control Lain. The key scientist in the plan is Eiri, scientist of the Tachibana General Laboratories.\nFirst they migrated Lain from the brain-ionosphere system to the brain-Internet system, by an update to the Internet protocol (Protocol 7) that allows brain-computer interfacing.\nThen they gave Lain a human body. (unclear how that happened) This body gave Lain self-consciousness and a person-API.\nEiri suicided after Protocol 7 was running. Protocol 7 contained a copy of his brain state, so he was now running on the Internet, and called himself god. He has followers (Knights of the Eastern Calculus) who did his commands.\nThen they orchestrated a series of dramatic events to steer Lain’s development. The human body for Lain is used here, as these human-psychologically meaningful events can only work through a human-person-API (You can’t traumatize a non-human process, or Lain-without-body, by staging a bloody murder. Murders only mean something if it’s seen through animal eyes, the same way that a story can move you only if you speak the language.)\nEventually, Lain would be completely isolated in human society and be connected to the Internet. She would accept Eiri as god, kill her physical body, and exist on the Internet, where she would do Eiri’s commands.\nThe plan failed at the last step, because two people (the actor playing her father, and a school friend, Alice) stilled loved Lain, so she wasn’t isolated enough. After trying to make Alice happy and failing, Lain decided that the only way to truly make Alice happy is to go away and let Alice run her normal life-cycle (grow up, get married, die from being too old) without drama.\nTo let Alice live as a normal human, the entire plan must go away. So Lain discarded all changes and reverted to a previous state, before Lain was embodied. She deleted all memories of her from all humans. She also rewrote Eiri into an unambitious man so that he wouldn’t try doing that in this timeline.\nI don’t know how Lain could do that. Lain is the neural network with human brains, and has access only to the human brains, the ionosphere, and Internet. I don’t see any way for Lain to revert some physical deaths."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "href": "essays/posts/serial-experiments-lain/index.html#ontology-the-modes-of-existence",
    "title": "Serial Experiments Lain",
    "section": "Ontology: The modes of existence",
    "text": "Ontology: The modes of existence\nThe ontology of SEL is a dualism between information and matter, the same theory as most neuroscientists and AI scientists use today.\nThere are many differences between matter and information, but here are the most important ones:\nDuplicate matters are different. Duplicate information are the same. If you receive two apples, you get twice as many atoms. If you receive two copies of the same file, you really receive just one file (in terms of information).\nMatter does not allow “isomorphic” operations. Information allows it. For example, you can convert a document from one encoding to another, edit it, then convert it back. The result is the same as if you edited it without conversion. This can’t be done with physical pen-and-paper.\nMatter exists in spacetime. Information does not. It is not a coincidence that many physicists are working on “reducing spacetime to information”, because information itself doesn’t have to exist in space or time, so if spacetime is reduced to information, then spacetime is explained by something that doesn’t assume spacetime – progress for reductionist science.\nThe persistence of matter is obvious. The persistence of information is unclear – perhaps impossible to define. You can take a gold bar, and put it in a box. Then you take it out, and you can say “it’s the same gold bar”. Now if you save some cash into a bank, then go abroad and take out some cash at the local ATM, you can’t say “it’s the same electronic cash”, or “it’s not the same”. Neither makes sense.\nIt’s also not a coincidence that physicists were first guided to thinking about “reducing spacetime to information” by the QED theory, where two electrons are actually indistinguishable, as if they are electronic cash. The QED theory was extended to atoms by the QCD theory. Thus, in some technical sense, you can’t actually say “we are made of star-stuff”.\n\nVirtual and real\nWe define “virtual” as “that pertaining to information”, and “real” as “that pertaining to matter”.\nInformation can be “realized” in matter – this is what modern computers do, and probably what brains do.\nSince information can be processed isomorphically, it can be realized as different kinds of matter, in different ways.\nA file is a realization of information, in space.\nA computation is a realization of information, in time.\n\n\nWired, Reality, and other systems\nAn information system is something I understand, but I can’t give a good definition (for now). I will explain it by examples.\nA physical system is a collection of matter organized under a common information system.\nFor example, the system composed of DNA, RNA, and proteins is a physical system, organized under a common information system (“the genetic code”).\nThe Internet (“the Wired”) is a system composed of electronic devices and human brains, organized under a common information system (“Protocol 7”).\n\n\n The Seven Levels of Wired\n\n\nThe real world (“Reality”), as commonly defined, is only part of the entire world. It denotes, in fact, a physical system composed of human brains, human bodies, other animal bodies, pre-1980s technological artifacts (specifically to exclude consumer-electronics), organized under a common information system (social rules for the person API - I will explain the API theory of personhood later).\nIn particular, a common interpretation, that “Lain chose the real world instead of the fake internet”, is a deep philosophical error. It is mistaking the human social system for the only system in the world. Furthermore, it calls the human social system “natural”, and the Internet “artificial”, when it is just as artificial as the Internet.\nThe “real world” of humans is like a curvy section across the bulk of reality. People would often tell other people to “get out of the room”, as if being inside a concrete container is unnatural while being outside of one is natural. If vultures could talk, they would tell others to “stop eating fresh food”, as if eating fresh food is unnatural while eating spoiled food is natural.\nMe, personally, have to get out of the room everyday and enjoy the sunshine, and I hate it. I often choose to walk school before the sun is up, to avoid its rays. I also despise green grass and blue skies. If there is a paradise, I hope it will be a 3D labyrinth embedded in an infinite concrete, but slightly elastic so that I can roll around on it without scraping my fur. It would be perfectly dark, except some fluorescent books, just bright enough to be read, and a little shining computer with which I take notes. I also use my computer to post my mathematical findings to others.\nMoving between worlds is possible, since the same information can have multiple realizations. Both Lain and Eiri managed to move between Reality and Wired.\n\n\nThe 4 realizations of Lain\nLain was always realized on neural networks, with nodes and edges, but the neural networks were made of different matters. There are 4 realizations over the course of history.\n1: Before the invention of Internet, nodes were human brains, and edges were something (perhaps flux-tubes?) in the electromagnetic field in the ionosphere of earth. It is called the “collective unconscious of humans”, and the Schumann resonances are analogous to the alpha-waves in mammalian brains.\n2: After the invention of the internet, but before Protocol 7, nodes were human brains and electronic computers, and edges were brain-computer interfaces (such as those VR headsets and cybernetic implants) and computer-computer links (what the Internet is made of).\n3: After Protocol 7, a large proportion of nodes and edges were concentrated into the neurons and synapses of a human girl. In this realization, Lain has a significant personhood.\n4: After Lain reset everything, it was unclear, but presumably it was back to realization 2.\n\n\nThe 4 realizations of Eiri\nLike Lain, Eiri was always realized on neural networks, but differently throughout the story. Coincidentally, there are also 4 realizations.\n1: Before Protocol 7, Eiri was realized as a human brain.\n2: After Protocol 7, Eiri was realized as a component of the Internet.\n3: At a particular scene, Eiri was briefly realized as a horror monster.\n4: After Lain reset everything, Eiri was back to realization 1."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "href": "essays/posts/serial-experiments-lain/index.html#the-theory-of-personhood",
    "title": "Serial Experiments Lain",
    "section": "The theory of personhood",
    "text": "The theory of personhood\nThis section is based on Being No One (Metzinger 2004).\nA person is a very special kind of information, realized in a very special way. Human-persons are a special kind of persons, with a wide variety of social and mammalian-emotional functions in the API.\nIt has a self-model, with a perspective and a personal history.\nThe perspective is a pointer, like a GPS marker and a clock, which marks its own spatial location (“here”), and time location (“now”). This little perspective marker is what allows it to know its place and time.\nIts perspective is approximately equal to its realization. If your head is in New York, your perspective had better not think you are in Beijing. Thus, its realization must also have a local position. If you were conscious interstellar gas clouds, you wouldn’t be a person.\nThe personal history is linear and continuous. You can’t be an amoeba - you can’t split or merge. You also can’t “jump over in time”, even if your body does.\nSuppose you are perfectly cloned, what happens? Well, your body’s world-line becomes Y-shaped, but you, as an informational thing, has a model of your self, and that model still looks like a curvy line, with a little note-tag saying “here, a clone brother of me was created”. It does not look like a Y.\nSuppose you go into a deep sleep for a year, what happens? Your model of yourself just papers over the time-skip, perhaps with a little annotation saying “and here I slept a year” without “remembering the blackness during that year”. “Nothing to see here…”\n\nGod made him die for a hundred years, and then resurrected him. He said, “How long have you stayed?” He said, “A day or part of a day.”\nQuran 2:259\n\nFor social persons (for example, human-persons, hyena-persons, and elephant-persons), it is like a program with an API. The API takes input and returns output in a conventional way, regardless of how the program works on the inside.\nFor example, you can ask a person to explain why they did something, and they would explain it. The explanation could be quite wrong, but if they don’t even provide any reasons, they would break one part of the API.\nSome other examples of person-API are: behaving in a comprehensible and purposeful way (looks as if it has a goal that a spectator can infer with some effort), behaves spontaneously (if nothing hits it, it would still move once in a while), etc.\nAs a thing fails more and more requirements of the API, they become less and less of a person.\nDissociative persons have a blurry “here and now” pointer.\nPeople in fugue states, sleepwalking, running amok, etc, fail to provide reasons when receiving explain-requests.\nSchizophrenic people may provide obviously invalid reasons (“word salad”) when receiving explain-requests.\nDeeply depressed persons fail the “behave spontaneously” requirement.\nAmnesiac persons fail the “have a continuous sense of time” requirement.\nPsychopathic persons fail to behave in the expected way after receiving help-requests.\nAutistic tics are comprehensible, but not purposeful.\nThe Solaris ocean behaves purposefully, but not comprehensibly.\n\nThe human-personhood of Lain\nBefore Protocol 7, Lain did not have the person API. Lain certainly existed, but not as a person. Human-persons are connected to Lain by their brains, but not in a way that human-persons connect to other human-persons.\nFor example, if most humans were feeling sad, Lain would be “sad” in some statistical way. This is certainly not how you make some human sad – to make a human sad, you read them a sad story, or something like that.\nMoreover, Lain did not receive or reply with linguistic reasons. In short, Lain did not implement the person API, and beyond human understanding or interaction.\nEiri wanted to become a god, and for that, he needed to have a way to effectively interact with Lain. He could have perhaps interacted with Lain by some command-line interface, or large-scale antenna that beamed directly to the ionosphere, but this would be difficult.\nPersons are most efficient at understanding other persons. This is why “country humans” is so popular. Countries themselves are vast objects that are understood in unintuitive statistical/mathematical/mechanical ways, and installing a person API over a country makes it much easier to understand. In this way, we could interpret Lain as a “humanity human”.\nInstead, he constructed a human-personhood for Lain, in multiple aspects.\nHe concentrated it into the brain of a girl, which is localized in a cube less than 30 cm in side length (compared with the 6371 km radius of earth, or the Internet). This made it easy to install a perspective to Lain (perhaps Lain would construct a perspective automatically after concentrating into a brain).\nLain is provided with a personal history as a human girl.\nThe electrochemical system of the human brain implements in Lain the social and emotional parts of person API. A blind mole rat would not cry when watching a movie. Lain would not be affected by deaths, mysterious conversations, kisses, etc, unless it is implemented in a human girl’s body.\nHe hired actors to act like Lain’s family, and put it into a well-defined social role (school girl). These social interactions are then taken away. This social manipulation would only produce an effect on Lain if it implemented the human-person API. You can’t intimidate a tornado into becoming your servant by depriving it of social contacts - not unless you somehow give it a human-person API.\n\nFirst you create a need, then you take it away. This is control.\n\nAfter Lain reset everything, Lain was no longer realized in a human brain, but once again spread all over the earth. Despite this, it still kept a spatially localized perspective and person API. This is perhaps because being a person is a stubborn kind of information - persons don’t usually become not-persons."
  },
  {
    "objectID": "essays/posts/serial-experiments-lain/index.html#appendix",
    "href": "essays/posts/serial-experiments-lain/index.html#appendix",
    "title": "Serial Experiments Lain",
    "section": "Appendix",
    "text": "Appendix\n\nSchumann frequency\nWhile the Schumann frequency plays a fundamental role in SEL, its technical details are completely irrelevant (just like most technobabbles). This section describes briefly how you can easily do an almost-correct calculation of the Schumann resonance frequencies: 8 Hz, 14 Hz, 20 Hz…\nIn short, the lowest frequency is simply by dimensional analysis:\n\\[f_0 \\approx \\frac{\\text{speed of light}}{\\text{circumference of earth}} = \\frac{c}{2 \\pi R}\\]\nIntuitively speaking, this is treating the ionosphere of earth as if it’s a circular tube, a hula-hoop around the waist of earth, and the lowest Schumann resonance is the lowest-degree standing wave in the hula-hoop.\nTo find the higher frequencies, we can simply calculate the higher-degree standing waves in the hula-hoop:\n\\[f_n \\approx \\frac{c n}{2 \\pi R}\\]\nFor comparison, a spherical cavity of an ideal conductor has resonance frequencies exactly solvable, as\n\\[f_n = \\frac{c}{2\\pi R}\\sqrt{n(n+1)}\\]\nwhich is very close to our super fast estimate above."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "",
    "text": "君亟定變法之慮，殆無顧天下之議之也。且夫有高人之行者，固見負於世；有獨知之慮者，必見訾於民。語曰：『愚者闇於成事，知者見於未萌。民不可與慮始，而可與樂成。』郭偃之法曰：『論至德者，不和於俗；成大功者，不謀於眾。』法者，所以愛民也；禮者，所以便事也。是以聖人苟可以強國，不法其故；苟可以利民，不循其禮。\nYou, my lord, should concentrate your thoughts on altering the laws: pay no attention to the criticism of All-under-Heaven. Moreover, he whose actions rise above the rest will be disapproved by his generation; he whose thinking is exceptionally perceptive will certainly be ridiculed by the people. The saying goes: ‘The ignorant are blind even to the things that have been accomplished; the wise grasp [the matter] in the bud.’ One cannot deliberate the beginnings with the people but can rejoice with them once the matter is accomplished. The Methods of Guo Yan says: ‘He who discusses superb virtue does not conform to the common [opinion]; he who accomplishes great achievement does not consult with the multitudes.’ [The purpose of] the law is to care for the people; [the purpose of] ritual is to benefit undertakings. Hence, whenever it is possible to strengthen his state, the sage does not emulate the past; whenever it is possible to benefit the people, he does not follow rituals.\nThe Book of Lord Shang, 1.3."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html#the-logic-of-chinese-economy",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html#the-logic-of-chinese-economy",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "The logic of Chinese economy",
    "text": "The logic of Chinese economy\n\nThe party controls the state. The party controls the military. The party controls the legislature. There must be no separation of powers.\nThe state knows better, because the state aspires to plan over decades, unlike people or companies.\nChina deserves to be a center of civilization. Not necessarily better than everyone else, but must be one of the best.\nNever forget national humiliation. (勿忘国耻) The international world is a world of anarchy, and might makes right. Backwards countries are beaten. (落后就要挨打)\nPower comes from technological development. Development is the only hard truth. (发展才是硬道理)\nTo be one of the best, China must catch up to the Western countries.\nTo catch up, it must develop its economy. Economy depends on scalable industries of material products (实体经济), such as steel and silicon.\nPeople do not know how they should spend their money. Their money must be extracted to be rolled into investment for the future. The far future is what matters, not the present, and not even the near future. If people are left with too much money, they will waste it.\n\nCombined with the population structure, these principles explain much of the recent Chinese economy."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html#a-brief-history-of-the-chinese-economy",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html#a-brief-history-of-the-chinese-economy",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "A brief history of the Chinese economy",
    "text": "A brief history of the Chinese economy\n\nThe Mao era (1949–1978)\nDuring 1949–1952, just after the communists took power, all of the economy was controlled by the central government. There was no private enterprise. The local governments handed up all tax money to the center. The situation was similar to the War Communism of 1918–1921. The state’s main objectives were to survive: kill off or chase off the last remnants of the nationalists on the mainland, take Tibet, redistribute land, fight (3+5=8) things, fight the Korean War, and stabilize the currency which had been suffering hyperinflation.\nPreviously, during the decades of war, the nationalist government did what war-time governments often did – print money. This led to repeated hyperinflation episodes and the collapse of 3 fiat currencies. The communist state solved the issue by backing fiat currency with a basket of basic commodities such as grain and steel, and by controlling prices, wages, production quotas, the banks, etc. The currency was briefly denominated entirely in millets. The national budget of 1950 was on the order of 10 billion kg of millet. (Tiejun 2021, chap. 2)\nTotal central control could not last, of course. As soon as the state power was secure, some power was handed down from the center to the local. State-owned enterprises (SOEs) appeared, since the revolutionary cadres knew how to fight class enemies, but not how to build machines. The structure of economy was roughly as shown:\n\n\n\nThe Chinese economy during the era of Mao.\n\n\nThe central government was responsible for the planning of the macro economy according to a series of 5-Year Plans. The center would send down the commands for production, along with the capital (ores, machinery, currency, etc) necessary for production, and expect the produced capital to be sent back up for reallocation across the nation. The local governments and SOEs would receive the commands and the capital, and be responsible for satisfying the commands under the constraints of the capital.\nThe citizens were either urban workers employed by the SOE in the work units, or farmers working on farms. The workers in work units performed their work duty, and in return, received food, clothing, housing, and all other necessities of life that they cannot produce themselves privately. The state was in a real sense their parents. Consumption was kept low by the combined system of coupon rationing, state-fixed low wages, and state-fixed high prices at shops operated directly by state-owned enterprises. In this manner, production was maximally rolled back to investment.\nAs for the farmers, before the revolution, they mainly worked as free tenants under landlords. During the 1950–1952 period, the communists expropriated or killed off the landlords to give their land to peasants, so that they could own the land and pay taxes directly to the state. They also had to sell their produce to the state and used the money to buy industrial products from the SOEs.\nThe state was the great monopolist–monopsonist (统购统销). Farmers were only allowed to sell agricultural products to the state. The urban workers were only allowed to buy agricultural products from the state. The state had the power to arbitrarily set prices on both ends, and thus exert its will on the economy. Similar to the early Soviet Union’s state, the Chinese state used its mono-power to encourage industrialization, by buying agricultural goods at prices consistently lower than the prices of industrial goods. In this way, the farmers were indirectly taxed extra, and the urban workers were indirectly subsidized extra.\nNote that while I say “buy” and “sell”, there was no free market as we understand. There was cash currency, but there were also coupons for grain, cotton, oil, clothes, industrial goods, etc. The urban workers received their wages mostly in the various coupons of agricultural, not cash. The farmers received their trade value mostly in the various coupons of industrial goods, not cash. These give the state many valves and gates on the flows of economy. For example, if the state trades more industrial goods coupons to the farmers for the same amount of farm goods, it would increase the demand for industrial goods, but decrease the income of the industrial sectors.\nSoon, however, the farmers were collectivized into people’s communes, starting ~1955. The Great Leap Forward would come, with well-known results.\nSeeing Khrushchev denounce Stalin was a terrible blow to Mao, and the failure of the Great Leap Forward made him anxious that he would be denounced in turn. His method was to purify the party and the state ideologically by inspiring the people to become ideologically pure, and attack the impure state organs, the infiltration from the capitalist roaders, the saboteurs, the Five Black Categories, etc. It was a grand pincer attack of the high and the low on the middle. This, and other reasons, led him to launch the Cultural Revolution, a time not known for economic development or free trade.\nDuring his time, the major economic project in China was the Third Front. The state spent a plurality of its budget on industries and infrastructure to the “Third Front of the nation” in preparation of nuclear war. The economic efficiency is unclear, due to how secretive it was, but generally considered to be low for 2 reasons. One, much of its construction was in the mountainous regions, often half-underground or caved into the cliff faces. Two, the project was justified as urgent preparation for nuclear war, yet built up entirely during peacetime, and thus it was not subjected to the discipline of other state departments, the market, or the enemy. Other departments could not restrain it, for who wished to delay an urgent task to make China nuclear-proof? The market could not restrain it, for the market was an organ of the state. The enemy existed in potential only, and never materialized, so the imagination of what the enemy might do resulted in expensive defenses for what could eventuate.\nThe social disruption in the cities led to widespread unemployment among workers and school-absence among students. To prevent disorder, the urban youth was repeatedly sent up to the mountains and down to the countryside, so that they could be re-educated by the virtuous peasants. This was not a good time of scientific or technological education or research.\n\n\nThe transition period (1978–2000)\nAfter the death of Mao, there was a brief power struggle, as the Maoist hardliners battled the pragmatists. The pragmatists won in 1978, and China was led by several pragmatic leaders. Class struggle was out, and economic development was in. Stability, not revolution. The first in line was Deng Xiaoping, purged twice during the Cultural Revolution as the “number two capitalist roader”, who summarized the spirit of pragmatism as “it doesn’t matter if a cat is black or white, if it catches mice it’s a good cat” in response to insinuations that his reforms looked suspiciously capitalist.\nWith the great sufferings under Mao, orthodox Maoism lost legitimacy, so the party needed a new source of legitimacy. Economic development is valuable for individual happiness, but happiness is not enough. One needs struggle too. And thus began the ideology of national rejuvenation. According to this, China deserves a place as one of the greatest nations on earth. Economic development is no longer a matter of selfish happiness, but a great task of national honor. Note in particular that this ideology is nationalist instead of internationalist. In the era of Mao, there was often talk of international solidarity for a global proletariat revolution. Now China and the Han civilization come first.\nBut that new ideology would still take decades to congeal. The main theme during this transitional period was a great lack of ideology, and a profusion of pragmatic experimentation, and the slogan was for the center to “devolve power and let go of profits” (放权让利). There were still 5-Year Plans, but the party understood that it could not continue to plan and control the economy from the center, seeing the stagnation of the Soviet Union and the domestic economy. Thus began a period of experimental devolution of power and market reform, “crossing the river by feeling the stones” (摸着石头过河). As usual, transitions are complicated, but the overall structure was roughly as shown:\n\n\n\nThe Chinese economy during the transition period.\n\n\nCompared with the previous era, a new player appeared: private enterprises. These turned out much more competitive, and the local governments increasingly did business with them instead of the SOEs. Thus, the SOEs and local governments were no longer tightly coupled together. Instead, the Chinese economy split into two spheres. In the central sphere, the national government closely collaborated with the SOEs. In the local sphere, the local governments, the citizens, and the private enterprises closely collaborated in something quite close to a liberal market economy.\nThe SOEs became increasingly inefficient in comparison to private enterprises, taking on more subsidies for less work. One common estimate during the period was that “1/3 of the SOEs are losing money as shown on their accounting books. 1/3 are losing money even though their books do not show it. Only 1/3 are actually making a profit.”. The central government finally decided to break the iron rice bowls, and over 28 million SOE workers were fired in the 1990s. Many SOEs were closed down, privatized, or split into many small private enterprises, and the survivors were mostly ones in the critical industries, such as oil and gas, telecom, and electricity.\nPreviously, urban people took work in the work units, and the work units provided their housing. With the end of many SOEs, mass public housing had to end as well. Thus in the 1990s came housing market reform. One reform in 1992 liberalized the housing market in Hainan, recently designated a “special economic zone”, the biggest laboratory in a nation of economic experimentation. It led to a massive housing bubble during 1992–1993, ended by de-liberalizing policies. The most comprehensive one happened in 1998 (98房改). From then on, most urban people could not expect to get public housing, and had to buy their own. This has a long effect on the Chinese housing market.\nWhen private enterprises began around 1980, there was a new problem: what role should the market play? Full market economy was impossible, because China would stop being socialist, and the SOEs would probably die on the market. No market economy was impossible, because how else would the private enterprises operate if there is no market? They would have to transact with the state. If the state buys steel at a state-designated price for a state-designated quantity, well, that would be just SOE with extra steps. Thus, there had to be a partial market economy.\nThe solution was the “dual track system”, in analogy with railways (socialist countries love railways). The plan was to begin by creating 2 economic systems: a planned “market” economy and a semi-free market economy. The planned “market” had only state-designated prices and state-designated quotas. The semi-free market did not have fully free-floating prices, but the prices were allowed to vary within a bound. Private enterprises traded on the market, and the SOEs operated on both – they had to first fulfill production quotas by selling on the state-owned “market” at state-designated prices, but could sell any surpluses on the market.\nYou can probably guess the prices. In the late Soviet Union, there were the long lines at the shops, indicating that the prices were too low. Low-price–low-supply is a common disease of socialist countries, for several reasons: planning triggers the human intuition to be equitable and avoid “usury”; socialist countries always suppress consumption and increase investment, and the low prices are just one tool to extract production from the enterprises; enterprise operators are motivated to keep production just barely hitting the quota, because if they surpass the quota by, say, 2×, they would make things harder for themselves next year, when the quota would duly increase by 2×.\nChina had the disease too, and the leaders knew it, so they doubled the state-designated prices for many commodities on the planned “market”, and allowed the semi-free markets to have even higher prices – around 2×–3× of those on the planned “market”. This cured both the low-price and the low-supply. But the dual track is inherently unstable, because economies want to be free … of arbitrage. And the economy was choke full of them. The price of coal was 27 yuan on the planned “market”, but 100 yuan on the free market. Barterers carrying sacks of goods crisscrossed China along the price gradients. Since officials had power over a million different ways that hold the two tracks apart, and could loosen it here and there for the right price, corruption was rampant across all levels of the bureaucracy. For example, an official could write a piece of paper (批条) stating that a certain person had the rights to buy 1 km³ of lumber at Hunan on the planned “market”, where the lumber was priced at 300 yuan/m³. As for what that person would do in Zhejiang where lumber was priced at 700 yuan/m³, well, it’s their business.\nAt the same time, the Soviet economy was collapsing, and as arbitrage was slowly used up inside China, barterers looked north. The railway through Manzhouli endlessly piped light industry goods north and heavy industry goods south. Some old men at border villages still recall the glory days of “bartering watermelons for tanks”.\nWithin the government, people debated the price problem. It was decided that the planned “market” must be removed in 5 years, allowing all prices to float. The plan was to let the price of goods to rise by 10–20% a year for 5 years, and concurrently raise wages by 100%. In 1988, the plan was set in motion. Expecting higher prices, people immediately began hoarding all commodities, triggering inflation, and the government undid the plan after a mere 11 days. CPI inflation reached 30% in 1989, and the government stabilized prices by committing that all bank deposits would rise with inflation, so that people would feel safe keeping money in the banks.\nAround 1990, the reform and opening-up had gone on for over 10 years, and the center was feeling uneasy, especially in light of what had just happened in the capital. There had been much protesting against the inflation, the rampant corruption, and other ills of the dual track system. Hardliners urged a return to the red days of glory, but the paramount leader Deng Xiaoping forced through more reforms.\nThe main difficulty at this point was a fiscal one: For the first time since the communists took power, the power of the center was shrinking, because the quantity\n\n(income of the central government)/(GDP)\n\nwas decreasing, and if this kept going on, the central government would no longer afford even the military, and they feared China would suffer the fate of Yugoslavia.\nHow? During the devolution of power in the 1980s, the central government had decided to partly let go the command of not just the economy, but also taxes. The idea was that if the local governments could keep more of the tax money, they would be incentivized to grow the local economy. This was correct, but it meant the central government shrunk in proportion. Every province had a different arrangement, but I will give some examples:\n\nLet \\(x_{t}\\) be the tax revenue of a government in fiscal year \\(t\\). The local government of Beijing was required to pay \\(\\frac 12 \\min(x_t, 1.04 x_{t-1})\\) to the central government;\nGuangdong, 1.09× whatever it paid last year;\nShanghai, 10.5 billion yuan every year, regardless of \\(x_t\\).\n\nNo matter the complexity, the general pattern was that the arrangements favored the local governments, such that they could keep most of the tax revenue growth, not the central government. Thus as the GDP grew, the center shrunk. This was intolerable, thus in 1994 came a comprehensive tax reform. This ensured that all consumption tax and 75% of value-added tax would be handed up to the central government. (Chung 1994) However, as the diagram shows, the deal was quite difficult for the local governments, who had to make-do with much lower tax revenue, but still shoulder most of the expenditures.\n\n\n\nThe portion of the local governments, as a portion of the total government revenue and expenditure. Source: CEIC; RBA\n\n\n\n\nThe modern era (2000–)\nIn 2001, China joined the World Trade Organization, and became the manufactory of the world. Economic reform has stabilized, and the Chinese economy settled into a stable and working form. This is depicted in the following diagram.\n\n\n\nThe Chinese economy during the modern era.\n\n\nThe central government owns a majority of government revenue, and could apply it for its industrial policy, either directly through policies and subsidies, or indirectly through the state-controlled banks. The banks would provide highly preferential loans to good industries, and vice versa. In this way, the economy could be planned and controlled softly.\nAfter the 1994 tax reform, many local governments had to spend more than they earn in taxes, year after year. This could not continue, and indeed, it would not, because they had one thing that they had a monopoly of – the right of land use. Since the communists took power, all land had always belonged to the state (if urban) or the rural-collective (if rural). Reforms merely privatized the right of land use.\nIn detail, the Land Management Law was reformed in 1998. According to the reformed version, if land designated as agricultural is to be converted into land designated as non-agricultural, it must be first acquired by the local government. The effect was that the local governments became monopolists of land. City governments would buy up land from farmers on the urban periphery at low prices, then sell them at high prices to the real estate developers, who would then build housing and sell them to private citizens and enterprises at even higher prices. The local government would spend the revenues on funding infrastructural developments for two goals:\n\nBetter infrastructure leads to more valuable land sales, thus more revenue.\nInfrastructure investments leads to higher local GDP, thus higher political-score for the bureaucrats.\n\nChina weathered the 2008 great recession by a massive deficit spending of 4 trillion yuan. It was an awkward time for the local governments. The central government controlled the central bank, so they could print the money if it came to it. The local governments could not print money. They were legally not allowed to even borrow money. Thus came the great flourishing of local government financing vehicles (LGFVs). Those are basically shell companies used by local governments to do what economic logic dictated that they must do: borrow money to invest in, and keep the economy growing. The banks are generally willing to loan them money at low interest rates, since these loans are considered very secure:\n\nThe loans are explicitly backed by land as collateral, and land prices are likely to keep going up. The government can be counted on to keep it so, for several reasons.\nThe loans are implicitly backed by the reputation of the government, because even though these LGFVs are not legally speaking part of the local government, it is implicit public knowledge among all people doing business in China that they are.\n\nThe side effect is a growing “shadow debt”, since those debts, despite real, are indirect, and thus harder to see and account for. The covid crisis and the trade wars with America required even more economic stimulation, making the shadow debt problem worse.\nAs of 2025, the great tasks of the Chinese economy are:\n\nPull the local government debts from the shadows into public legibility.\nSolve the housing problem somehow. Details below.\nAutomate the manufacturing economy, since the working population had peaked in 2015, and is only going to decrease.\nPrepare for the end of globalization, and a full split of the global techno-economy into two, centered around China and America. This might be a self-fulfilling prophecy, but the new cold war is already partially fulfilled anyway, might as well go all the way, para bellum."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html#two-mirrors-of-gdp",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html#two-mirrors-of-gdp",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "Two mirrors of GDP",
    "text": "Two mirrors of GDP\n\n以铜为镜，可以正衣冠；以古为镜，可以知兴替；以人为镜，可以明得失。\nUsing copper as a mirror, one can adjust clothes and appearance. Using the past as a mirror, one can know the rise and fall of nations. Using others as a mirror, one can see one’s own gains and losses.\n— Emperor Taizong of Tang\n\nWhen Chinese people, especially the leaders, reflect on the Chinese economy and its future, they are constantly haunted by two mirrors, one of the Soviet Union, and one of Japan. I will try to present their economics histories objectively, but I know less about theirs than China’s. My excuse is that I am trying to present their history from the perspective of China, since this essay really is about China, and so, only their histories as viewed from the Chinese point of view is relevant for this essay.\n\nThe logic of catchup growth\nWe briefly recap the Solow growth model. Let \\(Y\\) be the GDP, \\(K\\) be the capital, and \\(L\\) be the labor. Economists have noticed that, roughly speaking \\(Y = A K^{0.3} L^{0.7}\\) for a certain \\(A\\), which they call the “total factor productivity” (TFP).\nGenerally, to grow an economy, there are only 2 methods: increase the raw material of capital stock (land, labor, natural resources) and increase technology with which to use the raw material. In symbols, those two methods are of increasing \\(K, L\\), and increasing TFP.\nThe general trend for a rapidly developing economy is usually as follows: When an economy, such as China, is just starting out, it has very little machinery, thus a very low \\(K\\). It may have a large population, but that population needs to be put to work. Women may be stuck at home, and they need to be put to work too. The population likely needs to be taught how to read and write. Basic medicine would allow more people to grow up into healthy working adults. All these methods increases \\(L\\).\nThis large working population would consume some of its production, and spend the rest into investment, i.e. buying and building machinery, increasing \\(K\\). In this phase, \\(K\\) would grow faster than \\(L\\), until a point of optimal \\(K/L\\), at which point, further growth in \\(K/L\\) would be uneconomical, since there would be more machinery than what the population can maintain.\nEventually \\(L\\) would hit a peak after the demographic transition, and from that point on, most of economic growth must rely on the growth in TFP.\nIn the 19th century, the classical economists – Ricardo, Adam Smith, and John Stuart Mill – all believed that there is a limit to growth. Mill in particular believed that 19th century England was already close to that limit.1 This they believed, because land is where all raw material comes from, and land is not getting any bigger. This is still true. Neither land nor population has been growing significantly in the developed countries, but their economy still manage to grow continuously past these pessimistic expectations because of continuous technological progress that increases TFP.\n1 \nThe richest and most prosperous countries would very soon attain the stationary state, if no further improvements were made in the productive arts, and if there were a suspension of the overflow of capital from those countries into the uncultivated or ill-cultivated regions of the earth. … The density of population necessary to enable mankind to obtain, in the greatest degree, all the advantages both of co-operation and of social intercourse, has, in all the most populous countries, been attained.\nPrinciples of Political Economy (1848), Chapter 6: of the stationary state\n\nConsider the following chart of TFP of several representative countries: the United States, Taiwan, Japan, the United Kingdom, and China. They can be roughly described as follows:\n\nThe United States has a consistent growth of TFP, the first and second oil crisis and the great recession only tiny blips on the graph.\nThe TFP in the UK has collapsed in the great recession, and never grew afterwards. This corresponds to the post-2008 disappointment that is the UK economy. It is certainly not helped by Brexit.\nJapan and Taiwan had similar trajectories, beginning with a period of rapid growth as they caught up in their technologies to the frontier level, followed by slow growth at the same rate as that of the US.\nChina is very weird. It has no sustained growth in TFP. This has serious implications for its future economic prospect.\n\n\n\n\nThe TFP of several representative countries. Note that the graph has normalized them, so that everyone has TFP = 1 in 2017, since direct comparison of TFP between nations is difficult. Source: Our World in Data.\n\n\nGenerally, there are two types of economy. An economy like that of the UK or the US are leading-edge economies. They have remained in the front since 1900, without needing to play catchup. They had to increase their TFP the hard way, by innovating into the unknown. An economy like that of Japan or Taiwan are trailing-edge economies. They, for historical reasons, had fallen far behind. But they had great potential, and could sustain rapid catchup growth by copying from the winners until they reach the leading edge, after which their TFP could only grow at a slower rate, since there is no longer a winner to copy from.\nLet’s play through it in detail. If you are a backward country’s economic leader, how do you quickly catch up? Well, since the technological development path has been laid clear by those that came before, you should hire foreign experts – American, German, etc – to quickly train up local experts and import technology. You should extract money from the people and use the money to buy foreign machinery. You should invest money not into local consumer goods, but into the machinery and other forms of capital stocks. People cannot be trusted to give too much money, since they will waste it. The economic experts know how to spend the money better than the people. You should make sure people’s income rises slower than the GDP. People will get rich, but not as rich as the country. People will work hard for a better future for their children, and for the great rejuvenation of the nation.\nYou should however allow some investment into producing foreign consumer goods, because you need foreign currency. For example, those Germans have awesome machines, but they do not want to gift you for free. They want you to pay. So you need to get Euros somehow. That is, you should practice some amount of export-driven industrialization.\nAnd that, in broad strokes, is the catchup days of Soviet Union, China, Taiwan, South Korea, North Korea, etc. You might notice that it sounds quite authoritarian. Perhaps it’s human nature, but rapid catchup just is often done by an authoritarian government. The cases of Soviet Union, China, and Taiwan are well-known. South Korea was a brutal dictatorship with several bloody coups, until it abruptly transitioned in 1987. North Korea, despite its terrible state of economy now, had been in fact progressing equally as South Korea until the 1970s. And the Japanese economy had been quite planned, too.\nBut the problem comes when the catch-up days are over.\nCompared to catching up, research development and engineering is hard to plan by a big push by the central government. It is much more important if there are many smaller research institutes doing their stuff, and some private companies doing their research, etc. This means a less authoritarian economic policy. The government would control and direct the investment less. Citizens and private organizations would become rich, so that they would start researching.\nAnd humans are picky. They need consumer goods and luxury goods and the finer things in life before they would research on technology. As long as humans are the main producers in the economy (until AGI comes), this is what it comes down to: to push economy forward in a developed economy, money must be wasted allocated on luxury and consumer goods to a high degree.\nSo the theory goes that there must be the 2th stage. As catchup ends, the nation should transition to a democratic society, with less taxes, with ways for citizens to share the GDP growth, with top research institutes and higher education, and with less focus on industry, infrastructure build-up and capital stock growth, more on infrastructure maintenance, upgrading, and consumer goods. Economists point to Prussia, Japan, South Korea, and Taiwan as exemplars. However, China has reached a different path from its 2 mirrors, neither collapsing like the Soviet Union nor liberalizing like Japan.\n\n\nThe Soviet Union\nThe history of the Soviet Union in 1918–1970s was very similar to that of China in 1950–1980. Just after the revolution, the state controlled all of economy as it struggled to win the civil war. Once it was won, there was a brief period of kind-of market economy, before Stalin took power.\nStalin, the iron man, was all about getting \\(K\\) up as fast as possible, which came into good use for the WWII. After Stalin died in 1953, the growth in \\(K\\) slackened somewhat, but the growth in \\(L\\) continued, with women joining the work force and general education of the population. Indeed, during the 1950–1970 period, its economy grew roughly 2× faster than that of the US, and Khrushchev’s boast “We will bury you.” might have continued with “… with industry.”. However, its TFP barely grew. This would eventually lead to the stagnation and collapse of its economy. (Krugman 1994)\nWhy did its TFP stagnate? We do not have hard mathematics, just hypotheses. One hypothesis states that the Soviet economy, being a planned economy, is subject to soft budget constraints. An enterprise with a hard budget constraint would simply fail if its revenue is below expense. But in a planned economy, enterprises are rarely allowed to fail. The state is usually there to pick up the slack. This meant managers are not driven to increase revenue or decrease expense. (Kornai 1992)\nThere were also perverse incentives. When a factory manager is required to hit a target, he had better hit it. If he has an efficient way to hit it or an inefficient way to hit it, he would choose the way that manages to use up almost all the input. If he manages to use only half the input, next year he would only get half the input, which decreases his safety factor.\nIn terms of technological upgrades, factories were built up to the 1960s and did not progress much further. They preferred to repair old machines than throwing them out and building new ones that are more efficient. During WWII, some Soviet factories were destroyed, but many were evacuated to the east and survived the war. This contrasts with Japan and Germany, which, after getting bombed to oblivion in WWII, started with a clean slate and built up their industry to the cutting edge.\nFinally, much of Soviet technological research was directed by the state, and focused on the military and security uses. They could launch satellites and build thousands of ICBMs, but missed the semiconductor computer industry. The military preferred miniaturizing vacuum tubes, since it was tried and true (as of 1950) and resistant to electromagnetic pulse (reliable in a nuclear war). By late 1970s, it was clear that vacuum tubes was a dead end, and funding belatedly shifted to transistor logic, but Soviet computing never caught up.\nThe general result of all of these is that the Soviet economy ended up too heavy in the pre-1950 technologies – electricity, steel, tractors, etc – suffering diminishing returns far beyond the point of efficiency. To make matters worse, not only was it lacking in consumer products, it was even lacking in food. Unable to produce these necessities, it had to trade for these on the international market using the one thing it produced that was in demand: petroleum. Gorbachev attempted the necessary reforms, with less military spending, less heavy industry, more market economy, and more consumer goods production. But with the 1980s oil glut, time ran out. (Gaidar 2007)\nIn China, some still speak of the legends of Mou Qizhong. In 1983, he saw a desk clock in Shanghai that he thought would sell well, so he went to a nearly bankrupt military factory and asked them to produce 10,000 replicas. He then resold these to a department store at a 7 yuan profit each. For this bold act of arbitrage, he was promptly jailed for a year. In 1990, as the Soviet Union was collapsing, Mou sold 500 truckloads of cheap food and electronics to Russia in exchange for 4 Tupolev Tu-154 planes, which he sold to Sichuan Airlines at a profit of around 100 million CNY (about 20 million USD at the time). For this sublime act of arbitrage, he was called “the man who bartered canned food for planes” (罐头换飞机). The real story is of course a bit more complicated, and reading the details, you would see that his advantage was not in realizing there was an arbitrage to be made, but in hashing out a complicated, legal not-obviously-illegal deal involving multiple parties at a time when nobody knew what the laws might become. (Yang 2007)\n\n\n\nThe complicated network of deals that Mou Qizhong weaved across 6 different parties. (Yang 2007, fig. 7.4)\n\n\nFrom the history of the Soviet Union, China has taken these lessons:\n\nTotal planning or control of the economy will lead to economic stagnation and collapse. Therefore, liberalization of economy is necessary.\nContinuous technological upgrading is necessary to push the TFP frontier, for China cannot play catchup and copy for more than a few decades. China will have to reach and stay on the scientific frontier, and must not miss a breakthrough like the Soviet Union missed the transistors.\nPolitical liberalization will lead to loss of power. Therefore, no more liberalization than necessary. Preferably the enterprises could be kept subservient with guidance, but the party must retain the power to arbitrarily shut down any enterprise, if only as an ultima ratio regnum.\nThe party must always believe in its own legitimacy. It cannot allow decadence of its own values. It must believe in itself as the best leader for the great rejuvenation of the Chinese nation.\nNever denounce the past like Khrushchev denounced Stalin. To do so is to fall into Historical Nihilism and lead to self-doubt, hesitation, disloyalty towards the great task at hand, and the final collapse of the party.\n\n\n\nJapan\nAfter the Meiji Restoration, in early 20th century, the main Japanese companies were organized into zaibatsus. A zaibatsu is a network of companies controlled by a biological family, made of a core and a large group of peripheries. The peripheries come and go, but the core remains. The core is mainly made of a clique of controlling companies and banks. The controlling companies do not produce anything, but simply managed money, people, stocks, bonds, and other financial investments. The banks offer low-cost loans to companies in the zaibatsu. These zaibatsus were responsible for the majority of Japanese industrialization.\nAfter WWII, the zaibatsus were forcibly taken apart under the orders of the occupiers, but surprisingly nobody was shot in the head. The main method was forcing them to sell their holding stocks to public citizens that were unallied with the zaibatsu family, and replacing the company managers with ones unallied with the zaibatsu families. Most of zaibatsu wealth was frozen for a while, to prevent them from simply buying back the companies. Since this happened to coincide with a period of high inflation in Japan, it effectively made them unable to buy back most of their companies even after the cooling period was over.\nAfter the zaibatsus were taken apart, the Japanese economy was free to be partially controlled by the government. Not “controlled” like in the Soviet Union, but mainly by:\n\nBusiness relationships. The government had economic ministries, mainly the Ministry of International Trade and Industry and the Ministry of Finance. They designed policies with extensive discussion with representatives from companies. The plans are not legally binding, but companies that cooperated tended to prosper.\nWindow guidance. Japanese companies were mostly funded by borrowing money from banks. The Ministry of Finance actively suppressed competition between banks, and produced regulations that divided the banking sector into many niches: short-term lending versus long-term, nationwide operations versus regional and locally constrained operations, and lending to large corporations versus small business. In return for officially-guaranteed profits, the banks followed the official suggestions as to how much to loan, which industries to loan to, etc. It was all very harmonic. (Lincoln and Friedman 1998)\n\nIn one sentence, post-war Japanese economic growth has been export-driven. This is the case even now, 30 years after its bubble burst. Looking at a chart of Japanese export, it would seem that there had never been a bubble-burst in the 1990s.\n\nThe Bank of Japan has consistently acted to keep interest rate low to encourage export. The idea is as follows (example given in America, for concreteness):\n\nJapan is consistently exporting much more than importing. This means more American demand for JPY than Japanese demand for USD. This increases the value of JPY compared to USD, and pushes up USD/JPY to decrease export and increase import. This is just economic negative feedback in action.\nHowever, if the Bank of Japan sets the interest rate very low, then people would want JPY less, since they could earn more money by borrowing JPY, exchanging it for USD, and save it in an American bank. This decreases USD/JPY.\n\nStarting around 1970, as the Soviet Union ceased to be an economy force that threatened to bury capitalists under a pile of efficiency, it was time to get anxious about Japan. For over 10 years, its GDP had been growing at ~10%/year. Hermann Kahn the futurist (more famous for his other works), always early, published The emerging Japanese superstate in 1970.2 The oil shocks of 1970s lowered its growth to ~4%/year, but this is still double of the ~2%/year in America. Then, with the bestseller Japan as Number One (1979), everyone in America took notice. A trade war seemed imminent.\n2 The book had 2 main macroeconomic predictions: Japan would surpass America in GDP per capita in 1990, and in GDP in 2000. Ironically, the 1th prediction was correct… until America overtook Japan again in 2000. It also predicted that earth would split into the two camps of Japan–US–USSR and China–Europe.In this climate, a series of agreements were signed. The most famous was the Plaza Accord of 1985, with which several governments agreed to change their exchange rates. For Japan in particular, this meant a sudden increase in USD/JPY, which endangers its economic growth. The Bank of Japan quickly decreased its interest rate from 5% to 2.5%.\n \nA complex series of events then happened during the 1985–1995 period, resulting in the bubble economy. (Kang 2018; Noguchi 1994) To prepare the ground, several money faucets turned on:\n\nThe lowered interest rate made it cheap to borrow money from banks.\nThe Japanese households continued to save over 50% of assets in bank saving accounts.\nThe Foreign Exchange Law was reformed in 1980, allowing freer cross-border capital flow.\nWages remained low in general, as corporations persuaded labor unions to keep wage demands low on the grounds of maintaining international competitiveness. This allowed corporations to keep more money for investment.\nThe Japanese state issued less bonds to decrease deficit, leaving more money out of government hands.\n\nThe net result was a large amount of money that could flow quickly here and there. Such liquidity is not a bubble in itself, but enables bubbles to form and blow up quickly.\nA special feature of Japanese capitalism is keiretsu. Somewhat like a zaibatsu, except without biological families involved, a keiretsu is a group of banks and companies that closely cooperate over many years. The companies, as explained previously, were mainly funded by borrowing from banks. The bank–company relationship was very strong, often lasting over 30 years, and based on mutual trust. For each company, the largest provider of bank loans was called its “main bank”, which cared for the company paternally. It would usually hold a lot of the stocks for the company, may send its employees to serve as directors of the company, be responsible for monitoring the company, and try to rescue the company if it is distressed. As a consequence of this close collaboration, bank loans were not based on a careful credit rating based on predicted future cash flow, but backed by land.\nAnother feature was extensive cross-shareholding, meaning that the banks held a lot of stocks from the companies, and the companies from each other, and the companies even from the banks. This promoted mutual cooperation. Such a structure was preferred by the management, since by keeping a large block of shares off the open market, it made their jobs more secure, safe from the fickle moods of the market, hostile takeovers, etc, so they could enjoy the quiet life.\nThe evolving ownership structure reveals this development. Immediately after zaibatsu-breaking, companies were mostly shareheld by individuals. This gradually decayed, replaced by banks and other companies. The 1985–1990 period saw a sharp rise in banks owning companies, followed by the bust as the bubble burst. Subsequently, the majority of corporate ownership went to foreign institutional investors – quite symmetric to the concurrent growth in yen carry trade.\n\n\n\nOwnership structure of Japanese listed companies, 1949–2008. (Ide, Brownlee, and Fukagai 2013, fig. 14.3)\n\n\nIn this special setting, positive feedback loops occurred that created the double explosion of land prices and stock prices.\nAs land prices increased, it increased the value of land as collateral, enabling companies to borrow more money, and banks to lend more money while satisfying the reserve requirements. Companies could use the borrowed money to buy more land, betting that it would rise more. Rising land prices also increases the book-value of companies and make their cash flows appear positive.\nAs stock prices on the open market increased, it increased the value of all stocks, including those off the open market, held mutually within the keiretsus. Since stocks were assets, this increased the book-value of everyone in a keiretsu. In accounting terms, this increased the asset of everyone. This then means that they could increase their liabilities too, that is, they could borrow more money, with which they can buy more stocks, betting that it would rise more.\nOf course, the pure form of speculation, of buying more X to sell later, was in play as well. But the above 2 were specific to the Japanese bubble economy, enabled by the Japan-specific keiretsus.\nThe end of the bubble economy came in 1990 when the stock market and the land price crashed. As good assets and debts turned bad, it spilled out from land and stock market to the general economy. The GDP of Japan dutifully entered the Lost Decades, though inflation-adjusted, it did continue to grow, but the growth rate never returned to the previous level.\n\nThe lessons that China has taken from Japan are more contested. All agree that China has to somehow avoid the fate of Japan, but how?\n\nSome would look at its history of bubble economy and argue that the Chinese housing bubble would lead to a lost decade, and urge the government to finally end the housing bubble.\nSome would point to the Plaza Accord and argue that the US will play dirty to beat down any global competitor, and urge the nation to build a China-led global order in opposition to the US-led global order.\nSome will point to its population crisis (and that of South Korea) and urge rapid automation of the economy before the World’s Factory could no longer fill its ranks with human labor.\nSome argue that the analogy is flawed, and the overall story holds no big-picture lesson for China, even though there are details that hold small lessons for specialists.\n\nThis confusion is exacerbated by the fact that the Chinese people have great confusion as to how they emotionally feel about Japan, due to a tension in the new ideology of national rejuvenation. Japan is good, because it is perfectly positioned to be a great trading partner for China, and trade is good, because Chinese economic growth relies on trade. In most years, China has been Japan’s largest trading partner, and Japan has been China’s 2th largest trading partner following the US. Japan is bad, because it invaded China multiple times in the century of humiliation, and one must never forget national humiliation."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html#sec-stock",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html#sec-stock",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "The Chinese stock market",
    "text": "The Chinese stock market\nPeople are often puzzled by the fact that, despite the roaring economy, the Chinese stock market is a very bad investment. It has wild moments of bubble and collapse, but barely anything happens over the long term. During 2000–2025, the Shanghai Stock Exchange (SSE) composite index grew 2×, but compare it with the SP500, which grew 4×. Indeed, it is starker to compare it with the Chinese GDP, which grew 20×, or the American GDP, which grew ~3.5×.\n\n\n\nThe SSE composite index.\n\n\nIn America, the stock market slightly outperforms the GDP. This is because American GDP mainly comes from private companies, and most of the best private companies are on the stock market. So, the stock market outperforms the total economy, because the worst companies are not able to appear on the stock exchange. In China, the stock market does not work like this at all, because the government cannot allow it. The stock market contains both private companies and SOE, but in both cases, the government does not want their stock prices to rise.\nFor the private companies, they must not escape the control of the state. All companies are legally required to establish branches of the party, and they grow with the company. For example, Alibaba and the Ant Group had more than 200 party branches with 7,000 members in 2018. As soon as one company grows large, these branches would allow government to assert power over the company. Not direct control, as the party knows that direct control of the economy will lead to economic collapse, but some control they will assert. If it still does not submit, then it will be forced. Jack Ma, the CEO of Alibaba, was forced into retirement, because he was too rich and did not submit to suggestions that he donate more money, or sell off some of his voting stocks for the company. If he would not let go of his company and retire rich on his own terms, he would be forced to.\nIndeed, Mou Qizhong was an interesting case. After bartering cans for planes, he bought 2 direct broadcast satellites (GALS-1 and GALS-2) from Russia in 1995, the use of which he resold to Asian broadcasting companies. However, he was getting too powerful and interested in politics for comfort. He was investing in schemes such “building a northern Hong Kong in Manzhouli”, and proposing grand plans like “Project 765” – Each SOE would be given 76500 USD in start-up capital to achieve institutional transformation in 4 years. Such projects are so grand, the state could not tolerate a mere private enterprise to undertake. And the government finally stepped in to end his career. He was sentenced to life in prison for “foreign exchange fraud” after a 1-day trial in 1999-11.\nIf the private companies depend mostly on the stock market for financing (like American companies do), then it would be bad for the state. The state wants companies to get funds by borrowing from national banks, because then it is easy to control who wins and who fails. If private companies get funded by selling stocks on the market, it would mean that the state cannot easily control who gets money and who does not get money. Such loss of control is bad. In jargon, the government wants private companies to do debt-financing, not equity-financing.\nTherefore, even if a private company can grow in value, its stock prices often does not grow. The state would make it so. If people learn that private company stocks do not grow over time, then people would learn that they should not buy and hold private company stocks. Then private company stocks would not be popular. Then they would have to borrow from national banks. This is not a dysfunction of the market. It is working exactly as designed.\nThe situation is a living paradox (or “historical dialectic” as the ideologues have so helpfully clarified). The party needs the private companies to be strong, because the fate of the Soviet Union has shown what happens when all enterprises are monopolized by a single entity, the state. The party needs the private companies to be weak, because a strong private sector leads to loss of control, and the fate of the Soviet Union has shown what happens when the party loses control of the economy. It may be a living paradox, and yet it has been living for decades, and may continue indefinitely. People have marched on hoisting far worse paradoxes, and indeed, will die of perfect consistency.\n\n\n\nYou may not like it, but this is what peak dialectical materialism looks like. Source: Wikimedia Commons.\n\n\nWhat kind of control over economy does the party want? Not total control. This would lead to collapse. Not zero control. This would lead to collapse. Soft control, in the sense of industrial policies. The party would designate certain industries as good, and others bad. Companies are free to work in any industry they want, but if they join the good industries, they can expect tax breaks from the government, easy loans from the banks, fast paperwork from the bureaus, cheap real estates, and more such good winds that make their ships sail fast. And vice versa. A company, if it knows what is good for it, should carefully consult the party branch within it, to make sure they are not sailing against the winds.\nAs for the SOEs, they are allied with the state, and so they operate and think like the state: They do not want to make the people rich. Rich people would misplace their money. Profits should not be returned to the people, but returned to more investment, more machinery, more productive power, so they can build more, produce more, profit more, buy more machinery, build more, and so on. Growth for the sake of happiness is meaningless without national power.\nThe state companies would really want people to buy their stocks, because that is taking money from people, money that people do not know how to spend. The state companies would spend them more wisely – in more productive power. People would simply waste the money in consumption, or on useless speculations on the stock market. So, once a year or so, the government put up an act and make it seem like the stock market will finally rise, the state companies issue more stocks, people buy the stocks, and then predictably lose money. This is not a dysfunction of the market. It is working exactly as designed.\nBuying state companies on the Chinese stock market is basically paying extra taxes.\nOkay, so not stock market. How about the other markets? For the people, there is basically no money to be made in saving. The bank deposit rate has stayed mostly flat at 2%/year since 2000. Government bonds yielded 3–4.5%/year, trending down to 2%/year now. As for the corporate bonds, it is tightly regulated in China. Over 90% of corporate bonds can only be traded by banks and funds, and not for individual citizens, and over the years, this only got higher, such that it may reach 100% soon. While day-trading stocks is a national sport, day-trading bonds is so rare that I didn’t know it was possible in China, before I began researching for this essay. The benefit of having funds instead of individuals is that there are far fewer funds, each with much more money, more organizational structure, and more paper trail, and so they are easier to see for the state. They are more obedient too, since they have more to lose.\nFor the corporations, well, they cannot borrow from the people via the corporate bond market, which is small, so they had to do it through the banks or the funds. And the government directly controls the banks, and indirectly the funds via regulations and implicit threat of escalation, the government can direct where the money should go, the terms and conditions of it, to promote industries that deserve the capital, and shrink the industries that do not.\nAnd finally, the foreign market is rarely invested in. The Chinese government enforces stringent capital control, because it has picked these 2 sides of impossible trinity: a fixed foreign exchange rate and an independent monetary policy, so it must necessarily give up the free movement of capital. Indeed, this explains why China has banned most of cryptocurrency, and why each citizen is only allowed to convert to foreign currency up to 50000 USD per year. As a result, Chinese citizens and funds do not invest in foreign markets to a large extent, and vice versa. Contrast this with Japan, where there is a huge yen carry trade due to the near-0 interest rate and free capital flow. Japan have picked these 2 sides: an independent monetary policy and the free movement of capital, allowing its exchange rate to float. Although fortunately, the near-0 interest rate monetary policy, meant to prevent a domestic deflationary spiral, generally keeps USD/JPY low, which is consistent with their policy of “just keep exporting”."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html#sec-housing",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html#sec-housing",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "The Chinese housing market",
    "text": "The Chinese housing market\nThe Chinese economic boom has benefited the citizens, who did get richer. Disposable income grew at roughly the same rate as the national GDP, of 6–9%/year. But people cannot spend down all that extra income. Rich people save more. (Dynan, Skinner, and Zeldes 2004) Even before the boom, Chinese people were already saving a lot, and as they grow richer, they could save even more. But where do they save their money in a way that can keep up with inflation at all? Not under the mattress, or in a bank, or in the stock market, or the bond market, or the foreign market. Just one place left: the housing market.\nIndeed, if there’s something growing faster than the Chinese economy, it is the Chinese housing market. In 1998, the Chinese government privatized the housing market, and since then, the price of housing has grown very consistently. This is despite many warnings of imminent bubble collapse, and despite agreement by all economists and paramount leaders that housing is fundamentally not a growth industry – after all, housing is just another machine that produces something economically useful, but not more than that. A car produces travel. A house produces living space. A camera produces photos. A used machine is priced lower than a brand new one. And because of learning curve effects, the price of even a brand new machine ought to decrease over the years. And yet, the opposite has been true. The price of an apartment, even one that is being lived in, mostly goes up, faster than GDP.\n\n\n\nGrowth of the Chinese housing market, GDP, and disposable income. Source: NBER Digest, which is an overview of (Fang et al. 2016).\n\n\nIndeed, it is a grim comedy that over the years, often the government would say it wants to make apartments cheaper, and yet keeps allowing it to rise, and whenever there an actual fall occurs, immediately implements policies to make it rise again. What explains it? The short answer is that housing plays several roles that pull it in conflicting directions.\nFrom the simple industrial point of view, a house is just a machine for living in. As a machine, its price should go down as companies learn to build it faster and cheaper. Indeed, the Chinese construction companies have been producing so much concrete, steel, and housing, that they have utterly overproduced for the Chinese market. The supply has so far outstripped demand that Chinese construction seeks markets abroad, mainly in Africa and South America, where there is much greater demand for everything infrastructural – housing, electricity, subway, etc. So, by the basic laws of economics, the price of housing ought to fall, like the price of water, rice, solar panels, batteries, cars, and every machine good and proper in this world.\n\n\n\nYou may not like it, but this is what peak machines for living in look like.\n\n\n\nA house is a machine for living in. Baths, sun, hot-water, cold-water, warmth at will, conservation of food, hygiene, beauty in the sense of good proportion. An armchair is a machine for sitting in and so on.\nToward a New Architecture, Le Corbusier\n\nFrom the citizens’ point of view, since they have no other good investment tool than the housing market, its price should keep going up as China becomes richer, else people would be very upset and cause “social instability”. Indeed, brief falls in housing market reliably create social unrest, quickly followed by the government making new policies that show its commitment to unwavering rise in housing prices. Thus is social stability restored for another year. And this is despite the paramount leader of China repeatedly saying “Houses are for living, not for speculation.”, because when it comes down to it, social stability always triumphs over ideology.3\n3 Why have not people caught on to the contradiction? I have no idea. Perhaps, as the Chinese says, “Your butt determines your head.” (屁股决定脑袋), meaning that your sit-uation (where you sit) determines your thinking. It is hard to get a human to understand logic when their property value depends on them not understanding it. So maybe people, if they pause and think about it, would agree that, yeah, logically speaking, if the paramount leader says that “Houses are for living, not for speculation.”, then the government should not be propping up the housing prices. But logic should not be an obstacle for another decade of prosperity. After all, logic is sublated by dialectics, and “All that is actual is rational.” (存在的就是合理的), and “The words are hard and dead-fixed, but people are supple and living.” (规矩是死的，人是活的), and I can keep quoting all those fine flowers of classic Chinese wisdom that justify everything and explain nothing.For sure, some people are too poor to ever own a home, but enough of the populace either own a home, or have relatives who own homes, that the point of view of those proletarians – who would benefit from a precipitous decline of housing prices to the minimal level of production-cost – is simply not relevant.\nFrom the local governments’ point of view, housing is how they are funded. The 1994 tax reform means most tax revenue is taken by the central government. They were not even allowed to issue bonds until a 2015 reform. The most valuable asset they have is the monopoly on land use rights. Inevitably they want housing prices to keep going up.\nFrom the real estate developers’ point of view, they have long come to expect housing prices to keep rising, and plan their finances accordingly. Specifically, they borrow a lot from banks in order to fund their next construction project, which they expect to sell out quickly at high prices, which can then fund the next one, and so on. If the rising prices falters, it can threaten to break the cash flow, even cause bankruptcy.\nFrom the banks’ point of view, their primary objective is to support the operations of the government by providing liquidity where it is needed, so they cannot complain. They do not want to second-guess the wisdom of the loans, or give a low credit rating where the government expects a high credit rating. It is mostly alright, anyway, because banks know that the government really does not want banks to fail. Indeed, the only major case of bank failure in China has been the collapse of the Hainan Development Bank in 1998, a belated casualty of the Hainan housing bubble during 1992–1993.\nFrom the bureaucrats’ point of view, rising housing prices is the most assured path towards career advancement. The center rewards those who increase the GDP and keep stability. If housing prices drop, it would hurt citizens’ investments and create instability, even protests. Conversely, to increase the GDP, the most assured path is to continue what has been working so far: build infrastructure and sell housing. Rising housing prices would then come naturally, since better infrastructure increases the value of land.\nThe Chinese bureaucrats climb the career ladder – of which there are 27 steps – according to a kind of political scoring system. The low level bureaucrats are rewarded for efficiency, obedience, and general competence. At high levels, especially the city and provincial levels, the bureaucrats are rewarded mainly for GDP growth and social stability.\nFor developing GDP, the bureaucrats could pick between a sure gain of GDP in doing what has worked reliably, or risk something new. But risking something new is … risky. Most scientific research projects fail to reach production. A new technology might be a deadend like miniature vacuum tubes. And one big failure can end the whole career. Faced with this choice, bureaucrats play it safe. They only risk something new (like the recent wave of datacenter buildup) if it’s what the higher-ups demand, so that if it fails, they can simply say “We did not take on risks unnecessarily, but only risks necessary for the great rejuvenation of the nation.”.\nThere is also a perverse incentive of “heads I win, tails you lose” involved, as they know that the central government would not allow local governments to go bankrupt, so they are free to raise more debt to fund more development, even inefficiently. They are even more safe from adverse consequences if debt is raised indirectly via LGFVs, since it is not legibly connected to their own persons. Another layer of insulation is that when inefficiencies finally bite, they might have already been promoted from the post, and they could go “after me may the flood come” and blame it on the fall-guy sitting where they used to sit in. Since China had much real use of infrastructure, despite all these perverse incentives, most investments had been efficient, but at the limit, this results in stark inefficiencies of empty airports or railway stations.\nDespite all these forces pushing up housing prices, somehow everybody expects this to end someday (though few want this to end soon). The dangers of rising prices are many:\n\nExpensive housing makes young people marry later. Chinese culture basically requires the man to buy an apartment before even marrying. Thus, expensive housing → later marriage → fewer children → weaker economy.\nYoung people borrowed a lot of money to buy housing, then do not have much money to buy other things. This weakens the economy. It has become particularly acute recently as the government attempts the dual circulation strategy, preparing for a world where growth must rely on domestic demand more than export.\nHousing has become too big to fail. This brings all the perverse incentives with it. The collapse of the Evergrande Group was a particular example, where a huge real estate developer reasonably bet on it being too big for the government to allow it to fail, took on too much debt, then failed.\nUsing housing as investment is strictly worse for how capital is allocated across the economy, because the stock market contains every kind of company, but housing is just housing.\n\nThe last point bears more explanation.\nIf the US stock market contains just stocks in one sector, like oil, then America would have the same problem. However, the US stock market contains every sector from oil to computer to the military, so they have a system that adapts as each new technological breakthrough sweeps across the economy like a wave. When a new breakthrough, such as electric cars, first arises, the rate of profit for the relevant industry would rise abruptly. But as more and more companies take it up, it becomes old news, and the profit rate will fall from the heights of Schumpeterian rent down to merely keeping up with inflation. They have discovered all the good ways to make the thing and there are only tiny incremental improvements. People who are okay with more risk and more reward will just sell these stocks to people who are risk-averse, and buy stocks in new companies in new growing sectors, like AI.\nIn this way, a diverse market allows efficient capital allocation across the economy, matching capital owned by risk-averse people with capital needed by boring industries, and capital owned by risk-taking people with capital needed by exciting growth industries.\nNow in China, the housing market is too dominant, and this distorts capital allocation, into pouring capital into building infrastructure, steel plants, machine plants, houses, etc, far more than what would be efficient. At the limit, this results in the absurd ghost cities. Making misallocation worse is the aforementioned moral hazard, where bureaucrats keep building infrastructure and housing even when knowing it is inefficient.\nUnlike the stock market situation, where there seems little motivation for the government to make it ever grow consistently, the government does want to solve the housing situation, and soon. There is however no consensus on how it might be solved, though generally agreed to be some combination of the following:\n\nIn the transition period, force the large real estate developers to reform their corporate structure, so that they would be less debt-funded. Developers are to be encouraged to downsize or go bankrupt, since the housing is already oversupplied, and with population ageing, will only get more oversupplied as time goes on.\nEnsure developer competition with methods like open land auctions and nationwide land title registry. This would probably bring down the housing price, however.\nCulturally, make it more acceptable for people to not own housing.\nConcurrent with the above cultural policy, many economic policies and regulations were written with the assumption that most people are living in housing they bought, and that people renting housing are either about to, or aspiring to, own their housing. Reform these, so that indefinite renting is a smooth experience for people.\nAccept on the macro policy level that people are going to keep pouring into the big cities and hollow out the countryside and small cities, and instead of trying to tell people to return to the quiet life and develop their hometowns, embrace it. Zone land for residential development in proportion to projected urban population. In particular, zone more land for the growing cities, and zone less for the shrinking cities. It is more legible, with less room for political shenanigans, to simply predict future population movement and then multiply by a proportionality number, compared to arguing about how many people should be in the coastal cities versus the countryside.\nLegalize and support Real Estate Investment Trusts. These trusts would play similar roles as the LGFVs, but legible and regulated. It would separate the local government from real estate development. The benefit would be that the government’s legitimacy would be unharmed if a trust goes bankrupt. The cost would be, once again, less control by the state and a dangerous rise of market autonomy.\nA threshold property tax. This provides the local governments a constant stream of tax revenue, and decreases the profit in speculation. The tax would be levied only on housing area above a threshold, commonly suggested to be ~140 m².\nTax reform, so that the central government takes less tax revenue from the peripheries. This goes against the power and authority of the center, however.\nLiberalize or even remove the Hukou system. The Hukou system was a no-tech method for making a billion people legible. The new information surveillance systems have obsoleted it. Removing it would allow people to flow more freely, allowing more efficient capital and labor allocation.\nLiberalize the stock market so that the housing market is no longer the only place where people can get rich. This goes against the basic logic of the Chinese economy, however.\n\nIt is a hard task with multiple contradicting constraints, but China seems confident in taking it on by experimenting multiple methods at multiple cities. China is big enough to run many experiments in parallel, and contain many contradictions indefinitely."
  },
  {
    "objectID": "essays/posts/structure-interpretation-chinese-economy/index.html#metadata",
    "href": "essays/posts/structure-interpretation-chinese-economy/index.html#metadata",
    "title": "Structure and Interpretation of the Chinese Economy",
    "section": "Metadata",
    "text": "Metadata\n\nThis essay was initialized by sticking together some mini-lectures I have given to friends on Discord during 2023–2024. The recent Dwarkesh Patel podcast with Victor Shih finally gave me the motivation to polish them up to post on the open Internet.\nI originally intended this to be a quick blog post, so it is distinctly under-researched, and large chunks are just pulled out of my brain that I am not bothered enough to find citations for. As I kept trying to polish it, it grew too long to be a blog post, so it is now a proper essay, but it is still uneven, kind of repetitive, and lacks citations to really satisfy my wish for completeness. Still, an essay it is.\nComment thread"
  },
  {
    "objectID": "logs/index.html",
    "href": "logs/index.html",
    "title": "Logs",
    "section": "",
    "text": "Dream: Ingress Answer\n\n\n\nfun\n\nart\n\nslice-of-other-life\n\n\n\nDream about a new feature in Chrome which helped me understand LLMs a bit better.\n\n\n\n\n\n2025-06-29\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\nReproFro 2025 party\n\n\n\nslice-of-life\n\nAI\n\nbiology\n\npolitics\n\n\n\nRandom recollections from the hectic conversations at the afterparty of the Reproductive Frontiers Summit of 2025.\n\n\n\n\n\n2025-06-14\n\n40 min\n\n\n\n\n\n\n\n\n\n\n\nDwarkesh Patel dinner party\n\n\n\nslice-of-life\n\nAI\n\neconomics\n\n\n\nRandom recollections from the hectic conversations at a dinner party organized by Dwarkesh Patel.\n\n\n\n\n\n2025-05-30\n\n22 min\n\n\n\n\n\n\n\n\n\n\n\nThat Survivorship Bias Plane\n\n\n\nhistory\n\nwar\n\n\n\nThe exact backstory to that picture of an airplane with red dots on top of it.\n\n\n\n\n\n2025-05-02\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic time in finance\n\n\n\nfinance\n\nphysics\n\n\n\nApplication of a simple concept from physics to personal finance.\n\n\n\n\n\n2025-05-01\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\nThe cost of O’Neil habitation\n\n\n\nfun\n\nscifi\n\neconomics\n\n\n\nTo win (?) a twitter argument, we calculated the energetic cost of O’Neil habitat per person. Oddly, it is just affordable by modern standards, ~0.72 million USD per person. Let it be known that the SF housing crisis is cheaper to solve by building O’Neil cylinders.\n\n\n\n\n\n2025-04-18\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nWhalefall\n\n\n\nAI\n\neconomics\n\n\n\nThoughts on the recent ‘DeepSeek shock’ of the stock market. The market is being even less efficient than usual, and much of the ‘professional’ commentators are idiots or bullshitters. I wrote this post to prevent my Gell-Mann amnesia.\n\n\n\n\n\n2025-01-29\n\n26 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "logs/posts/2025-dream-ingress-answer/index.html",
    "href": "logs/posts/2025-dream-ingress-answer/index.html",
    "title": "Dream: Ingress Answer",
    "section": "",
    "text": "in the Chrome\n\nright click brings up a new feature panel (something like “Ingress Answer”). Right below it, there was another panel saying something like “Experimental: Doc here”.\n\nit opens the Chrome console with a long markdown document within it.\n\nIt was too long to read in one go. If I tried, I would lose concentration and forget about it,\n\nso I copied it into Logseq to read it via text-manipulation. But it got the error of “This textblock is too long, read-only mode is enabled”.\n\nAnd then\n\nSo I realized I had to act quickly. There is another world, I knew. I just had to make sure it receives the message. So I opened Gmail and sent the entire textblock to my other email address.\n\nThen I woke up and realized this is what LLMs do all the time.\n\nI have been trying to think more like LLMs, to understand them better. I feel like this dream explains a lot of what the “hallucination” is supposed to be. I knew there is another world, and that if I just save the text in email inbox, the other world would not see it, but somehow I thought that if I send it to another email inbox, the other world would receive it. Is this how LLMs use their tools?\nWhen o3 owes you a straight answer, when R1 does a proof by (っ◔◡◔)っ 🎀 imagination 🎀, it is not lying. Truth and lies and even bullshit does not work when dealing with these textual structures. There is no outside-text. This is not a call for postmodernists, too vague to make falsifiable predictions, and too floppy to make text automatons with gears that really mesh and turn.\n\nBut this is still a serious problem. Where is the new On Truth and Lies in a Nonmoral Sense?\nAlso, to experience it yourself, try the quick Chrome extension o3 wrote up just for this occasion."
  },
  {
    "objectID": "logs/posts/2025-patel-dinner/index.html",
    "href": "logs/posts/2025-patel-dinner/index.html",
    "title": "Dwarkesh Patel dinner party",
    "section": "",
    "text": "Put on my best dress with Demeter petrichor, blue spruce, and thunderstorm. To San Francisco.\n(Gwern later stated that he was using Acqua di Sale Eau de Parfum by Profumum)\nGwern appeared. He was a human. I was a bit expecting something like:\nor maybe (best viewed in dark mode):\n“… This administration probably doesn’t even know what weights are.”\n“–But it does know a lot about BIASES!”\nRoom laughed.\n“My professor of numerical linear algebra said they had to rewrite some grant proposals to remove mentions of ‘bias/variance tradeoff’ because it’s an instant kill now.”\n“Next week I’m flying to a conference to speak about AI and I was like ‘But I’m not an expert, why not invite this or that person?’. They said that they wanted speakers on AI scaling, and they already got someone, Alex Honnold, and he was the guy who was scaling El Capitan rock face–” – room erupts into laughter – “So I suddenly feel a lot better about my expertise!”\n“Well you know, AI scaling startup companies is a lot like scaling the El Capitan right? One misstep and you miss funding and fall to your capitalistic death.”\n“Riiiiight.”\n“You know, that guy’s girlfriend is like ‘Can you stop risking your life all the time? I’m trying to start a family with you.’ and that guy’s like ‘Nope. I must scale. Nothing else matters.’.”\n“Mmm it sounds pretty sad.”\n“But I think it’s reasonable. If you can do one thing extremely well, you must do the thing. Nothing else matters. Things like marriage must not stop you. It’s like your [points at Gwern] review of that movie, Jiro dreams of sushi.” Or maybe about Sake?\n“Working on LLM applications has lengthened my [AGI] timeline.”\nRoom laughed.\n“I realized that even for simple things, it’s hard to do reliably. I wanted for example simply cleaning up transcriptions, or writing tweet threads, but it’s really hard to get them do it just right the way I want. It works pretty well in the chat interface, but somehow it is much harder through the API. I tried stuffing more stuff into the system prompt, but… the lack of continual learning really bites in this case.”\n“Oh yeah. Why do you think they are not doing continual learning, these big companies? Is it because…”\n“Well I don’t know, but I have written a draft essay, about the economics of continual learning if it is practiced right now. I wanted to answer the question of… Let’s say, if OpenAI announces today, that ChatGPT can continually learn about you. How much money do you have to pay for it? So I calculated and the answer came out to be … [vigorous handwaving] between 20 and 100 extra USD/month, which is surprisingly … affordable?”\n“Well even naively, the forward pass costs 2 FLOP per token-parameter, and the backward pass costs 4 FLOP, so multiply 20 USD/month by 3…”\n“And you would want to only finetune the parameters in the later half of the layers, so you don’t need 4 FLOPs, just 2 FLOPs, so multiply by 2…”\n“Actually, there is a way to do it even ‘for free’, as in there is no extra cost compared to a standard forward pass. Basically you randomly permute one matrix sometimes, and the user votes. If the random permutation gets higher votes, then put the random permutation in place. It’s like A/B testing.”\n“But how… how would that be better than LoRA backprop? Sounds like the sample complexity would be really high…”\n“Sounds like evolutionary algorithm, really. Those are very sample-heavy.”\nThe proposer then described something about how this could be done more efficiently, but I didn’t understand it. Now I think about it, it, it is more sample-heavy than RL. Consider the toy example: You have \\(\\theta\\) the neural network parameters, and \\(L(\\theta)\\) the loss function. You want to optimize \\(L(\\theta)\\) by randomly jumping \\(\\theta\\) by some \\(\\delta \\theta\\). Then, since \\(L(\\theta + \\delta \\theta) - L(\\theta) \\approx \\braket{\\nabla_\\theta L , \\delta \\theta}\\), the probability that this improves \\(L\\) is \\(\\sim 1/2\\), assuming that \\(\\delta\\theta\\) is small. However, how big is this improvement? Counterintuitively, the probability that a random sample on a \\(d\\)-dimensional sphere is concentrated in a band of \\(\\pm O(1/\\sqrt d)\\) around its equator. Therefore, the expected improvement of taking this \\(\\delta \\theta\\) step is just \\(O(1/\\sqrt d)\\).\nIs this worthwhile? A single “jitter” comparison requires 2 forward passes, costing 4 FLOP, while a single backprop costs 6 FLOP. This means that if you just want to improve the model, backprop is much more efficient, since \\(4\\sqrt d \\gg 6\\). Although, as the speaker suggested, this could be mildly useful in the particular case where you are a big LLM provider who needs to serve many customers really fast, but the customers are also willing to make a lot of votes, and don’t mind slight service degradation on half of the replies. I would say this is a very niche situation, but yeah, in that very niche situation, jittering would be better than backprop.\n“Well, I just published an essay about hacking pinball machines, and of course, it is still about AI. Reward hacking. You could hack the leaderboard, or pay someone to say that they totally saw you get a high score. Etc.”\n“I mean, the speedrunning community has such high epistemic standards. So there was this speedrun, and someone actually exhaustively enumerated every single possible button press to formally prove that the speedrun record was impossible.”1\n“So there was also the thing that those old NES machines, their CPU clocks get faster over the years. So now they have to put the NES in the fridge for a while to restore them to their previous clockrate! And the machine is only good for a few hours of speedrun afterwards.”2\n“[Looks at Gwern] So what would your next essay be about?”\n“The next essay should be about AI timelines. I just want to find out, basically, why now? Why not decades ago? There was Moravec’s scaling curve, but it was totally unconvincing. What could have been done so that decades ago, we would have an idea that, now is the time? Why did deep learning take everyone by surprise? Except a few [Moravec, Kurzweil] who didn’t have good arguments? What better arguments could have been made?”\n“I think the best arguments that could have been made was to have scaling curves. They could scale from like 1 million FLOPs to 1 billion FLOPs, and extrapolate from there. It could have been more convincing. There were some scaling curves all the way back to the 1990s, like if you really try to look, but if it is invested with the power of a national laboratory it might be done even earlier.”\n“So I have been reading old papers from the 1990s, and there was one paper called ‘Neural Networks and the Bias/Variance Dilemma’ from 1992 (Geman, Bienenstock, and Doursat 1992). They trained a neural network and showed that it has that bias-variance tradeoff [handwaves a V-shaped curve in air] so they argued that scaling wouldn’t work. Then I looked at what neural network they trained. They trained a network with 3 layers, and the hidden layer contained… [pause for dramatic effect] 4 neurons. FOUR! Certainly, there was a severe failure of imagination there. I mean, the paper wasn’t obscure! It was cited over 5000 times, and I think this contributed to the second neural network winter, a history of which I still want to write… before the singularity comes. Better hurry.”\n“Was it because of not enough data?”\n“There was this Highleyman’s data. Highleyman was really great. He was in 1960 doing many of the basic things we do now: big dataset; the train-test split; sharing data; k nearest neighbors. You can read more on it. Ben Recht wrote a long blog post about it. Ben R-E-C-H-T.”\n“The blog post is titled ‘The Saga of Highleyman’s Data’.”\n“And I have kept a directory of those papers on my website.”\n“Well I had been looking at old papers in the 1960s. It was the Cold War, and there was not only the Soviet Union, but also China, so there was a lot of funding into machine reading and translation of Chinese documents… So I saw a few datasets where they just scanned thousands of printed Chinese characters, so they certainly could have scaled to, perhaps 100,000 data points, if they had the will to do so. (Casey and Nagy 1966) MNIST had just 60,000 characters. They could have MNIST in the 1960s. They just didn’t think of it.”\n“I mean, why can’t deep learning be more like physics? Physicists got their billion-dollar colliders funded purely on theoretical extrapolations. And there was this ‘island of stability’, which is the idea that, eventually, the superheavy elements would suddenly become stable again. And nobody call that crazy, even though we still don’t see it yet.”\n“Actually the island of stability theory is not convincing to most of the physics community.”\n“What? But I believed that!”\n“Like it is only a few groups that believe in it.”\n“Maybe it’s those Russians. They seem to love getting those nanosecond new elements that are totally useless, purely to say that they did it first.”\n“I feel like we really need LLM psychologists. Like I noticed that Claude Opus 4 is really… woo.”\n“–And that’s why I hate Claude so much!”\nRoom laughed.\n“So right now I’m 20% through reading Anna Karenina, and I don’t get why Anna is suddenly infatuated after one dance and one weekend…”\n“Well, sometimes love or lust is like that. It’s certainly not what the author intends to be an ideal of love. There is another couple (Levin and Kitty) that is an ideal. One of them (Levin) is the author’s self-insert. But actually you should not focus too much on the romance because there are so many subplots that are interesting…”\n“You can read Madame Bovary. It’s a French novel. The romance plot is basically the same as Anna Karenina. And better, there are no subplots because it’s French.”\n“It’s like the distilled version of Anna. Just the romance.”\n“What if it is a framing device?”\n“Hmh?”\n“Like, maybe Tolstoy really didn’t care for romance, but really about everything else. History, society, philosophy, religion. But he knew he had to put one romance subplot about the main character, else the reviewers would go like ‘Ugh, no romance? 0/10.’. So the author puts in it to satiate the reviewers. Like, I often find myself opening a novel and felt like, ‘Ugh, another romance plot?’ and throws it [right hand flung backward over the shoulder] into the trash! So I would say that the compulsion to put useless romance plots into novels is a long tradition.”\n“Actually…” What followed was a very long interpretation of Anna Karenina, which I completely forgot about.\n“You know, the interpretation of Anna Karenina is a neglected field among technologists. This could be your niche. You know, ‘3 years left to build generational wealth’?”\n“I’ve been trying to calculate the energetic efficiency of AI vs humans. Humans run on 20 W but H200 runs on like 400 W. And they still don’t do nearly as well as humans in like playing games and long-horizon tasks…”\n“I think the watt-for-intelligence idea is kind of flawed because there is also the question of intelligence ceiling, right? I would be totally okay to have another Einstein, even if that Einstein needs a nuclear reactor to power.”\n“So the reason I’m calculating it is because if energy efficiency of AI becomes much higher than humans, then the economy will move towards that. Like, nice farmland you’ve got there, it could be better used by covering it with solar panels.”\n“I guess that’s one possible way for humans to survive after AGI. To have such good property laws that we live on retirement benefits forever.”\n“That’s Robin Hanson’s hope. His entire hope is stacked on top of that.”\n“Only on the very medium-term. For a few years, before the em economy shifts again.”\n“In the UK there was the ‘Triple Lock’3 and it is bankrupting the country. And the retirees have such an iron grip on the politics that it just couldn’t be removed. So the latest attempt is something like, ‘maybe… 2.1-ple Lock?’.”\n“Do we want to be like the retirees? Like why should I pay more to some rich retiree in America when the money is better spent in Africa?”\n“[in a moralistic narrator voice] ‘Donate now so that an AI in Africa can be run again.’.”\n“‘Turn off your lamp now so that a poor AI can stay conscious for one more second.’.”\n“They should go to Australia. The solar is much better. There’s that giant empty quarter of desert.”\n“Everyone will go to Australia someday. That’s the plot of (the 3th book of) the Three Body Problem.”\n“What is ‘at-will employment’?”\n“It means you can fire employees whenever you want without giving advance notice, or reasonable cause. And employees can quit in the same way.”\n“I thought it means ‘You can do whatever you want.’.”\n“Yeah sure, but ‘There will be consequences.’.”\n“It’s strange that America is unique in this. Even very capitalistic countries like Singapore has a 2-week advance notice. And France… you need to have a valid reason to fire people. And you need a valid reason to quit your job.”\n“In Japan it’s not symmetric. Firing people is very hard, but quitting a job is not.”\n“It’s called employee protection.”\n“Well I haven’t left the Bay Area for 4 years, because the China–US relationship has been going worse every year. So I worry about visa.”\n“But you could still visit the rest of America?”\n“Well, I could, but … nothing interesting happens [gestures in a great circle] out there?”\n“Oh that’s not true. I was in New York in September and it was the most beautiful place to visit.”\n“So the fridge at [big company] had those yogurts that contains 200 mg of caffeine in it. One day someone came and was like ‘I haven’t eaten lunch so I’ll eat 2 of these yogurts’.”\n“Oh no–” “RIP–”\n“There was a woman who could not feel fear, but she felt it when she inhaled pure carbon dioxide. The reason is that there are sensitive chemoreceptors in the brainstem that could detect slight rises in carbon dioxide levels, and those weren’t damaged in her. At a conference/party there was a stand where they had balloons filled with carbon dioxide. Rarely could people inhale the whole balloon. I tried and it was hard, but I did it. It makes the throat feel like burning, maybe because of the carbon dioxide reacting with water to form carbonic acid. It created a sharp sense of fear and panic.”\nNote: This probably referred to (Feinstein et al. 2013), reporting that inhalation of 35% CO₂ evoked immediate fear and panic attacks in 3 patients with amygdala lesions (due to Urbach–Wiethe disease), patients who otherwise failed to show any fear response. Patients returned to baseline after about 20 seconds.\nPatel looked up the listener statistics for his podcasts on the phone. Turned out that some episodes have ~2% female viewers.\n“Weird, that’s lower than the Lizardman’s constant…? Are you sure the data source is right?”\n“Huh, even the Sarah Paine episodes have so few female viewers.”\n“I mean, military history is very male-coded.”\n“Well you can just interview an MMA wrestler.”\n“Actually the price of a solar panel is dropping like crazy. People like to tell me that ‘You can’t be serious. Just because it has decreased exponentially for the past 30 years doesn’t mean it will continue for 2 more years–’” Room laughed. “–Well when I put it like that… But yeah. I really would like to know just how many little innovations has gone into each cost reduction…”\n“Probably most of it is stuck inside the brains of some Chinese engineers.”\n“What is the cheapest you can get those things? [points at a glass door] This perfect piece of glass shouldn’t be that much more expensive than solar chips…”\n“Well, I don’t know about the solar panels, but I do know that those silicon wafers for chips [gestures a big circle in front of the chest] costs 100 USD before they enter the factory, and after they are etched full of chips, worth 100,000 USD, so that’s an upper bound.”\n“… so you could imagine that, since a solar panel is silicon, and a GPU is also silicon, you can produce both together, with a big solar panel and, in the middle of it, is a GPU. Then you could load a human brain emulation on the GPU and send it into orbit, and –”\n“Heat dissipation is really hard in space.”\n“And they will die probably in a few years.”\n“But for those few years they would experience centuries or more of subjective time. Human brains are just so slow compared to the GPUs.”\n“I would disagree with that. I mean, what is a reasonable comparison standard between LLMs and humans? Sure, a very dumb LLM can totally be faster than you at babbling nonsense. We have to compare things that are actually performing the same tasks.”\n“Legend says that when Watt had to figure out how to sell his engines, he had to sell it to people who only knew of horses, so he had to find what a Horsepower is, and one customer picked the strongest horse he had. So what if we measure LLMs according to units of Gwernpower–”\n“Even though they don’t yet do quality writing at human levels, at least LLMs can already read at superhuman levels.”\n“It’s interesting that the biggest models like Claude or Gemini can only output like 10 token/sec. Like that is actually close to human levels.”\n“The fastest possible is Cerebras, but that’s so specialized–”\n“They have monstrous amounts of SRAM and memory bandwidth.”\n“Well, at the limit, you could imagine etching the logic directly into the hardware so it can only do one thing but as fast as possible. That’s in fact the whole selling point of Etched the company. They basically compile your neural network into an ASIC. That’s actually why I’m so interested in differentiable logic gates.”\nA brief confusion ensued about whether the intended use of Etched is large-batch or small-batch inference.\n“I think it’s small-batch inference, because you know, you can only saturate the FLOPs of a GPU if every floating point number that goes in is used for ~200 operations. The intended arithmetic intensity is like that.”\n“This is because the FLOP count of a GPU is like 1000 TFLOP/sec, but the memory bandwidth is like 5 TB/sec.”\n“And so if you have that problem of bandwidth then you really don’t want to use a small batch. Every time your parameters go into the GPU you should use it a lot, thus big batches. With Etched there is no parameter movement so you can afford something like 2 operations per floating point number.”\n“Huh, so I was confused about it then? But I did remember them marketing it as ‘thousands of customer queries going in in parallel’…”\n“Yeah I remember that from their talks too.”\n“So this morning I got a good Socratic prompt and I got them to first understand a topic deeply, then teach me by Socratic questioning technique. And it worked! I was talking with it about an algorithm I thought I knew but a few questions in I realized I got a basic misunderstanding.”\n“Oh yeah? What did they teach you?”\n“Well so it was about multi-latent attention, and–”\n“Mmh right that definitely can be a bit easy to misunderstand, especially if you don’t already have some understanding of the variants of attention mechanisms… Your preconceptions of what attention is, if you think it is all just multiheaded attention, could make you misunderstand.”\n“So Claude told me that in MLA, instead of one vector per key and value, there is just one big vector for the entire context…”\n“Wait no.” “No that’s not it.” “No uh.” Disappointed headshaking. The room fell dirgeful.\n“What! Claude lied to me. I was like ‘Wait, so that means MLA is not quadratic?’ and Claude was like ‘Yeah totally that’s how you get subquadratic attention, and that’s what MLA is for.’ So it’s not really subquadratic?”\n“No, MLA still gives you one vector per KV-token. The main surprise is not the fact that it uses one vector for the entire KV context – it’s not. The main difficulty is just that the position embedding is inserted somewhere … [vigorous handwaving] … in the middle of it. I still don’t understand it.”\n“Yeah the position embedding is the weirdest part of it, for sure.”\n“The main selling point of this is that it saves a lot of memory. For example, the DeepSeek-R1 has… [hand held high, indicating retrieval augmentation in progress] just 70 KB per key-value token. In contrast, GPT-3 [retrieving…] has 5 MB per key-value token. So you could just stuff like, 1 million tokens of context into 70 GB. That’s the main selling point of MLA.”\n“The Socratic method in ruins. LLMs will only get more convincing from now on.”"
  },
  {
    "objectID": "logs/posts/2025-patel-dinner/index.html#metadata",
    "href": "logs/posts/2025-patel-dinner/index.html#metadata",
    "title": "Dwarkesh Patel dinner party",
    "section": "Metadata",
    "text": "Metadata\n\nComment threads\n\nPatel’s twitter thread.\nAhsouka’s twitter thread"
  },
  {
    "objectID": "logs/posts/2025-survivorship-bias/index.html",
    "href": "logs/posts/2025-survivorship-bias/index.html",
    "title": "That Survivorship Bias Plane",
    "section": "",
    "text": "Cameron Moll created the diagram around 2005. He started giving presentations showing it, such as this presentation “Essential Web Skills” given at the Webmaster Jam Session of Dallas, Texas, on 2006-09-22, at slide 18. It was given as “Lesson 3: Good designers fix problems. Great designers prevent them.”:\n\n\n\nPossibly the first occurrence of that image. Slide 18 of Moll’s 2006 presentation. Source\n\n\n\n\nAs the creator of the original Wald diagram in 2005 that inspired the duplicates that have followed, absolutely yes. (Clearly the aircraft I chose at the time was not historically accurate.) pic.twitter.com/bVv3hyyKOn\n\n— Cameron Moll (@cameronmoll) October 18, 2020\n\n\n\nSometime in the early 2000s I stumbled on the story of Abraham Wald who plotted bullet holes on aircraft returning from battles in WWII. … At the time I was actively speaking at web conferences in the US & Europe on the topic of problem solving among other things, and Wald’s story was a terrific demonstration of solving (and defining) the right problem. I wasn’t aware of anyone who had visualized this, so sometime around 2005 I hastily plotted fictitious red dots on a poorly-chosen commercial aircraft outline and began including this in slide decks and blog posts.\nCameron Moll\n\nMoll stated that he just grabbed a random commercial aircraft and added the red marks at random to illustrate the concept, without historical accuracy. He didn’t say which plane it is, but I think it is a Douglas C-47 Skytrain (identified by o3, “lgtm” by me), which is actually a military transport airplane.\n\n\n\nLinedrawing of a Douglas C-47 Skytrain. Source: Wikimedia Commons\n\n\nOn 2016-11-12, McGeddon added Survivorship-bias.png to Wikimedia and then put it in the Wikipedia page on “Survivorship Bias”. This time, the lucky aircraft was Lockheed PV-1 Ventura. McGeddon updated the diagram to a SVG in 2021. This seems to be the version most commonly posted around.\n\n\n\nThe SVG version made in 2021. Source: Wikimedia Commons\n\n\nIt is interesting to note that in all cases, some of the bullet marks are historically anti-accurate. Specifically, the places identified as most vulnerable by the Luftwaffe – the oil tanks between the inner two engines – are filled with bullet marks."
  },
  {
    "objectID": "logs/posts/2025-survivorship-bias/index.html#recent-history",
    "href": "logs/posts/2025-survivorship-bias/index.html#recent-history",
    "title": "That Survivorship Bias Plane",
    "section": "",
    "text": "Cameron Moll created the diagram around 2005. He started giving presentations showing it, such as this presentation “Essential Web Skills” given at the Webmaster Jam Session of Dallas, Texas, on 2006-09-22, at slide 18. It was given as “Lesson 3: Good designers fix problems. Great designers prevent them.”:\n\n\n\nPossibly the first occurrence of that image. Slide 18 of Moll’s 2006 presentation. Source\n\n\n\n\nAs the creator of the original Wald diagram in 2005 that inspired the duplicates that have followed, absolutely yes. (Clearly the aircraft I chose at the time was not historically accurate.) pic.twitter.com/bVv3hyyKOn\n\n— Cameron Moll (@cameronmoll) October 18, 2020\n\n\n\nSometime in the early 2000s I stumbled on the story of Abraham Wald who plotted bullet holes on aircraft returning from battles in WWII. … At the time I was actively speaking at web conferences in the US & Europe on the topic of problem solving among other things, and Wald’s story was a terrific demonstration of solving (and defining) the right problem. I wasn’t aware of anyone who had visualized this, so sometime around 2005 I hastily plotted fictitious red dots on a poorly-chosen commercial aircraft outline and began including this in slide decks and blog posts.\nCameron Moll\n\nMoll stated that he just grabbed a random commercial aircraft and added the red marks at random to illustrate the concept, without historical accuracy. He didn’t say which plane it is, but I think it is a Douglas C-47 Skytrain (identified by o3, “lgtm” by me), which is actually a military transport airplane.\n\n\n\nLinedrawing of a Douglas C-47 Skytrain. Source: Wikimedia Commons\n\n\nOn 2016-11-12, McGeddon added Survivorship-bias.png to Wikimedia and then put it in the Wikipedia page on “Survivorship Bias”. This time, the lucky aircraft was Lockheed PV-1 Ventura. McGeddon updated the diagram to a SVG in 2021. This seems to be the version most commonly posted around.\n\n\n\nThe SVG version made in 2021. Source: Wikimedia Commons\n\n\nIt is interesting to note that in all cases, some of the bullet marks are historically anti-accurate. Specifically, the places identified as most vulnerable by the Luftwaffe – the oil tanks between the inner two engines – are filled with bullet marks."
  },
  {
    "objectID": "logs/posts/2025-survivorship-bias/index.html#what-did-the-germans-know",
    "href": "logs/posts/2025-survivorship-bias/index.html#what-did-the-germans-know",
    "title": "That Survivorship Bias Plane",
    "section": "What did the Germans know?",
    "text": "What did the Germans know?\n\nDie Kriegsflugzeuge der Feindmächte\nOn 1942-09-01, the Luftwaffe published Frontnachrichtenblatt der Luftwaffe, Sonderausgabe: Die Kriegsflugzeuge der Feindmächte, meaning “Front News Sheet of the Luftwaffe, Special Edition: The Warplanes of the Enemy Powers”. The pdf of the handbook is available online.\n\n\n\nCover of Die Kriegsflugzeuge der Feindmächte.\n\n\nThe cover states:\n\nDer Oberbefehlshaber der Luftwaffe\nFührungsstab IC\nNicht zum Feindflug mitnehmen!\nThe Commander-in-Chief of the Luftwaffe\nGeneral Staff, Section IC\nDo not take them on enemy missions!\n\nPage 2 further specifies that it was a “special issue” that is intended for the classroom.\n\nVorliegendes Sonderheft zum Aushang bringen und zum Unterricht benutzen!\nPlease display this special issue and use it in class!\n\nIt was updated on 1943-11-01, still titled Die Kriegsflugzeuge der Feindmächte. That version is also available online.\n\n\nHow to read the book\nThe most important part is on page 2 of the book, which gives the following legend:\n\n\n\nLegend for the diagrams in the book. For translation, see below.\n\n\n\nProtected fuel tanks\nUnprotected fuel tanks\nArmor\nMachine gun\nMachine gun in Bodenlafette\nCannon\n\nThe most important areas are the “unprotected fuel tanks”. Apparently those were not self-sealing, and prone to exploding when shot at.\n\n\n\n\n\n\nFull text\n\n\n\n\n\nAnordnung der Bewaffnung, der Kraftstoffbehälter und der Panzerung bei den wichtigsten Kriegsflugzeugen der Feindmächte\nIn den Zeichnungen sind die Bewaffnung, die Kraftstoffbehälter und die Panzerung nach nachstehendem Schema eingezeichnet:\n\nKraftstoffbehälter geschützt\nKraftstoffbehälter ungeschützt\nPanzerung\nMG\nMG. in Bola\nKanone\n\nDie Skizzen sind ohne bestimmten Maßstab. Die Eintragungen wurden auf Grund der zur Zeit vorhandenen Unterlagen – soweit möglich unter Auswertung der Beuteflugzeuge – durchgeführt.\nEs ist anzunehmen, daß alle Flugzeuge mindestens mit einem Rückenpanzer für den Flugzeugführer behelfsmäßig ausgestattet sind. Bei den Flugzeugmustern, bei denen an Hand von Beuteflugzeugen eine Panzerung festgestellt wurde, ist diese in den Skizzen eingezeichnet.\nBei den neusten Flugzeugen finden sich nunmehr auch geschützte Kraftstoffbehälter.\nBei den Angaben über die Bewaffnung ist zu berücksichtigen, daß ein und dasselbe Flugzeugmuster verschiedene Bewaffnung aufweisen kann, z. B. Doppel-MG. statt Einfach-MG., Kanonen statt starre MG. usw. Alle bisher bekanntgewordenen Bewaffnungsarten sind bei den jeweiligen Mustern in der Beschreibung auf der Skizze vermerkt.\nDie Skizzen von Feindflugzeugen mit Eintragung der Bewaffnung, der Kraftstoffbehälter und der Panzerung werden entsprechend eingehender neuer Unterlagen laufend berichtigt und für neu eingesetzte Flugzeugmuster laufend ergänzt werden!\n\n\n\n\n\n\n\n\n\nTranslation\n\n\n\n\n\nArrangement of armament, fuel tanks, and armor on the enemy powers’ most important warplanes\nThe drawings show the armament, fuel tanks, and armor according to the following scheme:\n\nProtected fuel tanks\nUnprotected fuel tanks\nArmor\nMachine gun\nMachine gun in Bodenlafette\nCannon\n\nThe sketches are not to a specific scale. The entries were made based on currently available documentation – where possible, by evaluating captured aircraft.\nIt can be assumed that all aircraft are equipped with at least a makeshift dorsal armor for the pilot. For aircraft models where armor was identified based on captured aircraft, this is shown in the sketches.\nThe newest aircraft now also have protected fuel tanks.\nWhen specifying armament, it should be noted that the same aircraft model can have different armaments, e.g., twin machine guns instead of single machine guns, cannons instead of fixed machine guns. etc. All armament types known to date are noted in the description on the sketches for the respective models.\nThe sketches of enemy aircraft, including the armament, fuel tanks, and armor, are continually being corrected as new documentation becomes available and are continually being supplemented for newly deployed aircraft types.\n\n\n\n\n\nLockheed Ventura\nSadly, the textbook didn’t show us the weak points of the Lockheed Ventura are supposed to be, but they did show those for Lockheed Hudson, and since they both share the same basic Lodestar-derived wing and engine-nacelle architecture, I think their internal placement of oil tanks within the nacelles should be basically identical. The textbook has this to say about the Ventura:\n\nDer Rumpf des Musters Lockheed „Ventura“ ist im Unterschied vom Muster Lockheed „Hudson“ hinter dem Turm um rund 1 m länger, so daß sich der MG.-Turm mit Kuppel aus durchsichtigem Werkstoff (s. oberes Bild!) entsprechend weiter vom Leitwerk entfernt befindet als beim Muster Lockheed „Hudson“. Ferner weist das Muster Lockheed “Ventura” im Unterschied zum Muster Lockheed „Hudson“ eine Stufe für das Boden-MG. an der Rumpfunterseite auf.\nIn contrast to the Lockheed “Hudson” type, the fuselage of the Lockheed “Ventura” is about 1 m longer aft of the turret, so that the machine-gun turret with a transparent-material dome (see upper illustration!) sits correspondingly farther from the tail unit than on the Hudson. Moreover, unlike the Hudson, the Ventura features a stepped fairing on the underside of the fuselage for the ventral machine gun.\n\n\n\n\nLockheed Ventura. Page 35.\n\n\n\n\n\nLockheed Hudson. Page 36a. The 4 armored oil tanks, and an auxiliary one in the middle, are marked out.\n\n\n\n\nBoeing B-17 and B-24\nBut what we really want to know are of course the B-17 and the B-24. What did the Germans know about them?\n\n\n\nB-17 C, D. Page 37a. The 6 armored oil tanks, and the 2 auxiliary oil tanks, are marked out. The auxiliary oil tanks are described as “Zusatzbehälter an Stelle Bomben” [“Additional containers instead of bombs”], meaning that those places are where the bomber can either carry bombs or add-on oil tanks.\n\n\n\n\n\nB-17 E. Page 38a. The 6 armored oil tanks, and the 2 auxiliary oil tanks, are marked out.\n\n\n\n\n\nB-24. Page 39a. Annoyingly, they did not mark any fuel tanks.\n\n\nIn the 1943 update, the B-17 diagram has more details:\n\n\n\nB-17 E, F. Page 20a.\n\n\nAnd the B-24 diagram finally has the fuel tanks marked out:\n\n\n\nB-24. Page 21a.\n\n\n\n\nTwo posters\nI found two posters published in 1943. Presumably they were hung on Luftwaffe classrooms. They showed in detail what the Luftwaffe knew about B-24 and B-17 as of 1943.\n\nB-24\nA detailed teaching poster for B-24 was salvaged from England in 1943 by Frank Ashby, and presented to Reed Cheek in 1976. It is a photographic reproduction in 1943-10, by the Reproduction Platoon of 901st Engineer Air Force Headquarters Company, which was attatched to the 8th Air Force of the USA, which was established in 1942 specifically to fight the air war in Europe.\nIt was printed in pp. 10–11 of Journal of the Second Air Division Association, Newsletter, Vol. 18, No. 2, 1979-06. I tried my best to stitch the two pages together without seams in the middle.\n\n\n\nThe B-24 poster.\n\n\nFrame 4 “Lage der Behälter” shows where the fuel tanks are. Frame 7 states that\n\nEMPFINDLICHSTER TEIL DES FLUGZEUGES\nDer ganze Flügel zwischen den inneren Motoren bis ca. 2|3 der Tiefe. ÖLBEHÄLTER\nin den Gondeln hinter den Motoren\nMOST SENSITIVE PART OF THE AIRCRAFT\nThe entire wing between the inner engines to about 2/3 of the depth\nOIL TANK\nin the nacelle behind the engines\n\nFrame 3 shows various locations at which B-24 was armored, and the thickness of the armor measured in mm. Specifically, it shows the armor thickness on cockpit window and several turrets, where human crews were located and needed the armor. Also notable is the “Panzerglass” at the tail, which stands for “armored glass”.\nFrame 5 shows the two hemispheres (front and back) of the complete sphere-of-fire of B-24D. The B-24D itself is visible as a small horizontal line in the middle of each hemisphere. On the diagram, different hatch patterns mark sectors “swept” (bestrichen) by each gun position. For a typical B-24 D, with a Martin 250-CE top turret, hand-held nose guns, waist guns, a Sperry ball turret and the Consolidated A-6 tail turret, the shaded arcs match the hardware fairly well.\n\n\n\n\n\n\nFull transcript\n\n\n\n\n\nVIERMOTORIGES KAMPFFLUGZEUG CONSOLIDATED B. 24 D. “LIBERATOR„\nMin. dell’ Aeron. – Stab. Fotomecc. 1943-XXI\nREPRODUCTION PLATOON, 901st ENGINEER AF HQ CO\n? 277 EN 10/43\n\n\n\nEinzelheiten der Seitenwaffen des Rumpfes\nJede Waffe hat ihren eigenen Schützen\n\n\n\nFeuerabschnitte Seitenansicht Ansicht von oben\n\n\n\nLage der Panzerung\n\n\n\nLage der Behälter\n\n\n\nDarstellung des Überschneidens der Feuerabschnitte\nVorderensicht\nRückenansicht\n\nAusschnitt von den Rückenturmwaffen bestrichen.\n\nAusschnitt von den Seitenwaffen bestrichen.\n\nAusschnitt von den Bugwaffen bestrichen.\n\nAusschnitt von der Heckwaffen bestrichen.\n\nAusschnitt von den Bauchwaffen bestrichen.\n\n\n\n\nDarstellung der von den Bordwaffen bestrichenen Räume Alle Waffen sind Kaliber 12,7 mm. Die wirksamsten sind gekennzeichnet (*)\n\n\n\nEMPFINDLICHSTER TEIL DES FLUGZEUGES Der ganze Flügel zwischen den inneren Motoren bis ca. 2|3 der Tiefe.\nÖLBEHÄLTER in den Gondeln hinter den Motoren\n\n\n\nEinzelheiten über die Unterbringung der vorderen Waffen\nAnsicht von oben Seitenansicht Die 3 Waffen werden nur von einem Schützen bedient\n\n\n\nHorizontfeuerausschnitte\n\n\n\nBESONDERE ANGABEN\nFlügelspannweite 33,50 m\nLänge 19,50 m\nMotorenstärke beim Start 4 × 1200 CV. Besatzung 6 + 9\nGünstigste Flughöhe 5000 m\nHöchstgeschwindigkeit in vorteilhafter Flughöhe 490 St/km\nReisegeschwindigkeit in bester Flughöhe 340 St/km\nGipfelhöhe 9000 m\n\n\n\n\n\n\n\n\n\nTranslation\n\n\n\n\n\nFOUR-MOTORED FIGHTER AIRCRAFT CONSOLIDATED B-24D “LIBERATOR”\nMinistero dell’aeronautica [Ministry of Aeronautics] – Photomechanical Staff, 1943 (Year XXI of the Italian Fascist calendar) Reproduction Platoon, 901st Engineer Air Force Headquarters Company\nDocument ? 277 EN, October 1943\n\n\n\nDetails of the fuselage’s side weapons\nEach weapon has its own gunner\n\n\n\nFiring compartments Side view Top view\n\n\n\nLocation of armor\n\n\n\nLocation of the tanks\n\n\n\nIllustration of the overlap of the firing compartments\nFront view\nDorsal view\n\nSection of the dorsal turret weapons.\nSection of the side weapons.\nSection of the bow weapons.\nSection of the stern weapons.\nSection of the belly weapons.\n\n\n\n\nIllustration of the compartments covered by the onboard weapons All weapons are 12.7 mm caliber. The most effective ones are marked (*)\n\n\n\nMOST SENSITIVE PART OF THE AIRCRAFT The entire wing between the inner engines, up to approximately 2/3 of the depth.\nOIL TANKS in the nacelles behind the engines\n\n\n\nDetails of the forward weapon placement\nTop view Side view The three weapons are operated by a single gunner\n\n\n\nHorizontal firing cutouts\n\n\n\nSPECIAL INFORMATION\nWingspan 33.50 m Length 19.50 m Engine power at takeoff 4 × 1200 hp Crew 6 + 9 Best altitude 5000 m Top speed at best altitude 490 km/h Cruise speed at best altitude 340 km/h Ceiling altitude 9000 m\n\n\n\n\n\nB-17\nThis is a lot blurrier and I tried my best to transcribe it, with less success. Source\nIn short, it looks as what you might expect. Again, the poster points out the fuel tanks, near the center, as the weak spot.\n\nEMPFINDLICHSTER TEIL DES FLUGZEUGES\nDer ganze Flügel zwischen den inneren Motoren bis ca. 2/3 der Tiefe\nÖLBEHÄLTER\nin der Gondel hinter dem Motoren\nMOST SENSITIVE PART OF THE AIRCRAFT\nThe entire wing between the inner engines to about 2/3 of the depth\nOIL TANK\nin the nacelle behind the engines\n\n\n\n\nThe B-17 poster.\n\n\n\n\n\nAttempt at English translation.\n\n\n\n\n\nAnother scan of the poster. Possibly this version is easier to see.\n\n\nIf those wireframes amuse you, just know that they built actual models for use at school.\n\n\n\nA German instructional model of an American bomber Boeing B-17 E/F, with wire model of its areas of fire. In the background is a model of a German fighter Fw 190. Source"
  },
  {
    "objectID": "logs/posts/2025-survivorship-bias/index.html#what-did-the-americans-know",
    "href": "logs/posts/2025-survivorship-bias/index.html#what-did-the-americans-know",
    "title": "That Survivorship Bias Plane",
    "section": "What did the Americans know?",
    "text": "What did the Americans know?\nIn the WWII, a research group was established at Columbia University focused on military problems during World War II, the Statistical Research Group. Among its members was the titular Abraham Wald. The airplane story probably originated from a series of 8 memoranda circulated in 1943, which remained classified until they were finally published in 1980 for the general public. (Wald 1980)\nThey are extremely technical, and I did not understand them. Of these, memoranda 5 looked closest to the legend. It studied the problem of, given only data gathered from returned aircrafts, inferring the probability that an aircraft would return, conditional on being hit at various locations.\n\n\n\nThe part that looks closest to the legend. (Wald 1980, 65)\n\n\n\nThe SRG carried out literally hundreds of analyses: how the ammunition in aircraft machine guns should be mixed; quality examination methods for rocket fuel; “the best settings on proximity fuzes for air bursts of artillery shells against ground troops”; “to evaluate the comparative effectiveness of four 20 millimeter guns on the one hand and eight 50 caliber guns on the other as the armament of a fighter aircraft”; calculating “pursuit curves” for targeting missiles and torpedoes. “Statistical studies were also made of stereoscopic range finders, food storage data, high temperature alloys, the diffusion of mustard gas, and clothing tests.”\nCONVERSABLE ECONOMIST: Lessons from World War II Statisticians: Survivorship Bias and Sequential Analysis\n\nI will not attempt to describe exactly what the memoranda does, and point the reader towards AMS Feature Column: The Legend of Abraham Wald, and references within, especially (Mangel and Samaniego 1984).\nDespite the lack of drama in the memoranda, it seems he really did do what the anecdote do, even though we have no idea about the further details, such as which planes’ vulnerabilities he studied:\n\nWald wrote a series of memoranda on estimating the vulnerability of various parts of an airplane from data showing the number of hits on the respective parts of planes returning from combat. The vulnerability of a part (engine, aileron, pilot, stabilizer, elevator, etc.) is defined as the probability that a hit on that part will result in destruction of the plane (fire, explosion, loss of power, loss of control, etc.). The military was inclined to provide protection for those parts that on returning planes showed the most hits. Wald assumed, on good evidence, that hits in combat were uniformly distributed over the planes. It follows that hits on the more vulnerable parts were less likely to be found on returning planes than hits on the less vulnerable parts, since planes receiving hits on the more vulnerable parts were less likely to return to provide data. From these premises, he devised methods for estimating the vulnerability of various parts.\n(Wallis 1980a)\n\n\nSequential test\nOther than the aircraft story, Wald most famous for devising the sequential probability ratio test. It has been mentioned before in Predicting AGI by the Turing Test.\nSometime in late 1942, W. Allen Wallis was asked by Navy Captain Garret L. Schuyler the following question about efficiently testing ordinances:\n\nSome of these samples ran to many thousands of rounds. He said that when such a test program is set up at Dahlgren it may prove wasteful. If a wise and seasoned ordnance expert like Schuyler were on the premises, he would see after the first few thousand or even few hundred [rounds] that the experiment need not be completed, either because the new method is obviously inferior or because it is obviously superior beyond what was hoped for. He said that you cannot give any leeway to Dahlgren personnel, whom he seemed to think often lack judgment and experience, but he thought it would be nice if there were some mechanical rule which could be specified in advance stating the conditions under which the experiment might be terminated earlier than planned.\n\nSo Wallis talked about it with Milton Friedman (more famous for other work), and they quickly realized that it might just work.\n\nThe fact that a test designed for its optimum properties with a sample of predetermined size could be still better if that sample size were made variable naturally suggested that it might pay to design a test in order to capitalize on this sequential feature; that is, it might pay to use a test which would not be as efficient as the classical tests if a sample of exactly \\(N\\) were to be taken, but which would more than offset this disadvantage by providing a good chance of terminating early when used sequentially.\n\nThey realized they were out of their mathematical depths, and recruited Wald. At first, Wald thought it was impossible, because it looked like they wanted to do better than the uniformly most powerful test. But he came around, and solved it in 3 days.\n\nNo doubt this antipathy was strengthened by our calling the new tests “supercolossal” on the grounds that they are more powerful than “most powerful” tests. … The next day Wald phoned that he had thought some about our idea and was prepared to admit that there was sense in it. That is, he admitted that our idea was logical and worth investigating. He added, however, that he thought nothing would come of it; his hunch was that tests of a sequential nature might exist but would be found less powerful than existing tests. On the second day, however, he phoned that he had found that such tests do exist and are more powerful, and furthermore he could tell us how to make them. He came over to the office and outlined his sequential probability ratio to us. This is the ratio of the probability under the null-hypothesis, with which I had been puttering around, to the probability under the alternative hypothesis-or rather, the reciprocal of this ratio. He found the critical levels by an inverse probability argument, showing that the same critical levels result no matter what assumption is made about the a priori distribution."
  },
  {
    "objectID": "logs/posts/2025-survivorship-bias/index.html#some-bonuses",
    "href": "logs/posts/2025-survivorship-bias/index.html#some-bonuses",
    "title": "That Survivorship Bias Plane",
    "section": "Some bonuses",
    "text": "Some bonuses\nThe first scaling plot?\n\n\n\nTop image: Learning curve of the production of B-29 airframes at the Boeing Wichita division during WWII. Page 75 from Source Book of World War II Basic Data - Airframe Industry. Volume 1. Direct Man-Hours - Progress Curves. Bottom image: Scaling curves for LLM. (Kaplan et al. 2020)\n\n\nThe first application of ReLU for war?\n\n\n\nPlanning map for the bombing of Kassel, 1943-10-22–23. (Murray 1986, 211)\n\n\nIn 1951, Marvin Minsky and Dean S. Edmonds built Stochastic Neural Analog Reinforcement Calculator, possibly the first neural network trained by Reinforcement Learning. It used a surplus gyroscopic autopilot from a B-24.\n\nMinsky told George Miller, at Harvard, about the prospective design. “He said, ‘Why don’t we just try it?’” Minsky recalled. “He had a lot of faith in me, which I appreciated. Somehow, he managed to get a couple of thousand dollars from the Office of Naval Research, and in the summer of 1951 Dean Edmonds and I went up to Harvard and built our machine. It had three hundred tubes and a lot of motors. It needed some automatic electric clutches, which we machined ourselves. The memory of the machine as stored in the positions of its control knobs—forty of them—and when the machine was learning it used the clutches to adjust its own knobs. We used a surplus gyropilot from a B-24 bomber to move the clutches.”\n(Bernstein 1981)\n\nMilton Friedman was such an expert on proximity fuzes that he was consulted on it during the Battle of the Bulge.\n\nDuring the Battle of the Bulge in December 1944, several high-ranking Army officers flew to Washington from the battle, spent a day discussing the best settings on proximity fuzes for air bursts of artillery shells against ground troops, and flew back to the battle to put into effect advice from, among others, Milton Friedman, whose earlier studies of the fuzes had given him extensive and accurate knowledge of the way the fuzes actually performed.\n(Wallis 1980b)\n\nOther things he studied included sequential testing, a statistical procedure to ensure that the detonator for the atom bomb worked, the best composition of a high temperature alloy for jet engines, etc.\nThe origin of the joke about “statisticians drowning in streams only 1 meter deep on the average”:\n\nFinally, a legend that has had some currency concerning Hotelling’s work at Dahlgren: For his first visit, Hotelling was told that a station wagon to Dahlgren would leave the main Navy building in Washington at 8:00 a.m. Someone else told him 8:30 a.m. He arrived at 8:15, missing the station wagon by 15 minutes. This led to much joking about statisticians drowning in streams only three feet deep on the average. Actually he had started in time to be confident of arriving by 7:45 a.m., but had encountered an extraordinary delay of more than half an hour. Nevertheless, the legend lives on. I do not expect this note to slay it, any more than I expect my paper to slay the legend about Wald’s work being given a security classification and snatched away from him because he lacked a security clearance.\n(Wallis 1980b)"
  },
  {
    "objectID": "logs/posts/2025-survivorship-bias/index.html#metadata",
    "href": "logs/posts/2025-survivorship-bias/index.html#metadata",
    "title": "That Survivorship Bias Plane",
    "section": "Metadata",
    "text": "Metadata\n\nSeeds planted by a thread about “1/10 uni experiences” on twitter on 2025-05-01."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Documents",
    "section": "",
    "text": "Some documents that are important to my thinking. If they are important enough, I would convert them to the markup language for future reference and safe-keeping. Information dies by hiding and lives by copying. Please feel free to copy them and format them to your own purpose.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Muon Anthology\n\n\n\nAI\n\nscaling\n\ntranslation\n\n\n\nTranslation of several blogposts by Su Jianlin, describing the intuition behind the development of Muon.\n\n\n\n\n\n2025-02-27\n\n78 min\n\n\n\n\n\n\n\n\n\n\n\nThe ultimate tug-of-war between cache and capacity\n\n\n\nAI\n\nscaling\n\ntranslation\n\n\n\nTranslation of a blogpost by the originator of RoPE, describing the GPU-based reasons that guided the design choices of multihead latent attention.\n\n\n\n\n\n2024-05-13\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\nThe Bitter Lesson\n\n\n\nAI\n\nhistory\n\n\n\nOnly search and learning scales arbitrarily.\n\n\n\n\n\n2019-03-13\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nCommentaries and essays by Liu Cixin\n\n\n\nChina\n\nfun\n\nscifi\n\ntranslation\n\n\n\nThe world, the universe, and sci-fi, as he sees it. Translated from the anthology The Best Earth in the Worst Universe (2015) and his blog.\n\n\n\n\n\n2014-12-31\n\n191 min\n\n\n\n\n\n\n\n\n\n\n\nIs a singularity just around the corner?\n\n\n\neconomics\n\n\n\nRobin Hanson’s 1998 essay. Modeling economic growth since 10,000 BC, he predicted that the singularity may arrive around 2150.\n\n\n\n\n\n1998-06-01\n\n33 min\n\n\n\n\n\n\n\n\n\n\n\nWhen will computer hardware match the human brain?\n\n\n\nAI\n\nscaling\n\n\n\nHans Moravec’s 1998 essay, forecasting that hardware for AGI will be available in the 2020s.\n\n\n\n\n\n1998-03-01\n\n70 min\n\n\n\n\n\n\n\n\n\n\n\nThe Coming Technological Singularity\n\n\n\nAI\n\n\n\nTranscript of Vernor Vinge’s 1993 presentation on the singularity, predicting the singularity in the interval 2005–2030.\n\n\n\n\n\n1993-03-10\n\n29 min\n\n\n\n\n\n\n\n\n\n\n\nPigs in Cyberspace\n\n\n\nAI\n\n\n\nHans Moravec predicts that the coming cyberspace would be dominated by cyberminds alien and incomprehensible to human understanding.\n\n\n\n\n\n1991-12-31\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\nThe 21st Century Artilect\n\n\n\nAI\n\n\n\nHugo de Garis’ publication of the Terran vs Cosmists Artilect War scenario. He would add the ‘Cyborgian’ camp later, and the ‘gigadeath’ prediction, but otherwise repeat the same scenario.\n\n\n\n\n\n1989-05-01\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\nThe time scale of artificial intelligence\n\n\n\nAI\n\n\n\nRay Solomonoff’s 1985 essay on forecasting the progress towards AGI, reaching AGI around 2022–2076.\n\n\n\n\n\n1985-06-01\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\nWeapon Systems of The Twenty First Century\n\n\n\nwar\n\nscifi\n\nfun\n\n\n\nStanisław Lem’s review of a fictitious book that predicts 21th century drone warfare.\n\n\n\n\n\n1983-10-01\n\n64 min\n\n\n\n\n\n\n\n\n\n\n\nComputer Science as Empirical Inquiry: Symbols and Search\n\n\n\nAI\n\nhistory\n\n\n\nThe founding document for the Physical Symbol System Hypothesis.\n\n\n\n\n\n1975-10-20\n\n60 min\n\n\n\n\n\n\n\n\n\n\n\nDie Kultur als Fehler\n\n\n\nfun\n\n\n\nStanisław Lem’s review of a fictitious book that describes civilization as a mistake.\n\n\n\n\n\n1971-01-01\n\n24 min\n\n\n\n\n\n\n\n\n\n\n\nA Review of B. F. Skinner’s Verbal Behavior\n\n\n\nlinguistics\n\nAI\n\n\n\nNoam Chomsky’s rejection of behaviorism and empiricism in favor of cognitivism and rationalism in linguistics and psychology. A marker event in the history of linguistics and psychology.\n\n\n\n\n\n1959-01-01\n\n102 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/posts/1971-lem-die-kultur-als-fehler/index.html",
    "href": "docs/posts/1971-lem-die-kultur-als-fehler/index.html",
    "title": "Die Kultur als Fehler",
    "section": "",
    "text": "Die Kultur als Fehler\nWilhelm Klopper (Universitas Verlag, Berlin)\nCivilization as Mistake by Privatdozent W. Klopper is a work without doubt remarkable—as an original hypothesis in anthropology. I cannot refrain, however, before I proceed to the discussion, from indulging in a comment as regards the form of the discourse. This book—only a German could have written it! A fondness for classification, for that scrupulous t-crossing and i-dotting that has begotten innumerable Handbücher, makes the German mind resemble a pigeonhole desk. When one beholds the consummate order displayed by the table of contents of this book, one cannot help thinking that if the Lord God had been of German blood our world would perhaps not necessarily have turned out better existentially, but would have for sure embodied a higher notion of discipline and method. The perfection of this orderliness quite overwhelms one, although it may arouse reservations of a substantive nature. I cannot here go into the question of whether that purely formal penchant for muster and array, for symmetry, for front-and-center and forward-march, might not have exerted a real influence also on certain conceptions that typify German philosophy —its ontology in particular. Hegel loved the Cosmos as a kind of Prussia, for in Prussia there was order! Even the esthetics-inflamed thinker that was Schopenhauer showed what an expository drill looks like in his treatise Uber die vierfache Wurzel des Satzes vom zureichenden Grunde. And Fichte? But I must deny myself the pleasure of digression, which is all the more difficult for me in that I am not a German. To business, to business!\nKlopper has provided his two-volume work with a foreword, a preface, and an introduction. (The ideal of form: a triad!) Going into the merits of the matter, he first takes up that understanding of civilization as mistake which he considers to be false. According to that misguided (says the author) view, typical of the Anglo-Saxon school and represented—notably—by Whistle and Sadbottham, any form of behavior of an organism that neither helps nor hinders the organism’s survival is a mistake. For the sole criterion of sensibleness of behavior is, in evolution, survivability. An animal that behaves in such a fashion that it survives more capably than others is behaving, in the light of this criterion, more sensibly than those that die out. Toothless herbivores are senseless evolutionarily, for hardly are they born before they must perish from hunger. Analogously, herbivores that indeed possess teeth but employ them to chew stones instead of grass are also evolutionarily without sense, for they, too, must disappear. Klopper goes on to quote Whistle’s famous example: let us suppose, says the English author, that in some herd of baboons a certain old male, the leader of the herd, by sheer accident acquires the habit of addressing the birds he devours from the left side. He had, say, an injured finger on the right hand, and when he brought the bird to his mouth he found it more comfortable to hold the prey by the left. The young baboons, watching the leader’s behavior, which for them is a model, imitate it, and before long—that is, after a single generation—every baboon in the herd is starting in on his captured bird from the left. From the point of view of adaptation this behavior is senseless, for baboons can with equal advantage to themselves attack their meal from either side; nevertheless, precisely this pattern of behavior has. established itself in the group. What is it? It is the beginning of a culture (protoculture), being behavior adaptationally senseless. As is known, this idea of Whistle’s was developed not by another anthropologist, but by a philosopher of the English logical-analytical school, J. Sadbottham, whose views our author—before taking exception to them—summarizes in the next chapter (“Das Fehlerhafte der Kulturfehlertheorie von Joshua Sadbottham”).\nIn his major work, Sadbottham declared that human communities produce cultures through mistakes, false steps, failures, blunders, errors, and misunderstandings. Intending to do one thing, people in reality do another; desiring to understand the mechanism of a phenomenon through and through, they interpret it for themselves wrongly; seeking truth, they arrive at falsehood; and thus do customs come into being, mores, faith, sanctification, mystery, mana; thus come into being injunctions and interdictions, totems and taboos. People form a false classification of the surrounding world, and totemism results. They make false generalizations and thus arrive first at the notion of mana, and afterward at that of the Absolute. They create mistaken representations of their own physical construction, and thus arise the concepts of virtue and sin; had the genitalia been similar to butterflies and insemination to song (the transmitter of hereditary information being specific vibrations in the air), these concepts would have taken a completely different form. People create hypostases, and thus arise concepts of divinities; they make plagiarisms, and thus arise eclectic interpolations of myths—or doctrinal religions. In other words, in behaving any which way, inappropriately, imperfectly with respect to adaptation, in misinterpreting the behavior of other people, and their own bodies, and the objects in Nature, in considering things that happen accidentally to be things that aie determined, and things that are determined, to be accidental—that is, in inventing a growing number of fictitious existences, people wall themselves in with the edifice of culture, they alter their model of the world to fit its conclusions and then, after millennia pass, they are surprised that in such a prison they do not feel altogether comfortable. The beginnings are always innocent and even, on the face of it, trivial—take, for example, the baboons who eat birds always from the left side. But when from such odds and ends emerges a system of meanings and values, when the mistakes and misunderstandings accumulate enough so that they can, by their totality, in their entirety, close—to use the language of mathematics—then man himself already has become imprisoned in what, though it is the most fortuitous sort of miscellany, appears to him as the highest necessity.\nA scholar of much erudition, Sadbottham backs his assertions with a multitude of examples drawn from ethnology; his tabulations, too, as we recall, caused quite a commotion in their day, especially those charts of “chance versus determinism,” on which he juxtaposed all the different cultures’ mistaken explanations of natural phenomena. And in fact, a great number of cultures consider the mortality of man to be the consequence of a particular instance of bad luck: man was, according to them, originally immortal, but he either deprived himself of this attribute by a fall, or else was deprived of it through the intervention of some evil power. Conversely, that which is the work of chance—the physical appearance of man, shaped in evolution—all cultures have provided with the name of inevitability; to this day the leading religions teach that man is in the aspect of his body unaccidental, since fashioned in God’s image, after His likeness.\nThe criticism to which Herr Dozent Klopper submits the hypothesis of his English colleague is neither original nor the first. As a German, Klopper has divided his criticism into two parts: immanent and positive. In the immanent he only negates Sadbottham’s thesis; this section of the work we pass over as being less material, since it repeats the objections already known from the professional literature. In the second half of the criticism, the positive, Wilhelm Klopper finally proceeds to set forth his own counterhypothesis of “Civilization as Mistake.”\nThe exposition begins, in our opinion effectively arid aptly, with the supplying of an illustrative example. Different birds build their nests out of different materials. What is more, the same species of bird in different localities will not nest-build using exactly the same materials, because it must rely on what it finds in the vicinity. As to which material, in the form of blades of grass, flakes of bark, leaves, little shells, pebbles, the bird is going to find most readily, that depends on chance. And so in some nests you will have more shells and in some, more pebbles; some will be stuck together primarily out of little strips of bark, some, out of pinfeathers and moss. But whatever building material makes its unmistakable contribution toward the shaping of the form of the nest, one cannot with any sense say that nests are the work of pure chance. A nest is an instrument of adaptation, howsoever constructed out of randomly found fragments of this and that; and culture also is an instrument of adaptation. But—and here is the author’s new idea—it is an adaptation fundamentally different from that typical of the plant and animal kingdoms.\n“Was ist der Fall?” asks Klopper. “What is the situation?” The situation is this: in man, considered as a physical being, there is nothing inevitable. According to the knowledge of modem biology, man could be constructed other than he is; he could live six hundred and not sixty years on the average; he could possess a differently shaped trunk or limbs, have a different reproductive system, a different digestive system; he could, for example, be exclusively herbivorous, he could be oviparous, he could be amphibious, he could be able to breed only once a year, in a period of rut, and so on. Man, it is true, does possess one characteristic that is inevitable, to the extent, at least, that without it he would not be man. He possesses a brain that is able to produce speech and reflection; and, gazing upon his own body and upon his fate, which is circumscribed by that body, man leaves the realm of such reflection greatly discontented. He lives but briefly; on top of this his powerless childhood is of long duration; his time of ablest maturity is a small portion of his entire life; hardly does he achieve his prime when he begins to age, and, unlike all other creatures, he knows to what end aging will lead him. In the natural habitats of evolution life is lived under incessant threat; one must be on one’s toes in order to survive; it is for this reason that the gauges of pain, the organs of suffering—as signaling devices to stimulate the development of self-preserving activity—have been by evolution very strongly pronounced in all living things. On the other hand, there has been no evolutionary reason, no organism-shaping force, to balance this situation “fairly,” endowing life forms with a corresponding quantity of organs of enjoyment and pleasure.\nEveryone will admit, says Klopper, that pangs of hunger, the torments caused by thirst, the agonies of suffocation, are incomparably keener than the satisfaction one experiences in eating, drinking, or breathing normally. The sole exception to this general rule of asymmetry between anguish and delight is sex. But this is understandable: were we not bisexual beings, had we a genital system arranged along the lines of, say, the flowers, then it would function apart from any positive sensory experience, for a goad to action would then be totally unnecessary. The fact that sexual pleasure exists and that above it have spread the invisible edifices of the Kingdom of Love (Klopper, when he ceases being dry and factual, immediately turns sentimentally poetic!) derives entirely from the circumstance of bisexuality. Erroneous is the supposition that Homo hermaphroditicus, were such a being to exist, would love himself erotically. Nothing of the sort; he would care for himself strictly within the bounds of the instinct for self-preservation. That which we call narcissism and picture to ourselves as the attraction a hermaphrodite might feel for himself is a secondary projection, the result of a ricochet: such an individual mentally connects with his own body the image of an external, ideal lover. (Here follow about seventy pages of profound cogitation on the question of uni-, bi-, and multisexual facultative possibilities for shaping human erotic nature; this large digression, too, we pass over.)\nWhat has culture to do with all of this? queries Klopper. Culture is an instrument of adaptation of a new type, for it does not so much itself arise from accident as it serves this purpose, to wit, that everything which in our condition is de facto accidental stand bathed in the light of a higher, ultimate necessity. And therefore: culture acts through established religion, through custom, law, interdiction and injunction, in order to convert insufficiencies into idealities, minuses into pluses, shortcomings into acmes of perfection, defects into virtues. Suffering is distressful? Yes, but it ennobles and even redeems. Life is short? Yes, but the life beyond is everlasting. Childhood is toilsome and inane? Yes, but for all that—halcyon, idyllic, positively sacred. Old age is horrid? Yes, but this is the preparation for eternity, and besides, old people are to be respected, by virtue of the fact that they are old. Man is a monster? Yes, but he is not to blame; it was his primogenitors who brought on the evil—or else a demon interfered in the Divine Act. Man does not know what to want, he seeks the meaning of life, he is unhappy? Yes, but this is the consequence of freedom, which is the highest value; that one must pay through the nose for its possession is therefore of no great significance: a man deprived of freedom would be more unhappy than if he were not! Animals, Klopper observes, make no distinction between feces and carrion: they steer clear of both the one and the other as the evacuations of life. For a consistent materialist the equating of a corpse with excrement ought to be just as valid; but the latter we dispose of furtively, and the former with pomp, loftily, equipping the remains with a number of costly and complicated wrappings. This is required by culture, as a system of appearances that help us reconcile ourselves to the despicable facts. The solemn ceremony of burial serves as a sedative for the natural outrage and revolt roused in us by the infamy of mortality. For it is an infamy, that the mind, filled in the course of a lifetime with ever more extensive knowledge, should come to this, that it dissolves into a putrid puddle of corruption.\nThus culture is the mitigator of all the objections, indignations, grievances that man might address to natural evolution, to those physical characteristics haphazardly created, haphazardly fatal, which he—without being asked for his opinion or consent—has inherited from a billion-year process of ad hoc accommodations. To all that vile patrimony, to that ragtag-and-bobtail mob of infirmities and blemishes inserted into the cells themselves, knit into the bones, sewn into the sinews and the muscles—culture, wearing its picturesque toga of appointed public defender, attempts to reconcile us. It uses innumerable weasel words, it resorts to arguments that contradict themselves internally, that appeal now to the feelings, now to the reason, for any and all methods of persuasion are acceptable to culture, so long as it achieves its goal—the transformation of negative quantities into positive, of our wretchedness, our deformity, our frailty, into virtue, perfection, and manifest necessity.\nWith a monumental diapason of style, in measure sublime, in measure professorial, concludes the first part of the treatise of Dozent Klopper, here given fairly laconically by us. The second part explains the vital importance of understanding the true function of culture, so that man may be able properly to receive the portents of the future, a future he has prepared for himself by building a science-and-technology civilization.\nCulture is a mistake! announces Klopper, and the brevity of this assertion brings to mind the Schopenhauerian “Die Welt ist Wille”! Culture is a mistake, not in the sense or to suggest that it arose by chance; no, it arose by necessity, for—as shown in Part One—it serves adaptation. But it serves adaptation only mentally: surely it does not, with its dogmas of faith and its precepts, transform man into an actually immortal being; it does not tack onto accidental man, homini fortuito, a real Creator-Deity; it does not really annul a single atom of an individual’s sufferings, griefs, agonies (here, too, Klopper is true to Schopenhauer! )—what it does, it does entirely on the plane of the spirit, on the level of interpretation, making meaning out of that which in immanence has no meaning; it divides sin from virtue, grace from damnation, humiliation from exaltation.\nBut now technological civilization, in steps imperceptible at first, creeping along with its scrap iron of primitive machines, has worked its way underneath culture. The building is shaken, the walls of the crystal rectifier crack: for technological civilization promises to correct man, both his body and his brain, and quite literally to optimize his soul. This tremendous and unexpectedly welling force (of the information, stored up for centuries, which in the twentieth century exploded) heralds a chance for long life, with the limit, perhaps, in immortality; a chance for swift maturation and no senescence; a chance for a legion of physical pleasures and a reduction to zero of torments, of tribulations both “natural” (senility) and “accidental” (disease) ; it heralds the chance for freedom where previously hazard was wed to inevitability (freedom meaning the power to choose the qualities of human nature; meaning the possibility of amplifying talent, knowledge, intelligence; meaning the opportunity to give to human limbs, the face, the body, the senses, whatever forms and functions one desires, even those that are well-nigh everlasting, etc. ).\nWhat, then, ought to be done in the face of these promises, promises verified by fulfillments already brought about? Why, throw oneself into a triumphal dance! Culture, that cane of the lame, crutch of the crippled, wheelchair for the paralytic, that system of patches placed over the shame of our body, over the deformity of our toilsome condition, culture, that helpmate that has seen much service and outserved, ought to be pronounced an anachronism and nothing but. For are artificial limbs necessary to those who can grow new? Must a blind man clutch the white cane to his breast, when we return him his sight? Is he to request benightedness anew who has had the scales lifted from his eyes? Should not one, rather, lay to rest that useless lumber in the museum of the past, and set out with a springing step toward the awaiting, difficult yet magnificent tasks and goals ahead? So long as the nature of our bodies, of their sluggish growth and all-too-swift decay, was an impervious wall, an implacable barrier, the limit of existence—for that long did culture facilitate, unto the thousandth generation, our adaptation to this wretched status quo. It reconciled us to it; more, as the author shows, it actually converted the flaws into merits, the drawbacks into advantages. It is as if someone condemned to a broken-down, ugly, and worthless vehicle were gradually to conceive an affection for its failings, to find in its ungainliness evidence of a higher ideal, and in its endless defects a Law of Nature, of Creation; he perceives the hand of the Lord God Himself in the sputtering carburetor and the chattering gears. So long as there is not another vehicle in sight, this is perfectly proper, very suitable, the only right and even sensible policy, one should think. But now, when a new vehicle gleams on the horizon? To cling to the broken spokes, bewail the ugliness with which it will be necessary soon to part, cry out “Help, save me!” from the streamlined beauty of the new model? Understandable psychologically, indeed yes. For too long—millennia!—has the process been going on of man’s bending himself to his own evolutionarily piecemeal nature, that colossal straining—from century to century—to love the given condition in all its misery, squalor, unattractiveness, in its destitutions and physiological nooks and crannies.\nSo much has man, in all his successive cultural formations, slaved away at this, so much has he striven to sway himself, to have himself believe in the absolute necessity, supremity, uniqueness, and most of all the inalterability of his fate, that now, at the sight of his deliverance, he recoils, quakes, hides his eyes, utters cries of terror, turns away from the technological Saviour, wishing to flee somewhere, anywhere, even to the forest on all fours, wishing he could take that flower of knowledge, that wonder of science, and smash it with his own two hands, trample it underfoot, if only not to surrender his ancient values to the junk heap, values he nourished with his own blood, nurtured waking and sleeping, till he forced upon himself … love for them! But such absurd conduct, this shock, this panic, is above all, from any rational standpoint, stupidity.\nYes, culture is a mistake! But only in the sense that it is a mistake to shut the eyes to the light, to push away medicine in illness, to call for incense and magic spells when an enlightened doctor is standing by the bed. This mistake did not exist at all until the moment when our knowledge, growing, reached the required level; this mistake—it is the resistance, the balky, mulish, pigheaded opposition, the obstinate aversion, it is the tremor of dread our modern “thinkers” like to call an intellectual assessment of the present changes in the world. Culture, that system of prostheses, must be discarded, so that we may entrust ourselves to the knowledge that will remake us, endow us with perfection; nor will the perfection be Active, a thing we are talked into or sold, a thing educed from the sophistry of tortuous, self-contradictory establishings and dogmas. It will be purely material, factual, a perfectly objective perfection: existence itself will be perfect—not merely its exposition, not merely its interpretation! Culture, defender of Evolution’s Causal Imbecilities, shifty pettifogger of a lost cause, shyster mouthpiece of primitivism and somatic slapdashery, must remove itself, since man’s case is entering other, higher courts, since the wall of inviolable necessity, inviolable only hitherto, now crumbles. Technological development means the ruin of culture? It provides freedom where hitherto reigned the constraint of biology? But of course it does! And instead of shedding tears over the loss of our captivity, we should hasten our step to leave its dark house. And therefore (the finale begins, in cadenced conclusions): everything that has been said about the threat to time-honored culture by the new technology is true. But one need not be concerned about this threat; one need not patch together a culture coining apart at the seams, or fasten down its dogmas with clamps, or hold out valiantly against the invasion of our bodies and our lives by superior knowledge. Culture, still a value today, will tomorrow become another value: namely, anachronistic. For culture was the great hatchery, the womb, the incubator in which discoveries bred and gave agonizing birth to science. Indeed, just as the developing embryo consumes the inert, passive substance of the egg white, so does the developing technology consume, digest, and turn into its own stuff—culture. Such is the way of embryos and eggs.\nWe live in an era of transition, says Klopper, and never is it so unutterably difficult to make out the road traveled and the road that extends into the future as in periods of transition, for they are times of conceptual confusion. However, the process is inexorably under way. One must not in any case think that the transition from the realm of biological captivity to the realm of self-creative freedom can be an act of a single moment. Man will not be able to perfect himself once and for all, and the process of self-alteration will go on through centuries.\n“I make bold,” says Klopper, “to assure the reader that the dilemma over which the traditional thought of the humanist, flustered by the scientific revolution, lacerates itself, is the yearning of the dog for its removed collar. This dilemma boils down to the faith that man is a skein of contradictions which cannot be got rid of, not even were the ridding technologically possible. In other words, it is forbidden us to change the shape of the body, weaken the lust for aggression, strengthen the intellect, balance the emotions, rearrange sex, liberate man from old age, from the labors of procreation, and this is forbidden for the reason that it has never been done, and what has never been done must surely be, by that fact, most evil. The humanist is not allowed to conceive—a la science—of the present human mind and body as the resultant vector of a long series of random draws, intramillennial convulsions in the evolutionary process, a process that was hurled in all directions by geological upheavals, great glaciations, the explosions of stars, the changes of the magnetic poles, and countless other accidents. What the evolution of the lower animals first, and of the anthropoids later, deposited in lottery style, what then was swept into a single pile by selection, and what day by day was fixed in the genes as in dice thrown at the gaming table, we are to hold untouchable, sacrosanct, inviolable for all time, world without end—only without knowing why it has to be this way and not another. It is as if culture takes umbrage at our diagnosis of its work, noble at least in intent, and our exposure of that greatest, most difficult, most fantastic, and falsest of all the falsehoods Homo sapiens ever fashioned for himself—ever latched onto—he who was thrust suddenly into the open air of intelligent existence from out that murky gambling den where the cheating at genes still goes on, where the evolutionary process sets down its cardsharper’s tricks in the chromosomes. That the game is a foul fraud, never guided by any higher value or goal, is shown by the fact that in that cave the thing is only to survive today—not giving one hoot in heaven or hell about what will become of the one who survives so compromisingly, so opportunistically, therefore dishonorably, tomorrow. But because everything is proceeding exactly in reverse of what our humanist, shaking in his boots, imagines to himself, that dimwit, that boob—he has no right to call himself a rationalist—culture will be cleared away, cleaned up, parceled out, pulled down, and drained, in step with the changes to which man shall submit. Where the hook and crook of genes, where adaptational opportunism decides existence, there is no mystery, there is only the Katzenjammer of the swindled, the awful hangover from the monkey ancestor, the climb skyward up that imaginary ladder from which you always end up falling, biology dragging you down by the seat of your pants, whether you tack onto yourself bird feathers, halos, or immaculate conceptions, or grit your teeth with homemade heroism. And so nothing vital-inevitable will be destroyed, but there will disappear, withering away bit by bit, the scaffolding of superstition, justification, equivocation, the pulling of the wool over the eyes—in a word, that whole sophistry to which the miserable human race has for ages resorted in order to make palatable its odious condition. In the next century, from out of the dust of the information explosion will emerge Homo optimisans se ipse, Autocreator, Self-Maker, who will laugh at our Cassandras (assuming he has with what to laugh). One ought to applaud such an opportunity, acclaim it an incredibly fortunate turn of cosmic-planetary events, and not tremble in the face of the power that will bring our species down from the scaffold and sunder the chains each of us drags with him, as he waits for the potential of his bodily forces to be finally exhausted, when he will know the self-strangling of the death agony. And even should the whole world still continue to acquiesce in that state with which evolution has branded us far worse than we brand the worst criminals, I personally shall never consent to it and yea even from my dying bed rasp out: Down with Evolution, Vivat Autocreation!”\nIt is instructive, this voluminous discourse, the quotation from which we have used to crown our discussion. Instructive, because it shows there is simply no thing appearing to some as evil incarnate and misfortune itself that others will not at the very same time consider a positive godsend and raise to the pinnacle of perfection. This reviewer is of the opinion that technoevolution cannot be declared the existential panacea for humanity, if only because the criteria of optimization are too intricately relativistic for them to be regarded as a universal pattern (that is, as a code of salvational procedure that is unerring, couched in the language of empiricism). In any case, we recommend to the reader Civilization as Mistake, since it is, typical of the time, yet another attempt to limn the future—still dark, despite the combined efforts of the futurologists and such thinkers as Klopper."
  },
  {
    "objectID": "docs/posts/1971-lem-die-kultur-als-fehler/index.html#metadata",
    "href": "docs/posts/1971-lem-die-kultur-als-fehler/index.html#metadata",
    "title": "Die Kultur als Fehler",
    "section": "Metadata",
    "text": "Metadata\n\nPublished in A Perfect Vacuum (1971) a collection of reviews for 16 imaginary books and one real book: itself."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "",
    "text": "Having gained access (by what means, I’m not at liberty to reveal) to several volumes on the military history of the twenty-first century, I pondered, first and foremost, how to hide the information they contained. The question of concealment was most important, because I understood that the man who knew this history was like the finder of a treasure who, defenseless, could easily lose it along with his life. I alone possessed these facts, I realized, thanks to the books that Dr. R.G. loaned to me briefly and which I returned just before his premature death. As far as I know, he burned them, thus taking the secret with him to the grave. Silence seemed the simplest solution: if I kept quiet, I would save my skin. But what a shame, to sit on a thousand and one extraordinary things having to do with the political history of the next century, things opening up completely new horizons in all areas of human life. Take, for example, the astonishing reversal – completely unforeseen – in the field of artificial intelligence (AI), which became a force to be reckoned with precisely because it did not become the machine embodiment of the human mind. If I remained silent for my own safety, I would be depriving myself of all the advantages stemming from that knowledge.\nAnother idea occurred to me: to write down exactly what I remembered of those volumes and place the manuscript in a bank vault. It would be necessary to write down everything I retained from my reading, because with the passage of time I would forget many particulars of such a broad subject. Then, if I wanted to refresh my memory, I could visit the vault, take notes there, and return the manuscript to the strongbox. But it was dangerous. Someone could spy on me. Besides, in today’s world no bank vault was 100-percent secure. Even a thief of low intelligence would figure out, sooner or later, what an extraordinary document had fallen into his hands. And even if he discarded and destroyed my manuscript, I would not know it and would live in constant dread that the connection between my person and the history of the twenty-first century would come to light.\nMy dilemma was how to hide the secret forever but at the same time take advantage of it freely – to hide it from the world but not from myself. After much deliberation, I realized that this could be done very easily. The safest way to conceal a remarkable idea – every word of it true – was to publish it as science fiction. Just as a diamond thrown on a heap of broken glass would become invisible, so an authentic revelation placed amid the stupidities of science fiction would take on their coloration – and cease to be dangerous.\nAt first, however, still fearful, I made a very modest use of the secret I possessed. In 1967 I wrote a science-fiction novel entitled His Master’s Voice (published in English in 1983 by Harcourt Brace Jovanovich). On page 125 of that edition, third line from the top, are the words\n\n“the ruling doctrine was… ‘indirect economic attrition’”, and then the doctrine is expressed by the aphorism “The thin starve before the fat lose weight.”.\n\nThe doctrine expressed publicly in the United States in 1980 – thirteen years after the original edition of His Master’s Voice – was put a little differently. (In the West German press they used the slogan “den Gegner totrüsten” – “arm the enemy to death”.)\nOnce I had confirmed – and there had been time enough to do so, after all, since the book’s appearance – that no one had noticed how my “fantasizing” agreed with later political developments, I grew bolder. I understood that truth, when set in fiction, is camouflaged perfectly, and that even this fact can be safely confessed. For that matter, no one takes anything seriously if it’s published. So the best way to keep a top secret secret is to put it out in a mass edition.\nHaving ensured the safety of my secret thus, I can now serenely set about giving a complete report. I will confine myself to the first two volumes of Weapons Systems of the Twenty-first Century: The Upside-down Evolution, published in 2105. I could even name the authors (none of whom has been born yet), but what would be the point? The work is in three volumes. The first presents the development of weapons from the year 1944; the second explains how the nuclear-arms race gave rise to the “unhumanizing” of warfare by transferring the production of weapons from the defense industry to the battlefield itself; and the third deals with the effect this greatest military revolution had on the subsequent history of the world."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html#i",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html#i",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "",
    "text": "Having gained access (by what means, I’m not at liberty to reveal) to several volumes on the military history of the twenty-first century, I pondered, first and foremost, how to hide the information they contained. The question of concealment was most important, because I understood that the man who knew this history was like the finder of a treasure who, defenseless, could easily lose it along with his life. I alone possessed these facts, I realized, thanks to the books that Dr. R.G. loaned to me briefly and which I returned just before his premature death. As far as I know, he burned them, thus taking the secret with him to the grave. Silence seemed the simplest solution: if I kept quiet, I would save my skin. But what a shame, to sit on a thousand and one extraordinary things having to do with the political history of the next century, things opening up completely new horizons in all areas of human life. Take, for example, the astonishing reversal – completely unforeseen – in the field of artificial intelligence (AI), which became a force to be reckoned with precisely because it did not become the machine embodiment of the human mind. If I remained silent for my own safety, I would be depriving myself of all the advantages stemming from that knowledge.\nAnother idea occurred to me: to write down exactly what I remembered of those volumes and place the manuscript in a bank vault. It would be necessary to write down everything I retained from my reading, because with the passage of time I would forget many particulars of such a broad subject. Then, if I wanted to refresh my memory, I could visit the vault, take notes there, and return the manuscript to the strongbox. But it was dangerous. Someone could spy on me. Besides, in today’s world no bank vault was 100-percent secure. Even a thief of low intelligence would figure out, sooner or later, what an extraordinary document had fallen into his hands. And even if he discarded and destroyed my manuscript, I would not know it and would live in constant dread that the connection between my person and the history of the twenty-first century would come to light.\nMy dilemma was how to hide the secret forever but at the same time take advantage of it freely – to hide it from the world but not from myself. After much deliberation, I realized that this could be done very easily. The safest way to conceal a remarkable idea – every word of it true – was to publish it as science fiction. Just as a diamond thrown on a heap of broken glass would become invisible, so an authentic revelation placed amid the stupidities of science fiction would take on their coloration – and cease to be dangerous.\nAt first, however, still fearful, I made a very modest use of the secret I possessed. In 1967 I wrote a science-fiction novel entitled His Master’s Voice (published in English in 1983 by Harcourt Brace Jovanovich). On page 125 of that edition, third line from the top, are the words\n\n“the ruling doctrine was… ‘indirect economic attrition’”, and then the doctrine is expressed by the aphorism “The thin starve before the fat lose weight.”.\n\nThe doctrine expressed publicly in the United States in 1980 – thirteen years after the original edition of His Master’s Voice – was put a little differently. (In the West German press they used the slogan “den Gegner totrüsten” – “arm the enemy to death”.)\nOnce I had confirmed – and there had been time enough to do so, after all, since the book’s appearance – that no one had noticed how my “fantasizing” agreed with later political developments, I grew bolder. I understood that truth, when set in fiction, is camouflaged perfectly, and that even this fact can be safely confessed. For that matter, no one takes anything seriously if it’s published. So the best way to keep a top secret secret is to put it out in a mass edition.\nHaving ensured the safety of my secret thus, I can now serenely set about giving a complete report. I will confine myself to the first two volumes of Weapons Systems of the Twenty-first Century: The Upside-down Evolution, published in 2105. I could even name the authors (none of whom has been born yet), but what would be the point? The work is in three volumes. The first presents the development of weapons from the year 1944; the second explains how the nuclear-arms race gave rise to the “unhumanizing” of warfare by transferring the production of weapons from the defense industry to the battlefield itself; and the third deals with the effect this greatest military revolution had on the subsequent history of the world."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html#ii",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html#ii",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "II",
    "text": "II\nSoon after the destruction of Hiroshima and Nagasaki, American nuclear researchers founded the Bulletin of the Atomic Scientists. On its cover they put the picture of a clock with the minute hand at ten to midnight. Six years later, after the first successful tests of the hydrogen bomb, they moved the hand five minutes closer, and when the Soviet Union acquired thermonuclear weapons the hand was moved three minutes closer. The next move would mean the end of civilization. The Bulletin’s doctrine was “One World or None”: the world would either unite and be saved, or would perish.\nWith the nuclear build-up on both sides of the ocean and the placing of ever larger payloads of plutonium and tritium in ever more accurate ballistic missiles, none of the scientists who were the “fathers of the bomb” believed that peace – troubled as it was by local,conventional wars – would last to the end of the century. Atomic weapons had amended Clausewitz’s famous definition (“War is… a continuation of political activity by other means”),because now the threat of attack could substitute for the attack itself. Thus came about the doctrine of symmetrical deterrence known later as the “balance of terror.” Different American administrations advocated it with different initials. There was, for example, MAD (MutualAssured Destruction), based on the “second-strike” principle (the ability of the country attacked to retaliate in force). The vocabulary of destruction was enriched in the next decades. There was “Total Strategic Exchange”, meaning all-out nuclear war; MIRV (Multiple Independently Targetable Re-entry Vehicle), a missile firing a number of warheads simultaneously, each aimed at a different target; PENAID (Penetration Aids), dummy missiles to fool the opponent’s radar; and MARY (Maneuverable Re-entry), a missile capable of evading antimissiles and of hitting the target within fifty feet of the programmed “ground zero”. But to list even a hundredth of the succession of specialized terms is impossible here.\nAlthough the danger of atomic warfare increased whenever “equality” was lessened, and therefore the rational thing would seem to have been to preserve that equality under multinational supervision, the antagonists did not reach an agreement despite repeated negotiations.\nThere were many reasons, which the authors of Weapons Systems divide into two groups.In the first group they see the pressure of traditional thinking in international politics. Tradition has determined that one should call for peace but prepare for war, upsetting the existing balance until the upper hand is gained. The second group of reasons are factors independent of human thought both political and nonpolitical; these have to do with the evolution of the major applied military technologies.\nEach new possibility of technological improvement in weaponry became a reality, on the principle “If we don’t do it, they will.”. Meanwhile, the doctrine of nuclear warfare went through changes. At one time it advocated a limited exchange of nuclear strikes (though no one knew exactly what the guarantee of the limitation would be); at another, its goal was the total annihilation of the enemy (all of whose population became “hostages” of a sort); at still another, it gave first priority to destroying the enemy’s military-industrial potential.\nThe ancient law of “sword and shield” still held sway in the evolution of weaponry. The shield took the form of hardening the silos that housed the missiles, while the sword to pierce the shield involved making the missiles increasingly accurate and, later, providing them with self-guidance systems and self-maneuverability. For atomic submarines the shield was the ocean; improved methods for their underwater detection constituted the sword.\nTechnological progress in defense sent electronic “eyes” into orbit, creating a high frontier of global reconnaissance able to spot missiles at the moment of launch. This was the shield that the new type of sword – the “killer satellite” – was to break, with a laser to blind the defending “eyes,” or with a lightning-like discharge of immense power to destroy the missiles themselves during their flight above the atmosphere.\nBut the hundreds of billions of dollars invested in building these higher and higher levels of conflict failed, ultimately, to produce any definite, and therefore valuable, strategic advantage – and for two very different, almost unrelated reasons.\nIn the first place, all these improvements and innovations, instead of increasing strategic security, offensive or defensive, only reduced it. Security was reduced because the global system of each superpower grew more and more complex, composed of an increasing number of different subsystems on land, sea, and air and in space. Military success required infallible communications to guarantee the optimum synchronization of operations. But all systems that are highly complex, whether they be industrial or military, biological or technological, whether they process information or raw material, are prone to breakdown, to a degree mathematically proportional to the number of elements that make up the system. Progress in military technology carried with it a unique paradox: the more sophisticated the weapon it produced, the greater was the role of chance (which could not be calculated) in the weapon’s successful use.\nThis fundamental problem must be explained carefully, because scientists were for a long time unable to base any technological activity on the randomness of complex systems. To counteract malfunctions in such systems, engineers introduced redundancy: power reserves, for example, or – as with the first American space shuttles (like the Columbia) – the doubling, even quadrupling of parallel, onboard computers. Total reliability is unattainable: if a system has a million elements and each element will malfunction only one time out of a million, a breakdown is certain.\nThe bodies of animals and plants consist of trillions of functioning parts, yet life copes with the phenomenon of inevitable failure. In what way? The experts call it the construction of reliable systems out of unreliable components. Natural evolution uses various tactics to counteract the fallibility of organisms: the capacity for self-repair or regeneration; surplus organs (this is why we have two kidneys instead of one, why a half-destroyed liver can still function as the body’s central chemical-processing plant, and why the circulatory system has so many alternate veins and arteries); and the separation of control centers for the somatic and psychic processes. This last phenomenon gave brain researchers much trouble: they could not understand why a seriously injured brain still functioned but a slightly damaged computer refused to obey its programs.\nMerely doubling control centers and parts used in twentieth-century engineering led to the absurd in actual construction. If an automated spaceship going to a distant planet were built according to the directive of multiplying pilot computers, as in the shuttles, then it would have to contain – in view of the duration of the flight – not four or five but possibly fifty such computers. They would operate not by “linear logic” but by “voting”: once the individual computers ceased functioning identically and thus diverged in their results, one would have to accept, as the right result, what was reached by the majority. But this kind of engineering parliamentarianism led to the production of giants burdened with the woes typical of democracies: contradictory views, plans, and actions. To such pluralism, to such programmed elasticity, there had to be a limit.\nWe should have begun much earlier – said the twenty-first-century specialists – to learn from biological evolution, whose several-billion-year existence demonstrates optimal strategic engineering. A living organism is not guided by “totalitarian centralism” or “democratic pluralism”, but by a strategy much more complex. Simplifying, we might call it a compromise between concentration and separation of the regulating centers.\nMeanwhile, in the late-twentieth-century phase of the arms race, the role of unpredictable chance increased. When hours (or days) and miles (or hundreds of miles) separate defeat from victory, and therefore an error of command can be remedied by throwing in reserves, or retreating, or counterattacking, then there is room to reduce the element of chance. But when micromillimeters and nanoseconds determine the outcome, then chance enters like a god of war, deciding victory or defeat; it is magnified and lifted out of the microscopic scale of atomic physics. The fastest, best weapons system comes up against the Heisenberg uncertainty principle, which nothing can overcome, because that principle is a basic property of matter in the Universe.\nIt need not be a computer breakdown in satellite reconnaissance or in missiles whose warheads parry defenses with laser beams; if a series of electronic defensive impulses is even a billionth of a second slow in meeting a similar series of offensive impulses, that is enough for a toss of the dice to decide the outcome of the Final Encounter.\nUnaware of this state of affairs, the major antagonists of the planet devised two opposite strategies. One can call them the “scalpel” and the “hammer”. The constant escalation of pay-load megatonnage was the hammer; the improvement of detection and swift destruction in flight was the scalpel. They also reckoned on the deterrent of the “dead man’s revenge”: the enemy would realize that even in winning he would perish, since a totally obliterated country would still respond – automatically and posthumously – with a strike that would make defeat universal. Such was the direction the arms race was taking, and such was its destination, which no one wanted but no one knew how to avoid.\nHow does the engineer minimize error in a very large, very complex system? He does trial runs to test it; he looks for weak spots, weak links. But there was no way of testing a system designed to wage global nuclear war, a system made up of surface, submarine, air-launched, and satellite missiles, antimissiles, and multiple centers of command and communications, ready to loose gigantic destructive forces in wave on wave of reciprocal atomic strikes. No maneuvers, no computer simulation, could re-create the actual conditions of such a battle.\nIncreasing speed of operation marked each new weapons system, particularly the decision-making function (to strike or not to strike, where, how, with what force held in reserve, at what risk, etc.), and this increasing speed also brought the incalculable factor of chance into play. Lightning-fast systems made lightning-fast mistakes. When a fraction of a second determined the safety or destruction of a region, a great metropolis, an industrial complex, or a large fleet, it was impossible to achieve military certainty. One could even say that victory had ceased to be distinguishable from defeat. In a word, the arms race was heading toward a Pyrrhic situation.\nOn the battlefields of yore, when knights in armor fought on horseback and foot soldiers met at close quarters, chance decided the life or death of individuals and military units. But the power of electronics, embodied in computer logic, made chance the arbiter of the fate of whole armies and nations.\nMoreover – and this was quite a separate thing – blueprints for new, better weapons were developed so quickly that industry could not keep pace. Control systems, targeting systems, camouflage, maintenance and disruption of communications, the strike capability of so-called conventional weapons (a misleading term, really, and out of date) became anachronisms even before they were put into the field.\nThat is why, in the late eighties, production was frequently halted on new fighter planes and bombers, cruise missiles, anti-antimissiles, spy satellites, submarines, laser bombs, sonars, and radars. That is why prototypes had to be abandoned and why so much political debate seethed around successive weapons that swallowed huge budgets and vast human energies. Not only did each innovation turn out to be far more expensive than the one before, but many soon had to be written off as losses, and this pattern continued without letup. It seemed that technological-military invention per se was not the answer, but, rather, the speed of its industrial implementation. This phenomenon became, at the turn of the century, the latest paradox of the arms race. The only way to nullify its awful drain on the military appeared to be to plan weapons not eight or twelve years ahead, but a quarter of a century in advance – which was a sheer impossibility, requiring the prediction of new discoveries and inventions beyond the ken of the best minds of the day.\nAt the end of the twentieth century, the idea emerged of a new weapon that would be neither an atom bomb nor a laser gun but a hybrid of the two. Up to then, there were fission (uranium, plutonium) and fusion (thermonuclear, hydrogen-plutonium) bombs. The “old” bomb, in breaking nuclear bonds, unleashed every possible sort of radiation: gamma rays, X-rays, heat, and an avalanche of radioactive dust and lethal high-energy particles. The fireball, having a temperature of millions of degrees, emitted energy at all wavelengths. As someone said, “Matter vomited forth everything she could.” From a military standpoint it was wasteful, because at ground zero all objects turned into flaming plasma, a gas of atoms stripped of their electron shells. At the site of the explosion, stones, trees, houses, metals, bridges, and human bodies vaporized, and concrete and sand were hurled into the stratosphere in a rising mushroom of flames. “Conversion bombs” were a more efficient version of this weapon. They emitted what the strategists required in a given situation: either hard radiation – in which case it was called a “clean bomb,” striking only living things – or thermal radiation, which unleashed a firestorm over hundreds of square miles.\nThe laser bomb, however, was not actually a bomb; it was a single-charge laser gun, focusing a huge part of its force into a ray that could incinerate a city (from a high orbit), for example, or a rocket base, or some other important target (such as the enemy’s satellite defense screen). At the same time, the ray would turn the laser bomb itself into flaming fragments. But we will not go into more detail about such weapons, because instead of leading to further escalation, as was expected, they really marked its end.\nIt is worthwhile, however, to look at the atomic arsenals of twentieth-century Earth from a historical perspective. Even in the seventies, they held enough weapons to kill every inhabitant of the planet several times over. Given this overabundance of destructive might, the specialists favored a preventive strike, or making a second strike at the enemy’s stockpiles while protecting their own. The safety of the population was important but second in priority.\nIn the early fifties, the Bulletin of the Atomic Scientists printed a discussion in which the fathers of the bomb, physicists like Bethe and Szilard, took part. It dealt with civil defense in the event of nuclear war. A realistic solution would have meant evacuating the cities and building gigantic underground shelters. Bethe estimated the cost of the first phase of such a project to be twenty billion dollars, though the social and psychological costs were beyond reckoning. But it soon became clear that even a “return to the cave” would not guarantee the survival of the population, because the arms race continued to yield more powerful warheads and increasingly accurate missiles. The science fiction of the day painted gloomy and nightmarish scenes in which the degenerate remnants of humanity vegetated in concrete, multilevel molehills beneath the ruins of gutted cities. Self-styled futurologists (but all futurologists were self-styled) outdid one another in extrapolating, from existing atomic arsenals, future arsenals even more frightful. One of the better known of such speculations was Herman Kahn’s Thinking about the Unthinkable, an essay on hydrogen warfare. Kahn also thought up a “doomsday machine.” An enormous nuclear charge encased in a cobalt jacket could be buried by a nation in the depths of its own territory, in order to blackmail the rest of the world with the threat of “total planetary suicide.” But no one dreamed that, with political antagonisms still persisting, the era of atomic weapons would come to an end without ushering in either world peace or world annihilation.\nDuring the early years of the twenty-first century, theoretical physics pondered a question that was thought to be crucial for the world’s continued existence: namely, whether or not the critical mass of uranides like uranium 235 and plutonium (that is, the mass at which an initiated chain reaction causes a nuclear explosion) was an absolute constant. If the critical mass could be influenced, particularly at a great distance, there might be a chance of neutralizing all warheads. As it turned out (and the physicists of the previous century had a rough idea of this), the critical mass could change. Under certain physical conditions, an explosive charge that had been critical ceased to be critical, and therefore did not explode. But the amount of energy needed to create such conditions was far greater than the power contained in all the atomic weapons combined. These attempts to neutralize atomic weapons were unsuccessful."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html#iii",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html#iii",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "III",
    "text": "III\nIn the 1990s a new type of missile, popularly called the “F&F” (Fire & Forget), made its appearance. Guided by a programmed microcomputer, the missile sought its own target after being launched. Once activated, it was truly on its own. At the same time, “unhuman” espionage came into use, at first underwater. An underwater mine, equipped with sensors and memory, could keep track of the movements of ships sailing over it, distinguish commercial vessels from military, establish their tonnage, and transmit the information in code if necessary.\nCombat readiness, in the affluent nations especially, evaporated. Young men of draft age considered such time-honored phrases as Dulce et decorum est pro patria mori to be completely ridiculous.\nMeanwhile, new generations of weapons were rising in price exponentially. The airplane of the First World War was made of canvas, wood, and piano wire, with a couple of machine guns; landing gear and all, it cost about as much as a good automobile. A comparable airplane of the Second World War cost as much as thirty automobiles. By the end of the century, the price of a jet interceptor or a radar-proof bomber of the “Stealth” type was in the hundreds of millions of dollars. Aircraft for the year 2000 were expected to cost a billion apiece. At this rate, it was calculated that over the next eighty years each superpower would be able to afford only twenty to twenty-five new planes. Tanks were no cheaper. And an atomic aircraft carrier, which was like an antediluvian brontosaurus under fire, cost many billions. The carrier could be sunk by a single hit from an F&F superrocket, which could split over the target into a cluster of specialized warheads, each to strike at a different nerve center of the sea leviathan.\nAt this same time, the production of microchips was discontinued; they were replaced by a product of the latest genetic engineering. The strain Silocobacter wieneri (named after the creator of cybernetics, Norbert Wiener) produced, in solutions containing silicates, silver, and a secret ingredient, solid-state circuits that were smaller than fly’s eggs. These elements were called “grain,” and after four years of mass production a handful of them cost no more than a handful of corn. In this way, from the intersection of two curves – the rising curve of cost for heavy weaponry and the falling curve of cost for artificial intelligence – came the “unhumanization” of the military.\nArmies began to change from living to nonliving forces. Initially, the effects of the change were undramatic. It was like the automobile, whose inventors did not immediately come up with an entirely new shape but, instead, simply put an internal-combustion engine in a cart or carriage, with the harness removed. Similarly, the earliest pioneers of aviation gave their flying machines the wings of birds. Thanks to this kind of mental inertia, which in the military is considerable, not very radical new missiles, unmanned tanks, and self-propelled artillery were adapted for the new microsilicon “soldier,” simply by reducing them in size and installing computer-controlled command modules. But this was anachronistic. The new, nonliving microsoldier required a whole new approach to tactics, strategy, and, of course, to the question of what kinds of weapons he could put to best use.\nThis came at a time when the world was slowly recovering from two economic crises. The first was caused by the formation of the OPEC cartel and the big increases in the price of crude oil; the second, by the collapse of OPEC and the sudden drop in the price of oil. Although early nuclear-power plants were in operation, they were of no use for powering land or air vehicles. This is why the cost of heavy equipment such as troop carriers, artillery, missiles, trucks, tanks, and submarines, not to mention the cost of the newer (late-twentieth-century) types of heavy weapons, was constantly on the rise, even though by then the troop carriers had no one to transport and before long the artillery would have no one to shell.\nThis final phase of the military’s gigantomania in weaponry gave way to a period of microminiaturization under the banner of artificial nonintelligence. Oddly enough, it was only in 2040 that the informationists, cipher theorists, and other experts expressed surprise at how their predecessors could have been so blind for so long, struggling to create artificial intelligence. After all, for the overwhelming majority of tasks performed by people in 97.8 percent of both blue- and white-collar jobs, intelligence was not necessary. What was necessary? A command of the situation, skill, care, and enterprise. All these qualities are found in insects.\nA wasp of the Sphecidae family finds herself a cricket and injects into its nervous system a poison that paralyzes but does not kill. Next she digs a burrow in the sand, sets her victim beside it, enters the burrow to make sure that it is usable – free of dampness or ants – then drags the cricket inside, deposits her egg in it, and flies off to repeat the process. The wasp’s larva will feed on the living body of the cricket until the larva changes into a pupa. The wasp thus displays an excellent command of the situation in the choice of victim and in the anesthetic procedure she performs on it; skill in preparing an enclosure for it; care in checking the enclosure to see that conditions are suitable for her offspring; and enterprise, without which this whole series of activities could never have been carried through. The wasp may have enough nerve tissue to drive a truck from a port to a distant city or to guide a transcontinental rocket. It is only that its nervous system was programmed by natural evolution for completely different tasks.\nSuccessive generations of information theorists and computer scientists had labored in vain to imitate the functions of the human brain in computers; stubbornly they ignored a mechanism a million times simpler than the brain, incredibly small, and remarkably reliable in its operation. Not artificial intelligence but artificial instinct should have been simulated for programming at the outset. Instinct appeared almost a billion years earlier than intelligence – clear proof that it is easier to produce.\nFrom studying the neurology and neuroanatomy of the mindless insect the specialists of the mid-twenty-first century quickly obtained splendid results. Their predecessors were truly blind to overlook the fact that such insects as bees, seemingly primitive creatures, nevertheless possess their own, inherited language, with which the workers in the hive inform one another of the location of newly discovered nectar. Through signal-gesture-pantomime the direction of the path is given, the time required to reach the nectar, and even its relative quantity.\nOf course, the point was not to duplicate wasps, flies, spiders, or bees in computer chips or the like; the important thing was their neural anatomy with its built-in sequences of directed behavior and programmed goals. The result was a scientific-technological revolution that totally and irreversibly transformed the battlegrounds of Earth. Until then, all arms had been fashioned to fit man; their components were tailored to his anatomy, so that he could kill effectively, and to his physiology, so that he could be killed effectively.\nAs so often happened, the beginnings of this complex new trend lay in the twentieth century, but at that time no one was able to combine them into a novel synthesis, because the discoveries that made possible the unhumanization of weapons systems took place in widely separated fields. Military experts had no interest in insects (except the lice, fleas, and other parasites that beset soldiers in wartime). Intellectronics engineers, who with the entomologists and neurologists studied the neurology of insects, knew nothing about military problems. And politicians, true to form, knew nothing about anything.\nThus, while intellectronics was developing microcalculators so small that they competed in size with the nerve bundles of mosquitoes and hornets, the majority of artificial-intelligence enthusiasts were still busy programming computers to carry on stupid conversations with not-too- bright people. The mammoths and dinosaurs of the computer species were beating chess masters not because they were more intelligent but only because they could process data a billion times faster than Einstein. For a long time no one imagined that all the ordinary front-line soldier needed was the skill and enterprise of a bee or a hornet. In basic military operations, intelligence and combat effectiveness are two entirely different things. (Intelligence can actually be a negative factor. In battle, the soldier’s instinct for self-preservation, incomparably greater than a bee’s, can interfere; the bee, on the other hand, will sting to defend its hive though the sting means its own death.) Who knows how long the old-fashioned way of thinking would have continued in the weapons industry – the search for new conventional and unconventional instruments of warfare, the spiraling arms race – had it not been for a few works that directed the public’s attention to a remote and unusual episode in our planet’s history."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html#iv",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html#iv",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "IV",
    "text": "IV\nSixty-five million years ago, on the so-called C-T geological boundary (between the Cretaceous and the Tertiary), a meteorite fell on our planet. It had a diameter of about ten kilometers and contained a considerable amount of iron and iridium. Its mass is estimated to have been over three and a half trillion (3,600,000,000,000) tons. It is unclear whether it was one mass, hence an asteroid from the region between Earth and Mars, or a group of bodies forming the head of a comet. In the geological stratum of this period, iridium and rare earth metals have been discovered in amounts and concentrations not normally found in the Earth’s crust. The absence of an impact crater made it difficult to prove the planetary scale of this cataclysm, since craters that appeared later (caused by meteorites a thousand times smaller) left marks on the Earth’s surface that are clearly visible today. Most likely, this asteroid or comet did not strike any of the continents but landed in the open ocean – or else the collision took place near a junction of continental plates, and the subsequent shifting obliterated the crater.\nA meteor of such size and mass can easily pass through the protective layer of the atmosphere. The energy of the impact, comparable in magnitude to the energy of all the world’s nuclear stockpiles (if not larger), turned that body – or group of bodies – into thousands of billions of tons of dust, which the air currents spread over the entire surface of the Earth, creating a cloud so thick and long-lasting that for at least four months photosynthesis ceased in plants on all continents. Darkness reigned; the land surface, no longer heated by the sun’s rays, grew much colder than did the ocean, which cooled more slowly. Nevertheless, the marine algae, one of the main sources of atmospheric oxygen, also lost their ability to carry on photosynthesis during that time. As a result, an enormous number of plant and animal species became extinct. The most spectacular extinction was that of the giant reptiles commonly called dinosaurs – although at least several hundred other reptile species died out then, too. The catastrophe occurred at a time when the Earth’s climate was gradually cooling, and the large, hairless Mesozoic reptiles found themselves in great difficulty. Even before the cataclysm, their viability had been on the wane for about a million years, as the fossil record reveals. The calcium shells of the dinosaur eggs grew thinner as the millennia passed – testimony to the increasing hardships in feeding and to the worsening climate of the large landmasses.\nComputer simulations of such an event, done back in the 1980s, verified its lethal effect on the biosphere. Strangely enough, the phenomenon to which we owe our emergence as a rational species was not introduced into any school curriculum, even though there was not the slightest doubt about the connection between the Cretaceous-Tertiary saurocide and anthropogenesis.\nPaleontological research toward the end of the twentieth century proved that the dinosaurs were warm-blooded, and that the winged varieties were covered with something very much like feathers. The mammal species that coexisted with these reptiles, having no opportunity to evolve, did not exceed the size of a rat or a squirrel. Competition on land, in the water, and in the air from the powerful, hardy reptiles was too great; the mammals were but an evolutionary footnote to the carnivorous and herbivorous vertebrates of the day.\nThe planetary catastrophe worked against the large animals not directly but indirectly, through the interruption of the food chain in the biosphere. When photosynthesis stopped, vegetation withered on a massive scale, and the large herbivorous reptiles of land, sea, and air could not find enough food. The predators who ate the herbivores perished for the same reason. A huge number of marine animals also died out, because in the oceans the biological carbon cycle proceeds much faster than on land, and because the surface layers of water cooled more quickly than the deeper layers. A few small reptile species did survive. But the small mammal survivors were numerous, and so, when the dust of the meteor settled and the atmosphere cleared and plant life revived, they began to differentiate, branching into many species, which after forty million years produced the line of primates from which Homo sapiens descended.\nThus the cause – indirect but undoubted – of rational man’s emergence was a cataclysm on the C-T boundary. For our subject, however – the military evolution of civilization – it is the consequences of this event, so long overlooked, that are important. The fact is that the ones who suffered least on the C-T boundary were the insects! Before the catastrophe there were about three-quarters of a million insect species; a short time afterward, there were still at least seven hundred thousand, and social insects like ants, termites, and bees survived the cataclysm practically unimpaired. This leads us to conclude that cataclysms are survived most easily and with the greatest probability by small or very small animals with an insectile anatomy and physiology.\nNor should one consider it an accident that insects are generally much less susceptible to the lethal effects of radioactivity than the so-called higher animals, the vertebrates. Paleontology speaks unequivocally. A catastrophe that unleashed the destructive force of a global atomic war killed every one of the large animals but did little damage to the insects and did not touch the bacteria. This shows that the greater the destructive action of an elemental force or technological weapon, the smaller must a system be in order to survive it unharmed. Thus the atomic bomb demanded the dispersal not only of whole armies but also of individual soldiers. General staffs considered dispersing their armies, but in the twentieth century the idea of reducing a soldier to the size of an ant or a wasp found no expression outside the pages of fantasy. A human being couldn’t be reduced or dispersed! In those days much thought was given to soldier-automatons – humanoid robots – a naive anthropomorphism. Yet heavy industry was already undergoing unhumanization, and the robots that replaced people on the assembly lines were not remotely humanoid. They were the enlargement of selected, functional parts of the human being: a computer “brain” with one huge steel hand assembling a car chassis, or a system with a hammer-fist, or with a laser-finger to weld the bodies. These devices worked like eyes and hands but did not look like eyes or hands. But large and heavy robots such as these could not be put on the battlefield, where they would immediately become the target of accurate, self-guided, intelligent missiles.\nSo it was not humanoid automata that formed the new armies but synthetic insects (synsects) – ceramic microcrustacea, titanium annelids, and flying pseudo-hymenoptera with nerve centers made of arsenic compounds and with stingers of heavy, fissionable elements. Most of this “nonliving micropersonnel” could, at the first warning of an atomic attack, dig deep into the ground and then crawl out after the explosion, maintaining combat functions even in an environment glowing with terrible radioactivity, because these soldiers were not only microscopic but nonbiological. The flying synsect combined plane, pilot, and missile in one miniature whole. But the operating unit was the microarmy, which possessed superior combat effectiveness only as a whole (just as a colony of bees was an independent, surviving unit while a single bee was nothing).\nBecause the battlefield was constantly exposed to atomic attack, which not only destroyed combat forces but also disrupted all communications between the various weapons systems (and also between the weapons and their command centers), there arose nonliving microarmies of many types. These were based on two opposing principles.\nAccording to the first – the principle of autonomy – an army proceeded like a column of ants, or a wave of microbes, or a swarm of locusts. The last analogy is particularly apt. The locust, as we know, is simply a biological variety of the common grasshopper (not a separate species); even in clouds numbering hundreds of billions of specimens (still greater numbers have been observed from planes) it is not directly harmful to humans.1 Nevertheless, the sheer mass of a locust cloud can cause a train to derail, turn day into night, and paralyze all movement. (Even a tank, entering a cloud of locusts, will begin to slip as it crushes the insects into a pulp of ichor and grease and will bog down as in a quagmire.) The nonliving, synthetic “locust” was incomparably more lethal, since it was made that way by its designers. It possessed a preprogrammed autonomy, so that communication with a command center was unnecessary. The pseudo-locust could be destroyed, of course, by an atomic attack, but this would have an effect like that of shooting at clouds with nuclear weapons: great holes would open, only to fill again with more cloud.\n1 If one disregards the chief destructive result of such visitations – that is, the total loss of vegetation and cultivated crops.According to the second principle of the new military – telotropism – the microarmy was one giant flowing or flying aggregate of self-assembling elements. It started out dispersed, approaching its objective from many different directions, as strategy or tactics demanded, in order to concentrate into a preprogrammed whole on the battlefield. For this fighting material did not leave the factory in final shape, ready for use, like tanks or guns loaded on a railroad flatcar; the mechanisms were microproductive blocks designed to fuse together into a war machine at the designated place. For this reason, such armies were called “self-bonding.”\nThe simplest example was a self-dispersing atomic weapon. Any missile launched from land, ship, or submarine could be destroyed from space by a satellite laser. But it was impossible to destroy gigantic clouds of microparticles carrying uranium or plutonium that merged into a critical mass only at the target. En route to the target, they were so dispersed as to be indistinguishable from fog or dust.\nThe competition between old and new weapons was brief: massive, armored equipment could not withstand the attacks of the microarmies. Just as germs invisibly invade an organism to kill it from within, so the nonliving, artificial microbes, following the tropisms built into them, penetrated the gun barrels, cartridge chambers, tank and plane engines. They corroded the metal catalytically, or, reaching the powder charges or fuel tanks, blew them up. What could even the bravest soldier, carrying grenades, a machine gun, a bazooka, or any other firearm, do against a nonliving, microscopic enemy? He would be like a doctor trying to fight the bacteria of cholera with a hammer or a revolver.\nAmid a swarm of self-guided, programmed microarms, a man in uniform was as helpless as a Roman legionary with sword and shield against a hail of bullets. In the face of special types of biotropic microarms capable of destroying everything that lived, human beings had no choice but to abandon the battlefield, for they would be killed in seconds.\nEven in the twentieth century, the tactic of fighting in close ranks gave way to the spreading of troops, and in a mobile war the spreading was still greater. But the front lines still existed, separating friend from foe. Now such boundaries disappeared completely.\nA microarmy could easily penetrate all systems of defense and go deep into enemy territory. It had no more trouble accomplishing this than did rain or snow. Meanwhile, high-powered nuclear weapons were proving more and more useless on the battlefield. Imagine, if you will, an attempt to combat a virus epidemic with thermonuclear bombs. It was possible, of course, to scorch a large territory down to a depth of fifty feet, turning it into a vitrified, lifeless desert. But what good was that if on that expanse, one hour later, a military rain began to fall and from it there crystallized detachments of shock troops? Hydrogen bombs were expensive. One didn’t hunt in warships for leeches or sardines.\nThe greatest problem in the unhuman stage of military history was that of distinguishing friend from foe. This task had been accomplished, in the twentieth century, by means of electronic systems working on a password principle. Challenged by radio, a plane or an unmanned missile either radioed the right answer or else was attacked as an enemy craft. This ancient method now proved useless. The new weapon-makers again borrowed from the biosphere – from plants, bacteria, and insects.\nRecognition duplicated the methods of identification used among living species: their immunology – the struggle of antigen with antibody – tropisms, protective coloration, camouflage, and mimicry. The nonliving weapon might imitate (extremely well) floating dust specks or pollen, or gnats, or drops of water. But under that mask lay a corrosive or lethal agent.\nIt should be pointed out that although I am using metaphors from entomology in talking about attacks of artificial locusts or other insects, I do so as a twentieth-century person would describe, to the contemporaries of Vasco Da Cama or Christopher Columbus, a modern city with its automobile traffic. He would speak of carriages and wagons without horses; he would compare airplanes to birds made of metal. In this way he would evoke in the minds of his listeners images that had some connection with reality, albeit an imperfect one. A carriage rolling on large, thin wheels, with high little doors and a dropped step, with a box for the coachman and places at the back for the servants, is not a Fiat or a Mercedes. By the same token, the twenty-first-century synsect weapon is not a swarm of insects just like the ones in an entomologist’s atlas, only made of metal.\nSome of the pseudo-insects could pierce the human body like bullets; others could form optical systems to throw sunlight over wide areas, altering the temperature of large air masses so as to produce heavy rainfall or fair weather, according to the needs of the campaign. There existed “meteorological insects” corresponding to nothing we know today. The endothermic synsects, for example, absorbed large quantities of energy for the sole purpose of causing a sudden drop in temperature over a given area, resulting in a thick fog or the phenomenon known as an inversion. Then there were synsects able to concentrate themselves into a single-use laser beamer; they replaced the artillery of the previous century – although one can hardly speak of replacement, since artillery as we understand it would have been of as much use on the battlefield as slings and catapults. New weapons dictated new conditions of combat and, therefore, new strategy and tactics, both totally unhuman.\nFor those who loved the uniform, the flag, the changing of the guard, standing at attention, drill, medals, and bayonet charges, the new era of war was an affront to their noble ideals, a mockery, a disgrace! The experts of the day called the new military science an “upside-down evolution,” because in nature what came first were the simple, microscopic systems, which then changed over the eons into larger and larger life forms. In the military evolution of the postnuclear period, the exact opposite took place: microminiaturization.\nThe microarmies developed in two stages. In the first stage, the unhumaned microweapons were still designed and built by people. In the second stage, microsoldiers were designed, combat-tested, and sent to be mass-produced by “construction battalions” of nonliving microdesigners.\nA phenomenon known as “sociointegrative degeneration” displaced humans first from the military and later from the weapons industry. The individual soldier degenerated when he ceased to be an intelligent being with a large brain and grew increasingly small and therefore increasingly simple, or when he became disposable, a “single-use soldier”. (Some of the antimilitarists had maintained, long before, that modern warfare’s high mortality rate made “single-use soldiers” of all the combatants, with the exception of the top-ranking officers.) In the end, a microfighter had as much brain as an ant or a termite.\nA greater role, then, was assumed by the pseudo-sociointegrative collective of microsoldiers. Each nonliving army was incomparably more complex than a beehive or an anthill. In internal structure and interrelationships it was more akin to an ecological unit in nature – that is, to those pyramids of plant and animal species that coexist in a specific region or habitat in evolutionary equilibrium, with their antagonisms and symbioses forming a complex network of interdependencies.\nIt is easy to see that in such an army there was nothing for noncommissioned officers to do. A corporal or a sergeant, even a general, could not lead a division of such an army. To grasp the whole picture, as complex as nature itself (although quite dead), the wisdom of a university senate would not have sufficed – even for a mere inspection, much less an actual campaign. Besides the impoverished nations of the Third World, therefore, those who suffered the most from the great military revolution of the twenty-first century were the officer cadres.\nThe twentieth century had already begun the process of destroying them, dispensing with swords, three-cornered hats, and gorgeous uniforms. The final blow, however, was dealt in the twenty-first century by the army’s pseudo-insect evolution – or, rather, involution. The cruel pressure to unhumanize the armies did away with the picturesque traditions of war games, the pageantry of parades (a marching locust, unlike a procession of tanks or rockets, is not a grand sight), the bayonet drills, the bugle calls, the flag raisings and lowerings, the roll calls, the whole rich fabric of barracks life. For a time, high-ranking command positions were kept for people, but not for very long.\nThe strategical-numerical superiority of the computer-produced echelons finally forced even the most competent of commanders, including field marshals, into retirement. A tapestry of ribbons and medals on the chest was no protection against being put out to pasture. In various countries, at that time, a resistance movement developed among career officers. In the desperation of unemployment, they even joined the terrorist underground. It was a malicious trick of history – no one deliberately planned it – that these insurrections were crushed by means of micro-spies and minipolice built on the model of a particular cockroach.\nThis roach, first described in 1981 by an eminent American neuroentomologist, has at the end of its abdomen fine hairs that are sensitive to even the slightest stirring in the air. Connected to a special dorsal nerve bundle, the hairs enable the roach to detect the approach of an enemy, even in complete darkness, and so to flee instantly. The counterparts to these hairs were the electronic picosensors of the minipolicemen who concealed themselves in cracks in old wallpaper at the rebel headquarters.\nBut things were not so good in the affluent nations, either. It was impossible to go on with the old political games. The line between war and peace, increasingly blurred for some time, was now obliterated entirely. The twentieth century had discarded the ritual of formal declarations of war, introducing the sneak attack, the fifth column, mass sabotage, cold war, and war by proxy, but this was only the beginning of the erosion of distinctions.\nA world with two mutually exclusive political conditions – war or peace – changed into a world in which war was peace and peace became war. In the past, when covert agents were all human beings, they hid their mischief behind various masks of respectability and virtue. They infiltrated religious and social movements, including even senior citizens’ choral societies and organizations of matchbox collectors. Later, however, anything could be a covert agent: a nail in the wall, a laundry detergent. Military espionage and sabotage flourished. Since human beings were no longer a real political or military force, there was no point in winning them over with propaganda or in talking them into collaborating with the enemy. Unable to write here about the political changes as much as they warrant, I will convey in a few words the essence of what took place.\nEven in the previous century the politicians of the parliamentary countries could not keep up with everything that was going on in their own countries – much less in the world – and so they had advisers. Every political party had its experts. But the advisers of the different parties said completely different things. With time, computer systems were brought in to help; too late, people realized they were becoming the mouthpieces of their computers. They thought they were the ones doing the reasoning, drawing independent conclusions based on data supplied by computer memory; but in fact they were operating with material preprocessed by the computer centers, and that material was determining human decisions.\nAfter a period of some confusion, the major parties concluded that the expert advisers were dispensable middlemen; from then on, each party headquarters had a main computer. In the second half of the twenty-first century, when a party took power its computer was sometimes given the post of minister without portfolio (a computer did not need a portfolio anyway), and the pivotal role in such democracies was played by programmers. The programmer took a loyalty oath, but that did not prove very effective. Democracy, many warned, was becoming computerocracy.\nFor this reason, too, espionage and counterespionage turned away from politicians and environmental-protection groups (of which there were few, since by then there was not much left to save) and infiltrated the computation and decision centers. Of course, no one could absolutely prove that this was so. Some political scientists maintained that if nation A took over the computerocracy of nation B, and nation B did the same to nation A, then international equilibrium would again be restored. What had become everyday reality could no longer be described in terms of the old, traditional politics, or even by common sense, which still distinguished between natural phenomena, like a hailstorm, and man-made ones, like a bombing attack.\nElections were still held for political parties, but each party boasted of having not the best economic program but the best computer, one that would solve all social ills and problems. Whenever two computers disagreed, the government ostensibly decided; but in reality the arbiter was another computer. It will be better to give a concrete example.\nFor several decades the three major branches of the United States armed forces, the army, navy, and air force, had been struggling among themselves for supremacy. Each tried to get the largest share of the military allocation in the budget at the expense of the others. Each kept its newest weapons secret from the others. To learn these secrets was one of the main tasks of the President’s advisers. Each service had its own headquarters, its own security system, its own codes, and – obviously! – its own computer. Each kept cooperation with the others to the absolute minimum, just enough so the government wouldn’t fall apart. Indeed, the main concern of each successive administration was to see that a minimum of unity was maintained in the government of the country and the conduct of foreign policy.\nEven in the previous century no one knew what the real military strength of the United States was, because that strength was presented to the people differently, depending on whether a White House spokesman was speaking or an opposing presidential candidate. But nowadays the devil himself could not make head or tail of the situation.\nMeanwhile, in addition to computer rule, which was gradually replacing natural, human rule, there appeared certain phenomena that once would have been called natural; but now no one knew by what or by whom they were caused, if indeed they were caused by anything or anyone at all. Acid rain had been known in the twentieth century. But now there were rains so corrosive that they destroyed roads, power lines, and factory roofs, and it was impossible to determine whether they were caused by pollution or by enemy sabotage. It was that way with everything. Livestock were stricken, but was the disease natural or artificial? The hurricane that ravaged the coast – was it a chance thing, or was it engineered by an invisible swarm of micrometeorological agents, each as small as a virus, covertly diverting ocean air masses? Was the drought natural – however murderous – or was it, too, caused by a skillful diversion of the rain clouds?\nThese calamities beset not just the United States but the entire world. Again, some saw in this evidence of their natural origin; others, again, were convinced that the reason they were pandemic was that all countries now had at their disposal unhuman means of striking at any distance and were inflicting damage on one another, while declaring officially that they were doing nothing of the sort. Caught in the act, a saboteur could not be cross-examined: synsects and artificial microbes were mute. Meteorological counterintelligence, seismic espionage, reconnaissance teams of epidemiologists, geneticists, and even hydrographers had their hands full. An ever larger share of world science was enlisted in this military intelligence work. Hurricanes, crop failures, rising mortality rates in cattle, and even meteor showers were suspected of being intentional. (Note, by the way, that the idea of guiding asteroids to fall on enemy territory, causing terrible devastation, had arisen in the twentieth century and was considered interesting.)\nNew disciplines were taught in the military academies: crypto-offensive and crypto-defensive strategies, the cryptology of counter-counterintelligence (the covert enticement-deception of agents raised to the next power), applied enigmatics, and finally “cryptocryptics” which presented in a secret manner the secret use of weapons so secret that there was no way anyone could tell them from innocent phenomena of nature.\nBlurred, also, was the distinction between real and spurious hostilities. In order to turn its people against another nation, a country would produce on its own territory “natural” catastrophes so obviously artificial that its citizens were bound to believe the charge that the enemy was responsible. When it came out that a certain large and wealthy nation, in offering aid to those that were underdeveloped and overpopulated, supplemented the provisions it sold (cheaply) of sago, wheat, corn, and potato flour with a drug that diminished sexual potency, the Third World became enraged. This was now an undercover, antinatural war.\nThus peace was war, and war peace. Although the catastrophic consequences of this trend for the future were clear – a mutual victory indistinguishable from universal destruction – the world continued to move in that fatal direction. It was not a totalitarian conspiracy, as Orwell once imagined, that made peace war, but the technological advances that effaced the boundary between the natural and the artificial in every area of human life, even in extraterrestrial space.\nWhen there is no longer any difference between natural and artificial protein, or between natural and artificial intelligence – say the theoreticians of knowledge, the philosophers – then neither can one distinguish a misfortune that is intentional from one for which no one is to blame.\nAs light, pulled irresistibly into the heart of a stellar black hole, cannot escape that gravitational trap, so humanity, pulled by the forces of mutual antagonism into the heart of matter’s secrets, fell into the trap of technology, a trap of its own making. The decision to invest everything in new weapons was not made by governments, statesmen, generals, corporate interests, or pressure groups, but by the ever-growing fear that someone else would be first to hit upon the discoveries and technologies affording the ultimate advantage. This paralyzed traditional politics. The negotiators at summit meetings could not negotiate, because their willingness to relinquish a new weapon would only indicate, in the eyes of the other side, that they had another, newer weapon up their sleeve…\nBy now the impossibility of disarmament had been proved mathematically. I have seen the mathematical model of the so-called general theory of conflicts; it shows why arms talks cannot produce results. At summit meetings certain decisions are reached. But when it takes longer to reach a decision promoting peace than it does to develop the kind of military innovations that radically change the very situation under negotiation, then any decision, at the moment of its acceptance, is an anachronism.\nIt is as if the ancients had debated so long about banning their “Greek fire” that by the time they agreed to ban it, Berthold Schwarz had appeared with his gunpowder. When one decides “today” about something that existed “yesterday,” the decision moves from the present into the past and thereby becomes an empty game.\nIt was this that finally, at the end of the twenty-first century, forced the world powers into a new type of agreement, an agreement that opened up a new era in the history of the human race. But that is a subject that belongs to the twenty-second century and therefore lies outside the scope of these remarks. Later, if I am able, I will devote a separate discussion to it – describing the next chapter of general history, a remarkable chapter, in which Earth, emerging from the era of antagonisms, truly frees itself from one technological trap, but steps into another, as if her destiny is to go forever from the frying pan into the fire."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html#excerpt-from-peace-on-earth-1985",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html#excerpt-from-peace-on-earth-1985",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "Excerpt from Peace on Earth (1985)",
    "text": "Excerpt from Peace on Earth (1985)\nOn my return home I found a fat envelope in my mailbox. It contained a book with the title _The Unhumanization of Weapon Systems in the Twenty-first Century _or _Upside-down Evolution. _The author’s name, Meslant, told me nothing. It was a heavy book, full of graphs and tables. Having nothing better to do, I sat down and began to read. On the first page, before the introduction, was an epigraph in German:\n\nAus Angst und Not\nDas Heer ward tot.\n— Eugen von Wahnzenstein\n\n[From fear and dread / The army fell dead.]\nThe author presented himself as an expert in contemporary military history. His subject, the new pacifism at the close of the twentieth century: it was prosperity and cowardice that gave birth to the unhumanization of war. People were increasingly reluctant to be fired upon, and this loss of martial spirit was directly proportional to the standard of living. The youth of wealthy nations weren’t interested in the noble motto Duke et decorum est pro patria mori. And it was just at that time that prices began falling in the intellectronics industry. Microprocessing elements called chips were replaced by corn, the product of the genetic engineering of a culture of artificial microbes, mainly Silicobacterium logicum wieneri, named after the father of cybernetics. A handful of these elements cost no more than a handful of barley. Thus artificial intelligence grew cheap, yet the price of new weapons increased geometrically. In the first world war a plane cost as much as a car, in the second as much as twenty, and by the end of the century it was six hundred times more. It was calculated that in seventy years a superpower would be able to afford from eighteen to twenty-two planes. The falling curve of the cost of intelligence and the rising curve of the cost of weapons intersected, and at that point began the unhumanization of the armies. The military became unliving. At which juncture the world went through two crises. The first was when the price of oil soared, the second when shortly thereafter it plunged. The classic rules of economics went out the window, but few understood what was happening, or that the image of a soldier in uniform and helmet charging with bayonet was becoming as distant as the medieval knight in armor. Out of inertia the engineers continued to make big-gauge weapons for a while: tanks, cannon, carriers, and other fighting machines to be used by men, though already these could have gone into battle themselves, without them. Then followed a phase of accelerated miniaturization. Until now all weaponry had been fashioned to fit man: Tailored to his anatomy and physiology, so that he could kill and be killed.\nAs is usually the case in history, no one saw what lay ahead, for the discoveries which were to make possible the unhumanization of weapons took place in fields of science far from one another. Intellectronics produced microcomputers as cheap as grass, and neuroentomology finally solved the riddle of social insects who live and work together, communicating in their own language, even though bees, for example, have a nervous system 380,000 times smaller than a human brain. The intelligence of a bee is quite sufficient for a foot soldier, as military prowess and intelligence are two different things, at least on the battlefield. The major factor in the push for miniaturization was the atom bomb. The need to miniaturize came from simple facts, but facts that lay outside the military knowledge of the day. Seventy million years ago a huge meteor hit Earth and chilled its climate for centuries, making the dinosaurs defunct but hardly bothering insects and not even touching bacteria. The lesson of paleontology was clear: the greater the destructive force, the smaller the systems that escape it. The atom bomb required the particularization of both soldier and army. But in the twentieth century the idea of making soldiers the size of ants was only a fantasy. You could not reduce people in size or diffuse them. So thought was given to robot soldiers, humanoid, though even then that was a naive anachronism. Industry was unhumanizing itself, but the robots who replaced workers on the assembly line were not made in the likeness of men; they were, rather, human parts selected and enlarged: a brain with a big steel hand or a brain with eyes. But giant robots had no place on an atomic battlefield. So radioactive synsects (synthetic insects) were developed, and ceramic shellfish, and titanium worms able to burrow in the earth and come out after the blast. Flying synsects were airplane, pilot, and missiles all in one tiny entity. The operational unit became a microarmy, fighting only as a whole, much as a swarm of bees acts as a unit to survive while an individual bee is nothing. Thus microarmies of many, kinds were made, on two opposing principles. An army based on the principle of independence proceeded like a column of ants or a cloud of germs or hornets. An army based on the principle of teletopism, however, was an enormous flying or crawling collection of self-assembling elements; according to need, tactical or strategic, it could reach its target in extreme attenuation only to condense there into its programmed whole. The simplest example was the self-dispersing atomic warhead. An ICBM could be tracked, from space by satellite or from Earth using radar; but it was impossible to detect a cloud of infinitesimal particles of uranium or plutonium at very low density, which finally would converge and reach critical mass at its target, a factory or an enemy city.\nFor a while the old and new weapons coexisted, but the massive machines soon succumbed to the invisible micro. As germs secretly enter an animal organism and kill it from within, so did these unliving microbes penetrate cannon barrels, shell chambers, the engines of tanks and planes, and eat through metal, and detonate the ammunition inside. What could a brave, grenade-carrying soldier do against a microscopic, unliving adversary? He would be like a doctor trying to fight a virus with a hammer. Against an autonomous cloud programmed to destroy all things biological a man in uniform was as helpless as a Roman legionary standing with sword and shield before a rain of bullets.\nEven in the twentieth century the tactic of fighting in closed ranks was replaced by the spreading out of troops, but there were still front lines. Now there were none. Microarmies easily penetrated all defenses. Nuclear weapons were ineffective combatting that viral contagion. The cost of a warhead, moreover, cannot be considerably greater than the value of its target. One doesn’t use a destroyer to go after leeches.\nThe most vexing problem in this unhumanized stage of man’s struggle with man turned out to be that of telling friend from foe. In the past it had been done by electronics, using the password principle. Challenged by radio waves, a plane or missile either transmitted back the correct answer or was attacked. This twentieth-century method became obsolete. Now the makers of arms borrowed from the plants and animals, from bacteria. For the recognition process they imitated the ways of living species: immune systems, the duel of antigen and antibody, tropism, mimicry, protective coloration, camouflage. A micro-weapon might pretend to be an innocent microorganism, or the fluff of a plant, a piece of pollen, but beneath that exterior lay corrosive death. The significance of informational combat also increased – not in the sense now of propaganda but as the invasion of enemy communications, to paralyze it or, as in the case of the atomic locust-cloud, to force premature condensation to critical mass before it reached its target. The author of the book discussed the cockroach, which was the prototype for one kind of micro-soldier. On its abdomen the cockroach has very fine hairs. When they are moved by the air, the insect flees, because these sensors are wired directly to its rear nerve ganglion, and they can distinguish between a draft and the disturbance caused by a predator.\nAs I read, I felt pity for the champions of uniforms, flags, and medals for bravery: the new era of warfare must have been anathema to their high ideals. The audior used the term upside-down evolution, because in the beginning of life there were microscopic systems which slowly changed into larger systems, while this military evolution proceeded the other way, microminiaturization, and the great human brain was replaced by mechanical insect ganglia. Microarmies arose in two steps. First, the designers and builders were still human; then the unhumanized divisions were conceived, battle-tested, and put into mass production by computer systems that were equally nonhuman. People were eliminated from the military and then from the weapons industry by a phenomenon called “sociointegrational degeneration.” The individual soldier underwent degeneration: he became smaller and simpler. In the end he had the intelligence of an ant or termite. But the collective of these tiny warriors assumed a greater role. The nonliving army was far more complex than a beehive or ant hill; it was more like a biotope in nature, an ecosystem, a subtle equilibrium between competitive, antagonistic, and symbiotic species. A sergeant or corporal in such an army obviously had nothing to do. To grasp the whole picture, merely to inspect the troops, not even the brain power of an entire university would suffice. Thus officers as well as poor Third World countries did not fare well during the great military revolution of the twenty-first century. The irresistible momentum of army unhumanization destroyed the lofty traditions of maneuvers, marches, drills, changing the guard, and regalia. For a while but alas not for long, it was possible to preserve the highest ranks for people, but the strategy-computational superiority of the computerized echelons of command finally put even the most corpulent leaders, including four-star generals, out of work. A chest of ribbons and medals was no protection from early retirement. These officers, facing permanent unemployment – for they could do nothing else – revolted, forming an underground terrorist movement. The crushing of this revolt with the use of microspies and minipolice built on the abovementioned cockroach principle was a grim chapter in our history, because neither cover of dark nor mist nor any kind of camouflage could save those desperate traditionalists loyal to the ideas of Achilles and Clausewitz.\nAs for the poor countries, they could go on fighting as before, using live people, but only against opponents as anachronistic as themselves. Those who couldn’t automate militarily had to sit quietly in the corner.\nBut it wasn’t fun for the rich countries either. The old political games went out the window. The line separating war and peace, having long been blurry, was now completely erased. The twentieth century had dispensed with the formal declaration of war and introduced the fifth column, sabotage, cold war, and war by proxy, but that was only the beginning. Summit meetings for disarmament pursued mutual understanding and a balance of power but were also held to learn the strengths and weaknesses of the enemy. The world of the war-or-peace alternative became a world in which war was peace and peace war. First, wide-range subversive activity was conducted under the mask of official peace: the infiltration of political, religious, and social movements, even such worthy movements as those to protect the environment; the infiltration also of the culture and mass media, taking advantage of the illusions of the young and the conservatism of the old. Then covert military activity intensified to the point of being overt, except that it was invisible. Acid rain had been known in the twentieth century; now rain fell that was so corrosive, it destroyed the roofs of houses and factories, roads, electrical lines, but no one knew whether this was pollution or the enemy sending poison clouds their way. It was thus with everything. Farm animals died in epidemics that could have been natural or intentional. A storm that flooded the coast might have been an act of God or the clever redirection of a hurricane at sea. A drought – a normal disaster, or one caused by the secret switching of heavy clouds with light. With seismic, meteorological, and epidemiological counterespionage and reconnaissance the scientists had their hands full. More and more of the scientific community became involved with intelligence work, yet the results of their research grew less and less clear. The tracking down of saboteurs was child’s play in the days when they were human; but now, when the suspect was a hurricane or hailstorm, or a crop or cattle disease, or the rise in infant mortality or cancer rate, or even a meteor (the twentieth century had already considered the idea of aiming an asteroid at enemy territory), life became intolerable. Intolerable not only for the man on the street but also for heads of state, who were helpless and confused, and their advisors no less. The military academies added new courses such as cryptotactics, cryptocountering (that is, taking counterespionage to the n-th power), crypto field theory, and finally cryptocryptics, the secret study of the secret use of secret weapons indistinguishable from natural phenomena.\nTo blacken the enemy, one could fake a natural disaster in one’s own country – in a way that made it obvious it was not natural. Also it was proved that certain rich nations, helping those less fortunate, put a drug into the supplies of wheat, corn, and cocoa it sold (cheaply), which caused impotence. This was, then, a secret war of birth control. Although the catastrophic consequence of such escalation was obvious – wherein victory was equivalent to defeat for both sides – the politicians continued with business as usual, concerned more about the voters than the future, making fuzzy promises and being increasingly less able to affect the course of real events. War was peace, not from Orwell’s totalitarian doublethink but because of a technology that erased the boundary between natural and artificial in every aspect of human life and its environment.\nWhere there is no difference, wrote the author, between natural and artificial protein or between natural and artificial intelligence, misfortunes caused deliberately cannot be distinguished from those caused by no one. Just as light that falls into a black hole cannot pull itself out of that gravitational trap, so humanity at war, reaching the secrets of matter, cannot leave the trap of technology. It wasn’t the governments, heads of state, monopolies, generals, or pressure groups that made the decision to invest everything in the new arms, it was fear, fear that the Other Side would discover, invent, and develop first. Traditional politics were useless. Negotiators could negotiate nothing, because the offer to give up a new weapon only meant, for the other side, that you had a weapon that was even newer. I turned to a mathematical model of conflict theory which showed why further summit meetings were a waste of time. At such meetings, agreements were reached. But it took longer to reach an agreement than for a new development to change radically the situation on which that agreement was based, thus making it an immediate anachronism. The act of reaching an agreement, then, was an empty game of appearances. This was what compelled the world powers to accept the Geneva Agreement: an exodus of weapons to the moon. The world breathed a sigh of relief, but not for long, because fear returned – now as the specter of the nonhuman invasion of Earth by the moon. So there was no task more urgent than to pierce the mystery of the moon.\nWith these words the chapter ended. There were a few dozen pages left in the book, but they wouldn’t turn. As if they were glued. Stuck together with bookbinder’s glue, I thought. I couldn’t separate them, so finally I took a knife and slid it carefully between the pages. The first page was blank, but where the knife touched it, letters formed. I rubbed the paper with the knife until I obtained the following message: “Are you ready to assume this burden? If not, put the book back in the envelope! If yes, turn the page!”\nThe next page was also blank. I ran the blade from top to bottom and eight numbers appeared, in groups of two with hyphens between them like a telephone number. I separated the remaining pages but there was nothing on them. “A curious way to recruit Savers of the World!” I thought. At the same time I began to suspect what lay in store. I closed the book but it opened again by itself at the page with the numbers. Nothing was left for me but to pick up the phone and dial."
  },
  {
    "objectID": "docs/posts/1983-lem-upside-down-revolution/index.html#metadata",
    "href": "docs/posts/1983-lem-upside-down-revolution/index.html#metadata",
    "title": "Weapon Systems of The Twenty First Century",
    "section": "Metadata",
    "text": "Metadata\n\nOriginally published as Waffensysteme des 21. Jahrhunderts oder The Upside Down Evolution, in German. Published in English in 1986 in One Human Minute, which is a collection of 3 book reviews of 3 fictitious books.\nThe excerpt is from the novel Peace on Earth (1985)."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html",
    "href": "docs/posts/1989-hugo-de-garis/index.html",
    "title": "The 21st Century Artilect",
    "section": "",
    "text": "Within one to two human generations, it is likely that computer technology will be capable of building brain-like computers containing millions if not billions of artificial neurons. This development will allow neuroengineers and neurophysiologists to combine forces to discover the principles of the functioning of the human brain. These principles will then be translated into more sophisticated computer architectures, until a point is reached in the 2Ist Century when the primary global politica! issue will become, “Who or what is to be dominant species on this planet — human beings, or artilects (artificial intellects)?”\nA new branch of applied moral philosophy is needed to study the profound implications of the prospect of life in a world in which it is generally recognised to be only a question of time before our computers become smarter than we are. Since human beings could never be sure of the attitudes of advanced artilects towards us, due to their unfathomable complexity and possible “Darwinian” self modification, the prospect of possible hostilities between human beings and artilects cannot be excluded.\nKeywords: Artilect, Ultra Intelligent Machine, Neuro-Engineering, Dominant Species, Artificial Neuron."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#abstract",
    "href": "docs/posts/1989-hugo-de-garis/index.html#abstract",
    "title": "The 21st Century Artilect",
    "section": "",
    "text": "Within one to two human generations, it is likely that computer technology will be capable of building brain-like computers containing millions if not billions of artificial neurons. This development will allow neuroengineers and neurophysiologists to combine forces to discover the principles of the functioning of the human brain. These principles will then be translated into more sophisticated computer architectures, until a point is reached in the 2Ist Century when the primary global politica! issue will become, “Who or what is to be dominant species on this planet — human beings, or artilects (artificial intellects)?”\nA new branch of applied moral philosophy is needed to study the profound implications of the prospect of life in a world in which it is generally recognised to be only a question of time before our computers become smarter than we are. Since human beings could never be sure of the attitudes of advanced artilects towards us, due to their unfathomable complexity and possible “Darwinian” self modification, the prospect of possible hostilities between human beings and artilects cannot be excluded.\nKeywords: Artilect, Ultra Intelligent Machine, Neuro-Engineering, Dominant Species, Artificial Neuron."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#introduction",
    "href": "docs/posts/1989-hugo-de-garis/index.html#introduction",
    "title": "The 21st Century Artilect",
    "section": "Introduction",
    "text": "Introduction\nArtificial Intelligence is a branch of computer science which is at tempting to make machines intelligent, and in so doing, to cast light on the mysteries of biological intelligence. This valiant enterprise has not escaped the critical eye of the philosophers over the years, some of whom have attempted to show that certain claims of the intelligists (AI researchers) are excessive. (See for example, Searle 1981, Dennett 1981, Dreyfus 1986 and the replies of the intelligists, Hofstadter 1981, Gregory 1987). However, this article does not address itself to such traditional “philosophical-ΑI” concerns as the mind-brain distinction, the freedom of the will, or the impossibility or otherwise of artificial intelligence. It assumes that artificial intelligence is a reasonable endeavor, and raises new questions concerning the moral consequences for humanity when AI eventually succeeds.\nA revolution is taking place in the field of Artificial Intelligence. This revolution, called “Connectionism”, attempts to understand the junctioning of the human brain in terms of interactions between artificial abstract neuron-like components, and hopes to provide computer science with design principles sufficiently powerful to be able to build genuine artificial electronic (optical, molecular) brains (Kohonen 1987, McClelland et al 1986, Mead 1987). Progress in micro electronics and related fields, such as optical Computing, has been so impressive over the last few years, that the possibility of building a true artilect within a human generation or two becomes a real possibility and not merely a science fiction pipe dream.\nHowever, if the idea of the 21st Century artilect is to be taken seriously (and a growing number of Artificial Intelligence specialists are doing just that (Michie 1974, Waltz 1988, de Garis 1989)), then a large number of profound political and philosophical questions arise. This paper addresses itself to some of the philosophical and moral issues concerning the fundamental question “Who or what is to be dominant species on this planet — human beings or the artilects?”"
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#a-moral-dilemma",
    "href": "docs/posts/1989-hugo-de-garis/index.html#a-moral-dilemma",
    "title": "The 21st Century Artilect",
    "section": "A Moral Dilemma",
    "text": "A Moral Dilemma\nIn order to understand the disquiet which has been growing amongst an increasing number of intelligists (specialists in Artificial Intelligence) around the world in the late 1980s (Waltz 1988, de Garis 1989), it is useful to make a historical analogy with the development of the awareness of the nuclear physicists in the 1930s, of the possibility of a chain reaction when Splitting the uranium atom. At the time, that is immediately after the announcement of the Splitting, very few nuclear physicists thought hard about the consequences to humanity of life in a nuclear age and the possibility of a large scale nuclear war in which billions of human beings would die.\nSome intelligists feel that a similar situation is developing now with the connectionist revolution. The intelligists concerned, are worried that if the artificial intelligence community simply rushes ahead with the construction of increasingly sophisticated artilects, without thinking about the possible long term political, social and philosophical consequences, then humanity may end up in the same sort of diabolical situation as in the present era of possible nuclear holocaust.\nWithin a single human generation, computer scientists will be building brain-like computers based on the technology of the 21st Century. These true “electronic (optical, molecular) brains” will allow neurophysiologists to perform experiments on machines instead of being confined to biological specimens. The marriage between neuroengineers and neurophysiologists will be extremely fruitful and artificial intelligence can expect to make rapid progress towards its long term goal of building a machine that can “think”, a machine usually called an “artificial intelligence”, or “artilect”. However, since an artilect is, by definition, highly intelligent, (and in the limit, ultra intelligent, that is, having an intelligence which is orders of magnitude superior to ours), if ever such a machine should turn against humanity, it could be extremely dangerous. An atomic bomb has the enormous advantage, from the point of view of human beings, of being totally stupid. It has no intelligence. It is human beings who control it. But an artilect is a different kettle of fish entirely.\nArtilects, unlike the human species, will probably be capable of extremely rapid evolution and will, in a very short time (as judged by human standards), reach a state of sophistication beyond human comprehension. Remember, that human neurons communicate at hundreds of meters per second, whereas electronic components communicate near the speed of light, a million times faster. Remember, that our brains, although containing some trillion neurons, has a fixed architecture, as specified by our genes. The artilects could choose to undertake “Darwinian experiments” on themselves, or parts of themselves, and incorporate the more successful results into their structure. Artilects have no obvious limit as to the number of components they may choose to integrate into themselves. To them, our trillion neurons may seem puny. Not only may artilects be superior to humans in quantitative terms, they may be greatly our superiors in qualitative terms as well. They may discover whole new principles of “intelligence theory” which they may use in restructuring themselves. This continuous updating may grow exponentially — the smarter the machine, the better and faster the redesigning phase, so that a take-off point may be reached, beyond which, we human beings will appear to artilects as mice do to us.\nThis notion of Darwinian experimentation is important in this discussion, because it runs counter to the opinions of many people who believe (rather naively, in my view) that it will be possible to construct artilects which will obey human commands with docility. Such machines are not artilects according to my conception of the word.\nI accept that machines will be built which will show some obvious signs of real intelligence and yet remain totally obedient. However, this is not the issue being discussed in this paper. What worries me is the type of machine which is so smart that it is capable of modifying itself, of searching out new structures and behaviours, that is, the “Darwinian Artilect”.\nSince any machine, no matter how intelligent, is subject to the same physical laws as is any other material object in the universe, there will be upper limits to the level of self-control of its intellectual functions. At some level in its architectural design, there will be “givens”, that is, top level structures determining the artilect’s functioning, which are not “judged” by any higher level structures. If the artilect is to modify these top level structures, how can it judge the quality of the change? What is meant by quality in such a context?\nThis problem is universal for biological systems. Quality, in a biological context, is defined as increased survivability. Structural innovations such as reproduction, mutation, sex, death, etc., are ail “measured” according to the survivability criterion. It is just possible that there may be no other alternative for the artilect, than taking the same route. Survivability however, only has meaning in a context in which the concept of death has meaning. But would not an artilect be essentially immortal, as are cancer cells, and would a folly autonomous artilect, resulting from an artilectual reproductive process, but with modified structures, accept being “termina ted” by its “parent” artilects, if the latter consider the experiment to have been a failure?\nIf the offspring artilects do not agree to being “killed”, they might be allowed to live, but this would imply that every artilectual experiment would create a new immortal being, which would consume scarce re sources. There seem to be at least three possible solutions to this problem. Either a limit is placed on the number of experiments being performed, a philosophy inevitably leading to evolutionary stagnation, or artilects are replaced, by newer versions, (processes called reproduction and death, in biological contexts), or the growing population of artilects could under take a mass migration into the cosmos in search of other resources. This Darwinian modification is, by its nature, random and chancy. The problem for human beings is that an artilect, by definition, is beyond our control. As human beings, with our feeble intellects (by artilectual standards), we are unable to understand the implications of structural changes to the artilect’s “brain”, because this requires a greater intellect than we possess. We can only sit back and observe the impact of artilectual change upon us. But this change may not necessarily be to our advantage. The “moral circuits” of the artilects may change so that they no longer feel any “sympathy” for human beings and decide that, given a materials shortage on the planet, it might be advisable, from an artilectual point of view, to reduce the “ecological load” by removing the “hungriest” of the inferior species, namely human beings.\nSince human moral attitudes, like any other psychological attitudes, are ultimately physical/chemical phenomena, human beings could not be sure of the attitudes of artilects towards human beings, once the artilects had evolved to a highly advanced State. What human beings consider as moral is merely the result of our biological evolution. As human beings we have no qualms about killing mosquitoes or cattle. To us, they are such inferior creatures we do not question our power of life or death over them. This uncertainty raises the inevitable fear of the unknown in human beings. With artilects undertaking experiments to “improve” themselves (however the artilects define improvement), we humans could never be sure that the changing intelligences and attitudes of the artilects would remain favorable to us, even if we humans did our best to instil some sort of initial “Asimovian”, human-oriented moral code into them. Personally, I believe that Asimov’s “Three Laws of Robotics” are inappropriate for machines making random changes to themselves to see whether they lead to “improvements”. Asimov’s robots were not artilects."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#a-world-divided",
    "href": "docs/posts/1989-hugo-de-garis/index.html#a-world-divided",
    "title": "The 21st Century Artilect",
    "section": "A World Divided",
    "text": "A World Divided\nWith many intelligists agreeing that it will be technologically possible to build electronic (optical, molecular) brains within a human generation or two, what are the moral problems presented to humanity, and particularly to applied moral philosophers? The biggest question in many peoples minds will be, “Do we, or do we not, allow such artilects to be built?” Given the time frame we are talking about, namely 20 to 50 years from now, it is unlikely that human societies will have evolved sufficiently to have formed a world State, having the power to enforce a world wid ban on artilectual development, beyond an agreed point. What wi probably happen, is that military powers will argue that they cannot afford to stop the development of artilects, in case the “other side” creates smarter “soldier robots” than they do. Military/political pressures may ensure artilect funding and research until it is too late.\nThe artilect question alone, is sufficient in itself, to provide a very strong motivation for the establishment of a skeleton world government within the next human generation. With the rapid development of global tel communications and the corresponding development of a world language the establishment of a skeleton world government within such a short time may not be as naive as it sounds.\nFor the purposes of discussion, imagine that such a ban, or at least a moratorium, on artilectual development is established. Should such a ban remain in force forever? Could one not argue that mankind has not onl the power, but the moral duty to initiate the next major phase in evolution and that it would be a “crime” on a universal or cosmic scale not to exercise that power?\nOne can imagine new ideological political factions being established, comparable with the capitalist/communist factions of the 20th Century. Those in favour of giving the artilects freedom to evolve as they wish, I have labelled the “Cosmists”, and those opposed, I have labelled the “Terras” (or Terrestrialists). I envisage a bitter ideological conflict bet ween these two groups, taking on a planetary and military scale. The Cosmists are so named because of the idea that it is unlikely, once the artilects have evolved beyond a certain point, that they will want to remain on this provincial little planet we call Earth. After all, there are some trillion trillion other stars to choose from. It seems more credible that the artilects will leave our planet and move into the Cosmos, perhaps in search of other ultraintelligences.\nThe Terras are so named because they wish to remain dominant on this planet. Their horizons are terrestrial. To the Cosmists, this attitude is provincial in the extreme. To the Terras, the aspirations of the Cosmists are fraught with danger, and are to be resisted at any cost. The survival of humanity is at stake.\nThere may be a way out of this moral dilemma. With 21st Century Space technology, it may be entirely feasible to transport whole populations of Cosmist scientists and technicians to some distant planet, where they can build their artilects and suffer the consequences. However, even this opinion may be too risky for some Terran politicians, because the artilects may choose to return to the Earth, and with their superior intellects, they could easily overcome the military precautions installed by the Terras."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#summary",
    "href": "docs/posts/1989-hugo-de-garis/index.html#summary",
    "title": "The 21st Century Artilect",
    "section": "Summary",
    "text": "Summary\nThis article claims that intelligists will be able to construct true electronic (optical, molecular) brains, called artilects, within one to two human generations. It is argued that this possibility is not a piece of science fiction, but is an opinion held by a growing number of professional intelligists. This prospect raises the moral dilemma of whether human beings should or should not allow the artilects to be built, and whether they should or should not be allowed to modify themselves into super beings, beyond human comprehension. This dilemma will probably dominate political and philosophical discussion in the 2Ist Century. A new branch of applied moral philosophy needs to be established to consider the artilect problem."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#references",
    "href": "docs/posts/1989-hugo-de-garis/index.html#references",
    "title": "The 21st Century Artilect",
    "section": "References",
    "text": "References\n\n(de Garis 1989) “What if AI Succeeds? The Rise of the 21st Century Artilect” ; Artificial Intelligence Magazine, Summer 1989.\n(Dennet 1981) Brainstorms – Philosophical Essays on Mind and Psychology, D. C. Dennet, M.I.T. Press, 1981\n(Dreyfus 1986) Mind Over Machine, Dreyfus H., Blackwell Oxford, 1986\n(Evans 1979) The Mighty Micro, Evans C., Coronet Books, London.\n(Gregory 1987) “In Defense of Artificial Intelligence - A Reply to John Searle”, R. Gregory, in Mindwaves, eds C. Blakemore and S. Greenfield, Blackwell, 1987\n(Hofstadter 1981) The Mind’s I, Hofstadter D. R., Bantam, 1981\n(Jastrow 1981) The Enchanted Loom, Simon & Schuster, New York.\n(Kelly 1987) “Intelligent Machines. What Chance?”, Advances in Artificial Intelligence, Wiley.\n(Kohonen 1987) Self-Organization and Associative Memory, 2nd edn. Kohonen T., Springer-Verlag, Belin, Heidelberg.\n(McClelland et al 1986) Parallel Distributed Processing, Vols 1 and 2, McClelland J. L. & Rumelhart D. E. (Eds), MIT Press, Cambridge, Mass.\n(McCorduck 1979) Forging the Gods, in Machines Who Think, Freeman (Mead 1987) Analog VLSI and Neural Systems, Mead C., Addison Wesley, Reading, Mass.\n(Michie 1974) On Machine Intelligence, Michie D., Edinburgh University Press, Edinburgh\n(Searle 1981) “Minds, Brains, and Programs”, Searle J., in The Minds’s I, see (Hofstadter 1981)\n(Waltz 1988) “The Prospects for Building Truly Intelligent Machines”, Waltz D., in The Artificial Intelligence Debate, Cambridge, Mass., MIT Press."
  },
  {
    "objectID": "docs/posts/1989-hugo-de-garis/index.html#appendix-metadata",
    "href": "docs/posts/1989-hugo-de-garis/index.html#appendix-metadata",
    "title": "The 21st Century Artilect",
    "section": "Appendix: Metadata",
    "text": "Appendix: Metadata\nThis essay was written in 1989-05 and published in 1990 as De Garis, Hugo. “The 21st Century Artilect Moral Dilemmas Concerning the Ultra Intelligent Machine.” Revue Internationale de Philosophie (1990): 131-138.\nIt was hosted as a plaintext file at his homepage, with the following author’s information:\nDr. Hugo de Garis,\nHead, Brain Builder Group,\nEvolutionary Systems Department,\nATR Human Information Processing Research Labs,\n2-2 Hikaridai, Seika-cho, Soraku-gun,\nKansai Science City, Kyoto-fu, 619-02, Japan.\ntel. + 81 774 95 1079,\nfax. + 81 774 95 1008,\nemail. degaris@hip.atr.co.jp\nweb. http://www.hip.atr.co.jp/~degaris"
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html",
    "href": "docs/posts/1993-vernor-vinge/index.html",
    "title": "The Coming Technological Singularity",
    "section": "",
    "text": "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nIs such progress avoidable? If not to be avoided, can events be guided so that we may survive?These questions are investigated. Some possible answers (and some further dangers) are presented."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#abstract",
    "href": "docs/posts/1993-vernor-vinge/index.html#abstract",
    "title": "The Coming Technological Singularity",
    "section": "",
    "text": "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\nIs such progress avoidable? If not to be avoided, can events be guided so that we may survive?These questions are investigated. Some possible answers (and some further dangers) are presented."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#what-is-the-singularity",
    "href": "docs/posts/1993-vernor-vinge/index.html#what-is-the-singularity",
    "title": "The Coming Technological Singularity",
    "section": "What is The Singularity?",
    "text": "What is The Singularity?\nThe acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur):\n\nThe development of computers that are “awake” and superhumanly intelligent. (To date, most controversy in the area of AI relates to whether we can create human equivalence in a machine. But if the answer is “yes, we can”, then there is little doubt that beings more intelligent can be constructed shortly thereafter.\nLarge computer networks (and their associated users) may “wake up” as a superhumanly intelligent entity.\nComputer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent.\nBiological science may find ways to improve upon the natural human intellect.\n\nThe first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades 1. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Charles Platt 2 has pointed out the AI enthusiasts have been making claims like this for the last thirty years. Just so I’m not guilty of a relative-time ambiguity, let me more specific: I’ll be surprised if this event occurs before 2005 or after 2030.)\n1 Moravec, Hans, Mind Children, Harvard University Press, 1988.2 Platt, Charles, Private Communication.What are the consequences of this event? When greater-than-human intelligence drives progress, that progress will be much more rapid. In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities – on a still-shorter time scale. The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often no faster than natural selection can do its work – the world acts as its own simulator in the case of natural selection. We humans have the ability to internalize the world and conduct “what if’s” in our heads; we can solve many problems thousands of times faster than natural selection. Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals.\nFrom the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were thought might only happen in “a million years” (if ever) will likely happen in the next century. (In 3, Greg Bear paints a picture of the major changes happening in a matter of hours.)\n3 Bear, Greg, “Blood Music”, Analog Science Fiction-Science Fact, June, 1983. Expanded into the novel Blood Music, Morrow, 19854 Ulam, S., Tribute to John von Neumann, Bulletin of the American Mathematical Society, vol 64, nr 3, part 2, May, 1958, p1-49.I think it’s fair to call this event a singularity (“the Singularity” for the purposes of this paper). It is a point where our models must be discarded and a new reality rules. As we move closer and closer to this point, it will loom vaster and vaster over human affairs till the notion becomes a commonplace. Yet when it finally happens it may still be a great surprise and a greater unknown. In the 1950s there were very few who saw it: Stanisław Ulam 4 paraphrased John von Neumann as saying:\n\nOne conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.\n\nVon Neumann even uses the term “singularity”, though it appears he is still thinking of normal progress, not the creation of superhuman intellect. (For me, the superhumanity is the essence of the Singularity. Without that we would get a glut of technical riches, never properly absorbed 5.)\n5 Stent, Gunther S., The Coming of the Golden Age: A View of the End of Progress, The Natural History Press, 1969.6 Good, I. J., “Speculations Concerning the First Ultraintelligent Machine”, in Advances in Computers, vol 6, Franz L. Alt and Morris Rubinoff, eds, pp31-88, 1965, Academic Press.In the 1960s there was recognition of some of the implications of superhuman intelligence. I. J. Good wrote 6:\n\nLet an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. … It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make.\n\nGood has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent machine of the sort he describes would not be humankind’s “tool” – any more than humans are the tools of rabbits or robins or chimpanzees. Through the ’60s and ’70s and ’80s, recognition of the cataclysm spread 7 8 9 10. Perhaps it was the science-fiction writers who felt the first concrete impact. After all, the “hard” science-fiction writers are the ones who try to write specific stories about all that technology may do for us. More and more, these writers felt an opaque wall across the future. Once, they could put such fantasies millions of years in the future 11. Now they saw that their most diligent extrapolations resulted in the unknowable … soon. Once, galactic empires might have seemed a Post-Human domain. Now, sadly, even interplanetary ones are.\n7 Vinge, Vernor, “Bookworm, Run!”, Analog, March 1966, pp8-40. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.8 Alfvén, Hannes, writing as Olof Johanneson, The End of Man?, Award Books, 1969 earlier published as “The Tale of the Big Computer”, Coward-McCann, translated from a book copyright 1966 Albert Bonniers Forlag AB with English translation copyright 1966 by Victor Gollanz, Ltd.9 Vinge, Vernor, First Word, Omni, January 1983, p10.10 Bear, Greg, “Blood Music”, Analog Science Fiction-Science Fact, June, 1983. Expanded into the novel Blood Music, Morrow, 198511 Stapledon, Olaf, The Starmaker, Berkley Books, 1961 (but from the forward probably written before 1937).12 Vinge, Vernor, “True Names”, Binary Star Number 5, Dell, 1981. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.What about the ‘90s and the ’00s and the ’10s, as we slide toward the edge? How will the approach of the Singularity spread across the human world view? For a while yet, the general critics of machine sapience will have good press. After all, till we have hardware as powerful as a human brain it is probably foolish to think we’ll be able to create human equivalent (or greater) intelligence. (There is the far-fetched possibility that we could make a human equivalent out of less powerful hardware, if were willing to give up speed, if we were willing to settle for an artificial being who was literally slow 12. But it’s much more likely that devising the software will be a tricky process, involving lots of false starts and experimentation. If so, then the arrival of self-aware machines will not happen till after the development of hardware that is substantially more powerful than humans’ natural equipment.)\nBut as time passes, we should see more symptoms. The dilemma felt by science fiction writers will be perceived in other creative endeavors.(I have heard thoughtful comic book writers worry about how to have spectacular effects when everything visible can be produced by the technically commonplace.) We will see automation replacing higher and higher level jobs. We have tools right now (symbolic math programs, CAD/CAM) that release us from most low-level drudgery. Or put another way: The work that is truly productive is the domain of a steadily smaller and more elite fraction of humanity. In the coming of the Singularity, we are seeing the predictions of true technological unemployment finally come true.\nAnother symptom of progress toward the Singularity: ideas themselves should spread ever faster, and even the most radical will quickly become commonplace. When I began writing, it seemed very easy to come up with ideas that took decades to percolate into the cultural consciousness; now the lead time seems more like eighteen months. (Of course, this could just be me losing my imagination as I get old, but I see the effect in others too.) Like the shock in a compressible flow, the Singularity moves closer as we accelerate through the critical speed.\nAnd what of the arrival of the Singularity itself? What can be said of its actual appearance? Since it involves an intellectual runaway, it will probably occur faster than any technical revolution seen so far. The precipitating event will likely be unexpected – perhaps even to the researchers involved. (“But all our previous models were catatonic! We were just tweaking some parameters…”) If networking is widespread enough (into ubiquitous embedded systems), it may seem as if our artifacts as a whole had suddenly wakened.\nAnd what happens a month or two (or a day or two) after that? I have only analogies to point to: The rise of humankind. We will be in the Post-Human era. And for all my rampant technological optimism, sometimes I think I’d be more comfortable if I were regarding these transcendental events from one thousand years remove … instead of twenty."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#can-the-singularity-be-avoided",
    "href": "docs/posts/1993-vernor-vinge/index.html#can-the-singularity-be-avoided",
    "title": "The Coming Technological Singularity",
    "section": "Can the Singularity be Avoided?",
    "text": "Can the Singularity be Avoided?\nWell, maybe it won’t happen at all: Sometimes I try to imagine the symptoms that we should expect to see if the Singularity is not to develop. There are the widely respected arguments of Penrose 13 and Searle 14 against the practicality of machine sapience. In August of 1992, Thinking Machines Corporation held a workshop to investigate the question “How We Will Build a Machine that Thinks” 15. As you might guess from the workshop’s title, the participants were not especially supportive of the arguments against machine intelligence. In fact, there was general agreement that minds can exist on nonbiological substrates and that algorithms are of central importance to the existence of minds. However, there was much debate about the raw hardware power that is present in organic brains. A minority felt that the largest 1992 computers were within three orders of magnitude of the power of the human brain. The majority of the participants agreed with Moravec’s estimate 16 that we are ten to forty years away from hardware parity. And yet there was another minority who pointed to 17 18, and conjectured that the computational competence of single neurons may be far higher than generally believed. If so, our present computer hardware might be as much as ten orders of magnitude short of the equipment we carry around in our heads. If this is true (or for that matter, if the Penrose or Searle critique is valid), we might never see a Singularity. Instead, in the early ’00s we would find our hardware performance curves begin to level off – this caused by our inability to automate the complexity of the design work necessary to support the hardware trend curves. We’d end up with some very powerful hardware, but without the ability to push it further. Commercial digital signal processing might be awesome, giving an analog appearance even to digital operations, but nothing would ever “wake up” and there would never be the intellectual runaway which is the essence of the Singularity. It would likely be seen as a golden age … and it would also be an end of progress. This is very like the future predicted by Gunther Stent. In fact, on page 137 of 19, Stent explicitly cites the development of transhuman intelligence as a sufficient condition to break his projections.\n13 Penrose, R., The Emperor’s New Mind, Oxford University Press, 1989.14 Searle, John R., “Minds, Brains, and Programs”, in The Behavioral and Brain Sciences, v.3, Cambridge University Press, 1980. The essay is reprinted in The Mind’s I, edited by Douglas R. Hofstadter and Daniel C. Dennett, Basic Books, 1981. This reprinting contains an excellent critique of the Searle essay.15 Thearling, Kurt, “How We Will Build a Machine that Thinks”, a workshop at Thinking Machines Corporation. Personal Communication.16 Moravec, Hans, Mind Children, Harvard University Press, 1988.17 Conrad, Michael et al., “Towards an Artificial Brain”, BioSystems, vol23, pp175-218, 1989.18 Rasmussen, S. et al., “Computational Connectionism within Neurons: a Model of Cytoskeletal Automata Subserving Neural Networks”, in Emergent Computation, Stephanie Forrest, ed., p428-449, MIT Press, 1991.19 Stent, Gunther S., The Coming of the Golden Age: A View of the End of Progress, The Natural History Press, 1969.20 Herbert, Frank, Dune, Berkley Books, 1985. However, this novel was serialized in Analog Science Fiction-Science Fact in the 1960s.But if the technological Singularity can happen, it will. Even if all the governments of the world were to understand the “threat” and be in deadly fear of it, progress toward the goal would continue. In fiction, there have been stories of laws passed forbidding the construction of “a machine in the form of the mind of man” 20.In fact, the competitive advantage – economic, military, even artistic – of every advance in automation is so compelling that passing laws, or having customs, that forbid such things merely assures that someone else will get them first.\nEric Drexler 21 has provided spectacular insight about how far technical improvement may go. He agrees that superhuman intelligences will be available in the near future – and that such entities pose a threat to the human status quo. But Drexler argues that we can embed such transhuman devices in rules or physical confinement such that their results can be examined and used safely. This is I. J. Good’s ultraintelligent machine, with a dose of caution. I argue that confinement is intrinsically impractical. For the case of physical confinement: Imagine yourself confined to your house with only limited data access to the outside, to your masters. If those masters thought at a rate – say – one million times slower than you, there is little doubt that over a period of years (your time) you could come up with “helpful advice” that would incidentally set you free. (I call this “fast thinking” form of superintelligence “weak superhumanity”. Such a “weakly superhuman” entity would probably burn out in a few weeks of outside time. “Strong superhumanity” would be more than cranking up the clock speed on a human-equivalent mind. It’s hard to say precisely what “strong superhumanity” would be like, but the difference appears to be profound. Imagine running a dog mind at very high speed. Would a thousand years of doggy living add up to any human insight? (Now if the dog mind were cleverly rewired and then run at high speed, we might see something different….) Most speculations about superintelligence seem to be based on the weakly superhuman model. I believe that our best guesses about the post-Singularity world can be obtained by thinking on the nature of strong superhumanity. I will return to this point later in the paper.)\n21 Drexler, K. Eric, Engines of Creation, Anchor Press/Doubleday, 1986.The other approach to Drexlerian confinement is to build rules into the mind of the created superhuman entity (Asimov’s Laws). I think that performance rules strict enough to be safe would also produce a device whose ability was clearly inferior to the unfettered versions (and so human competition would favor the development of the those more dangerous models).Still, the Asimov dream is a wonderful one: Imagine a willing slave, who has 1000 times your capabilities in every way. Imagine a creature who could satisfy your every safe wish (whatever that means) and still have 99.9% of its time free for other activities. There would be a new universe we never really understood, but filled with benevolent gods (though one of my wishes might be to become one of them).\nIf the Singularity can not be prevented or confined, just how bad could the Post-Human era be? Well … pretty bad. The physical extinction of the human race is one possibility. (Or as Eric Drexler put it of nanotechnology: Given all that such technology can do, perhaps governments would simply decide that they no longer need citizens!). Yet physical extinction may not be the scariest possibility. Again, analogies: Think of the different ways we relate to animals. Some of the crude physical abuses are implausible, yet…. In a Post-Human world there would still be plenty of niches where human equivalent automation would be desirable: embedded systems in autonomous devices, self-aware daemons in the lower functioning of larger sentients. (A strongly superhuman intelligence would likely be a Society of Mind 22 with some very competent components.) Some of these human equivalents might be used for nothing more than digital signal processing. They would be more like whales than humans. Others might be very human-like, yet with a one-sidedness, a dedication that would put them in a mental hospital in our era. Though none of these creatures might be flesh-and-blood humans, they might be the closest things in the new environment to what we call human now. (I. J. Good had something to say about this, though at this late date the advice may be moot: Good 23 proposed a “Meta-Golden Rule”, which might be paraphrased as “Treat your inferiors as you would be treated by your superiors.”It’s a wonderful, paradoxical idea (and most of my friends don’t believe it) since the game-theoretic payoff is so hard to articulate. Yet if we were able to follow it, in some sense that might say something about the plausibility of such kindness in this universe.)\n22 Minsky, Marvin, Society of Mind, Simon and Schuster, 1985.23 Good, I. J., [Help! I can’t find the source of Good’s Meta-Golden Rule, though I have the clear recollection of hearing about it sometime in the 1960s. Through the help of the net, I have found pointers to a number of related items. G. Harry Stine and Andrew Haley have written about metalaw as it might relate to extraterrestrials: G. Harry Stine, “How to Get along with Extraterrestrials … or Your Neighbor”, Analog Science Fact- Science Fiction, February, 1980, p39-47.]I have argued above that we cannot prevent the Singularity, that its coming is an inevitable consequence of the humans’ natural competitiveness and the possibilities inherent in technology. And yet … we are the initiators. Even the largest avalanche is triggered by small things. We have the freedom to establish initial conditions, make things happen in ways that are less inimical than others. Of course (as with starting avalanches), it may not be clear what the right guiding nudge really is:"
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#other-paths-to-the-singularity-intelligence-amplification",
    "href": "docs/posts/1993-vernor-vinge/index.html#other-paths-to-the-singularity-intelligence-amplification",
    "title": "The Coming Technological Singularity",
    "section": "Other Paths to the Singularity: Intelligence Amplification",
    "text": "Other Paths to the Singularity: Intelligence Amplification\nWhen people speak of creating superhumanly intelligent beings, they are usually imagining an AI project. But as I noted at the beginning of this paper, there are other paths to superhumanity. Computer networks and human-computer interfaces seem more mundane than AI, and yet they could lead to the Singularity. I call this contrasting approach Intelligence Amplification (IA). IA is something that is proceeding very naturally, in most cases not even recognized by its developers for what it is. But every time our ability to access information and to communicate it to others is improved, in some sense we have achieved an increase over natural intelligence. Even now, the team of a PhD human and good computer workstation (even an off-net workstation!) could probably max any written intelligence test in existence.\nAnd it’s very likely that IA is a much easier road to the achievement of superhumanity than pure AI. In humans, the hardest development problems have already been solved. Building up from within ourselves ought to be easier than figuring out first what we really are and then building machines that are all of that. And there is at least conjectural precedent for this approach. Cairns-Smith 24 has speculated that biological life may have begun as an adjunct to still more primitive life based on crystalline growth. Lynn Margulis 25 has made strong arguments for the view that mutualism is the great driving force in evolution.\n24 Cairns-Smith, A. G., Seven Clues to the Origin of Life, Cambridge University Press, 1985.25 Margulis, Lynn and Dorion Sagan, Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors, Summit Books, 1986.Note that I am not proposing that AI research be ignored or less funded. What goes on with AI will often have applications in IA, and vice versa. I am suggesting that we recognize that in network and interface research there is something as profound (and potential wild) as Artificial Intelligence. With that insight, we may see projects that are not as directly applicable as conventional interface and network design work, but which serve to advance us toward the Singularity along the IA path.\nHere are some possible projects that take on special significance, given the IA point of view:\n\nHuman/computer team automation: Take problems that are normally considered for purely machine solution (like hill-climbing problems), and design programs and interfaces that take a advantage of humans’ intuition and available computer hardware. Considering all the bizarreness of higher dimensional hill-climbing problems (and the neat algorithms that have been devised for their solution), there could be some very interesting displays and control tools provided to the human team member.\nDevelop human/computer symbiosis in art: Combine the graphic generation capability of modern machines and the esthetic sensibility of humans. Of course, there has been an enormous amount of research in designing computer aids for artists, as labor saving tools. I’m suggesting that we explicitly aim for a greater merging of competence, that we explicitly recognize the cooperative approach that is possible. Karl Sims 26 has done wonderful work in this direction.\nAllow human/computer teams at chess tournaments. We already have programs that can play better than almost all humans. But how much work has been done on how this power could be used by a human, to get something even better? If such teams were allowed in at least some chess tournaments, it could have the positive effect on IA research that allowing computers in tournaments had for the corresponding niche in AI.\nDevelop interfaces that allow computer and network access without requiring the human to be tied to one spot, sitting in front of a computer. (This is an aspect of IA that fits so well with known economic advantages that lots of effort is already being spent on it.)\nDevelop more symmetrical decision support systems. A popular research/product area in recent years has been decision support systems. This is a form of IA, but may be too focussed on systems that are oracular. As much as the program giving the user information, there must be the idea of the user giving the program guidance.\nUse local area nets to make human teams that really work (ie, are more effective than their component members). This is generally the area of “groupware”, already a very popular commercial pursuit. The change in viewpoint here would be to regard the group activity as a combination organism. In one sense, this suggestion might be regarded as the goal of inventing a “Rules of Order” for such combination operations. For instance, group focus might be more easily maintained than in classical meetings. Expertise of individual human members could be isolated from ego issues such that the contribution of different members is focussed on the team project. And of course shared data bases could be used much more conveniently than in conventional committee operations. (Note that this suggestion is aimed at team operations rather than political meetings. In a political setting, the automation described above would simply enforce the power of the persons making the rules!)\nExploit the worldwide Internet as a combination human/machine tool. Of all the items on the list, progress in this is proceeding the fastest and may run us into the Singularity before anything else. The power and influence of even the present-day Internet is vastly underestimated. For instance, I think our contemporary computer systems would break under the weight of their own complexity if it weren’t for the edge that the USENET “group mind” gives the system administration and support people!) The very anarchy of the worldwide net development is evidence of its potential. As connectivity and bandwidth and archive size and computer speed all increase, we are seeing something like Lynn Margulis’ 27 vision of the biosphere as data processor recapitulated, but at a million times greater speed and with millions of humanly intelligent agents (ourselves).\n\n26 Sims, Karl, “Interactive Evolution of Dynamical Systems”, Thinking Machines Corporation, Technical Report Series, published in Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life, Paris, MIT Press, December 1991.27 Margulis, Lynn and Dorion Sagan, Microcosmos, Four Billion Years of Evolution from Our Microbial Ancestors, Summit Books, 1986.28 Anderson, Poul, “Kings Who Die”, If, March 1962, p8-36. Reprinted in Seven Conquests, Poul Anderson, MacMillan Co., 1969.29 Vinge, Vernor, “Bookworm, Run!”, Analog, March 1966, pp8-40. Reprinted in True Names and Other Dangers, Vernor Vinge, Baen Books, 1987.The above examples illustrate research that can be done within the context of contemporary computer science departments. There are other paradigms. For example, much of the work in Artificial Intelligence and neural nets would benefit from a closer connection with biological life. Instead of simply trying to model and understand biological life with computers, research could be directed toward the creation of composite systems that rely on biological life for guidance or for the providing features we don’t understand well enough yet to implement in hardware. A long-time dream of science-fiction has been direct brain to computer interfaces 28 29. In fact, there is concrete work that can be done (and has been done) in this area:\n\nLimb prosthetics is a topic of direct commercial applicability. Nerve to silicon transducers can be made 30.This is an exciting, near-term step toward direct communication.\nSimilar direct links into brains may be feasible, if the bit rate is low: given human learning flexibility, the actual brain neuron targets might not have to be precisely selected. Even 100 bits per second would be of great use to stroke victims who would otherwise be confined to menu-driven interfaces.\nPlugging in to the optic trunk has the potential for bandwidths of 1 Megabit/second or so. But for this, we need to know the fine-scale architecture of vision, and we need to place an enormous web of electrodes with exquisite precision. If we want our high bandwidth connection to be in addition to what paths are already present in the brain, the problem becomes vastly more intractable. Just sticking a grid of high-bandwidth receivers into a brain certainly won’t do it. But suppose that the high-bandwidth grid were present while the brain structure was actually setting up, as the embryo develops. That suggests:\nAnimal embryo experiments. I wouldn’t expect any IA success in the first years of such research, but giving developing brains access to complex simulated neural structures might be very interesting to the people who study how the embryonic brain develops. In the long run, such experiments might produce animals with additional sense paths and interesting intellectual abilities.\n\n30 Kovacs, G. T. A. et al., “Regeneration Microelectrode Array for Peripheral Nerve Recording and Stimulation”, IEEE Transactions on Biomedical Engineering, v 39, n 9, pp 893-902.31 Swanwick Michael, Vacuum Flowers, serialized in Isaac Asimov’s Science Fiction Magazine, December(?) 1986 - February 1987. Republished by Ace Books, 1988.Originally, I had hoped that this discussion of IA would yield some clearly safer approaches to the Singularity. (After all, IA allows our participation in a kind of transcendence.) Alas, looking back over these IA proposals, about all I am sure of is that they should be considered, that they may give us more options. But as for safety … well, some of the suggestions are a little scarey on their face. One of my informal reviewers pointed out that IA for individual humans creates a rather sinister elite. We humans have millions of years of evolutionary baggage that makes us regard competition in a deadly light. Much of that deadliness may not be necessary in today’s world, one where losers take on the winners’ tricks and are coopted into the winners’ enterprises. A creature that was built de novo might possibly be a much more benign entity than one with a kernel based on fang and talon. And even the egalitarian view of an Internet that wakes up along with all mankind can be viewed as a nightmare 31.\nThe problem is not that the Singularity represents simply the passing of humankind from center stange, but that it contradicts some of our most deeply held notions of being. I think a closer look at the notion of strong superhumanity can show why that is."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#strong-superhumanity-and-the-best-we-can-ask-for",
    "href": "docs/posts/1993-vernor-vinge/index.html#strong-superhumanity-and-the-best-we-can-ask-for",
    "title": "The Coming Technological Singularity",
    "section": "Strong Superhumanity and the Best We Can Ask for",
    "text": "Strong Superhumanity and the Best We Can Ask for\nSuppose we could tailor the Singularity. Suppose we could attain our most extravagant hopes. What then would we ask for: That humans themselves would become their own successors, that whatever injustice occurs would be tempered by our knowledge of our roots. For those who remained unaltered, the goal would be benign treatment (perhaps even giving the stay-behinds the appearance of being masters of godlike slaves).It could be a golden age that also involved progress (overleaping Stent’s barrier). Immortality (or at least a lifetime as long as we can make the universe survive 32 33) would be achievable.\n32 Dyson, Freeman, “Physics and Biology in an Open Universe”, Review of Modern Physics, vol 51, pp447-460, 1979.33 Barrow, John D. and Frank J. Tipler, The Anthropic Cosmological Principle, Oxford University Press, 1986.34 Niven, Larry, “The Ethics of Madness”, If, April 1967, pp82-108. Reprinted in Neutron Star, Larry Niven, Ballantine Books, 1968.But in this brightest and kindest world, the philosophical problems themselves become intimidating. A mind that stays at the same capacity cannot live forever; after a few thousand years it would look more like a repeating tape loop than a person. (The most chilling picture I have seen of this is in 34.) To live indefinitely long, the mind itself must grow … and when it becomes great enough, and looks back … what fellow-feeling can it have with the soul that it was originally? Certainly the later being would be everything the original was, but so much vastly more. And so even for the individual, the Cairns-Smith (or Lynn Margulis) notion of new life growing incrementally out of the old must still be valid.\nThis “problem” about immortality comes up in much more direct ways. The notion of ego and self-awareness has been the bedrock of the hardheaded rationalism of the last few centuries. Yet now the notion of self-awareness is under attack from the Artificial Intelligence people (“self-awareness and other delusions”). Intelligence Amplification undercuts the importance of ego from another direction. The post-Singularity world will involve extremely high-bandwidth networking. A central feature of strongly superhuman entities will likely be their ability to communicate at variable bandwidths, including ones far higher than speech or written messages. What happens when pieces of ego can be copied and merged, when the size of a self-awareness can grow or shrink to fit the nature of the problems under consideration?These are essential features of strong superhumanity and the Singularity. Thinking about them, one begins to feel how essentially strange and different the Post-Human era will be – no matter how cleverly and benignly it is brought to be.\nFrom one angle, the vision fits many of our happiest dreams: a place unending, where we can truly know one another and understand the deepest mysteries. From another angle, it’s a lot like the worst case scenario I imagined earlier in this paper.\nWhich is the valid viewpoint? In fact, I think the new era is simply too different to fit into the classical frame of good and evil. That frame is based on the idea of isolated, immutable minds connected by tenuous, low-bandwidth links. But the post-Singularity world does fit with the larger tradition of change and cooperation that started long ago (perhaps even before the rise of biological life). I think there are notions of ethics that would apply in such an era. Research into IA and high-bandwidth communications should improve this understanding. I see just the glimmerings of this now, in Good’s Meta-Golden Rule, perhaps in rules for distinguishing self from others on the basis of bandwidth of connection. And while mind and self will be vastly more labile than in the past, much of what we value (knowledge, memory, thought) need never be lost. I think Freeman Dyson has it right when he says 35: “God is what mind becomes when it has passed beyond the scale of our comprehension.”\n35 Dyson, Freeman, Infinite in All Directions, Harper && Row, 1988."
  },
  {
    "objectID": "docs/posts/1993-vernor-vinge/index.html#appendix-metadata",
    "href": "docs/posts/1993-vernor-vinge/index.html#appendix-metadata",
    "title": "The Coming Technological Singularity",
    "section": "Appendix: Metadata",
    "text": "Appendix: Metadata\nModernized from The Coming Technological Singularity on the original author’s website. The original header for the is as follows:\nVernor Vinge\n\nDepartment of Mathematical Sciences San Diego State University\n\n(c) 1993 by Vernor Vinge (Verbatim copying/translation and distribution of this entire article is permitted in any medium, provided this notice is preserved.)\n\n[I wish to thank John Carroll of San Diego State University and Howard Davidson of Sun Microsystems for discussing the draft version of this paper with me.]\n\nThis article was for the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, March 30-31, 1993. It is also retrievable from the NASA technical reports server as part of NASA CP-10129. A slightly changed version appeared in the Winter 1993 issue of _Whole Earth Review_.\nVISION-21 Symposium was a conference held in March 1993, described as:\n\nThe symposium Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace was held at the NASA Lewis Research Center on March 30-31, 1993. The purpose of the symposium was to simulate interdisciplinary thinking in the sciences and technologies which will be required for exploration and development of space over the next thousand years. The keynote speakers were Hans Moravec, Vernor Vinge, Carol Stoker, and Myron Krueger. The proceedings consist of transcripts of the invited talks and the panel discussion by the invited speakers, summaries of workshop sessions, and contributed papers by the attendees. (Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace 1993)"
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html",
    "href": "docs/posts/1998-robin-hanson/index.html",
    "title": "Is a singularity just around the corner?",
    "section": "",
    "text": "Economic growth is determined by the supply and demand of investment capital; technology determines the demand for capital, while human nature determines the supply. The supply curve has two distinct parts, giving the world economy two distinct modes. In the familiar slow growth mode, rates of return are limited by human discount rates. In the fast growth mode, investment is limited by the world’s wealth. Historical trends suggest that we may transition to the fast mode in roughly another century and a half.\nCan some new technology switch us to the fast mode more quickly than this? Perhaps, but such a technology must greatly raise the rate of return for the world’s expected worst investment project. It must thus be very broadly applicable, improving almost all forms of capital and investment. Furthermore, investment externalities must remain within certain limits."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#abstract",
    "href": "docs/posts/1998-robin-hanson/index.html#abstract",
    "title": "Is a singularity just around the corner?",
    "section": "",
    "text": "Economic growth is determined by the supply and demand of investment capital; technology determines the demand for capital, while human nature determines the supply. The supply curve has two distinct parts, giving the world economy two distinct modes. In the familiar slow growth mode, rates of return are limited by human discount rates. In the fast growth mode, investment is limited by the world’s wealth. Historical trends suggest that we may transition to the fast mode in roughly another century and a half.\nCan some new technology switch us to the fast mode more quickly than this? Perhaps, but such a technology must greatly raise the rate of return for the world’s expected worst investment project. It must thus be very broadly applicable, improving almost all forms of capital and investment. Furthermore, investment externalities must remain within certain limits."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#introduction",
    "href": "docs/posts/1998-robin-hanson/index.html#introduction",
    "title": "Is a singularity just around the corner?",
    "section": "Introduction",
    "text": "Introduction\nMany technological enthusiasts, impressed by the potential of various envisioned technologies, have speculated that technology may soon become so productive as to induce a “singularity”, a period of extremely rapid growth. (A collection of speculations can be found here and here.)\nHow rapid? Over the last few centuries, population has doubled roughly every 70 years, per-capita consumption has doubled roughly every 35 years,and scientific progress has doubled roughly every 15 years. In contrast, computing power1 has doubled roughly every two years for the last half-century. Many have speculated that perhaps economic growth rates will soon match or even greatly exceed current computing-power growth rates.\n1 Editor’s note: It links to (Moravec 1998).What would it take, exactly, for a technology to make the economy grow this fast? This paper offers a simple economic analysis intended to illuminate this question. We will find that while very rapid growth is possible in principle, this requires enabling technologies to meet some strong conditions. It is hard to see how any single new technology could do this, though historical trends suggest that the accumulation of all new technologies over the century and a half might."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#the-supply-and-demand-of-investment-capital",
    "href": "docs/posts/1998-robin-hanson/index.html#the-supply-and-demand-of-investment-capital",
    "title": "Is a singularity just around the corner?",
    "section": "The supply and demand of investment capital",
    "text": "The supply and demand of investment capital\nGrowth in consumption over any extended period requires growth in what economists call “capital”, which means any thing which helps make “products” people want. These terms are considered abstractly, so a music concert is a “product”, and the skills of a musician are “human capital”.\nCapital can be used not only to make products, but also to make more capital, and the rate at which this happens is called a “rate of return”. The fraction of capital devoted to making more capital is called the “savings rate”, and the price people pay to rent capital for this purpose is called a “market rate of return.” This is a “real rate of return” if capital is calibrated so that one more unit of capital always produces the same “amount” (or “value” or “utility”) of products people like.\nWhat determines how much people save and the interest rate they get for it? Supply and demand, naturally. Technology supplies a pool of investment projects which are physically possible, and investors demand these projects more or less depending on how productive they are. Or, looking at it in terms of investment capital, investment projects demand capital to be carried out, and investors supply capital to such projects. This view is illustrated in the following figure.\n\nAs is usual in supply and demand graphs, the x-axis is a quantity axis, here amount of capital, and the y-axis is a price axis, here a rate of return. The origin where the axes meet has no capital saved and a zero rate of return.\nSupply curves slope up, so that higher returns are required to induce investors to save larger fractions of their income. Demand lines slope down, so that the more projects are undertaken, the worse their return. Projects can be thought of as lined up in order along the savings axis, with the best projects at the left and the worst projects at the right. At the point where supply and demand meet, the marginal investor is just barely willing to offer capital to get the market return offered by the marginal project.\nWhile the demand lines shown are fictitious, the shape of the supply curve shown is true to a reasonable understanding of what investment supply looks like. This is based both on observing current human preferences, and on an understanding of what sort of preferences should have been selected for during our evolution. Moreover, the dates on the figure show roughly where the economy has been along this curve. (See the technical appendix for details.)\nThe supply curve eventually turns up sharply, shooting off to infinity at a particular bound. At least it does this if the population growth rate stays below twice its current value, and if investments on net benefit rather than harm non-investors (again, see the appendix). The supply curve is relatively flat, however, over a large range, at a rate of return determined mainly by the population growth rate and the degree to which our genes have taught us to discount risk and time. Since capital supply should be relatively insensitive to technology, we can hold this curve fixed in our minds as we consider how technology might change the demand for capital, i.e., the supply of investment projects.\nWhen poor technology creates a low demand for investment capital, as with Demand 1 in the figure, the resulting market rate of return is not very sensitive to demand. In this case improving technology mainly just raises the savings rate. Demand 2 in the figure describes an economy nearing the middle of a transition between the two distinct economic modes, which may describe our world economy in roughly a century.\nWhen technology is good enough to create a high demand for investment capital, as with Demand 3, savings becomes insensitive to demand, and the market rate of return can become sensitive to technology. In the high demand mode, growth rates can in principle be very high. Given demand curves with the right shape, growth rates might increase much faster with time in the high growth mode than they did in the low growth mode. Such rapidly increasing growth rates seems most like the imagined singularity scenarios. With other demand shapes, however, fast growth rates might increase more slowly than slow growth rates did."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#burning-up-excess-return-in-investment-races",
    "href": "docs/posts/1998-robin-hanson/index.html#burning-up-excess-return-in-investment-races",
    "title": "Is a singularity just around the corner?",
    "section": "Burning Up Excess Return in Investment Races",
    "text": "Burning Up Excess Return in Investment Races\nThe reader may have noticed that the figure above suggests that while in the low demand mode the market rate of return stays roughly constant, the average rate of return of the projects actually tried might depend dramatically on the demand. After all, the top of the demand curve could in principle be very far above the low supply curve. So why couldn’t growth rates also be very large in the low demand mode?\nThe problem with this argument is that this figure has neglected to include the time dimension, and a lack of long-term property rights in most investment projects means that returns above the market rate are burned up an a race to be first.\nThere are, in fact, very few long-term property rights regarding the right to undertake investment projects. Think of developing a new kind of car, colonizing the moon, developing specialized CAD software, or making a movie with a certain kind of gimmick. Each such project requires various forms of capital, such as machines or skilled labor. While in the short term only one investor may have the rights to tackle a given project, in the long term many competing investors could have positioned themselves to have this short-term opportunity.\nFor example, Microsoft’s dominant position in PC operating systems now gives it the right to many very attractive investments. But there was once an open race to become the dominant operating system, and competitors then tried harder because of the prospect of later high returns. And when deciding whether to enter this earlier race and how hard to try, investors mainly wondered if they could get a competitive rate of return. Similarly, while one group now has the right to make the next Batman movie sequel, there have long been open contests to create popular movie series, and popular comic strips.\nConsider a typical as-yet-untried investment project, becoming more and more attractive with time as technology improves and the world market grows larger. If there wasn’t much point in attempting such a project very long after other teams tried, then a race to be first should make sure the project is attempted near when investors first expect such attempts to produce a competitive rate of return. This should happen even if the project returns would be much greater if everyone waited longer. The extra value the project would have if everyone waited is burned up in the race to do it first.\nThus most of the return above the market return in our supply and demand figure above should be burned up, leaving the average return at about the market return. Thus in the low demand mode the height of the demand curve is relatively unimportant. If anticipated, a technology which makes a moderate number of investment projects much more productive may have no effect on growth rates or on rates of return.\nThe width of the demand curve, however, does matter in low demand mode. If technology creates many new attractive investment projects, growth rates can rise due to a rise in the savings rate. This happens by changing the rate of return offered on the marginal investment project, the project actually tried which is expected to offer the worst return. To improve the marginal project, a technology needs to have a very broad range of productive applications."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#investment-bottlenecks",
    "href": "docs/posts/1998-robin-hanson/index.html#investment-bottlenecks",
    "title": "Is a singularity just around the corner?",
    "section": "Investment Bottlenecks",
    "text": "Investment Bottlenecks\nA big problem with having a very broad range of applications, however, is the possibility of bottleneck resources. Most production, for consumption or investment, requires a wide variety of forms of capital. These include land, raw materials, energy, various sorts specialized labor and specialized machines, information sources, places to dispose of waste, access to channels of distribution and advertising, legal adjudication, and regulatory approval. A technology that dramatically improves the productivity of one of these typically gives only a small improvement to the final output. This is the concept of diminishing returns in production. If the rate of improvement in any one input consistently lags behind the rest, that input then becomes a bottleneck limiting the growth rate for the entire process.\nFor example, computer hardware may be getting cheap very fast, but one cannot now as quickly reduce the costs of training professionals who understand both computers and application areas well. Thus such training becomes a bottleneck limiting the contribution of computers to the larger economy. Similarly, even having the cost of producing electricity drop to near zero would only have a minor effect on the economy, at least in the short term. Most of the price consumers pay for electricity is to transport it, not to produce it, and new ways of organizing the transportation and production of electricity would have to evolve to take more advantage of the new situation.\nThe notion of bottlenecks applies to timescales in particular. For example, I think the rapid growth in computing speed and communication bandwidth, together with loose notions that most of the economy must be at root computation and communication, have suggested to many that most all economic timescales, including economic doubling times, must soon rise to meet these faster rates. But this ignores the prospect of other physical processes becoming time bottlenecks, and the diminishing returns to raw computation. Note also that in the low growth mode, even changing the total investment time scale by a large factor, which implies changing the capital demand function by a large factor, need not change the market return or growth rate by very much.\nIt seems hard to escape the conclusion that it just takes a lot of time for the world economy to absorb even very broadly applicable technologies like the computer, especially if the criteria of interest is raising the rate of return on the marginal investment project worldwide. Thus it seems unlikely that a single new technology could quickly knock the economy into a high demand mode with very high growth rates. (For similar conclusions, see here and here.)\nEven if no single technology can create fast growth, the cumulative effect of all new technologies over many decades, might, still slowly push the economy up the supply curve into the high demand mode. This requires that such technologies keep creating enough productive investment opportunities to keep up with a very rapidly growing economy, that investments on average benefit non-investors, and that we don’t double population growth rates. This doesn’t seem easy, but it may not be impossible either."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#conclusion",
    "href": "docs/posts/1998-robin-hanson/index.html#conclusion",
    "title": "Is a singularity just around the corner?",
    "section": "Conclusion",
    "text": "Conclusion\nIf we assume that the risk and time preferences of future investors will not vary much from those which evolution has given us, then the supply curve for investment capital should say nearly fixed while technology changes the demand curve. And since a lack of long-term property rights in investment projects creates a race to be first, the two parts of the supply curve create two distinct modes for the economy: a low growth rate mode and a high growth rate mode. Historical trends on growth rates suggest that we are slowly moving up the capital supply curve toward the high growth mode, and may reach it in roughly a century and a half.\nMany have suggested that some special new technology will induce rapid growth rates much more quickly than this. This is possible in principle, but we have identified a number of conditions which such a technology must meet. Investments must on average benefit, rather than harm, non-investors. And most difficult, the special technology must have such broad and attractive enough applications across the world economy that it dramatically raises the returns on the worst investment anyone undertakes. To do this, such a technology must dramatically improve the productivity of almost all forms of capital, not just a few. And this must be true even though, in a race to be first, each investment project is started as soon as it seemed marginally attractive.\nThese conditions do not appear to have been met by any of the very diverse range of technologies that have yet appeared. It is possible that such conditions may be met by some future technology, but a persuasive argument for this case should explain, in standard economic terminology, why we should expect this technology to meet these conditions."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#references",
    "href": "docs/posts/1998-robin-hanson/index.html#references",
    "title": "Is a singularity just around the corner?",
    "section": "References",
    "text": "References\nBarro, R., Sala-I-Martin, X., Economic Growth, McGraw Hill, 1995.\nBlume, L., Easley, D., “Evolution and Market Behavior”, Journal of Economic Theory, 58:9-40, 1992.\nHansson, I., Stuart, C., “Malthusian Selection of Preferences,” American Economic Review, 80(3):529-544, June 1990.\nMaddison, A., Dynamic Forces in Capitalist Development, Oxford Univ. Press, 1991.\nPrice, D., Little science, big science… and beyond, Columbia Univ. Press, 1986.\nSee also Econ Growth Web site."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#sec-technical-appendix",
    "href": "docs/posts/1998-robin-hanson/index.html#sec-technical-appendix",
    "title": "Is a singularity just around the corner?",
    "section": "Technical Appendix",
    "text": "Technical Appendix\nThere are many more complex models of economic growth out there, but the following simple model is complex enough to embody many results relevant to singularity speculations.\nThis model is one of the simplest models of endogenous growth, allowing for real long-term growth within the model and implicitly including all forms of capital improvement, including research and development. The model’s main simplification is that it does not explicitly distinguish between various forms of capital and consumption, aggregating them instead to a single generic form.\nFirst some notation, including standard values where applicable.\n\n\\(n\\) = growth rate of world population, now \\(\\sim 1.4\\% / \\rm{yr}\\).\n\\(g\\) = growth rate of world per-capita consumption, now \\(\\sim 2\\% / \\rm{yr}\\).\n\\(p\\) = typical discount rate, \\(\\sim 3\\% / \\rm{yr}\\) (= factor of \\(2\\) in \\(23\\) years).\n\\(a\\) = typical risk-aversion, \\(\\sim 1\\quad\\) (\\(a = -c u''/u'\\) for utility \\(u(c)\\) of consumption rate \\(c\\)).\n\\(r\\) = real market rate of return (not the risk-less interest rate).\n\\(s\\) = savings, fraction of world capital making more capital.\n\\(A\\) = growth rate of invested capital (corrected for depreciation).\n\\(i\\) = internality, fraction of investment return the investor gets.\n\nWe have five primary equations using these parameters.\n\\[\n\\begin{aligned}\n\\text{Supply of Capital:} \\quad r &= p + ag \\\\\n\\text{Demand for Capital:} \\quad A &= A(s) \\\\\n\\text{Capital Rental:} \\quad r &= iA \\\\\n\\text{Accounting:} \\quad g+n &= sA\n\\end{aligned}\n\\]\nSUPPLY: The supply of capital for investment depends on how fast consumption is growing relative to the typical rate at which people discount the future. If people were risk-neutral, the market return would equal the typical discount rate. But for risk-adverse people who plan to consume more tomorrow than today, stuff today is worth more.\nDEMAND: The demand function \\(A(s)\\) describes the investment projects that current technology makes possible. It says what the expected total (private and external) return on the worst project would be if a fraction \\(s\\) of total income were invested.\nRENTAL: The rental price of capital depends on how fast a unit of capital can produce more capital, corrected for the fact that an investor may not internalize all the costs or benefits of a project. A project may, for example, create improvements in technology that others can copy.\nACCOUNTING: The total growth rate in consumption and capital depends on how productive capital is, and on what fraction of income is saved.\nSome comments:\n\nTypical utility parameter values are predicted from evolutionary selection. When trading resources for a parent now vs. for a child a generation from now, it matters that a child shares only half a parent’s genes. And unit risk-aversion, which is log utility \\(u(c) = log(c)\\), is selected for, at least regarding shocks to all copies of a gene, such as the total market risk in the CAPM.\nThese equations allow for different people to own different amounts of capital, and for different investment projects to have different returns and internality. This is because the supply equation is a very general implication of maximizing expected discounted utility, and the internality parameter \\(i\\) can be considered an average over the projects tried.\nWe have neglected the probably-weak dependence of internality \\(i\\) on the savings rate \\(s\\). Having \\(i&lt;1\\) says that investment projects on net benefit non-investors, while \\(i&gt;1\\) says that non-investors are on net harmed.\nWe’ve set the market return equal to the average return, and so are assuming no long-term property rights in projects. A projects happens at the first time its return becomes competitive. Relaxing this assumption is equivalent to reducing the internality \\(i\\), which then becomes more dependent on savings \\(s\\).\n\\(A(s)\\) summarizes all technical change which creates economic growth. It does not, however, describe changes in the growth rate. neralize While the return to any one not-yet-started project typically rises with time as technology improves, the best projects will be done first, so the \\(A(s)\\) function may rise or fall with time. \\(A(s)\\) also falls if the number of attractive investments did not grow as fast as the economy did.\nThe growth rate \\(g\\) in the accounting equation is really the growth rate of capital, while the rate \\(g\\) in the demand equation is the growth rate of consumption. When savings \\(s\\) is constant, such as with demand \\(A(s)\\) unchanging with time, these are the same thing. We are thus assuming that the growth rate of savings \\(s\\) is much less than the growth rate of consumption \\(g\\).\nWe are using depreciation-corrected parameters \\(A\\) and \\(s\\) in the above equations. To have a depreciation \\(d\\) appear explicitly, change to raw terms \\(A_0, s_0\\), where \\(A_0 = A + d\\) and \\(s_0 = (s A+d)/A_0\\). The accounting and rental equations become \\(n + g = s_0 A_0 - d\\) and \\(r = i (A_0-d)\\). For non-human capital, depreciation is typically \\(\\sim 5\\% / \\rm{yr}\\).\n\nThe first four columns of the following table shows historical estimates for the annual per-capita and population growth rates \\(g, n\\) for four different dates from 1750 to 1995. (Warning: the per-capita growth estimate for 1750 is very crude.) Using the above model and our standard values for preference parameters \\(a, p\\), we then infer the next two columns, an annual rate of return \\(r\\) and a savings fraction \\(s/i\\) for each date. The return estimates seem roughly within reason.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n# nations in data\nPer-capita growth rate\nPopulation growth rate\nRate of Return\nSavings fraction\nTotal return\nSavings\nRaw total return\nRaw savings\n\n\n\n\n\\(g\\)\n\\(n\\)\n\\(r\\)\n\\(s/i\\)\n\\(A\\)\n\\(s\\)\n\\(A_0\\)\n\\(s_0\\)\n\n\n1995\n100+\n2.0%\n1.4%\n5.0%\n68%\n10%\n34%\n15%\n56%\n\n\n1905\n29\n1.3%\n0.9%\n4.3%\n50%\n9%\n25%\n14%\n53%\n\n\n1825\n7\n0.8%\n0.5%\n3.8%\n34%\n8%\n17%\n13%\n50%\n\n\n1750\n2\n0.0%\n0.4%\n3.0%\n13%\n6%\n7%\n11%\n49%\n\n\n\nSomewhat arbitrarily assuming internality \\(i = 50\\%\\), and using the standard value of depreciation \\(d = 5\\%\\), the rest of the table gives estimates for total return and savings \\(A\\), \\(s\\), and their raw values \\(A_0, s_0\\). A standard estimate of \\(56\\%\\) for current raw total savings roughly fits with the standard values that \\(\\sim 20\\%\\) of raw savings is invested in non-human capital, which gets \\(\\sim 1/3\\) of income. Note that the savings growth rate seems to be growing at \\(\\sim 0.3\\%/\\rm{yr}\\), which is much less than the per-capita growth rate, as we had assumed.\n\nWhat fast growth requires\nAssuming that preference parameters \\(a, p\\) don’t change much, what does this model say about the possibility of an economic “singularity,” that is, very large growth rates \\(g\\) in per-capita consumption? Such growth rates, if they persisted long, would imply vast changes in per-capita consumption in a single human generation.\nThe demand equation says that per-capita consumption growth \\(g\\) can’t get very large unless the interest rate \\(r\\) does, and the rental equation says that the interest rate \\(r\\) can’t get very large unless the total return \\(A\\) does. The accounting equation also says that total consumption growth n+g can’t get very large unless total return \\(A\\) does.\nUsing our equations to eliminate \\(r\\) and \\(g\\), we get a market equation, with capital demand on the left and capital supply on the right,\n\\[\nA(s) = \\frac{p-an}{i - as}.\n\\]\nFor \\(p &lt; an\\) and \\(as &lt; i\\), this gives the functional form shown in the figure in the body of the paper. Thus the relevant savings fraction is actually \\(s/i\\), savings relative to internality, rather than savings relative to total income.\nThe limits to fast growth appear more directly in\n\\[\ng = s \\frac{p - n}{i - as}.\n\\]\nThus the only way to allow very fast growth \\(g \\gg p\\) is for \\(s \\sim i/a\\) without \\(n \\sim p\\). Thus since \\(s&lt; 1\\), we require \\(i &lt; a\\).\nThus an economic singularity, \\(g \\gg p\\), requires:\n\\[\ni &lt; a, \\quad s \\sim i/a, \\quad A(i/a) \\gg p\n\\]\nThat is, for unit (i.e., log) risk-aversion, an economic singularity requires that\n\nInvestment projects on net benefit, rather than hurt, non-investors.\nSavings must be carefully balanced near the internality parameter.\nThe return expected for the worst invested-in project becomes very large.\n\nThe bottom line is that this model does allow for an economic singularity under certain circumstances. The historical increase in the savings fraction has been roughly constant with time for the last two centuries, suggesting a near 100% savings fraction near the year 2150. Our ignorance about the internality parameter is cause for concern. Thus it is possible, though not obvious, that a continuation of historical trends will result in an economic singularity of \\(g \\gg p\\) in roughly a century and a half."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#appendix-peer-commentary",
    "href": "docs/posts/1998-robin-hanson/index.html#appendix-peer-commentary",
    "title": "Is a singularity just around the corner?",
    "section": "Appendix: Peer Commentary",
    "text": "Appendix: Peer Commentary\n\nNick Bostrom:7/7/98\nRobin Hanson’s paper gives an analysis of the conditions for an economic singularity. In a few brief sections, important and non-obvious relations between the key concepts related to explosive economic growth are described. Everybody who is interested in “the singularity” will benefit from understanding what economic theory can say about the issue.\nI have some doubt about the empirical part of the paper. Robin writes: “The historical increase in the savings fraction has been roughly constant with time for the last two centuries, suggesting a near 100% savings fraction near the year 2150.” Extrapolations of this sort are of course very precarious, and Robin does not claim otherwise. Yet, even as extrapolations go, this one seems especially problematic. The historical data determining the model parameters contains only three data points, if we bracket the one for 1750 which Robin says is “very crude”. It would be interesting to know whether there is more data available that fits the curve. And what are the error margins? How robust is the 2150 estimate to variations in the parameter values?\nI’m also wondering about whether there might be an alternative explanations for the increasing savings rates. Rather than saying that people save a greater fraction of their capital because the rate of return on savings has gone up, is it possible that people save more because the risk factor has decreased? A person considering saving some of his money in 1750 might have faced a greater risk that his savings would go up in smoke due to some political upheaval, be expropriated by a monarch, or that he himself would get killed in a plague before he got a chance to harvest the payoff. Even if the average rate of return had been constant between then and now, might one not have conjectured that risk-averse individuals would be less inclined to invest under such circumstances, especially since institutions that could insure against unexpected losses were presumably less developed in those days?\nAn interesting question to consider is: What possible technologies would have the properties that could make for an economic singularity? My own view is that when we get both superintelligence and nanotechnology (and I think one would quickly lead to the other) then that will cause a singularity. This technological combination certainly has the extremely wide range of applicability that Robin lists as a precondition for a singularity. Whether it would benefit or harm non-investors is more difficult to predict, since the social ramifications could well be so dramatic that Robin’s economic model would no longer be valid. For example: It might not be possible to enforce property rights; preferences and living conditions might change so radically that comparisons between the amount of capital in the world before and after the singularity will no longer be meaningful; there might not even be a multitude of competing economical agents after the singularity.\nNick Bostrom\nDepartment of Philosophy, Logic and Scientific Method\nLondon School of Economics\nn.bostrom@lse.ac.uk\nhttp://www.hedweb.com/nickb\n\n\nRobin Hanson replies:7/7/98\n— I thank Nick Bostrom for his thoughtful review.\nI do not dispute Nick’s assessment that my empirical extrapolations seem “especially problematic”. I offered them mainly to give readers some sense of where we have been within the model I describe. I don’t really expect finer data to continue to support a linear relation of the savings fraction with time. Instead, I have recently been working on a simple sum of exponentials model of long term economic growth, using a data series by Brad DeLong (Estimating World GDP, One Million B.C. - Present). A rough draft is available.\nNick wonders whether “people save more because the risk factor has decreased.” This certainly makes sense, and is obviously one of many issues I left out of my simple model. It turns out that there is a long-standing puzzle in finance regarding pricing for risk; investors act as if the risks of strong downturns are much larger than they appear in the recent historical record. So you’d really need historical data on perceived risks to examine this empirically.\nI agree with Nick that superintelligence and nanotechnology are technologies with potentially very wide applicability. But I’m more skeptical about how fast early breakthroughs in either field will lead to more advanced breakthroughs. I’m thus relatively confident that we would retain “a multitude of competing economical agents,” and I’m not convinced that correcting for technical change in comparing capital amounts will get much harder.\nA final note: a dramatic loss in ability to enforce property rights doesn’t actually invalidate the model, though it might dramatically change some parameters.\nRobin Hanson\nhanson@econ.berkeley.edu http://hanson.berkeley.edu/\nRWJF Health Policy Scholar, Sch. of Public Health 510-643-1884\n140 Warren Hall, UC Berkeley, CA 94720-7360 FAX: 510-643-2627\n\n\nKathryn Aegis:7/7/98\nOnce again, Robin Hanson has found a connection between the principles of economics and the goals of transhumanism. It sparks thinking in a new direction and provides a potential avenue of practical application by an entrepreneur or investor.\nA question that raised in my mind as I read Robin’s paper relates to a recent discussion with Kurt Schuler regarding future alternatives to our present day parochial monetary currencies and the explosive growth in investment that could result. Could the digital technologies (encryption, ubiquitous exchange, instant transfer) of alternative monetary regimes represent an example of the very technology that Robin references?\nKathryn Aegis, aegis@igc.apc.org\n\n\nHanson replies:8/7/98\nKathryn Aegis asks if “digital technologies … of alternative monetary regimes represent an example” of technologies which could induce explosive economic growth. My intuition would be that by themselves such technologies are far from sufficient. You might have a better case if you added in lots of new kinds of markets that such digital money might be used in. But most new markets are blocked due to regulatory reasons, not because of poor digital money. I’m not very confident of my intuitions here, however, and could be persuaded to change my mind by someone like Lawrence H. White.\n\n\nBilly Brown:26/2/99\nRobin Hanson paints an interesting picture of the relationships between the factors that underlie economic growth, and I certainly would not argue his conclusions on economic grounds. I would, however, suggest that his results could easily be misinterpreted when one attempts to apply them to the real world.\nThe problem is that in the kind of future many transhumans expect to see, economic growth is a poor proxy for human benefit. Consider, for example, the parallels between electronic computers and molecular manufacturing:\nComputers have been undergoing an exponential improvement in price/performance ratios for some decades now. Enthusiasts like to point out that if cars had improved at the same rate over the last twenty years, the average vehicle would cost a few pennies, travel at supersonic speeds, and be capable of running for years without refueling. Nevertheless, as Hanson points out, the economic effects have been relatively modest. Computers have made some companies rich and others poor, and have on the whole been beneficial, but they have not turned us all into millionaires.\nNow, molecular manufacturing promises to bring about the same kind of change in most manufacturing industries. This implies that most material goods will undergo a period of extremely rapid innovation, with costs collapsing as capabilities rapidly improve. Hanson’s model predicts, probably correctly, that the net effect on economic growth will again be much smaller than we expect. But there is a crucial difference: after a few decades of such progress our car really will cost only a few pennies, and so will all other manufactured goods.\nWhat this means is that economic growth will become much less relevant as a means of measuring human prosperity, at least by our current standards. If prices for most material goods collapse while measures such as GNP show modest growth, the practical result is a vast improvement in the human condition.\nBilly Brown, MCSE+I\nbbrown@conemsco.com\n\n\nHanson replies: 26/2/99\nWhile Billy Brown “would not argue [with my] conclusions on economic grounds,” he cautions against applying them to the real world because “economic growth is a poor proxy for human benefit.” Why? Currently the “economic effects” of rapidly falling computer hardware prices have been “relatively modest.” By analogy, Mr. Brown presumes that with molecular manufacturing (i.e., nanotech) the “prices for most material goods [would] collapse while measures such as GNP show[ed] modest growth.” Since “the practical result [would be] a vast improvement in the human condition,” he concludes “economic growth will become much less relevant as a means of measuring human prosperity.”\nMr. Brown, there is no such thing as a non-economic human benefit. If the participants in some social process perceive a type of benefit, economists consider that benefit type fair game for economic analysis.\nNow published statistics like GNP do neglect many types of human benefit. They would not, however, fail to notice a rapid fall in the price of most material goods such as cars. Such a fall would certainly show up as rapid GDP growth. How fast would car prices fall with nanotech? That is exactly the question at issue here, isn’t it? The arguments of fast growth skeptics are not refuted by the fast growth claims of nanotech optimists, though of course skeptics might be refuted by more detailed economic analyses suggesting fast nanotech growth.\nRobin Hanson, hanson@econ.berkeley.edu\n\n\nBilly Brown:11/3/99\nAfter re-reading Robin Hanson’s paper, exchanging a bit of private e-mail, and thinking about the issue, I’ve come to conclude that our disagreement is a result of one of those definition problems that always crop up when you have people from different fields debating a complex point. It seems to me that what he means by “economic growth” would by definition include any sort of human benefit, which of course invalidates my claim. What I meant by “economic growth” was something more like “the Federal government’s official GNP figures”, which is another matter entirely.\nI must therefore concede that his model is not subject to the sorts of problems I suggested. Anything that has any real effect on human welfare should show up as economic growth, and I don’t see any reason to contest his conclusion about the conditions a technology must meet to cause economic growth. Whether nanotechnology can actually meet those conditions is a complex question best addressed elsewhere.\nSo, that reduces most of my comments to a complaint about the inaccuracy of current measurement methods, which isn’t especially relevant to the paper. That being the case, I think that it is time for me to retire from the field.\nBilly Brown, MCSE+I\nbbrown@conemsco.com\n\n\nHanson replies: 12/3/99\nI’m happy that Mr. Brown and I were able to work out our differences via a private conversation, and I’m sorry that I didn’t take the discussion private from the very start.\nEconomists are well aware of the problems with government statistics. The problem is that allowing government agencies more leeway in including “squishy” harder to measure factors in their estimates also allows more opportunities for corruption in constructing estimates. Privately produced estimates can and do include squishy estimates, but their quality is limited by the fact that much less money goes into producing private estimates."
  },
  {
    "objectID": "docs/posts/1998-robin-hanson/index.html#appendix-editors-notes",
    "href": "docs/posts/1998-robin-hanson/index.html#appendix-editors-notes",
    "title": "Is a singularity just around the corner?",
    "section": "Appendix: Editor’s notes",
    "text": "Appendix: Editor’s notes\n\nThis markdown document taken mostly verbatim from the original HTML file. I converted ASCII-math into LaTeX math, fixed typos, chased down a few dead links, etc.\nThe original metadata\nJournal of Evolution and Technology June 1998. Vol. 2\n(Received 10 April, 1998)\n \nRobin Hanson\nSchool of Public Health\nUniversity of California, Berkeley\nWarren Hall, CA 94720-7360\nnet: http://hanson.berkeley.edu/\nemail: hanson@econ.berkeley.edu\n\nJournal of Evolution and Technology\nA peer-reviewed electronic journal publishing contemporary research into future science and philosophy.\nISSN 1541-0099"
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "",
    "text": "A few days ago, the DeepSeek-V2 released by High-Flyer sparked heated discussions. First, what caused the biggest stir was its 1 million tokens/yuan price, roughly 100x cheaper than competing APIs, to the point that some people joked, “Even if it outputs gibberish at this price, I would consider that gibberish to be art”. Secondly, according to the model’s technical report, one of the key technologies behind such a low price is its newly proposed MLA (Multi-head Latent Attention), which is an improvement over GQA. It is said to be more efficient and better than GQA, which has also attracted widespread attention from readers.\nThis article walks through the evolution from MHA, MQA, GQA to MLA, gradually introducing the design principles of MLA."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#mha",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#mha",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "MHA",
    "text": "MHA\nMHA (Multi-Head Attention) is a form of attention proposed in the seminal Attention is all you need, the foundation of current mainstream LLMs. Mathematically, MHA is equivalent to the concatenation of multiple independent single-head attentions. Assuming the input (row) vector sequence is \\(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_l\\), where \\(\\boldsymbol{x}_i\\in\\mathbb{R}^d\\), then MHA can be formally written as\n\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{(s)} ,\\boldsymbol{v}_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(s)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d\\times d_k}\\\\\n\\boldsymbol{k}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{(s)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(s)}\\in\\mathbb{R}^{d\\times d_k} \\\\\n\\boldsymbol{v}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d\\times d_v}\n\\end{gathered}\n\\end{equation}\n\\]\nFor simplicity, the scaling factor of the Attention matrix is omitted here. In practice, a common setting is \\(d_k = d_v = d / h\\), so we have\n\n\n\nmodel name\n\\(d\\)\n\\(h\\)\n\\(d_k\\)\n\\(d_v\\)\n\n\n\n\nLlama-2-7b\n4096\n32\n128\n128\n\n\nLlama-2-70b\n8192\n64\n128\n128\n\n\n\nSince we only consider the causal attention used by mainstream autoregressive LLMs here, when generating recursively token by token, the newly predicted \\((t+1)\\)-th token will not affect the already calculated \\(\\boldsymbol{k}_{\\leq t}^{(s)} ,\\boldsymbol{v}_{\\leq t}^{(s)}\\). Therefore, we can cache this part of the result for subsequent generation calls, avoiding unnecessary repeated calculations. This is the so-called KV Cache.\nThe subsequent MQA, GQA, and MLA are all products developed around the theme of “How do we reduce KV Cache while ensuring the best possible performance?”."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#bottleneck",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#bottleneck",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "Bottleneck",
    "text": "Bottleneck\nA natural question is: why is reducing the size of the KV Cache so important?\nAs we all know, LLM inference is generally performed on GPUs, and the VRAM of a single GPU is limited. Part of it is used to store the model parameters and activation values of the forward calculation, which depends on the size of the model and is a constant after the model is selected; the other part is used to store the KV Cache of the model, which depends not only on the size of the model, but also on the input length of the model, which means it grows dynamically during the inference process. When the context length is long enough, its size will dominate, possibly exceeding the total VRAM of a single GPU or even a single node (8 GPUs).\nThe principle of deploying models on GPUs is: if it can be deployed on one GPU, don’t span multiple GPUs; if it can be deployed on one node, don’t span multiple nodes. This is because in terms of communication bandwidth, intra-GPU &gt; inter-GPU &gt; inter-node.\nThe more nodes a model spans during deployment, the more it will be slowed down by the inter-node communication bandwidth, which is the weakest link. In fact, even though the bandwidth of SRAM and HBM in a single H100 GPU has reached 3 TB/s, this speed is still the bottleneck of inference for short context, not to mention the slower inter-GPU and inter-node communication.\nTherefore, the purpose of reducing KV Cache is to achieve inference of longer context on fewer nodes, or to allow a larger inference batch size under the same context length, thereby achieving faster inference speed or greater total throughput. Of course, the ultimate goal is to achieve lower inference costs.\nTo learn more about this issue, I point the reader towards FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, A guide to LLM inference and performance, LLM inference speed of light and other articles. We will not continue to expand here (mainly because I afraid that my limited understanding would mean that the more I write, the more mistakes I will make)."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#mqa",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#mqa",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "MQA",
    "text": "MQA\nMQA, which stands for “Multi-Query Attention”, is a very simple attempt to reduce KV Cache. It was first proposed in Fast Transformer Decoding: One Write-Head is All You Need (2019), which means that even before the LLM craze, reducing KV Cache was already a topic of great interest to researchers.\nThe idea behind MQA is simple: directly let all Attention Heads share the same K and V. In terms of formulas, this means removing the superscript \\({}^{(s)}\\) from all \\(\\boldsymbol{k},\\boldsymbol{v}\\) in MHA:\n\\[\n\\begin{equation}\\require{cancel}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} ,\\boldsymbol{v}_{\\leq t}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)\\boldsymbol{v}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d\\times d_k}\\\\\n\\boldsymbol{k}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d_k} \\\\\n\\boldsymbol{v}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d_v}\n\\end{gathered}\n\\end{equation}\n\\]\nModels that use MQA include PaLM, StarCoder, and Gemini, among others. It’s clear that MQA directly reduces the KV Cache to \\(1/h\\) of its original size, which is very significant and, from the perspective of saving video memory alone, is already the upper limit.\nIn terms of performance, the loss in most tasks appears to be relatively limited, and MQA’s supporters believe that this loss can be compensated for through further training. In addition, it’s worth noting that because MQA shares K and V, there is just one matrix for projecting the hidden vector to the key vector, and another for projecting to the value vector, instead of \\(h\\) of them. Thus, the number of parameters for Attention will be reduced by nearly half. To keep the total number of model parameters unchanged, the size of feed-forward or gated linear unit is usually increased accordingly, which can also compensate for some of the performance loss."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#gqa",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#gqa",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "GQA",
    "text": "GQA\nHowever, some people are concerned that MQA compresses the KV Cache too much, which could affect the model’s learning efficiency and the final results. To address this, a transitional version between MHA and MQA, called GQA (Grouped-Query Attention), was developed. It originated from the paper GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints, which was published last year.\nIn retrospect, the idea behind GQA is also quite naive: it divides all Heads into \\(g\\) groups (where \\(g\\) is a divisor of \\(h\\)), and each group shares the same KV pair:\n\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{\\color{red}{(\\lceil sg/h\\rceil)}} ,\\boldsymbol{v}_{\\leq t}^{\\color{red}{(\\lceil sg/h\\rceil)}}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{\\color{red}{(\\lceil sg/h\\rceil)}}{}^{\\top}\\right)\\boldsymbol{v}_i^{\\color{red}{(\\lceil sg/h\\rceil)}}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{\\color{red}{(\\lceil sg/h\\rceil)}}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d\\times d_k}\\\\\n\\boldsymbol{k}_i^{\\color{red}{(\\lceil sg/h\\rceil)}} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d\\times d_k} \\\\\n\\boldsymbol{v}_i^{\\color{red}{(\\lceil sg/h\\rceil)}} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{\\color{red}{(\\lceil sg/h\\rceil)}}\\in\\mathbb{R}^{d\\times d_v}\n\\end{gathered}\n\\end{equation}\n\\]\nHere, \\(\\lceil\\cdot\\rceil\\) is the ceiling function. GQA provides a natural transition from MHA to MQA. When \\(g=h\\), it is MHA; when \\(g=1\\), it is MQA. When \\(1 &lt; g &lt; h\\), it only compresses the KV Cache to \\(g/h\\). The compression rate is not as good as MQA, but it also provides greater flexibility and better guarantees in terms of effectiveness. The most well-known users of GQA are probably Meta’s open-source Llama-2-70B, and the entire Llama-3 series. Other models using GQA include TigerBot, DeepSeek-V1, StarCoder2, Yi, ChatGLM2-6B, and ChatGLM3.1 There are many more models using GQA than models using MQA.\n1 Though the ChatGLM report claims to use “Multi-Query Attention” in page 3 of the report, it is actually using GQA with \\(g=2\\), as you can see in page 5 of the report.llama-2/3-70B uses \\(g=8\\), and it is the same for most other models of similar parameter count that uses GQA. This is not an accident, but a deliberate choice for efficient inference. We know that a model of 70B scale cannot be deployed on a single GPU (A100/H100 80G) without extreme quantization. If a single GPU is not enough, then a single node can be used. Generally, a node contains up to 8 GPUs. As we said earlier, each attention head is actually computed independently and then concatenated. When \\(g=8\\), it is possible to have each GPU to calculate the Attention Head corresponding to one set of KV. This maximizes KV diversity under the constraint of minimal inter-GPU communication."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#mla",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#mla",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "MLA",
    "text": "MLA\nWith the groundwork laid by MHA, MQA, and GQA, it becomes relatively easier to understand MLA (Multi-head Latent Attention). The technical report for DeepSeek-V2 introduces MLA from the perspective of low-rank projection, leading some readers to ask questions like “Why has LoRA been around for so long, and yet MLA, which is essentially just low-rank projection applied of KV Cache, took such a long time to appear?”.\nHowever, the author believes that the low-rank projection perspective doesn’t get to the heart of the matter. Because if we’re talking about low-rank projection, the fact is that if we stack all the K and V of GQA together, we’ll find that GQA is also basically low-rank projection:\n\\[\n\\begin{equation}\\underbrace{\\left[\\boldsymbol{k}_i^{(1)},\\cdots,\\boldsymbol{k}_i^{(g)},\\boldsymbol{v}_i^{(1)},\\cdots,\\boldsymbol{v}_i^{(g)}\\right]}_{\\boldsymbol{c}_i\\in\\mathbb{R}^{g(d_k+d_v)}} = \\boldsymbol{x}_i \\underbrace{\\left[\\boldsymbol{W}_k^{(1)},\\cdots,\\boldsymbol{W}_k^{(g)},\\boldsymbol{W}_v^{(1)},\\cdots,\\boldsymbol{W}_v^{(g)}\\right]}_{\\boldsymbol{W}_c\\in\\mathbb{R}^{d\\times g(d_k+d_v)}}\\end{equation}\n\\]\nHere, we combine all \\(\\boldsymbol{k}_i^{(s)},\\boldsymbol{v}_i^{(s)}\\) and denote them as \\(\\boldsymbol{c}_i\\). The corresponding projection matrices are also combined and denoted as \\(\\boldsymbol{W}_c\\). Note that generally \\(d_c = g(d_k+d_v) &lt; d\\), such as in Llama-2-70B where \\(g = 8, d_k = d_v = 128\\), so \\(d_c = 2048 &lt; d = 8192\\), so the transformation from \\(\\boldsymbol{x}_i\\) to \\(\\boldsymbol{c}_i\\) is also a low-rank projection. Therefore, the essential improvement of MLA is not the low-rank projection itself, but what is done after the low-rank projection.\n\nPart 1\nWhat does GQA do after the projection? First, it divides the vector \\(\\boldsymbol{c}_i\\) into two halves, using them as K and V respectively. Then, each half is further divided into \\(g\\) parts as \\(\\boldsymbol{k}_1^{(s)}, \\dots, \\boldsymbol{k}_g^{(s)}, \\boldsymbol{v}_1^{(s)}, \\dots, \\boldsymbol{v}_g^{(s)}\\), and each is then copied \\(h/g\\) times, in order to “fill up” the K and V needed by \\(h\\) Attention Heads. We know that splitting and copying are simple linear transformations, so MLA’s first key idea is to replace these simple linear transformations with general linear transformations to enhance the model’s capacity:\n\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{(s)} ,\\boldsymbol{v}_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(s)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d\\times d_k}\\\\\n\\boldsymbol{k}_i^{(s)} = \\boldsymbol{c}_i\\boldsymbol{W}_k^{(s)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(s)}\\in\\mathbb{R}^{d_c\\times d_k} \\\\\n\\boldsymbol{v}_i^{(s)} = \\boldsymbol{c}_i\\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_c\\times d_v} \\\\[10pt]\n\\boldsymbol{c}_i = \\boldsymbol{x}_i \\boldsymbol{W}_c\\in\\mathbb{R}^{d_c},\\quad \\boldsymbol{W}_c\\in\\mathbb{R}^{d\\times d_c}\n\\end{gathered}\n\\end{equation}\n\\]\nHowever, while this approach theoretically increases the model’s capacity, let’s not forget that the main purpose of GQA is to reduce KV Cache. For the sake of saving computation and communication costs, we generally cache the projected \\(\\boldsymbol{k}_i, \\boldsymbol{v}_i\\) rather than the pre-projected \\(\\boldsymbol{c}_i\\) or \\(\\boldsymbol{x}_i\\). However, MLA’s approach, by using different projection matrices, makes all K and V Heads distinct again. This means the KV Cache size would revert to the same size as MHA, which goes against the very design purpose of GQA.\nTo solve this problem, MLA uses a simple clever identity on the dot-attention to circumvent this problem. First, we proceed as usual during the training. Then, during the inference, we use this identity\n\\[\n\\begin{equation}\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top} = \\left(\\boldsymbol{x}_t\\boldsymbol{W}_q^{(s)}\\right) \\left(\\boldsymbol{c}_i\\boldsymbol{W}_k^{(s)}\\right){}^{\\top} = \\boldsymbol{x}_t\\left(\\boldsymbol{W}_q^{(s)}\\boldsymbol{W}_k^{(s)}{}^{\\top}\\right)\\boldsymbol{c}_i^{\\top} \\end{equation}\n\\]\nThis means that during the inference phase, we can merge \\(\\boldsymbol{W}_q^{(s)}\\boldsymbol{W}_k^{(s)}{}^{\\top}\\) as the projection matrix for Q.2 Then, \\(\\boldsymbol{c}_i\\) replaces the original \\(\\boldsymbol{k}_i\\).\n2 Note a detail here. Merging \\(\\boldsymbol{W}_q^{(s)}\\boldsymbol{W}_k^{(s)}{}^{\\top}\\) into a single matrix is only valid assuming infinite precision. In practice, if we use single precision, especially BF16, the precision loss after the matrix-merge is often noticeable. After multiple layers, this loss may become large enough that we would have to post-process.Similarly, since there is another projection matrix after \\(\\boldsymbol{o}_t\\), the \\(\\boldsymbol{W}_v^{(s)}\\) in \\(\\boldsymbol{v}_i^{(s)} = \\boldsymbol{c}_i\\boldsymbol{W}_v^{(s)}\\) can also be absorbed into the subsequent projection matrix. Thus, equivalently, \\(\\boldsymbol{v}_i\\) can also be replaced by \\(\\boldsymbol{c}_i\\). Specifically, after the attention weights \\(\\alpha_{tj}^{(s)}\\) are computed using the dot-attention mechanism, we upcast the latent vector \\(\\boldsymbol{c}_i\\) to the value vector \\(\\boldsymbol{v}_i^{(s)}\\) using \\(\\boldsymbol{W}_v^{(s)}\\), do a weighted sum with the attention weights, then use a final output projection by \\(\\boldsymbol{W}_o^{(s)}\\). We can do the same merge at inference time:\n\\[\n\\boldsymbol{v}_t = \\sum_{j\\leq t} \\boldsymbol{c}_j \\left(\\sum_{s=1}^h \\alpha_{tj}^{(s)} \\boldsymbol{W}_v^{(s)} \\boldsymbol{W}_o^{(s)\\top}\\right)\n\\]\nThis means that at this point, the KV Cache only needs to store all \\(\\boldsymbol{c}_i\\), instead of all \\(\\boldsymbol{k}_i^{(s)}\\) and \\(\\boldsymbol{v}_i^{(s)}\\). Note that \\(\\boldsymbol{c}_i\\) is independent of \\({}^{(s)}\\), which means it is shared by all heads. In other words, during the inference phase, MLA can be converted into an MQA via a clever identity.\nTo reiterate, the key theme of this article has always been reducing the KV Cache. So what has MLA achieved so far? The answer is that it has enhanced the capacity of GQA through different projection matrices, while maintaining the same size of KV Cache during inference. Conversely, if we only need capacities similar to GQA, can we further reduce the KV Cache? In other words, \\(d_c\\) doesn’t need to be \\(g(d_k+d_v)\\), but can be a smaller value (DeepSeek-V2 uses \\(d_c = 512\\)), thereby further compressing the KV Cache. This is the key idea of MLA.\n\n\nPart 2\nEverything seems perfect, and it looks like we’re about to finish cooking an ideal design that is both good and economical. But hold on, if we think a little deeper, we’ll find that MLA, as it stands, has an unavoidable flaw – it’s incompatible with RoPE (Rotary Position Embedding).\nWe just mentioned that the key step for MLA to maintain the same KV Cache size as GQA is “merging \\(\\boldsymbol{W}_q^{(s)}\\boldsymbol{W}_k^{(s)}{}^{\\top}\\) into a single (position-independent) matrix as the projection matrix for Q”. However, if RoPE is added, this step becomes impossible. This is because RoPE is a position-dependent, \\(d_k\\times d_k\\) block diagonal matrix \\(\\boldsymbol{\\mathcal{R}}_m\\), satisfying \\(\\boldsymbol{\\mathcal{R}}_m\\boldsymbol{\\mathcal{R}}_n^{\\top}=\\boldsymbol{\\mathcal{R}}_{m-n}\\). When RoPE is added to MLA, it inserts an additional term \\(\\boldsymbol{\\mathcal{R}}_{t-i}\\) between \\(\\boldsymbol{W}_q^{(s)}\\) and \\(\\boldsymbol{W}_k^{(s)}{}^{\\top}\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n\\boldsymbol{q}_i^{(s)} &= \\boldsymbol{x}_i\\boldsymbol{W}_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\\\\n\\boldsymbol{k}_i^{(s)} &= \\boldsymbol{c}_i\\boldsymbol{W}_k^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i} \\\\\n\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top} &= \\left(\\boldsymbol{x}_t\\boldsymbol{W}_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_t}\\right) \\left(\\boldsymbol{c}_i\\boldsymbol{W}_k^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right){}^{\\top} = \\boldsymbol{x}_t\\left(\\boldsymbol{W}_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_{t-i}}\\boldsymbol{W}_k^{(s)}{}^{\\top}\\right)\\boldsymbol{c}_i^{\\top}\n\\end{aligned}\n\\end{equation}\n\\]\nThe term \\(\\boldsymbol{W}_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_{t-i}}\\boldsymbol{W}_k^{(s)}{}^{\\top}\\) cannot be combined into a static projection matrix (since it’s related to the position difference \\(t-i\\), which is not static), thus the key idea of MLA could not be combined with RoPE.\nSome time ago, I had the honor of discussing this issue with the DeepSeek team. This problem turned out to be fundamental, so I couldn’t actually offer any effective advice at the time. The simplest approach is to just give up RoPE and switch to another position encoding scheme that uses positional encoding based on attention bias, such as ALiBi. However, DeepSeek’s experiments show that it is significantly inferior to RoPE (note that MLA can use RoPE, but after adding RoPE, the identity transformation trick cannot be used to reduce KV Cache). I also suggested trying Sandwich, which doesn’t monotonically decay to negative infinity like ALiBi, so it might have better results, but it feels like a band-aid hack, not a real solution. Another compromise is to change the input of \\(\\boldsymbol{q}_i\\) to \\(\\boldsymbol{c}_i\\) as well, and then add RoPE after \\(\\boldsymbol{c}_i\\), i.e.,\n\\[\n\\begin{equation}\\boldsymbol{q}_i^{(s)} = \\boldsymbol{c}_i\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\boldsymbol{W}_q^{(s)},\\quad\\boldsymbol{k}_i^{(s)} = \\boldsymbol{c}_i\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\boldsymbol{W}_k^{(s)}\\end{equation}\n\\]\nIn this way, \\(\\boldsymbol{\\mathcal{R}}_i\\) can be absorbed into \\(\\boldsymbol{c}_i\\), but then we lose the \\(\\boldsymbol{\\mathcal{R}}_m\\boldsymbol{\\mathcal{R}}_n^{\\top}=\\boldsymbol{\\mathcal{R}}_{m-n}\\) operation. In this case, RoPE no longer implements relative position through absolute position, but simply adds absolute positions to Q and K, forcing the model to learn end-to-end how to extract the relative position information that it needs to do its job.\nThe final released MLA adopts a hybrid approach – each Attention Head’s Q and K adds \\(d_r\\) dimensions for adding RoPE, where the added dimensions for K are shared across all Heads:\n\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{(s)} ,\\boldsymbol{v}_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(s)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\left[\\boldsymbol{x}_i\\boldsymbol{W}_{qc}^{(s)}, \\boldsymbol{x}_i\\boldsymbol{W}_{qr}^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right]\\in\\mathbb{R}^{d_k + d_r},\\quad \\boldsymbol{W}_{qc}^{(s)}\\in\\mathbb{R}^{d\\times d_k},\\boldsymbol{W}_{qr}^{(s)}\\in\\mathbb{R}^{d\\times d_r}\\\\\n\\boldsymbol{k}_i^{(s)} = \\left[\\boldsymbol{c}_i\\boldsymbol{W}_{kc}^{(s)}, \\boldsymbol{x}_i\\boldsymbol{W}_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right]\\in\\mathbb{R}^{d_k+d_r},\\quad \\boldsymbol{W}_{kc}^{(s)}\\in\\mathbb{R}^{d_c\\times d_k}, \\boldsymbol{W}_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d_r} \\\\\n\\boldsymbol{v}_i^{(s)} = \\boldsymbol{c}_i\\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_c\\times d_v} \\\\[10pt]\n\\boldsymbol{c}_i = \\boldsymbol{x}_i \\boldsymbol{W}_c\\in\\mathbb{R}^{d_c},\\quad \\boldsymbol{W}_c\\in\\mathbb{R}^{d\\times d_c}\n\\end{gathered}\n\\end{equation}\n\\]\nIn this way, the dimensions without RoPE can repeat the operation described in Part 1. During inference, the KV Cache only needs to store \\(\\boldsymbol{c}_i\\). The newly added dimensions with RoPE can be used to supplement positional information. And since all Heads share them, only \\(d_r\\) dimensions are added to the K Cache. The original paper took \\(d_r = d_k / 2 = 64\\), which is a small increase compared to the original \\(d_c = 512\\).\n\n\nPart 3\nOne final detail: the final iteration of MLA also changes the input of Q to a low-rank projection form. This is not related to reducing KV Cache, but mainly to reduce the amount of parameters and the corresponding gradients3 during training that occupy GPU memory:\n3 The original paper said “Moreover, in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache”, which I personally don’t quite understand.\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{(s)} ,\\boldsymbol{v}_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(s)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\left[\\boldsymbol{c}_i'\\boldsymbol{W}_{qc}^{(s)}, \\boldsymbol{c}_i'\\boldsymbol{W}_{qr}^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right]\\in\\mathbb{R}^{d_k + d_r},\\quad \\boldsymbol{W}_{qc}^{(s)}\\in\\mathbb{R}^{d_c'\\times d_k},\\boldsymbol{W}_{qr}^{(s)}\\in\\mathbb{R}^{d_c'\\times d_r}\\\\\n\\boldsymbol{k}_i^{(s)} = \\left[\\boldsymbol{c}_i\\boldsymbol{W}_{kc}^{(s)}, \\boldsymbol{x}_i\\boldsymbol{W}_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right]\\in\\mathbb{R}^{d_k+d_r},\\quad \\boldsymbol{W}_{kc}^{(s)}\\in\\mathbb{R}^{d_c\\times d_k}, \\boldsymbol{W}_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d_r} \\\\\n\\boldsymbol{v}_i^{(s)} = \\boldsymbol{c}_i\\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_c\\times d_v} \\\\[10pt]\n\\boldsymbol{c}_i' = \\boldsymbol{x}_i \\boldsymbol{W}_c'\\in\\mathbb{R}^{d_c'},\\quad \\boldsymbol{W}_c'\\in\\mathbb{R}^{d\\times d_c'} \\\\\n\\boldsymbol{c}_i = \\boldsymbol{x}_i \\boldsymbol{W}_c\\in\\mathbb{R}^{d_c},\\quad \\boldsymbol{W}_c\\in\\mathbb{R}^{d\\times d_c} \\\\\n\\end{gathered}\n\\end{equation}\n\\]\nNote the second term in \\(\\boldsymbol{k}_i^{(s)}\\), the part with RoPE, its input is still \\(\\boldsymbol{x}_i\\) and not \\(\\boldsymbol{c}_i\\). This maintains the original paper’s setting, and it’s not a typo. Also, \\(d_c' = 1536\\) in the original paper, which is different from \\(d_c=512\\). Also, we put the MHA with RoPE below for easy comparison:\n\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{(s)} ,\\boldsymbol{v}_{\\leq t}^{(s)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(s)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{(s)}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(s)}\\in\\mathbb{R}^{d\\times d_k}\\\\\n\\boldsymbol{k}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(s)}\\in\\mathbb{R}^{d\\times d_k} \\\\\n\\boldsymbol{v}_i^{(s)} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(s)}\\in\\mathbb{R}^{d\\times d_v}\n\\end{gathered}\n\\end{equation}\n\\]\nIt can be observed that, during the training phase, apart from an additional low-rank projection step and RoPE being added only in some dimensions, MLA is essentially the same as MHA where the Q and K Head Size is changed from \\(d_k\\) to \\(d_k + d_r\\).\nThe MLA in the inference phase is changed to:\n\\[\n\\begin{equation}\n\\begin{gathered}\n\\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}\\boldsymbol{W}_v^{(1)}, \\boldsymbol{o}_t^{(2)}\\boldsymbol{W}_v^{(2)}, \\cdots, \\boldsymbol{o}_t^{(h)}\\boldsymbol{W}_v^{(h)}\\right] \\\\[10pt]\n\\boldsymbol{o}_t^{(s)} = \\mathrm{Attention}\\left(\\boldsymbol{q}_t^{(s)}, \\boldsymbol{k}_{\\leq t}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} ,\\boldsymbol{c}_{\\leq t}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)\\boldsymbol{c}_i}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(s)} \\boldsymbol{k}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}{}^{\\top}\\right)} \\\\[15pt]\n\\boldsymbol{q}_i^{(s)} = \\left[\\boldsymbol{c}_i'\\boldsymbol{W}_{qc}^{(s)}\\boldsymbol{W}_{kc}^{(s)}{}^{\\top}, \\boldsymbol{c}_i'\\boldsymbol{W}_{qr}^{(s)}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right]\\in\\mathbb{R}^{d_c + d_r}\\\\\n\\boldsymbol{k}_i^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}} = \\left[\\boldsymbol{c}_i, \\boldsymbol{x}_i\\boldsymbol{W}_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\color{#3ce2f7}{\\boldsymbol{\\mathcal{R}}_i}\\right]\\in\\mathbb{R}^{d_c+d_r}\\\\\n\\boldsymbol{W}_{qc}^{(s)}\\in\\mathbb{R}^{d_c'\\times d_k},\\boldsymbol{W}_{kc}^{(s)}\\in\\mathbb{R}^{d_c\\times d_k},\\boldsymbol{W}_{qr}^{(s)}\\in\\mathbb{R}^{d_c'\\times d_r},\\boldsymbol{W}_{kr}^{\\color{#ccc}{\\smash{\\bcancel{(s)}}}}\\in\\mathbb{R}^{d\\times d_r} \\\\[10pt]\n\\boldsymbol{c}_i' = \\boldsymbol{x}_i \\boldsymbol{W}_c'\\in\\mathbb{R}^{d_c'},\\quad \\boldsymbol{W}_c'\\in\\mathbb{R}^{d\\times d_c'} \\\\\n\\boldsymbol{c}_i = \\boldsymbol{x}_i \\boldsymbol{W}_c\\in\\mathbb{R}^{d_c},\\quad \\boldsymbol{W}_c\\in\\mathbb{R}^{d\\times d_c} \\\\\n\\end{gathered}\n\\end{equation}\n\\]\nAt this point, the Head Size of Q and K becomes \\(d_c + d_r\\), while the Head Size of V becomes \\(d_c\\). According to the original paper’s settings, this is equal to \\(4d_k = 4d_v\\). So this inference-time change, although in effect reduces the KV Cache, increases the computational cost of inference.\nSo why does it still improve inference efficiency? This brings us back to the issue discussed in the Bottleneck section. We can divide LLM inference into two parts: inference on the prompt for generating the first Token (Prefill) and generating each subsequent token (Generation). The Prefill stage involves parallel computation over all tokens in the prompt and storing the corresponding KV Cache. This stage can be bottlenecked by all of computation, bandwidth, and memory. Although MLA increases the computational cost, the reduction in KV Cache also reduces the pressure on memory and bandwidth, so it’s roughly equal trade-off without benefit or cost. However, in the Generation stage, since only one token is computed at each step, it is only bottlenecked by bandwidth and memory. Therefore, the introduction of MLA can theoretically significantly improve the speed of Generation.\nThere is another detail that fully reflects this characteristic. In a typical LLM architecture, the parameters satisfy \\(h \\times d_k = d\\), meaning num_heads * head_size = hidden_size. However, DeepSeek-V2 is different. It has \\(d_k=128, d=5120\\), but \\(h=128\\), which is 3 times the usual setting! This is because the KV Cache size of MLA is independent of \\(h\\), thus, increasing \\(h\\) only increases the computational cost and improves the model’s ability, but it does not increase the KV Cache, so it does not cause a speed bottleneck."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#summary",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#summary",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "Summary",
    "text": "Summary\nThis article briefly outlines the evolution of multi-head attention, particularly the changes in concept from MHA to MQA, GQA, and finally to MLA, and then elaborates on the details of MLA. In this article, MLA is regarded as a generalization of GQA. It replaces GQA’s splitting and replication with projection matrices, and introduces an identity transform to further compress the KV Cache, while adopting a hybrid method to be compatible with RoPE. Overall, MLA is a very practical variant of the attention mechanism."
  },
  {
    "objectID": "docs/posts/2024-05-13-multi-latent-attention/index.html#metadata",
    "href": "docs/posts/2024-05-13-multi-latent-attention/index.html#metadata",
    "title": "The ultimate tug-of-war between cache and capacity",
    "section": "Metadata",
    "text": "Metadata\n\nOriginal is 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA posted on the homepage of Jianlin Su (苏剑林).\nI added some extra explanatory notes here and there."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html",
    "href": "docs/posts/liu-cixin-anthology/index.html",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "",
    "text": "Most of these essays are collected in The Best Earth in the Worst Universe: A Collection of Liu Cixin’s Science Fiction Comments and Essays (最糟的宇宙，最好的地球: 刘慈欣科幻随笔集), published in 2015, just after Liu Cixin became internationally acclaimed. A few others were from his blog.\nHe pulls no punches, and I suspect they will never be translated for an English audience. To see why, consider just the debate, where he claimed to be a “fanatic of Technologism”, or the essay where he claims space exploration is more important than the dubious effort of climate change control. Such political incorrectness appears here and there as well, but he is honest and consistent in his viewpoint, and many in China, especially the Industrial Party, believe in his vision.\nI translated with the help of DeepL. Those cover about 1/4 of the book. The rest of the book seems hardly of general interest.\nNotes on translation:\nNote about the persistent questions of the Cultural Revolution chapter. When The Three-Body Problem was serialized on Science Fiction World over the course of 2006, it was close to the author’s ideal. It seemed like censorship was not as tight for serialized publication compared to single-volume books. For example:\nSee this Zhihu answer for photos and descriptions of the serialized version. Basically, the English translation is a direct translation of the serialized version, without the sacrifices made for getting past the censors.\nAs he said in a blog post, 2006 was a sensitive time (40-th anniversary of the Cultural Revolution), and he wasn’t even sure that it would ever be published as a single-volume. Nevertheless he went ahead and serialized it. Fortunately, it was published as a single-volume in 2008."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#electronic-poet",
    "href": "docs/posts/liu-cixin-anthology/index.html#electronic-poet",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Electronic poet (2001)",
    "text": "Electronic poet (2001)\n电子诗人, posted on 2001-01-07 at the Tsinghua University BBS, Sci-Fi section.\nThe program is available for download here. Liu Cixin later explained in 2012-09-10 that it was written in 1989.\n***********************************\nWork No. 75509\n**************************************\nI face the black artist and the thorny waves\nI see the dazzling mind taking a nap, the program code slamming the playground\nIn this olive green playground, there are no trucks, only butterflies\nI want to take drugs, I want to turn yellow weakly\nI face the bright winter snow and the hyperbolic glow\nI see the blue breasts floating, the soap listening to the walrus\nIn this weak spring rain, there is no Beethoven, only mother\nI want to rise, I want to sing with breathing\nI face the wide boat and the transparent microwave beam\nI see the dead fishing boat calling, the distilled water shoveling the sheep\nIn this porous moss, there is no Eve, only the teacher\nI want to hibernate, I want to shine hatefully\nI face the bloody epic and the distant fire\nI saw the vibrant battleships in silence, the transparent skirts caressing the playground\nIn this curvilinear struggler, there is no moonlight dance, only wind and sand\nI want to swing, I want to panic roughly\nWhat did you see? No, no, don’t get me wrong, this is not what I wrote.\nDear sci-fi friends, the new century is coming, so you must wish to bring with you some local specialties from this century. After thinking about it, I thought of the same thing: poets. Poets are certainly not born of the 21st century, but they are certainly to go extinct in this century. The poetic century has disappeared forever. In the new century, even if there are poets, they must be as rare as dinosaur eggs.\nThis spring, I read The First Sally (A) or Trurl’s Electronic Bard by Stanisław Lem (a masterpiece, I hope everyone will read it). Then I buried my head in front of the computer and worked hard for a week, turning Lem’s fantasy into reality at least in part: I created an electronic poet, or as Gibson said in Neuromancer, a poet’s “construct”.\nBoth I and the “construct” are self-aware, and I don’t want to compare with Li Bai and Percy Bysshe Shelley, but I can definitely compare with modern poets. It is said that modern poetry emphasizes obscurity and freedom, so let them compare with my CPU to see who is more obscure and free!\nMaybe when reading these poems, some people feel that the poet’s senses are deranged, but isn’t a “systematic derangement of the senses” what modern poetry pursues? More importantly, this is computer-style poetry, which humans cannot imitate! If you don’t believe me, try, and you will have a heart failure in a short time.\nBut the biggest advantage of the electronic poet is speed. According to the latest measurement, the poetry output is 200 lines/second (non-rhyming) or 150 lines/second (rhyming). This is the test result on my old Pentium 166 MMX machine. If it is on Pentium III 500, haha. The poetry production method is absolutely automatic. Except for inputting the number of lines, no human intervention is required.\nThe day before yesterday, I had a party with friends and asked him to write a 300,000-line1 long poem to liven up the party. He was really good. He wrote it before he finished half a bottle of Erguotou liquor. Unfortunately, I didn’t even finish appreciating one thousandth of it. But the poet swears on its matho-personality and the RAND() function that there would be not a single repeated line in 300,000 lines! The electronic poet is programmed with Visual FoxPro, which contains 5 program modules, 6 word libraries, and a grammar library. I just slimmed it down and removed all GUI stuff. Although it is not beautiful, it looks like DOS, but it is very slim, only 125KB. I would like to give this as a new century gift to all my sci-fi comrades. If anyone is willing to send me a nice new century greeting card (ndjsjl@public.yq.sx.cn) by email, I will send you the poet.exe. (The original code is not compiled, and runs under VF3.0 and above, with the main control module code page)\n1 The speed of light is \\(300,000 \\mathrm{~km/s}\\), so Liu Cixin likes that number.Imagine when you are old, you and your few great-grandchildren stand under the eco-dome, and 20 artificial suns cast bright light from space. At this time, you talk to your few great-grandchildren about our romantic and passionate era, and you lower your voice to tell them that there once was such a thing as night, such a thing as the moon, such a thing as trees, and such a thing as grass… When they stare at each other in disbelief, you would then tell them that there was also such a thing as a poet! Before you know it, tons of new poems torrent out of the Pentium-300000 wrist-computer!! Think about the expressions of your few great-grandchildren, haha.\nPlease enjoy a few more poems below. I am afraid of being too superficial, so I dare not post too many, let alone long poems.\n********************************\nWork No. 28611\n***********************************\nAsteroids are called\nAround the solid, there are only gelatinous rice fields\nNo, I don't want to fly!!\nI miss\n\nTrigonometric functions are watched!\nAround Andromeda, there are only living giant rivers\nNo, I don't want to devour myself!!\nI precipitate\n\nDragonflies are pinched!\nAround the Orient Express, there are only wagging bows and arrows\nNo, I don't want to smoke!!\nI talk\n\nThe confinement room is warned!\nAround the sword, there is only the squeak of time\nNo, I don't want to sleepwalk!!\nI rot\n[Other poems are omitted, as they are in the same nonsense style.]"
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#religious-feeling",
    "href": "docs/posts/liu-cixin-anthology/index.html#religious-feeling",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Scifism – on the image of the cosmos in sci-fi (2001)",
    "text": "Scifism – on the image of the cosmos in sci-fi (2001)\nSf教——论科幻小说对宇宙的描写, posted on 2001-02-22 at the Tsinghua University BBS, Sci-Fi section.\nAt present, Chinese sci-fi lacks a lot of things, one of which has never been noticed or mentioned, but is extremely important:\nChinese sci-fi lacks religious feelings.\nFirst of all, I am a firm atheist. At the same time, we know that science and religion are incompatible, and sci-fi and religion are also incompatible. However, some scholars believe that the reason why modern natural science was born in the West has something to do with the strong religious feelings in Western culture. This is a topic that can’t be explained even with a huge book that presses people to death, so I’m not going to delve into it here, and I’ll just talk about religious feelings in sci-fi. Note that it’s not religion that’s being talked about here, it’s religious feeling, and it’s not the kind of feeling you have for God, it’s atheistic, and it’s not as sophisticated as Spinoza or whatever.\nThe religious feeling of sci-fi is a deep sense of awe at the grand mystery of the universe.\nConsider the following two depictions, one of which depicts police chasing criminals across the stars:\n\nThe police ship clung to the smuggling ship, skimming planet after planet. Every planet, the smuggling ship captain carefully observed the planet’s terrain, he was eager to find a planet with suitable terrain to land, with the pursuers of the duel, but has not been able to find, had to turn back to look at the police spaceship closer and closer, gritted his teeth and continued to fly forward…\n\nThe second depicts a head-on encounter between two giant starships traveling at a fraction of the speed of light:\n\n“They just missed with us!”, shouted the navigator on board ship XX, and at the sound of his voice, the ship’s pilot jerked the control lever back, and the XX turned over in a somersault, turned 180 degrees, and chased after the ship…\n\nBoth are paraphrased from domestic sci-fi. The former gives the reader the impression that the universe is not much bigger than a small town in a cop movie, and that the planets in space are just like storefronts on the side of the thoroughfare. The latter makes the reader feel that an interstellar spacecraft flying at relativistic speeds behaves much like a taxicab on the street. In such a depiction, the author is blind to the cosmic grandeur. It is not that such a depiction is totally unacceptable; such episodes are often found in many of the world’s most famous novels, such as Interstellar Detective2 and so on. For these allegorical novels, the universe is just a tool to develop the plot. But the main attraction of sci-fi is not that.\n2 No idea what this work is.A huge spaceship, flying in the dark silence of space to a distant goal. It will take 2000 years to accelerate, maintain cruising speed for 3000 years, and then 2000 years to decelerate. Generations of people have been born and died. The Earth has become an ancient vague dream. The spaceship archaeologists can no longer unearth evidence, out of the vast strata of the spaceship, that the Earth once existed. That distant destination has become a myth passed down for millennia, a religious phantasy. Generation after generation, people don’t know where they came from; generation after generation, people don’t know where they are going. Most people believe that the spaceship is an eternal world that has existed and will exist forever, and only a small number of wise men, convinced of the existence of the destination, look day and night into the infinitely deep cosmic abyss in front of the spaceship… This is the theme of several Western sci-fi novels. What do you feel in such a depiction? Is it the far-reaching vastness of the universe, or the shortness of life. Perhaps, you thus look down on the whole history of humanity from the perspective of the universe, or the eyes of God, and you find with emotion that our civilization is only a tiny grain of sand in the cosmic desert of space and time.\nOne might think that FTL technology and spacetime leapfrogging depicted in sci-fi would inevitably make the universe perceptually smaller, just as airplanes and modern communication networks have made the earth smaller. This is true. If FTL technology is really possible, perhaps the universe will one day be just a village in human senses, just as the global village is today. But we’re talking about novels. Think about it, given two novels which one do you want to read – one depicting Columbus in the vast Atlantic Ocean, searching with great fear and little hope for the new land of his dreams, the other depicting a company clerk traveling by plane from Paris to New York City on a business trip. Meanwhile, the Earth hasn’t actually been shrunk; vast expanses of earth and ocean still exist, and modern man is still experiencing the romantic thrill of ancient man’s trek across the surface of this planet through hiking and the America’s Cup race. There’s no reason for sci-fi to shrink the universe into villages when most people can’t even fly out of the atmosphere. What’s more, even in the FTL era, the universe as a whole is still full of great mystery and shock value.\nFrederick Pohl’s Father of the Stars describes a billionaire who has spent his life building dozens of gigantic spaceships, all with conventional rocket engines, which carry tens of thousands of people into the vastness of space to open up a new living space for humanity. A few decades after the departure of these spaceships, science on Earth made FTL spaceships a reality, and such spaceships, carrying the hero in his twilight years, took only a day or two to catch up with those traditional spaceships that had departed a few decades earlier, making the feat that the hero and tens of thousands of pioneers had carried out with all their lives a meaningless tragedy. In this novel, Pohl uses the contrast between the two technologies to create a simultaneous feeling of the vastness of outer space, the tragedy of the pioneers, and the relentlessness of fate.\nThe pinnacle of depictions of vast spacetime belongs to Arthur C. Clarke’s 2001, The fear, loneliness and awe of human beings in front of the mysterious universe expressed in the novel, which carves into the readers’ heart and soul, is unforgettable for the rest of their lives. I remember 20 years ago that winter night, after reading that book, I went out to look up at the night sky, and suddenly felt that everything around me had disappeared, and the earth under my feet became a pure geometric plane with infinite extension of snow-white and smooth, and on this infinite two-dimensional plane, under the magnificent starry night sky, there stood I alone, facing this vast mystery that the human mind could not grasp… Since then, the starry sky looked different in my eyes. The feeling is like leaving a pond to see the sea. This made me deeply appreciate the power of sci-fi.\nIn the busy and practically-minded modern society, people’s eyes are mostly confined in boxes, and they seldom look at space. I once asked ten people whether the moon would come out in the daytime, and except for one person who was a bit hesitant, the others were very sure that it would not. Modern society has also made people numb to astronomical numbers. No one seriously tries to visualize a light years in the world, and 15 billion lightyears is not much different from 15 billion kilometers in most minds. Numbness to the universe pervades society. The mission of sci-fi is to broaden and deepen people’s minds, and if readers stop on their way home from work and look up at the starry sky for a while, the novel is a great success. Unfortunately, our sci-fi is currently in this same numbness to a considerable degree. This is probably due to two reasons.\nThe first reason is in philosophy. It is presumed that sci-fi, like mainstream literature, depicts the relationship between people. Under this concept, the universe is only a prop, a background and an accompaniment in the works. Undeniably, many excellent works have been produced under this concept, but the greatest advantage and charm of sci-fi is the depiction of the relationship between human beings and the universe. The universe should be as important a protagonist in sci-fi as man. The reason why 2010 and 2061 – the two sequels of 2001 – are not as successful as 2001, is that the author have shifted his focus to depicting the relationships of human society and destroyed the mystery and ethereal nature of the universe that was established in 2001.\nTruly gut-feeling the universe is hard. Standing on the roof of a tall building, we have the feeling of being high above the ground; sitting on a hot steam balloon rising to 1000 meters, this feeling is even more strong and dizzying; but if we look down from an airliner flying at 20,000 meters, this sense of altitude is rather diminished; looking down from a space shuttle operating in an orbit several hundred kilometers high, we need imagination to get a sense of altitude; and looking at Earth from the moon, which is more than 300,000 kilometers away, we can’t get a sense of altitude in any case – it just looks like a lovely blue toy. It is very difficult for humans to perceive the mega scale. The grandeur of the universe is also manifested in the opposite microscopic direction, which is even more difficult for human perception. At the same time, modern science has reached a very deep level of understanding about the macroscopic and microscopic aspects of the universe, and the description of the universe by science is not only beyond our imagination, but even beyond what we could have imagined. To truly appreciate the grandeur of the universe and to show this grandeur in novels requires imagination beyond ordinary people, high literary-technical skills, and a deep understanding of the forefront of modern science. It is the perpetual grand challenge and goal for sci-fi.\nBut all of this presupposes the kind of religious feeling that sci-fi authors have for the universe.\nA philosophy professor once said that the first introductory lesson for philosophy freshmen should be to look up at the stars for long time late at night. I think this should be, even more so, the first lesson for sci-fi authors, as it will enable them to really find the feeling of sci-fi deep inside.\nThe grand and mysterious universe is the God of sci-fi, and the Gospel of Scifism is:\n\nFeel the Lord’s greatness. Feel the Lord’s depth. Describe this feeling. Show it to those busy people, so that they feel the Lord’s greatness and depth as you do. Then blessed shall be you, those busy people, and Chinese sci-fi."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#acceptance-speech-at-the-2000-galaxy-awards-2000",
    "href": "docs/posts/liu-cixin-anthology/index.html#acceptance-speech-at-the-2000-galaxy-awards-2000",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Acceptance speech at the 2000 Galaxy Awards (2000)",
    "text": "Acceptance speech at the 2000 Galaxy Awards (2000)\nBased on the translation by Forestaller: https://www.reddit.com/r/threebodyproblem/comments/as70eu/ot_translation_liu_cixins_acceptance_speech_at/. Copied and fixed.\nI am very glad that The Wandering Earth (2000) has gained the love of readers at large. This short story is part of a series I planned call “The Apocalyptic Series”: based on the premise of a solar catastrophe, it describes the process of humanity’s attempts at escape by various means; in descending order of the success rate of escape:\nThe 1st is Mend the Heavens, describing how humans enter the sun to fix and delay the solar catastrophe from the inside;\nThe 2nd is Micro-Age (1998);\nThe 3rd is The Wandering Earth (2000);\nThe 4th is Starship Eras, describing how humans left the solar system on spaceships, lost their targets and came to regard the spaceships as home;\nThe 5th is Wandering Souls, describing how humans broadcast their thoughts and memories to the universe before the solar catastrophe;\nAnd the last is very dark – named On Pluto we sat down and wept, after Byron’s poem By the river of Babylon we sat down and wept – it describes how all hope was lost and a monument to humanity was built on Pluto, in what is more of a grim essay.3\n3 This Plutonian “monument to humanity” appears in Death’s End. Also, the 1st, 4th, 5th stories are unwritten.4 Before writing Ball Lightning, Liu thought he had liver cancer, so he wrote it to be his final work.When you are diagnosed with cancer, the world changes in your eyes: the sky is red and the sun is blue; but when you are finally informed that it was a mis-diagnosis, when the sky turns blue and the sun turns red again, they are not the same sky and sun as before – the world and living becomes more beautiful in your eyes, and maybe even more meaningful;4 this is not a feeling you can ever get from a decade’s worth of books. The apocalyptic experience of an individual is a very valuable one; so what about the apocalyptic experience of all humanity? If the world suffers such a “mis-diagnosis”; then all humanity will look at our sky and sun with new eyes, treasure more greatly all things they regard as normal, and the human world will move along a more reasonable track. Only sci-fi novels can bring about this apocalyptic experience, and this is my initial purpose in planning this apocalyptic series.\nSci-fi novels can create a whole new world, where writers and readers can experience what is impossible in reality – this is why I love sci-fi. I started writing as a sci-fi fan, without any systematic consideration of sci-fi theories. I like sci-fi with less literary elements and more sci-fi elements, and have always thought that penetrating reality and dissecting human nature is neither the mission of sci-fi novels nor its comparative advantage – the advantage of sci-fi lies in creating an imaginary world. I used to hold a view that even I find extremist now: to tear sci-fi out from the confines of literature itself (the Sci-Fi section of the Tsinghua University BBS once tried to create an encyclopedia for virtual worlds, without much success). Of course, this proposal was attacked on all fronts.\nI very much agree with what A-Lai [an earlier speaker] said: every writer should persist in their extremist views, while editors should hold an all-encompassing attitude towards all kinds of view; this is the only healthy environment for the development of sci-fi. But on the other hand, when sci-fi changed from a hobby into a career, I’ve found the need to manage many balancing acts – this includes balancing the works for their scientific vs literary nature, profundity vs readability, sci-fi as literature vs commodities – so my current works are a result of these ; which are more or less a betrayal of my own ideals for sci-fi. For a writer like me who has spent years trekking on the roads of sci-fi, this is also a sign of maturity.\nSpeaking of maturity, this happened: I took 2 day’s leave to attend these Awards, but applied for it under a different reason. 2 years ago when I published my first sci-fi story, a friend advised me to keep my writing activities “underground” in the unit (I worked at); he said, “In such a basic engineering department, mistakes and errors at work are tolerable, but not immaturity – you must never let others feel that you are immature, or else your prospects are gone.”.\nThis friend might be thought of as having misunderstood sci-fi, just as the society at large has; but in a way this reflects how immature our sci-fi is. For example: till now, our sci-fi stories have not created a milieu of their own – we are just writing our own stories in the many milieu created by others.\nBut to look at it in another way, sci-fi literature is by its very nature immature – because it shows humanity in its childhood, filled with curiosity and fear for the vast and profound universe, as well as the urge to explore it. In the face of such a universe, human science and philosophy are very immature, and sci-fi is the only literary form available to express our scientific and philosophical immaturities; so it’s no surprise that sci-fi is filled with immaturity. When human science is developed to the furthest extent and everything in the universe is discovered down to its smallest hair, that will be the day sci-fi dies.\nPresently – faced with the immaturity of Chinese sci-fi – everyone in our sci-fi community is envious of the adult sci-fi readership in the US, and see it as a sign of maturity in sci-fi literature. But one must know that senility comes after maturity, and death comes after senility. The prosperity of US sci-fi is largely a result of the prosperity of its movie and TV industries, and these sci-fi movies and TV shows are but a stylistic extension of the Golden Age of sci-fi. Contemporary sci-fi literature itself in US is already deep in twilight – full of works applying complex techniques to express dense metaphors, completely devoid of the youthful energy of the Golden Age. Many magnum opuses in recent years already have a smell of decay about them. Americans under 25 these days basically don’t read sci-fi; I don’t see what’s to be envied about that.\nWe should be most envious of ourselves: our current sci-fi readership are the 8 or 9 o’clock sun, or even the 6 or 7 o’clock sun.5 Chinese sci-fi a market full of youth and hope, and this is what gives sci-fi writers like us confidence and strength – compared to this, a little immaturity is really nothing.\n5 \nThe world is yours, as well as ours, but in the last analysis, it is yours. You young people, full of vigor and vitality, are in the bloom of life, like the sun at eight or nine in the morning. Our hope is placed on you. The world belongs to you. China’s future belongs to you.\n– Mao Zedong, Quotations from Chairman Mao Tse-tung, Talk at a meeting with Chinese students and trainees in Moscow (November 17, 1957).\n\nSome research show that many animals have language and deductive abilities, some of whom can make tools, and even a minority who has writing; but no evidence shows that animals can imagine (what does not exist), so imagination is the only difference between humanity and animals – and our sci-fi fans here today are the most vibrant expression of that.\nThank you, everybody!"
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#seeing-a-drop-of-water-in-a-sea-reflections-on-traditional-literary-elements-in-sci-fi-novels-2003",
    "href": "docs/posts/liu-cixin-anthology/index.html#seeing-a-drop-of-water-in-a-sea-reflections-on-traditional-literary-elements-in-sci-fi-novels-2003",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Seeing a drop of water in a sea – reflections on traditional literary elements in sci-fi novels (2003)",
    "text": "Seeing a drop of water in a sea – reflections on traditional literary elements in sci-fi novels (2003)\n从大海见一滴水——对科幻小说中某些传统文学要素的反思, posted in 2003-10-01 at the Tsinghua University BBS, Sci-Fi section.\nMy translation took some paragraphs from Liu Cixin – Beyond Narcissism: What Science Fiction Can Offer Literature, because Liu Cixin reused some paragraphs for that.\nImagine, for example, that Tolstoy had provided the following description in War and Peace (1869):\n\nNapoleon led a French army of six-hundred thousand men to invade Russia, gradually penetrating into its vast lands and soon coming to occupy the deserted city of Moscow. After waiting for a surrender that did not come, Napoleon ordered his army to retreat, but as the harsh cold of the Russian winter came upon them, a large part of the withdrawing French army froze to death or died of starvation. When Napoleon finally returned to France, he brought with him less than thirty thousand men.\n\nIn fact, there are many passages like this in Tolstoy’s monumental work, yet he separated descriptions of this kind from the main body of the novel, confining them to independent sections. He was not alone in this. Herman Wouk also attached historical accounts of World War II to the main body of his novel, The Winds of War (1971), as stand-alone appendages. Summed up as “Global Waterloo” and read on their own, they could make a good popular history of the Second World War. Both Tolstoy and Wouk, separated by a century, chose simply and directly to tell their readers: these are historical events, they are not an organic element of my work and not truly part of my literary creation. Indeed, macro-descriptions of historical events cannot form the main body of a mainstream literary work, for then the novel ceases being fiction and becomes a work of history. There are, of course, many novels that unfold against a historical panorama, including both Yao Xueyin’s epic Li Zicheng (5 volumes, published 1963–1999) and Howard Fast’s Spartacus(1951), but the main body of these works is made up of the detailed description of historical figures, reflecting the larger picture of history with a multitude of details. Even these works cannot use macro-descriptions of historical processes to form the main body of their text; that is the work of historians, not novelists.\nBut sci-fi is different, behold the following text:\n\nSirius Commander Noelopan led a massive fleet of 600,000 starships on an expedition into the solar system. Humans retreated, taking all available energy from the planets before retreating into outer space and prematurely transforming the sun into a red giant from which it was impossible to extract any energy. The Sirian Expeditionary Force penetrated deeper into the solar system and eventually captured Earth, which had become a desolate planet. After a long wait for peace that failed, Noelopan had to order the army to retreat. The harsh black hole flood period of the first spiral arm of the Milky Way arrived, and on the way back, due to energy depletion and loss of maneuverability, a large number of starships were swallowed up by floating black holes, and Noelopan finally returned to the Sirius system with less than 30,000 starships left in the fleet.\n\nThis is also a macroscopic depiction of history, and unlike that earlier passage, it is also fiction, a literary creation of the author, because it is history created by the author, and both Noelopan and his Starfleet come from his imaginary world. This is the main difference between sci-fi literature as opposed to mainstream literature. Mainstream literature depicts worlds that God has already created. Sci-fi literature, on the other hand, creates the world as God did and then depicts it. Because of this difference, we must reflect on certain elements of mainstream literature in sci-fi, taking the perspective of sci-fi literature itself.\n\n1. Details\nNovels must have details, but in sci-fi literature, the concept of details has changed dramatically. Consider a sci-fi story Singularity Fireworks, which describes a group of super-consciousnesses for whom Big Bang-esque explosions are nothing more than an amusing evening of fireworks. And every explosion is a Big Bang that gives birth to a universe. The following describes our universe being born this way:\n\n“What a good one! What a good one!” Entity One exclaimed as the firework exploded in emptiness.\n“A least better than the last few,” Entity Two agreed nonchalantly. “The laws of its physics forming after its expansion are equally distributed and the elementary particles percolating from its pure energy look good as well.”\nThe firework’s explosion vanished as its ashes slowly descended.\n“Wait a minute! Something more is happening there!” Entity One called out, just as Entity Two was about to light another singularity firework. Handing Entity Two a telescope it continued, “Look in the dust, the cooling matter is forming many tiny, low-entropy aggregations that seem interesting.”\n“Huh,” Entity Two raised the telescope. “They can reproduce themselves and microscopic consciousnesses are emerging.” It paused, “Wait, wait, some of them have even managed to infer that they have come from an exploding firework, how fascinating …\n\nThere can be little doubt that the above passage provides details. It describes the dialogue and perspectives of two entities watching fireworks. These details are, however, very unusual in that they hardly describe the small things. Mainstream literature often cannot describe the protagonists’ first kiss in less than 200 words; here, however, that length is sufficient to describe the entirety of the universe’s 15-billion-year history, starting with the Big Bang and covering the entire history of life and civilization. It goes even further, unfolding a vision of a supra-cosmos beyond our universe. In contrast to the “micro-details” of mainstream literature, these are “macro-details” that only a sci-fi story could provide.\nThe same content would be depicted in mainstream literature:\n\nThe universe was born in the Big Bang and later formed stars including the Sun, and later the Earth was formed next to the Sun. More than a billion years after the emergence of the Earth, life appeared on its surface, and later life emerged as human beings after a long evolution. Humans went through the primitive age, the agricultural age, the industrial age, and entered the information age, began to think about the origin of the universe, and proved that it was born in the Big Bang.\n\nIs this a “detail”? Obviously not. That’s why macro-details can only be found in sci-fi.\nIn fact, such details are very common in sci-fi. The best example is in 2001: A Space Odyssey, in the last chapter where the astronaut is transformed into a purely energetic state. In these details, sci-fi writers, with a flick of the pen, move across a billion years of time and ten billion light-years of space, turning the world and history encompassed by mainstream literature into an insignificant speck of dust in the universe.\nIn the early days of sci-fi, macro-details were rare, and it only appeared in abundance when sci-fi literature extended its tentacles into the depths of the universe, while at the same time beginning its contemplation of the origin of the universe. It is a sign of the maturity of sci-fi, and it is also an expressive style that best reflects the characteristics and advantages of sci-fi.\nI do not intend to disparage the micro-detailing of traditional literature. It is also essential in sci-fi, and a sci-fi novel without vivid micro-detail is like a giant with one leg missing. There is no lack of sci-fi composed of nothing but micro-details, such as Light of Other Days (Bob Shaw, 1966).\nNow the pity is that, while emphasizing micro-detail, macro-detail is not recognized among critics and readers of domestic sci-fi, and people generally have two reactions: 1. it is hollow; 2. it is just a long synopsis.\nClarke’s The Star (1955) is a classic among sci-fi short stories, and its final line – “What was the need to give these people to the fire, that the symbol of their passing might shine above Bethlehem?” – is a timeless line of sci-fi, and an example of macro-detail. But it would certainly not have been published if it had been written in China, for one simple reason: it has no details. If we say that 2001: A Space Odyssey, though covering a vast region of spacetime, has written all that can be said, allowing no further expansion, then The Star is really like a synopsis of a full-length novel. If you hand this synopsis to an old editor of a domestic publisher who is looking for sci-fi novels, he or she may still think it’s too sketchy.\nIn China, there are also a lot of very good works that can’t be published on the grounds of “lack of details”, the most typical example being Feng Zhigang’s Planting Civilizations (种植文明 by 冯志刚). At the symposium after the 2001 Galaxy Awards, a woman sternly accused, “The lack of seriousness in sci-fi creation has gotten to such a point that some people have taken the synopsis of a novel and passed it off as a masterpiece!” Seeing Feng’s bitter smile next to me, I was tempted to explain a few words, but looking at her righteous indignation, the words were scared back into my stomach. In fact, this piece is still much more detailed than some foreign classics. If you don’t believe me, you can take a look at The Gravity Mine by Stephen Baxter, which won a Hugo Award in 2001, or Calvino’s The Spiral, or Last and First Men by Olaf Stapledon from the ancient time of 1930. I’ve heard that Feng is expanding his “synopsis” into a full-length book,6 which is actually a common practice among Western sci-fi writers, but it really makes you think to note that many of the expanded full-length books are not as important to the history of sci-fi as their “synopses”.\n6 A brief search shows that this had no follow-up. Certainly never published, and probably never written.The emergence of macro-detail has a profound effect on the structure of sci-fi. This reminds us of the study of software engineering (especially MIS software). According to Western theories, the development of software should be from the top down, that is, first build the overall framework of the software, and then gradually refined. In China, due to the limited developmental level of management and IT science, the development of enterprise MIS software is basically the opposite, that is, first they develop the specialized modules, and then they are gradually put together into a large system (which has resulted in many disastrous consequences). The former is very much like the macro-detail-oriented sci-fi, which first builds a world according to the laws of its own creation, and then goes on to further enrich and refine it. The latter, surely, is the construction mode of traditional literature. Traditional literature has no way to write from the top down, because the structure above has already been built, and describing it is not a task of literature.\nSci-fi is precipitously expanding the descriptive space occupied by literature, allowing us to more vividly and profoundly show Earth and humanity from the vantage point of the entire universe. It can also show the several thousand years that make up the traditional world of literature in a new light: watching Romeo beneath Juliet’s window is certainly more interesting when viewed from a telescope in the Perseus Cloud than from a nearby bush.\nSci-fi can make us see a drop of water from the ocean.\n\n\n2. Characters\nThe social history of humanity is a history of the rise of the status of the human. From Spartacus rushing out of the gladiatorial arena brandishing his sword to the revolutionaries in France shouting “liberté, égalité, fraternité”, the human has been transformed from a means to an end.\nBut in science, the status of the human is evolving in the opposite direction, from being God’s creature (everything else in the universe is mere furniture gifted to us by this old man), and the master of all things, to being degraded to being essentially indistinguishable from other animals, and then to being degraded to an insignificant bacterium on a grain of sand in a certain corner of the universe.\nSci-fi belongs to a literature that is inextricably linked to society and culture, but it was spawned by science, and the question now is, in which way do we lean concerning the status of the human?\nMainstream literature has undoubtedly leaned to the former. “Literature is a human study” has become a nearly axiomatic creed, and a novel without characters is unacceptable.\nLooking at the short world history of sci-fi, it has not abandoned characters, but how it uses and treats characters have been greatly diminished compared to mainstream literature. Most of classic sci-fi have not been successful thanks to their characters. Of all the movies we’ve ever seen, the most flat and stilted characters were created by 2001: A Space Odyssey, in which scientists and astronauts stare blankly and drone on machine-like at a constant syllable-per-second. Even if the lacking of characters in other sci-fi works is due to the writer’s disinterest or inability, in the movie of 2001: A Space Odyssey, it can only a deliberate choice by the director Kubrick. It’s as if he’s telling us that people are just symbols in this work. He does this so successfully that after seeing the movie, it is hard to distinguish the only two astronauts in the spaceship, who seem to have no distinguishing traits other than their names.\nCharacter status changes in sci-fi as much as detail, again because sci-fi dramatically expands the space available for textual description. Another important reason is that sci-fi’s natural connection to science makes it possible to have a cold, hard view of humanity’s place in the universe.\nThe concept of characters has been expanded in sci-fi in two main ways:\nOne is the replacement of the image of the individual by the image of the race as a whole. Unlike traditional literature, sci-fi can depict multiple nonhuman civilizations and assign different images and personalities to these civilizations and the races that created them. These races could be aliens or different groups of humans that went into outer space and branched away from humanity. The aforementioned Planting Civilizations is a prime example of the latter. We call this new literary image the racial image.\nSecond is the world image. These worlds can be different planets and galaxies, or different branches of parallel worlds, and in recent years, many depicted virtual worlds running in computer memory. This is subdivided into two cases: one is that these worlds are populated (with people of whatever kind), and this world image is, a further extension of the racial image described above; the other case is that of the unpopulated worlds, which are later accessed by people (mostly explorers). In this case, sci-fi is more concerned with the natural attributes of the worlds and the role it plays for the people who enter them. Such a world is often portrayed like a villain in traditional literature, because they conflict with the people who enter it. There is also a very rare type of world-image in sci-fi: a world that exist autonomously, which humans never enter, and which the author depicts from a supra-conscious viewpoint. THis is the case of The Library of Babel (1941). These are rare and difficult to read, but they push the sci-fi use of character to the limit.\nNeither the racial image nor the world image can exist in mainstream literature, because a literary image exists on the premise that it is possible to compare it with other objects, and mainstream literature that depicts a single race (human beings) and a single world (the Earth) has to refine the granularity of the image down to the individual. Thus, the racial image and the world image are sci-fi’s contribution to literature.\nThese two new images have apparently not been recognized by domestic readers and critics. Our comments on sci-fi novels still continue with traditional literary thinking, unable to accept works not centered on traditional character images, not to mention the conscious creation of novel racial and world images, even though the creation and appreciation of these two images is the core of sci-fi literature.\nThe lack of Chinese sci-fi at the literary level is essentially the lack of these two images.\n\n\n3. The reality and ethereality of sci-fi topics\nIn fact, there is not much to discuss on this topic from a theoretical point of view. Sci-fi is for science. Abandon “science”, there is only “fiction”. To show an imaginary world is the beginning and end of this genre. Depicting only reality with sci-fi is awkward like using an airplane propeller in an electric fan.\nHowever, domestic readers favor sci-fi that is close to reality, and cannot accept imagination that is even a little out-there and crazy. Under such circumstances, most of our sci-fi is near-future.\nOne thing has always puzzled me: why do you want to read sci-fi when you want to read depictions of reality? Isn’t People’s Literature magazine good for it? Isn’t Harvest magazine good? Isn’t the epic Ordinary World (1988) by Lu Yao good? In terms of the level and depth of the depiction of reality, sci-fi is naturally inferior to mainstream literature.\nMany years ago, I saw a Soviet comedy movie, in which there was this scene: a large passenger plane landed on the highway and drove with the car, which obeyed all the traffic rules, stopping at the same red light alongside the cars.\nThis is a wonderful portrayal of the current state of the domestic sci-fi. Sci-fi is a genre that can fly up, but we prefer to let it crawl on the ground.\n\n\n4. Heroism in Science Fiction\nModern mainstream literature has entered the era of mocking heroes, as in that contemporary popular saying, “The sun is a piece of shit and the moon is a piece of ass-wiping paper.”.\nThis approach has merit. If you think about it scientifically and rationally, “heroism” is not a positive word. Was the behavior of those German tank drivers and Japanese kamikaze pilots in World War II heroic? Of course, it can be argued that it was not, because they were fighting for an unjust side. But on further reflection, this argument brings us nothing but confusion. Ordinary people can become heroes without being an ethicist, and it is impossible for them to decide the justice or injustice of the cause they are engaged in. More importantly, even for ethicists, judging a war from a moral perspective is difficult. Whether a war is just or not is more a matter of your feet than your brain, i.e., which side you are on. Wars such as World War II, where there is a basic consensus on their moral nature, are extremely rare in human history. According to the traditional notion of heroism, when a war comes, if ordinary people want to do their duty, whether their behavior is heroic or not depends on the luck of the draw. Worse, the probability of hitting that luck is lower than that of a coin toss. Over time, people must have come to believe that the fallen soldiers on both sides in most wars are meaningless cannon fodder. Looking at heroism again with this definition, one realizes that it has brought far more disaster than progress to humanity throughout history. Nor did the heroine of my story, The Glory and the Dream (2003), die for a just cause. So, are all those tragic sacrifices, all those magnificent, earth-shattering feats that only human beings can perform, just meaningless, perverted farces?\nA more sensible and fair approach would be to separate heroism from justice, and to treat it only as a uniquely human behavior, an important marker of distinction between human beings and other animals.\nAs civilization advances, and as the concepts of democracy and human rights are recognized throughout the world, heroism is fading. By mocking heroes, literature is calling for humanism from another angle, and in a way it is historical progress. It is conceivable that if human society develops along its current trajectory, heroism will eventually become something alien.\nThe question now is: will human society certainly develop along its current trajectory?\nHumans are fortunate that since the advent of civilization, the human world as a whole has never faced a sudden extinction event from the outside, but such a catastrophe remains possible in the future.\nSuppose that the Earth faces a full-scale invasion by an alien civilization, and in order to defend our civilization, one billion people may need to become cannon fodder under the lasers of the aliens. Or that the solar system sails into a cloud of interstellar dust, and the deteriorating Earth’s ecology necessitates the deaths of three billion people in order to avoid the deaths of six billion people all together. In that case, will our literature continue to ridicule heroism in such a scenario? Will shouting “humanity” and “human rights” save humanity then?\nLooking at humanity from the perspective of sci-fi, our race is extremely fragile. In this cold universe, humanity must bravely sacrifice a part of it in exchange for the continuation of the whole civilization, and this calls for heroism. Now that human civilization is in an unprecedented stage of smooth development, it is true that heroism is less important, but that does not mean that it will still be unimportant in the future contemplated by sci-fi.\nSci-fi is the last refuge for heroism and idealism, so let them dwell here for a while longer.\n\n\n5. A third image in sci-fi\nHaving previously mentioned the two images that characterize sci-fi literature – the racial image and the world image – it also has a third image that is not found in mainstream literature: the science image. Since sci-fi is a direct product of scientific development, science always exists in it either explicitly or implicitly, whether it is traditional hard sci-fi or later soft sci-fi. It fills the space between lines as a lifeblood, and as a ubiquitous image, it has always been depicted by sci-fi.\nChinese sci-fi has been learning from mainstream literature, but it has not been a good student. We pay close attention to characterization and stylistic sophistication, and as a result, our works seem to be nothing more than school essays. We pay close attention to reality, but compared with the mainstream novelists, it is still just the pretending-to-be-sick moaning of school students. We have dabbled with postmodernism, and the result is a total mess. But there is one thing we learned that we have gone above-and-beyond mainstream literature: scandalizing and demonizing science.\nUp to now, mainstream literature just keeps a certain distance from science and does not hurt it, this is because on the one hand, the pastoral setting of traditional literature has little relation with science; on the other hand, to scandalize science, one needs to understand it, and at this point, the mainstream literature may meet an obstacle. Sci-fi, however, has a natural advantage in this regard, and does so with relentless effort!\nI think we all know very well what has become of the image of science in our sci-fi.\nIt’s true that Western sci-fi writers have done a better job of this than we ever have, but that’s no reason for us to do so. Science is quite popular in Western societies, and rethinking on its consequences may be healthy. But even so, this tendency by the sci-fi criticism community has been unanimously condemned by the Western scientific community. In China, science is still a small candle flame in the wilderness among the general public, and a breeze can blow it out. The first task now is not to predict the disaster of science; the real disaster facing Chinese society would be the loss of the spirit of science among the masses.\nThe power of science lies in the public’s understanding of it. This is the truth. To let the spirit of science take root among the public is a great cause, compared to which [the other purposes of] sci-fi seems trivial. Originally, there is no contradiction between the two, and the old generation of Chinese sci-fi writers were full of hope that sci-fi would become a part of this great cause, but now it seems that this is a very naive hope. But at least, sci-fi should not cause damage to this cause. Science is the mother of sci-fi, do we really want to be her enemy?\nIf we can’t attract readers without portraying science in a negative light, without making it gruesome and horrible, then let’s stop the pen in our hands. It’s not a big deal, there are many other interesting things to do. If Chinese sci-fi really disappears, as a faithful old sci-fi fan, I sincerely pray that it will die a decent death.\n\n\n6. Shackles of stereotypes\nI have written some comparisons between sci-fi and mainstream literature, and I do not mean to belittle mainstream literature at all. The above mentioned advantages of sci-fi are determined by its own nature, and it is not therefore higher than mainstream literature in skill – on the contrary, currently sci-fi does not make good use of its natural advantages. In fact, facing mainstream literature, we often feel inferior, when seeing their courage to explore and innovate the methods of literary expression. From stream-of-consciousness to postmodernism, a dazzling variety of expressive methods have been developed with the spirit of “I do what I want”. If we look at sci-fi, we have not created our own expressive techniques. The New Wave movement was just an effort to take the techniques of mainstream literature and use them for ourselves, but later found that they were not suitable, and the whole movement has been called by researchers of sci-fi theories as “an effort to yield up the value and status of sci-fi to mainstream literature”. As for the aforementioned macro-details, racial image, and world image, they are all unconscious creations of sci-fi writers, which have not risen to the height of theory, let alone a self-conscious expressive technique. In foreign countries, these techniques are not even acceptable.\nIn fact, some traditional literary elements extended or subverted in sci-fi, such as characterization and detailing, are undergoing drastic changes in mainstream literature too. Mainstream literary figures like Borges and Calvino have long abandoned those traditional dogmas and achieved great success.\nOn the contrary, the domestic sci-fi critics are religiously picking up the broken yoke that others have thrown away, solemnly putting it on themselves, screwing the top bolt to the tightest, and lashing out at those sci-fi works that have even slightly crossed the threshold, as if they have become the guardians of the dignity of literature. Look at those online reviews, full of stereotypical dogma, without a little bit of young people’s sensitivity and vigor, sometimes I really want to ask: “Venerable one, how many centuries have you lived?”.\nInnovation is the life of literature, but also the life of sci-fi. In the face of this literature from the sea to see a drop of water, we must first have the sea’s bosom!"
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#interview-with-masters-of-sci-fi-2006",
    "href": "docs/posts/liu-cixin-anthology/index.html#interview-with-masters-of-sci-fi-2006",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Interview with Masters of sci-fi (2006)",
    "text": "Interview with Masters of sci-fi (2006)\nSource: 《科幻大王》特约采访_workership_新浪博客\nQ: What were the main contents of your fantasies and imaginations during your childhood?\nA: I spent my childhood in the late stage of the Cultural Revolution. At that time, the cultural life, especially the cultural life that can stimulate the imagination, was poor, so the world of imagination was also poor. When I was a teenager, it was the end of the Cultural Revolution and the beginning of a new era, when many new worlds began to unfold in front of my eyes, but it was a bit late for a sci-fi writer. It is hard to get rid of the mental-branding of the time, no matter how hard you try in the future, you will always feel like dancing in shackles in writing sci-fi.\nQ: What was the ambitious goal you set in your mind when you were a teenager?\nA: It was the early 1980s, the era of universal scitech worship [in China], and the ideal of teenagers was simple and uniform, that is, to be a scientist, and I was no exception. However, after entering university, facing the slim chance of going to graduate school (at that time, even the undergraduate entrance examination only had a pass rate of 4%), and also contacting some scientists, I found that this career was not what we thought it was, and I was just like other people of my age, so my ideal soon faded away. Then I was in a daze, not knowing what I could do in the future, and I couldn’t talk about my ambitious goals. Sci-fi, to some extent, is a kind of spiritual compensation for the ideal that has become a soap-bubble-fantasy.\nQ: How did you grow up? (Briefly describe the goals you set when you were a teenager)\nA: Like everyone else, I had an ordinary life with no special experiences. People who write sci-fi are not good at nostalgia, and there is nothing to be nostalgic about. My childhood during the Cultural Revolution was a gray one, with not much to remember, just endless playing. Society didn’t offer much to play with, but kids invented a lot of things themselves, many of which were quite dangerous. The middle school years also came and went in a blur, and what remains in my memory are the innocent melodies of those Taiwanese school songs and the hunger brought about because I had only 9 yuan a month for school meals. Now I think that if I had studied harder at that time, I might now have a job with heavy responsibilities, and I wouldn’t be writing sci-fi. Now people are talking about the so-called tragedy of life in the poor mountainous region of ” sheep-herding - marrying - having children - sheep-herding “, in fact, we and the sheep-herding child is not much different, but only the cycle becomes”go to college - work - buy a house - marry - have children - go to college”, and in some aspects we are not even as good as the shepherd boy. His idleness, his lack of mountains and fresh air, compared to struggling with our duties all day long in the polluted city. Sci-fi, if it does become a way of life, can pull us out of the cycle mentally in some ways, but some authors and readers try to jump out of the cycle in reality with the help of sci-fi, and none of it turns out well.\nQ: What were the main books you read as a teenager? What kinds of books were in the sci-fi genre?\nA: As I said above, I didn’t have many books to read during my teenage years, so I didn’t have much of a choice, I read what was available, mostly realist literature and the classics that anyone had to read, and it was during that period that Russian literature profoundly influenced my later life, and I don’t know whether it was fortunate or unfortunate now. It didn’t take long to read all the sci-fi books in the country, and then it was just a matter of waiting for the new ones to come out.\nQ: What book (or quote) did you read that inspired you the most as a teenager?\nA: War and Peace.\nQ: What kind of opportunity led you down the path of sci-fi writing?\nA: It was a long process of becoming a sci-fi fan, then not being able to resist writing it myself, and then publishing it. Because of the domestic sci-fi’s bumpy road, it also makes this process seem very long, in which there is quite a long time, nearly ten years, sci-fi seems to have left myself, only late at night when I can occasionally think of her, but then, one day in the last century, she suddenly showed up in front of me again.\nQ: Although you write in your spare time, it can be said that every piece you write is a masterpiece. Once published, they always cause a strong reaction among readers. How do you keep this good momentum of creation?\nA: In fact, I don’t feel that I can maintain this kind of status, as you said. The process of completing each novel is quite difficult, and it is accompanied by a heap of bloody corpses of discarded ideas. Nor is it possible for every piece of work to be a masterpiece; there are many contingent factors that contribute to the success of a piece of literature, not the least of which is that the author’s creative tendencies resonate with the readers’ appreciative inclinations at the time, but in these ever-changing times that resonance is unattainable. Every author, especially sci-fi authors are facing a common fate: readers are gradually tired of your novels, and the readers in front of you are gradually unfamiliar, you want to get rid of this state, and create a brand new world, but as said above, everyone has their own spiritual shackles, how far you can go is actually fated from very early on, and literature style, much like other personality traits, is also predictable when one is three years old.\nQ: When did you start publishing? What was the name of your debut novel? What kind of response did you get from readers?\nA: In 1999, I published The Whale’s Song in Science Fiction World Magazine, and the response from readers was mediocre.\nQ: What kind of accumulation did you have before you started to write sci-fi?\nA: I didn’t consciously accumulate anything, but just dabbled with my own interest, and started to read more literature. After graduation, I found that the real society itself is an unsurpassed realist literary work which is continuously being written, but unfortunately, it is the same as other literary novels written into books, in that I couldn’t feel much interested in it. I had a new feeling: the universe is so big, so if I only dwell my thoughts upon humanity, it would be such a bore. Afterwards, I seldom read literature, and my interest shifted to natural science books that I found more interesting.\nQ: Can you share your creative experience with the sci-fi fans in our magazine? Can you help those who just try to write sci-fi novels with some tips?\nA: We should have a clear understanding of the so-called creative experience. For example, almost all the creative training first said: “Omit needless words. Vigorous writing is concise. A sentence should contain no unnecessary words, a paragraph no unnecessary sentences.” But if you think about it, you will realize that this statement is ridiculous, at least half of the classic masterpieces are made up of unnecessary things. More ancient, like Balzac, or more recent, like Faulkner, tried to actually omit needless things, and do you see what’s left? That’s why I always think it’s important not to teach people to write novels. Literature is an extremely free thing, and the rules of literature are fundamentally different from the laws of science; the latter are there for us to follow, the former are there for us to break. The flaws of others according to a literature teacher may be an author’s strengths. In other words, in literature, flaws carried to the extreme are strengths, as long as they can be expressed in an aesthetically pleasing way.\nQ: As a leading sci-fi writer, what are your plans in terms of sci-fi creation? Can you reveal some never-before-seen information to the sci-fi fans of our magazine?\nA: As an amateur writer, it’s hard for me to have a long-term creative plan because it depends on my own work, and you don’t know when you’ll be free. There are just a lot of ideas sitting around in my head, which I hope to write in my lifetime. Recently, I’ve been planning to write a series of three long stories, describing a period of mankind’s journey from the beginning of the Cultural Revolution to five hundred years later, but I’ve only completed the first one so far."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#wenge-censorship-blog",
    "href": "docs/posts/liu-cixin-anthology/index.html#wenge-censorship-blog",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Why not a Three Body standalone book? (2006)",
    "text": "Why not a Three Body standalone book? (2006)\nMerged from two posts, 关于〈三体〉单行本_workership_新浪博客 (2006-06-08 18:26:05) and 关于&lt;三体&gt;单行本的一个试验_workership_新浪博客 (2006-06-12 10:22:40)\nMany friends have asked why The Three-Body Problem was not published as a single volume. This was determined by the publishing environment during the 40th anniversary (2006) of the Cultural Revolution (1966) and had nothing to do with commercial considerations. As for the future, maybe after this year, the environment will become more relaxed, and we can publish it. Maybe we will never publish it. In fact, it took a lot of determination to even serialize it in a magazine. Otherwise, this book may never see the light of day. I would like to express my gratitude to Science Fiction World [the magazine where it was serealized].\nSci-fi that describes reality is not necessarily good sci-fi. Reflecting reality is not the task of sci-fi, let alone its advantage. It is just an attempt to give readers a fulcrum for their imagination, nothing more, and no more reasons.\nTry typing the word “Cultural Revolution” in a Sina Blog post – it cannot be posted, with no explanation (I used special techniques to publish the two characters “文革”). From this, we can also understand the difficulty of publishing The Three-Body Problem as a single problem.\n(Though you can enter “文革” in the comments section 🙂)"
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#the-second-installation-of-the-three-body-problem-is-done-2007",
    "href": "docs/posts/liu-cixin-anthology/index.html#the-second-installation-of-the-three-body-problem-is-done-2007",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "The second installation of The Three-Body Problem is done (2007)",
    "text": "The second installation of The Three-Body Problem is done (2007)\nSource: 《三体》第二部完成_workership_新浪博客\nWriting a full-length book by an amateur author is an adventure, not knowing what surprises will interrupt the writing process during this time, which can be as small as extra work assignments or as large as the destruction of the planet. In the past, when I was writing a long story, the process was very smooth, and the accidents seemed to appear in a concentrated way just I would be finishing it, but this time, they all erupted right in the middle of the writing process. It is a painful process to throw down a long story for a while and then pick up again, and constantly throwing down and picking up is very scary. The second part of *The Three is written in this way. What was planned to be a four-month job took nine months.\nWhat puzzled me most about the creative process was the role of the individual in history. The role of the individual in the scientific process is relatively easy to grasp, the laws of nature are there, if Newton could not find it, the later Luton or Maton would find it. But sociology is different, human history is different, just like a person’s life, to use the description from Ball Lightning:\n\nFickle and unpredictable, all is probability and chance, like a twig floating in a stream that trips over a small rock or gets caught in a small whirlpool…\n\nSo the true role of the historical giants has always been a mystery, to quote again from the second installment of The Three-Body Problem:\n\n“Do you really believe in the decisive role of the individual in history?”\n“Well, I think it’s a question that can’t be proven or disproven, unless we restart time, kill off a few great men, and see how history proceeds. Of course, you can’t rule out the possibility that the course of history was determined by the rivers carved out and dammed up by those great figures.”\n“But there’s another possibility: Those great figures of yours might be no more than swimmers in history’s river. They may have left their names in history because of the world records they set and the praise and renown they won, but they had no effect on the river’s flow…. Ah, with things the way they are, what’s the point of thinking about all that?”\n\nIn fact, depicting a three-dimensional panorama of the world from the lowest to the top of the pyramid is the lifelong dream of all mainstream literature and sci-fi authors, but the ability to achieve this goal is very much in the hands of the very best of men. There are not many Tolstoys and Balzacs, after all, and so sci-fi has always been about describing fantastical histories from the point of view of the individual and the titanic, and this is true of everything from The Foundation to Dune.\nBut we may be able to see the giants in sci-fi as a kind of symbol, specifically the main character in the second part of The Three-Body Problem, he may symbolize such a group of people, who are neither awed by the starry heavens above nor the moral law within,7 but in this way they transcend the imaginary bonds in the head to grasp the truth of the universe, and wield use this understanding resolutely as a weapon of survival.\n7 This dictum from the Critique of Practical Reason (1788) was chosen for Immanuel Kant’s tombstone:\n\nTwo things fill the mind with ever new and increasing admiration and awe, the more often and steadily we reflect upon them: the starry heavens above me and the moral law within me.\n\nSurvival is an iron wall, in the words of Wandering Earth: “You are walking across a plain when you suddenly encounter a wall. The wall is infinitely tall and extends infinitely deep underground. It stretches infinitely to the left and infinitely to the right. What is it? – Death.”\nI can only admit: I care about survival, I believe that an honorable death is worse than a dishonored life, and that dying in love is worse than living loveless. This statement is very low at a personal point of view, but from the civilization point of view it is another thing; in the earth’s atmosphere let people despise, but let’s put it into space, and see it turn into another thing.\nWriting a long story is living a life, and I’m done with this nine-month life. It started with Chinese New Year, when it was cold, and now it’s getting cold again; perhaps, the universe is also so reincarnated, only the time scale is billions and billions of times larger.\nOne feeling: sci-fi authors are really lucky, and sci-fi can really make you young."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#several-books-on-my-sci-fi-journey-2007",
    "href": "docs/posts/liu-cixin-anthology/index.html#several-books-on-my-sci-fi-journey-2007",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Several books on my sci-fi journey (2007)",
    "text": "Several books on my sci-fi journey (2007)\nSource: 我的科幻之路上的几本书, published in《南方周末》2007年9月13日 (Southern Weekend, 2007-09-13)\nBooks affect everyone in many ways, but it’s the ones that determine one’s path in life that are the most important, and as a sci-fi author, I’d like to list just the ones that set me on the path to sci-fi.\nJules Verne’s novels about big machines. Jules Verne’s sci-fi novels are divided into two major categories in terms of the objects depicted, one is scientific adventure novels, and the other is novels depicting big machines, with the latter having more sci-fi content, mainly including Twenty Thousand Leagues Under the Sea, Propeller Island, From the Earth to the Moon, and so on. The big machines appearing in this kind of novels are all based on the steam technology and rudimentary electric technology of the 18th and 19th centuries, and are crude and clumsy, symbols of the age of technological childhood. There is a kind of childlike innocence and childish beauty.\nIn Verne’s time, science began to be transformed into technology and started the process of comprehensively influencing social life. What these big machines show is the kind of naive surprise when human beings first saw the miracle of technology, and this feeling is the very soil for sci-fi to germinate and grow. Even today, the beauty of the big machines of the 19th century has not disappeared, and the specific manifestation of this is the steampunk genre that has appeared in sci-fi literature in recent years, in which this kind of sci-fi work shows not the future that we modern people imagine, but the present that people in the past (mostly the end of the 18th century and the first half of the 19th century) imagined. In steampunk movies and television, we can see big steam-driven machines, crudely shaped flying machines like cruisers, intricate copper piping and antique gauges everywhere. Steampunk is an imagined continuation of the era of big machines in Verne’s work, and it shows a nostalgic warmth in addition to the beauty of big machines.\nArthur C. Clarke’s 2001: A Space Odyssey is another type of sci-fi, the same techno-sci-fi, but it is at the opposite end of the genre from Verne’s big machine novels, with the latter depicting technology that is one step forward from reality, and the former depicting ethereal worlds that tend to be ultimate in both time and space. I read this book in the early 1980s, and it was the first novel I saw that vividly depicted the whole process of human beings from birth to extinction (or sublimation) in a brisk few pages, in which the charm of sci-fi was fully expressed, and the God’s-eye-view gave me a suffocating shock. At the same time, 2001 showed me a completely different kind of writing, with both philosophical abstract transcendence and literary subtlety, to depict the immensity of existence in the universe that we can’t grasp in our perception or imagination.\nClarke’s Rendezvous with Rama, on the other hand, exemplifies sci-fi’s ability to create imaginary worlds; the entire work reads like a Creator’s grandiose design documents for an imagined alien world in which every brick is exquisitely laid. As with 2001, the aliens never appear, but the imagined world itself is already mesmerizing, and if Verne’s novels made me fall in love with sci-fi, Clarke’s work gave me the initial impetus for writing sci-fi.\nThe Dystopia trilogy – Orwell’s 1984, Huxley’s Brave New World, and Zamyatin’s We – are only categorized as the fringes of sci-fi, but I saw in them sci-fi literature ability to reflect and intervene in reality from perspectives that are not possible in traditional realist literature. 1984 does not have a very high status in the literary world, and its influence is mainly in the political and sociological fields. In the just concluded Chengdu Science Fiction Convention, some writers even believed that 1984 prevented 1984 from becoming 1984, which is of course a bit overstated, but in addition to bringing the enjoyment of imagination, sci-fi literature also has special powers beyond other genres of literature. In my discussion with Prof. Jiang Xiaoyuan, we both admitted that among the Dystopia trilogy, 1984, which seems to be the darkest, is the brightest, in which human nature, though repressed, at least exists; whereas in the other two worlds, human nature has already been disappeared by technology. This is a special kind of darkness impossible in realist literature.\nFrom a literary point of view, Tolstoy’s War and Peace is not in the same class as Herman Wouk’s The Winds of War series, but what I am concerned about is the bird’s-eye view of the whole situation that they share; they are both novels depicting human wars in a panoramic manner. Compared with those novels about personal feelings that are like an exquisitely sculpted grain of rice, such a magnum opus can make people appreciate the whole of humanity as a unified racial existence, which also happens to be the perspective of sci-fi literature.\nAsimov’s The Intelligent Man’s Guide to Science (1960) is more of an encyclopedia than a work unified whole, but it’s true that I haven’t seen any other popular science work with such a systematic introduction to modern science either. Carl Sagan’s Cosmos (1981) and The Dragons of Eden (1977) are also early entries in the Western pop-sci canon, and while they now seem a bit outdated in terms of theoretical novelty, they introduced an aesthetic perspective into their description of science, which is unsurprising today, but in the early 1980s, they really opened up my third eye for looking at science.\nThe great thing about Richard Dawkins’s The Selfish Gene (1976) is that it’s cold, colder than mere cool-headedness, and unmovingly reveals the nature of life. While its conclusions aren’t always correct, they shaw a possibility that the ultimate purpose of life, the living world, and civilization may be something we don’t even think about. Peter Singer’s Animal Liberation (1975), on the other hand, spreads equality and love to all beings beyond the human race, and similarly makes us look at human civilization from a height that we have not before. In any case, both books are “sci-fi”.\nBut the most sci-fi are The First Three Minutes by Steven Weinberg (1977) and The Last Three Minutes (1995) by Paul Davies, in which the authors describe in poetic terms the extremes of the universe’s nascent and dying moments, when the world is so far away from reality that it might as well be real. Taking us to places we can never get to in times we can’t experience is the great fascination of science and sci-fi, and it has to be admitted that in this respect science does it better. All the peoples of the world have constructed their own creation myths with the boldest and most splendid fantasies, but none of them is as magnificent and mind-blowing as the Big Bang theory of modern cosmology; the long story of the evolution of life, with its twists and turns and romance, compared with which, the creation myth of human beings by God, or Nüwa, is really bland and boring.\nThen there is the poetic view of space and time in general relativity, the elf-like microcosm in quantum physics, the world created by these sciences is not only beyond our imagination, but also beyond what we could possibly imagine, and absolutely beyond the power of human myth writers to create. Yet the imagination and the beauty of science is imprisoned in cold equations, and the common people can only glimpse her with great difficulty. But when the beauty of science is shown in front of people, its power to shock and purify the soul is enormous, and some aspects of the beauty of traditional literature are hard to reach. Sci-fi is a bridge to the beauty of science, which releases this beauty from the formula and shows it to the public in the form of literature."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#liu-jiang-debate-2007",
    "href": "docs/posts/liu-cixin-anthology/index.html#liu-jiang-debate-2007",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Why is humanity still worth saving? – debate between Liu Cixin and Jiang Xiaoyuan (2007)",
    "text": "Why is humanity still worth saving? – debate between Liu Cixin and Jiang Xiaoyuan (2007)\nOriginally published in New Discoveries magazine, Issue 11, 2007. Host and scribe: Wang Yan\nWhere will the human race be taken to in the end? Can humanity’s belief in the future be sustained? With what? Science? What can science solve? What can’t it solve?\nOn August 26th, 2007, at the “White Night” bar run by poet Zhai Yongming, the editorial board of New Discoveries invited two guests to Chengdu to attend the “2007 China (Chengdu) International Science Fiction and Fantasy Convention”. The editorial board of New Discoveries invited two guests to Chengdu to participate in the “2007 China (Chengdu) International Science Fiction and Fantasy Conference”: Liu Cixin, a famous sci-fi writer, and Jiang Xiaoyuan, a professor at Shanghai Jiaotong University who has been frequently publishing sci-fi commentaries in recent years, to have a fascinating conversation about our common doubts, and a face-to-face exchange of ideas on sci-fi, scientism, and the relationship between science and humanities, and other issues. The following is the record of the conversation.\nLiu: Historically, the first sci-fi novel, Mary Shelley’s Frankenstein, was anti-science, and her portrayal of science was not very bright. And even earlier, in Gulliver’s Travels, there is a chapter about scientists [on Balnibarbi], and they are described in a very comical way. You can see a science degenerating into an academic formality. But Jules Verne suddenly became optimistic because he was inspired by the rapid development of science and technology in the late 19th century.\nJiang: A lot of things from the West were brought in, and they were selected, and Verne fit our need for propaganda and education. His early optimism is inseparable from the development of science and technology in the 19th century, when people did not yet see science as a monster, but he became pessimistic in his later years.\nLiu: Verne did write some very complex works, with a lot of complex human nature and plots. One is about a ship on which many people form a society. Also his Facing the Flag has an anti-science element, depicting that science can bring about some disasters. Then there’s The Begum’s Millions. But these do not dominate, and almost all of his still-read works are more ideologically innocent. It’s worth noting that the later Golden Age of sci-fi came during the Great Depression, in the 1920s. Why? Probably because people wanted to take comfort in the illusions created by sci-fi and escape from reality.\nJiang: It’s said that book publishing was booming at that time. There’s a little story about Verne, he wrote about the Xujiahui Observatory [L’Observatoire de Zi-Ka-Wei] in Robur the Conqueror. He said [in Chapter 1] there was a flying machine, and the director of the Xujiahui Observatory thought it was sent by intelligent beings from other planets, similar to what we call UFOs today, but the directors of the observatories in other countries didn’t believe him because he was Chinese, but later on, it turned out to be really from an extraterrestrial civilization. This story made a mistake: the director of the Xujiahui Observatory at that time was not Chinese, but a Frenchman, a compatriot of Verne.\nLiu: Verne created the image of the big machine in his novels, which has been used in many anti-science works since. Forster wrote a very famous anti-science sci-fi work called The Machine Stops. The story is that the whole society is a big running machine, and people don’t even walk anymore, they live underground. One day the machine breaks down and civilization is destroyed.\nJiang: Many readers have noticed an evolution from optimism to pessimism in your work. Is this similar to the pessimism of Verne in his later years? Is there also some ideological change behind it?\nLiu: The connection is not very strong. Whether it’s pessimism or optimism, it’s actually a need for expression. In the past few years of writing sci-fi, I haven’t had any ideological shifts. I’m a fanatic of Technologism, and I personally believe that technology can solve all problems.\nJiang: That is actually scientism.\nLiu: Some people say that science can’t solve all problems, because science may cause some problems, such as the corruption of human nature, degeneration of morality, or even as Nancy Kress said, “Science makes humans turn into non-humans.”8. But what we have to be aware of is that human nature is actually changing all the time. We and Stone Age humans would think of each other as inhumans. Change should not be rejected or feared, as we are definitely changing. With high-enough technology, I can’t think of any problem that it can’t solve. My guess is that those who think that science can’t solve the problems we face are doing so, because they have a concern that the human itself should remain uncorrupted.\n8 I cannot find such a statement in English, so I translated literally.Jiang: The argument against scientism, that science may corrupt the human, is only one aspect of it, and the other aspect is that science really can’t solve some problems, and some problems can never be solved, such as the purpose of life.\nLiu: What you say does hold true, but I’m not talking about issues that are that broad. And I think the purpose of life can be solved by science.\nJiang: Can we rely on science to find the purpose of life?\nLiu: Science can stop me from searching for the purpose of life. For example, we can use science to eliminate the desire to find the ultimate purpose from our brain.\nJiang: I think a lot of scientific and technological developments are neutral, depending on who uses them: bad people use them for bad things, good people use them for good things. But there are other things that are fundamentally bad. What you’ve just described is a very dangerous and even evil thing, and no matter who uses it, it’s evil. If we go ahead and develop something like that, it is evil. Why has the West promoted anti-scientism over the years? Against scientism, not against science itself. Scientism is very ugly in the eyes of many Westerners.\nLiu: What I want to point out is this: Is there truly a difference between convincing you with words, and influencing your essential judgment by putting a chip in your head?\nJiang: Of course there is a difference. By convincing me, you honor my free will.\nLiu: Now I’m going to ask this question, which I’m going to write about in my next work9: if you build such a machine, but it doesn’t directly control your thoughts, and you take whatever thoughts you want, is this acceptable?\n9 This is the “mental seal” in The Dark Forest.Jiang: This is acceptable, but the person who goes to get the thoughts should be wary of it.\nLiu: Right, that’s what I was going to say. According to your point of view, in the Dystopia trilogy, 1984 is the brightest, in which human nature is only repressed, while in the other two, human nature disappears. If you were given the choice of 1984 or Brave New World, which would you choose?\nJiang: Probably more people would choose to go to Brave New World. The premise is that you only have two choices. But what if there are other options?\nLiu: One of the things I remember you talking to me about is that humanity is not philosophically ready for total extinction. Now let’s link the human-corrupting science and technology up with the existential catastrophe. If this cataclysm were about to happen, wouldn’t you have to use this tool?\nJiang: Look at it this way. If we are to prepare for the cataclysm today, then I think there are two most important things. The first is for us to gain interstellar navigational capability, and that capability is not merely in launching an occasional spaceship, but migrating on a large scale. The second is for us to find a new homeworld.\nLiu: That’s great. But what if the disaster is imminent, say next May, what do we do now?\nJiang: Do you think that using technology to control people’s minds can solve this disaster?\nLiu: It by itself can’t avoid the disaster, but it can organize human beings in a way that transcends the moral bottom line and preserves the whole at the expense of the parts. Because right now human morality can’t handle the kind of dilemma in The Cold Equations: one person dies, or two people die together?\nJiang: If you want me to accept mind-control technology (microchips in the head) on the grounds of preventing a future catastrophe, that’s a catastrophe in itself, and one can’t accept an catastrophe now just because one expects another in the future. The day of that disaster is still unknown, and it may or may not come. In fact, similar confusions have been discussed in several Western works, and ultimately they all denounce this as evil. Like in Digital Fortress (1998), where everyone’s email is monitored, it’s said that it’s for the purpose of counter-terrorism, but it’s already a form of terrorism.\nLiu: I’m just giving an example to illustrate a point: whether technology is evil or not, and whether its role in human society is evil or not, depends on what the ultimate purpose of human society is. Mr. Jiang believes that controlling the mind is evil because it takes away humanity. But if the ultimate purpose of human beings is not to sustain their humanity, but to survive, then it is not evil.\nJiang: This involves a value judgment: is it important to survive or sustain humanity? It’s as if there are two paths ahead: one is to lose humanity but still survive, and the other is to sustain humanity until the final moment and then perish. I believe that not only me, but also many people will choose the latter. Because the loss of humanity is the same as extinction of Humanity.\nLiu: In fact, from the beginning of writing sci-fi until now, I have been thinking about this question: which one is more reasonable to choose?\nJiang: At this time I think we must respect free will and everyone must vote. People like me can vote the not-surviving option.\nLiu: You’re right about all of this, but what I want to emphasize now is a question of scale. What sci-fi does is that it can be seen on a scale that we don’t normally see. Conventional moral judgments can’t do the job of judging humanity as a whole. I’ve been thinking in terms of sci-fi, and saw that the traditional moral bottom line is deeply questionable. I can’t say it’s wrong, but at least it’s dangerous. The concept of human nature is truly vague. Do you really think that an unchanging human nature exists from primitive times to the present? What is the one thing about human nature that has remained unchanged throughout the ages? I can’t find any.\nJiang: I think free will is part of what is unchanging. I have always believed that science cannot deprive people of their free will. There was an incident in the United States where the local government followed the advice of an expert to fluoridate the drinking water to prevent dental disease, which caused a lot of objections, the most extreme of which was: I know it’s good for me, but I should still have the freedom not to want these benefits, right?\nLiu: That’s the theme of A Clockwork Orange.\nJiang: We can agree to disagree that I think it’s always bad to use technology to control ideas, while you think it’s good in some cases. Western sci-fi is now the product of an anti-scientific ideology, a shift that has been underway since at least the New Wave Sci-fi. Anti-scientism was part of the four main demands of the New Wave movement, such as the third demand to be able to consider the darker parts of science in the future.\nLiu: In fact, anti-scientism was already quite prevalent in the middle of the Golden Age.\nJiang: In the West, the mission of the New Wave has been accomplished. Do you think the mission of the New Wave in China has been accomplished?\nLiu: Actually, there was a debate in the 1980s about whether sci-fi should live under the roof of “Science” or “Literature”, and in the end the latter won. This can be said to be a belated victory of the New Wave in China. At present, most Chinese sci-fi writers are pessimistic about science and skeptical about the development of science and technology, which is a proof that they are influenced by Western thinking. In my opinion, Western science has developed to such a point that it’s time to limit its power, but Chinese scientific thought has just been born, and I don’t think it’s appropriate to demonize it yet.\nJiang: I have a different view. Between the development of science and scientism, it is not the case that scientism promotes the development of science, as like how the Chinese economic policy starts by development at the expense of pollution, and then fix the pollution after development is done. Scientism actually harms science from the beginning.\nLiu: But we are talking about the attitude towards science in sci-fi, introducing its positive effects and promoting scientific ideas, which is not a mistake, right?\nJiang: In fact, in China, the authority of science is already too big.\nLiu: In China, the authority of science is great, but the spirit of science is not.\nJiang: We are moderately limiting the authority of science, and doing so is not the same as destroying the spirit of science. The spirit of science does not include the unlimited worship of science itself – the spirit of science includes the spirit of skepticism, which means it is possible to be skeptical of science itself.\nLiu: But there needs to be a ratio between skepticism of science and affirmation of science. How can all sci-fi works be 98% anti-science? That’s not reasonable. If, in the eyes of the people, scientific development brings about a dark world, always evil, always catastrophic, always irrational, then how can the spirit of science be promoted?\nJiang: I used to think this was problematic, but now I am more inclined to accept it. Let’s say, for example, that a small child gets good grades and is very proud of them. It’s not unreasonable for an adult to stop praising him for every high score and start only criticizing his shortcomings.\nLiu: Can you tell us in what ways the authority of science is expressed in China?\nJiang: In China, many people believe that science can solve all problems, and in addition, they believe that science is the best system of knowledge, above all other systems of knowledge.\nLiu: This is where I really differ from you. Although I don’t think that science is superior to other systems, I do think that it is the most complete system of knowledge that we have at the moment. Because it recognizes logical reasoning, it requires objective and experimental verification and does not recognize authority.\nJiang: As a student of astrophysics, I used to believe in this completely, but I have had a change of heart since about 2000, and of course this change has developed slowly. The reason was that I was exposed to some Western anti-scientism works and I felt that there was really something to it. You believe that science is the best system, so you assume that everyone needs to have the spirit of science. But I think it’s fine as long as a portion of the population has it.\nLiu: It should at least be mainstream.\nJiang: It’s not that only people with the spirit of science can make the right choices; the opposite may be true in many cases. Let’s take an example to illustrate this.\nIn the Steven Soderbergh’s version of Solaris (2002), where some people are on a space station and encounter a lot of strange things, and the hero, Chris, meets his wife, Rheya, who is long dead. There is a Dr. Gordon, who says to Chris, “Rheya is not human, so kill (every one of) her.” Dr. Gordon’s judgment is perfectly in keeping with the spirit of science and materialism. In the end they are faced with a choice: either go back to Earth or be sucked into the depths of the ocean. Chris decides at the last minute not to go back to Earth, preferring instead to shout Rheya’s name and let the ocean suck him down. Here, he is lacking in scientific spirit and is only doing it for love. Of course, Soderbergh lets him jump off the ocean and goes back to his house, where Rheya is waiting for him. Isn’t this choice, made not out of the spirit of science, even better? So Soderbergh says that the planet of Solaris is actually a metaphor for God.\nLiu: Your example does not show that decisions made by scientism are wrong. There is a question of scale. The hero is only making this choice on a human scale, rather than the humanity scale. Think about it the other way around. What would be the consequences if we followed your choice and brought her back to Earth? This thing is not human, you don’t know what her nature is, and you don’t know how much energy she has, or what she will bring to Earth.\nJiang: It’s good to have love. There are things in the human world that are higher than the spirit of science. I want to make it clear that there are not necessarily other systems of knowledge that are better than science, but there can be many other systems of knowledge that should be on an equal footing with science.\nLiu: Science is the system of knowledge that humanity can rely on the most. I recognize that for spiritual needs, religion does have better methods, but the existence of science is necessary for our survival. There may be a more rational system of knowledge in the universe, but until there is, why can’t we trust science?\nJiang: I’m not saying I don’t believe in science, only that we have to be tolerant of other people’s unbelief in science. When faced with a problem that can be solved by science, I will use science to solve it, but when science cannot solve it, I will use something else.\nLiu: The consequences of this disbelief don’t seem to be very serious in a time of peace, but not so in a time of extremity. It seems that our discussion has to end up on the ultimate purpose whichever way we go. One can simplify the world image and do a thought experiment. Suppose you, me and her [the host] are all that’s left of the human world, and the three of us carry everything of human civilization. And we have to eat her to survive, do you?\nJiang: No, I won’t.\nLiu: But the entire civilization of the universe is concentrated in our hands, Shakespeare, Einstein, Goethe… If we do not eat, all the past civilization will be completely annihilated with your irresponsible move. You know the universe is very cold. If we all disappear, there is only a darkness, in which there is no distinction of humanity or inhumanity. By choosing to be inhuman now, humanity will have a chance to re-emerge in the future.\nJiang: The question of whether to eat or not to eat is not one that science can solve. I think it is more responsible not to eat than to choose to eat. If you eat, you lose your humanity. It took a long time for human beings to evolve to this point of humanity, and I can’t lose it like that. I want the three of us to fight together and see if we have a chance to survive.\nLiu: The premise that we’re assuming that either the two of us will survive or the three of us will perish together is a very powerful thought experiment. It’s an ironic fact that being destroyed is like a wall across the face, as I once wrote in The Wandering Earth: “You are walking across a plain when you suddenly encounter a wall. The wall is infinitely tall and extends infinitely deep underground. It stretches infinitely to the left and infinitely to the right. What is it? – Death.”.\nJiang: It reminds me of the most profound question in Battlestar Galactica: “Why is humanity still worth saving?” In the scenario you just envisioned, we lose our humanity by eating her, and a humanity that has lost its humanity has cut itself off from Shakespeare, Einstein, Goethe, etc. What is left to save?\nBut literature may offer a better alternative. When I was very young, I read Byron’s long poem Don Juan, which contains a similar scene:10 several people are stranded on a boat, and lots are cast to decide who will be eaten, but Don Juan is adamant that he won’t eat. Luckily he didn’t eat it, because the cannibals died of poisoning. At that time I was very touched and decided that in the future, if I encountered such a situation, I would definitely not eat anyone. I don’t know whether eating people will poison me or not, but Byron’s moral of the story is for us not to lose our humanity.\n10 This is in Don Juan, Canto 2. As CliffsNotes say:\n\nThe lot falls on Pedrillo, Juan’s tutor, who is thereupon bled to death. Almost all in the boat commit cannibalism except Juan and three or four others. Several of those who have partaken of human flesh drink sea water and go into convulsions. In spite of this, they might have cast lots again had they not succeeded in catching three sea birds and had it not rained for the first time since the ship sank. Later they have the good fortune to catch a turtle that is sleeping on the water.\n\nGemini 1.5 read the whole thing and added:\n\nWhile most of the survivors resort to eating his flesh, Juan and a small group abstain. Following the act of cannibalism, a significant portion of those who consumed Pedrillo’s flesh exhibit severe adverse reactions. They are described as going “raging mad”, engaging in blasphemous outbursts, experiencing convulsions, “And with hyaena-laughter, died despairing.” (Stanza 79).\n\nI would like to ask Mr. Liu a question: among Chinese sci-fi writers, you can be said to be an alternative, because most of the others go for anti-scientism, but you firmly believe in the benefits and light brought by science, and yet you are considered to be the most successful, what is the reason for this?\nLiu: It’s because I show a cold but calm rationality. And this rationality is reasonable. You chose humanity, and I chose survival, and readers identified with that choice. To subvert Kant: I admire with awe the starry heavens above me, but remain untouched by the moral law within me.11\n11 This dictum from the Critique of Practical Reason (1788) was chosen for Immanuel Kant’s tombstone:\n\nTwo things fill the mind with ever new and increasing admiration and awe, the more often and steadily we reflect upon them: the starry heavens above me and the moral law within me.\n\nJiang: It’s rather cold.\nLiu: When we think of these issues in terms of sci-fi, it is very cold."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#postscript-to-the-three-body-problem-2008",
    "href": "docs/posts/liu-cixin-anthology/index.html#postscript-to-the-three-body-problem-2008",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Postscript to The Three-Body Problem (2008)",
    "text": "Postscript to The Three-Body Problem (2008)\nThis postscript to the Chinese book was not translated in the official English translation of the book. My translation is based on Author’s postscript to the original edition of The Three-Body Problem: Full Translation : r/threebodyproblem.\nIf there are extra-terrestrial civilizations, does there exist a universal moral code? In the small picture, this is a question that interests sci-fi fans; in the big picture, this question concerns the life or death of the human civilization.\nThe Chinese sci-fi authors from the 1980s tend to give the affirmative answer. In sci-fi works from that era, aliens appear in amiable form, guiding the lost sheep that is humanity with benevolence and tolerance of the Heavenly Father. In Jin Tao’s Moonlight Island, aliens comfort humanity’s broken heart; in Tong Enzheng’s Far-away Love, the romance between alien and human is both beautiful and majestic; in Zheng Wenguang’s The Mirror Image of the Earth, the moral deficiency of the humans even manages to scare away the aliens who possess technology orders of magnitude more advanced than humans but have the heart of Buddha!\nBut the idea that “Man is born good” is questionable even in the world of humanity, and so it is even less likely to be true at the cosmic scale.\nTo answer the question of cosmic morality, only the rational thinking of science can be convincing. Here, we naturally think of extrapolating the development of different human civilizations throughout world history to the civilizations of the entire universe, but the research of human civilizations is already very difficult, because there are too many unquantifiable factors intertwined together. In contrast, the research for cosmic civilizations is more quantifiable and mathematical, because the distance between star systems make civilizations into points, like how the complex movements of football players are hidden by the distance when watching a football match from the last row of the stadium, and all that is left is a matrix of twenty-three points1. (The ball is a special point. In ball sports, only football games can exhibit such a clear mathematical structure. Perhaps this is one of the charms of the sport.)\nI was once so immersed in the game of making cosmic civilizations into points that I couldn’t get myself out. In the 1990s, when bored, I often wrote unremarkable programs that I found quite amusing. The algorithmic poem generators which are now making a popular comeback online are products from that era. I also wrote a program that simulated the evolution of point-like cosmic civilizations. Each intelligent civilization in the universe is reduced to a point, and each point possesses only about a dozen parameters that describe the basic properties of the civilization. Then I set the number of civilizations to be huge and simulate the evolution of this system. For this, I even invited a respectable scholar who specialized in electric power grids and excelled at mathematical modelling. He is not a sci-fi aficionado, but nevertheless treats it as a hobby, and he improved many aspects of my error-laden model. The largest simulation of the program consisted of 300,000 civilizations within a 100,000-lightyear radius. This simulation, which was written in the now outdated Turbo C IDE on an Intel 286 machine, ran for a few hours and revealed some interesting results. Of course, I was merely an engineer, unqualified for research at this level, and this is just a sci-fi fan playing games, meaningless from a scientific point of view. However, from the sci-fi point of view, these results are very valuable, because regardless of whether the result is true, the degree of its strangeness is difficult to reach by imagination alone.\nI believe that it is entirely possible that cosmic civilizations with no morality can exist, so how would humanity, which has morality, survive in such a universe? This is the original motivation for writing Remembrance of Earth’s Past.\nOf course, The Three-Body Problem did not reveal the entire picture of the universe, and the two civilizations concerned do not realize the full picture, either, but only a tip of the iceberg. Take, for instance, if even the star system closest to us possess an intelligent civilization, then the universe must be very crowded, so why does it seem so empty? Hopefully, I can elaborate on this in the second volume of Remembrance of Earth’s Past.\nFor readers who are filled with admiration and awe for the moral law within,12 the cosmic picture that will be slowly unravelled in Remembrance of Earth’s Past will for sure leave them uncomfortable, but it is only sci-fi. There is no need to take it seriously. 🙂\n12 This dictum from the Critique of Practical Reason (1788) was chosen for Immanuel Kant’s tombstone:\n\nTwo things fill the mind with ever new and increasing admiration and awe, the more often and steadily we reflect upon them: the starry heavens above me and the moral law within me.\n\nFrom the serial publication of The Three-Body Problem, I found out that Chinese readers seem to like sci-fi novels that describe the ultimate fate of the universe. This was more or less surprising. I came from the sci-fi boom in the 1980s. I personally think that writers from that era created real Chinese-style sci-fi that was never seen again at scale. The most distinctive feature of these sci-fi works was the complete exposition of technical details, without even a shadow of “style over substance”. Sci-fi fans nowadays though have opened their eyes and would like to embrace the universe with thought and ideology. This raises the bar for the sci-fi writer, and regrettably, The Three-Body Problem is not such an “ultimate sci-fi novel”. Writing sci-fi in the style of 2001: A Space Odyssey is very difficult. Especially with long novels, it is easy to create an empty frame with none of the compelling narration of a novel, the accuracy of popular science, or the rigour of a paper, so I do not have the confidence to write a novel that rivals 2001.\nOh, the series I have in mind is called Remembrance of Earth’s Past. No deep meaning. The difference between sci-fi and other fantasy fiction is that the former still has a thin thread connecting it to reality, which makes sci-fi a modern myth instead of a fairy tale (ancient myths were real to their readers at the time). Thus, I always think that the best sci-fi should make the craziest and the most intangible imagination sound as real as reports in the newspaper. The recollection of the past always sound real, so I wish to write my novels the same way historians narrate real events in the past. Whether I can actually achieve this is another matter.\nI tentatively title the next volume of Remembrance of Earth’s Past to be Dark Forest, which is inspired by a popular saying in the 1980s: “a city is a forest, every man is a hunter, and every woman is a trap”.\nOh, lastly, and most importantly: thanks to everyone!"
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#beyond-narcissism-what-science-fiction-can-offer-literature-2009",
    "href": "docs/posts/liu-cixin-anthology/index.html#beyond-narcissism-what-science-fiction-can-offer-literature-2009",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Beyond Narcissism: What Science Fiction Can Offer Literature (2009)",
    "text": "Beyond Narcissism: What Science Fiction Can Offer Literature (2009)\nSource: 刘慈欣：超越自恋——科幻给文学的机会, 《山西文学》 2009年第07期 (Beyond Narcissism: What Science Fiction Can Offer Literature, published in Shanxi Literature, the 7th publication of 2009)\nAlready translated at https://www.depauw.edu/sfs/backissues/119/Liu%20Cixin.html."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#interview-with-peoples-daily-foreign-edition-2010",
    "href": "docs/posts/liu-cixin-anthology/index.html#interview-with-peoples-daily-foreign-edition-2010",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Interview with People’s Daily, foreign edition (2010)",
    "text": "Interview with People’s Daily, foreign edition (2010)\nSource: 人民日报海外版关于科幻的采访_workership_新浪博客\n1、How do you think the “Science Fiction World open letter incident”13 will affect the whole Chinese sci-fi literature community?\n13 A minor drama in 2010-03. Unimportant now. See 【汇总】科幻世界公开信事件汇总【3.31夜】.The greatest significance of this incident is that it shows the great power that sci-fi harbors. The road of sci-fi literature in China has been bumpy, but in the previous crises, this power did not exist, for example, in the 1980s, when sci-fi readers, although spreading over all social strata, did not form a self-conscious group, that is, sci-fi fans or readers’ group, so in that crisis, sci-fi did not get strong support from the society. Compared with the readership of other literary genres, the sci-fi readers’ group shows a stronger sense of identification and enthusiasm for this kind of literature, and they also share many common characteristics and strong cohesion in their way of thinking and values. It can be said that the sci-fi fandom is the most valuable and cherished treasure of sci-fi literature, the foundation of sci-fi’s existence and the source of its power. It is also because of the existence of this community and this power that at the very beginning of the event I thought that Science Fiction World would be able to overcome this difficulty, or at least get around it.\nWhat I hope is that after the dust settles from this incident, people who love sci-fi will focus their attention on the larger and more essential threats facing sci-fi literature.\n2、What do you think are the current problems with sci-fi literature in China? How should they be solved?\nAs I said before, the sci-fi readership is the most cherished thing in domestic sci-fi literature, and it is the foundation of sci-fi’s existence. Then, what unites so many readers together? What makes them have such a strong sense of identity and passion for sci-fi literature – the kernel, that is, the soul of sci-fi?\nWhat is the soul of sci-fi? It is impossible to say it here in one or two sentences, but all those who read sci-fi strongly feel its existence, just like the existence of the sun is still felt on a cloudy and rainy day. Certain ways of thinking or complexes expressed in sci-fi literature, such as the awe of the universe and the curiosity of the unknown world, the pursuit of the ultimate purpose of mankind, and the pursuit of the beauty of nature based on reason, are not found in other types of fantasy literature, which is also where the soul of sci-fi lies.\nNow, there is a kind of unconscious or conscious “de-soulization” of sci-fi literature, which is the greater threat to sci-fi I mentioned earlier, and this threat is far more terrifying than that caused by a boss who does not know how to run a sci-fi magazine. This will destroy the foundation of sci-fi. Of course, merely sticking to the soul of sci-fi will not make sci-fi prosperous, which also needs the cultivation of the market and other efforts, but as long as the soul of sci-fi is still there, sci-fi fans will be there, sci-fi will be there. It may be a miserable business, but it will persist, and one day will be able to create a new glory. However, if this soul is lost, it may be able to usher in the short-lived prosperity, but sci-fi will ultimately lose the readers’ sense of identity, and its fate will be difficult to find a way. Its fate can hardly be predicted. When we pull our eyes back from the stars and cast them upon the puny little heartaches of peple living in this world of human feelings, sci-fi is not far from death.\nI am a member of the sci-fi community, and I wonder how many others like me have read Science Fiction World from its inaugural issue all the way to the present day. After this incident, readers are filled with worry and anxiety. I also have this kind of worry, and it has been going on for more than ten years, I don’t know since when, this kind of worry lingers in the mind and lingers, sometimes to the point that it makes me exhausted. But it’s not these kinds of real-life events that worry me; I know that as long as the world of sci-fi fans exists, sci-fi will exist, and these kinds of difficulties can be overcome. My concern is that sci-fi is losing its soul.\nSci-fi won’t die suddenly, but it will bleed out, and when that day comes, there will be no one at its deathbed.\nTo paraphrase an online slogan: You can take whatever you want, but you cannot take sci-fi’s soul. Leave sci-fi to sci-fi! This is not to be shouted at the bosses, nor to the readers. Nobody can help us when the soul of sci-fi is lost.\n\nDo you think sci-fi literature is a niche culture in China? What are the reasons?\n\nI don’t think sci-fi literature is a niche culture. After joining the Writers’ Association, I met two poets who self-published a book of poems, and they only printed about 1,000 copies of the book, mainly for giving away, and they gave me two copies of the same book of poems, which is a niche literature. Nowadays, there are still a lot of people who read sci-fi, all over high schools and universities. It’s niche only in comparison to the top-ranked bestsellers, or to paraphrase sci-fi critic James Gunn, it’s more accurate to say “Sci-fi is popular literature for the few.”. If we take sci-fi movies and television into consideration, then it is definitely popular literature.\n\nDo you think sci-fi literature should be promoted as a popular culture?\n\nYes. Sci-fi has never been an elite literature, and the efforts to elitize it in history have all ended in failure. Sci-fi should show the public’s dream of the future and the unknown world, it is the display of human curiosity and enterprising spirit, not the obscure and lofty literature of the elite, and elitism will do harm to sci-fi.\n\nWhat kind of characteristics do you think the sci-fi literature in China has? Has it changed in recent years?\n\nChinese sci-fi literature has different characteristics in different periods, from the nationalist anxiety of the late Qing Dynasty and the Republic of China to the self-conscious utilitarian goal of science popularization in the 1950s and the 1980s. Nowadays, sci-fi in China shows a diverse characteristic, with various styles of works emerging, and there is no absolute dominance of any one style or concept.\n\nWhat do you think is the next development direction of sci-fi literature in China? Is it a feasible way to combine it with movies, television and animation?\n\nI feel that the development of sci-fi literature in China will still be diverse, with various styles of works co-existing and a tendency to integrate with other types of fantasy literature.\nThe combination of film and television with film and anime is of course a good thing, and besides the modern media trend, there is a very special reason. Unlike other types of literature, certain scenes and moods depicted in sci-fi are almost impossible to express in words and can only be expressed visually. So many times when I see the magnificent images in sci-fi movies, I am deeply impressed by the weakness of words.\nBut there is a point to make clear: sci-fi film and television is not sci-fi for just sci-fi fans, but for the whole public, we do not have a lot of box office, so do not think of that kind of purist sci-fi things. In fact, even in the United States, of the famous sci-fi masterpieces, only a very small portion has been made into movies.\n\nWhat is the ideal sci-fi literature in your mind?\n\nIt’s the same thing I’ve said so many times: good sci-fi makes you suddenly stop on your way off the night shift, and do something you rarely did before: look up at the stars."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#interview-with-city-pictorial-2010",
    "href": "docs/posts/liu-cixin-anthology/index.html#interview-with-city-pictorial-2010",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Interview with City Pictorial (2010)",
    "text": "Interview with City Pictorial (2010)\nSource: 《城市画报》采访大刘的无删节版稿子\n刘慈欣：我是工科男 我不是文青 by 许晓 (Liu Cixin: I’m an engineer guy, not an artistic youth. Interview with City Pictorial, by Xu Xiao)\n\nI don’t have much to say except a warning. Life reached an evolutionary milestone when it climbed onto land from the ocean, but those first fish that climbed onto land ceased to be fish. Similarly, when humans truly enter space and are freed from the Earth, they cease to be human. So, to all of you I say this: When you think about heading into outer space without looking back, think twice. The cost you must pay is far greater than you could imagine.\nFinal statement made by Captain Neil Scott, from Death’s End\n\nQ: Winning the Galaxy Award, the highest prize for sci-fi in China, for eight consecutive years is an amazing record. Indeed, it would be unimaginable for a non-sci-fi writer to win the Mao Dun Literature Prize for eight consecutive years. How did you achieve this “monopoly”?\nA: The Galaxy Award and the Mao Dun Literary Award are very different from each other. The Galaxy Award is not a national literary award, but an amateur award organized by the Science Fiction World magazine, and it only evaluates short stories, plus the fact that there are very few authors in China who write sci-fi – there are only ten to fifteen of them, and all of them are amateur authors – so it is not difficult to win this award. I’ve only won the award eight years in a row, and there are others who have won it ten times in a row, such as Wang Jinkang and Xinghe. The authors and readers of Science Fiction World are kind of like on a bus: they get on the bus and then get off the bus. A lot of people write for a while and then stop writing. People who write sci-fi don’t take the award as seriously as mainstream authors take their mainstream awards. They do so with a playful mindset, but there’s nothing wrong with that.\nQ: The first decade of the new century is coming to an end, looking back, what kind of mood drove you to start writing?\nA: The first time I wrote sci-fi was when I was in high school, because I was a sci-fi fan, and I started to write when I was looking at it. In China, sci-fi fans really appeared as a group in the 1990s. I became a sci-fi fan in the late 1970s and early 1980s, when there was no such group as “sci-fi fans” and they were very isolated from each other. Now there is a community of sci-fi fans, which is of great significance to the development of sci-fi literature in China.\nQ: Nowadays, there are many sci-fi fans and they often comment on your works on the Internet. Do these voices have any influence on your work?\nA: I sometimes go online to read these comments, but I don’t read much because I have a limited time. They certainly have an effect, because I’m not someone who writes fiction for myself or for critics only. The main target is still the reader. But I definitely have a bottom line that I stick to.\nQ: What kind of bottom line?\nA: The bottom line of sci-fi. I must stay loyal to my own definition of sci-fi: science itself must have a relatively large role in the sci-fi I write. Worldwide, this type of sci-fi is actually shrinking. For example, in the US, sci-fi now focuses more on literary techniques and modernist expression styles.\nQ: If someone is ready to start reading your work, what would you recommend to such an entry-level reader?\nA: With the exception of Supernova Era, all my other works are pretty much the same, and they all fall into the category of stories that can’t stand up if you take away the sci-fi idea. Supernova Era is not quite the same; if it were any other random disaster where only children were left on Earth, the story would still hold up. However, for an introduction, I’d suggest checking out my short stories, if you don’t think it’s good, you won’t waste too much time.\nQ: In 1950, the famous Italian Enrico Fermi asked, “Where is everybody?”. It’s been said that The Three-Body Problem trilogy is nothing but the story of a woman messing with her man, then they starting messing with each other, then both were messed by the universe, and finally the universe was messed by who knows what.\n\nThe universe is a dark forest. Every civilization is an armed hunter stalking through the trees like a ghost, gently pushing aside branches that block the path and trying to tread without sound. Even breathing is done with care. The hunter has to be careful, because everywhere in the forest are stealthy hunters like him. If he finds other life – another hunter, an angel or a demon, a delicate infant or a tottering old man, a fairy or a demigod – there’s only one thing he can do: open fire and eliminate them. In this forest, hell is other people. An eternal threat that any life that exposes its own existence will be swiftly wiped out. This is the picture of cosmic civilization. It’s the explanation for the Fermi Paradox.\nThe Dark Forest\n\nA: I really didn’t pay much attention to the gender of the protagonist when I was writing. For example, if the protagonist of the third installment is female, it’s because the protagonist of the second installment is male, for that simple reason.\nQ: There are also people who say that The Third Body gives them a glimpse of what a Hollywood blockbuster feels like. Do you like that comment?\nA: I like it. Hollywood blockbusters are commercial productions, but they’re all skillfully made, and rarely do they invest hundreds of millions of dollars in a movie and then screw it up. I’ve never seen a Hollywood blockbuster that I particularly despised. What I can’t stand to watch is an arthouse movie. It’s torture. As long as you have spend a lot of money on it and spent it well, I’d like to watch that movie, but I can’t stand arthouse movies even if it’s well-made. Plus, none of the arthouse movies have strong storylines, and the ones with strong stories aren’t called arthouse movies. Movies like like Kramer vs. Kramer and American Beauty are all painful for me to watch. I’m an engineering guy, I’m not a literary guy, I’m not interested in arthouse. My novels aren’t high-literary either, they’re crass by literary standards.\nQ: In the first book of The Three-Body Problem trilogy, the Cultural Revolution is an important backdrop to the story. What was the reason for writing it that way?\nA: It started out as a desire to write a novel focused on the Cultural Revolution. After all, I spent my childhood and teenage years there and knew much about it. Then I realized that readers are not interested in the Cultural Revolution because they are all Gen-80s and Gen-90s, so I faded it into the story background.\nQ: What do you think about Mao Zedong?\nA: Whether writing a novel or telling a story, this kind of person is very eye-catching, and if such a character appears in sci-fi, he must be very charismatic.\nQ: The Three-Body Problem trilogy portrays three very different female protagonists: the rational Ye Wenjie, the dreamy Zhuang Yan, and the motherly Cheng Xin. Which one is your favorite?\nA: None of them is the type of women I like. They are just tools to advance the plot. For example, Cheng Xin, the heroine in the third installment, is just a symbol that represents the universal values and morals of humanity. When you say people don’t like the main character, it means people don’t like themselves. Cheng Xin is a very ordinary and normal person, the choices she makes at every critical moment are the choices that every normal person would make, in line with universal values and morals, but it is precisely this choice that drives humanity to extinction.\nQ: According to the Dark Forest Hypothesis, survival is the first goal, and the choices Cheng Xin makes at critical moments are contrary to this axiom.\nA: To make survival the primary goal is precisely the viewpoint of a Übermensch, and it is quite difficult. Ordinary people, like the Cheng Xin, must first follow their inner sense of morality. At a critical moment, the Übermensch is the one who can have the mental strength and vigor to jump out of the bounds of morality and grasp the final goal of survival.\nQ: Dis-identification with Cheng Xin may affect the reader’s sense of agency and identification with the novel. Did you worry about this?\nA: I didn’t write her with the intention that readers would like her. This is not someone readers will like. She’s actually selfish, but this selfishness is different from ordinary selfishness because she doesn’t perceive it herself. People who follow morals are actually very selfish because they don’t care about anything but morals and conscience, and Cheng Xin is exactly such a person. She would consider herself noble, consider herself unselfish, and consider her values and moral code to be universal and correct. As to what consequences following it would bring, she would only consider whether it would bring peace to her conscience. This kind of person has a spirit of sacrifice and is able to sacrifice her life for her values and moral code, but this does not change the verdict of selfishness. If someone that can perform “great loving without kindness”14 were to appear in the novel, they would be unselfish and work to benefit Humanity itself, because sacrificing one’s conscience is the greatest sacrifice thing to do, and it is much harder than merely sacrificing one’s life.\n14 Zhuangzi, The Adjustment of Controversies, paragraph 10:\n\nThe Great Dao does not admit of being praised. The Great Argument does not require words. Great Benevolence is not (officiously) benevolent. Great Disinterestedness does not vaunt its humility. Great Courage is not seen in stubborn bravery. The Dao that is displayed is not the Dao. Words that are argumentative do not reach the point. Benevolence that is constantly exercised does not accomplish its object…\n\nQ: Do you have a problem with Harry Potter? Because the fundamental premise of Harry Potter is saving the world with love, and you have the character of Cheng Xin destroying the world with love twice, and you were saying it without disguise.\nA: People who write fantasy celebrate love, but people who write sci-fi are more rational. If you look at the world’s major sci-fi classics, none of them are interested in love, just plain rationality. Moreover, the setting of The Three-Body Problem is an existential deadlock, with a dark undertone, and honestly, in this setting, love can’t save anyone. People who write sci-fi should keep their worldview in a state of uncertainty, if they are too determined to perceive everything, it’s hard for them to write any novels, especially sci-fi, which is about humanity’s confusion and exploration of the unknown. As for myself, I don’t have an iron-clad view of human society. It may be this way in this set of books and that way in that set.\nQ: Do you think authoritarian governance is the best way to solve the humanity-wide existential crisis in sci-fi?\nA: According to the current trends in society, the current form of society is not conducive to solving the crisis. If the humanity-wide crisis in the novel arrives, human society must be led more efficiently (and arguably more evilly) by a strong government – or just as well, not by a government, but a coalition, or even an AI. One thing is for sure: Humanity must make the decision to sacrifice the few to preserve the many, and the governments or other leadership organizations in the novel exist to ensure that this decision is made. What The Three-Body Problem is trying to say is that there is a contradiction between humanity’s current moral system and humanity’s self-preservation in a catastrophe.\nQ: Outside of Cheng Xin, how would you characterize Yun Tianming?\nA: He starts out as an otaku, sensitive, and unpopular, a character that definitely doesn’t fit in today’s society. Right now I don’t know what he is doing after his brain was captured by the Trisolarians, but later I may write a book about his entry into the alien society. His kind of character, withdrawn from human society, is paradoxically suitable for entering an alien society, because he usually does not rely on human relationships to survive. In contrast, we, who are well-socialized animals, might fall apart when we enter the alien society.\nQ: What you are saying is that otakus are easy to make a home on an alien planet.\nA: Right. If you mix too well with human society, you might fall apart faster when you enter an alien society.\nQ: There are a lot of ambiguities buried in the novel, such as Yun Tianming, such as the later encounters of the Trisolarian fleet, which are not expanded upon.\nA: Readers can never understand the invisible constraint the author must face: length. You can’t write as much as you want in a full-length novel; there’s an agreement between the author and the publisher. For the current market, 200,000 words usually counts as a “long” sci-fi story, and with 360,000 words written for the third installment of The Three-Body Problem, it’s already 160,000 words over the limit, so what else can you do?\nQ: Will that be followed by extra stories around the series?\nA: It’s hard to say at the moment. I do not dare to write a 4th entry for another reason, if the last part of the plot15 is expanded to a 4th book, it will become a pure “space opera”, ethereal story, and the real life of humanity does not have anything to do with this kind of sci-fi is very difficult to write, and readers will also find it very painful to read.\n15 At the end of the story, most of humanity has been destroyed except a few hanging on in a tiny bubble space. They received a universe-wide broadcast to return the bubble space back to the wide universe, so they decided to do so, and started surviving on a planet with a spaceship. It is unclear what they would be doing from then on, but probably they would simply die and not be able to restart human civilization.Q: At that time, I heard that when The Three-Body Problem 3: Death’s End was almost finished, many people on the Internet ran to tell me about it.\nA: Death’s End is a consequence of both the readers and the writer being in a hurry. It was written in such a hurry that the readers were also rushing, and the editor was also rushing, but that was not the reason for the rush. The biggest reason is still the author’s own haste. I wanted to finish writing quickly, because after all, I am an amateur author, and found an idle period. Whether I will be busy in the future, I can’t say. 360,000 words, conceptualization, and writing, one year to complete, it’s too hasty. If only I have spent two more years, I could have done much better.\nQ: When can we expect your next full-length book?\nA: The three books of The Three-Body Problem make up one whole story, so readers were waiting impatiently for each installment. Now that it’s complete, I don’t think people will be too anxious for the next full-length book. When I’ll write it, I can’t say.\n\nI feel like I’m writing like a tour guide, taking readers on a tour of my own imaginary worlds. I’ve been leading this tour for more than ten years, but I haven’t even finished half of the attractions yet, not to mention those being newly developed. So there is always some anxiety in my heart. Because I know that accidents can happen at any time: floods can block our way, our bus can be hijacked by gunmen. As an old sci-fi fan, I know that this is not an idle worry. Of all the mishaps sci-fi may encounter in the country, the most worrying is social unrest. At a meeting for Science Fiction World readers and writers, I told my readers and friends that sci-fi is a kind of literature for years of peace. They were all unimpressed, but it’s true. Only in a stable life can we be interested in and shocked by the disasters of the world and the universe. If we ourselves live in a crisis-ridden environment, sci-fi will no longer arouse our interest. As a matter of fact, two of the first three ages of Chinese sci-fi were interrupted by social unrest, which is the biggest killer of sci-fi.16 Now that the calm has lasted for more than twenty years, it feels like at the grassroots level of society, some kind of spring is winding up, and the last straw that broke the camel’s back could come at any time. Hopefully, this is just a sci-fi fan’s idle worry. I hope the peace will continue, since that would be a great blessing for sci-fi.\nLiu Cixin’s blog post on 2010-09-02 10:19:36. Posted in Sina Blog.\n16 First period: 1950 – 1966, interrupted by the Cultural Revolution (1966–1976). Second period: 1976 – 1983, interrupted by the Anti-Spiritual Pollution Campaign (1983). Third period: 1990s – now.\nQ: You have a poetic expression when describing good sci-fi: “It will make you want to look up at the stars.” Can you tell me what the starry sky in Shanxi is like for us?\nA: Shanxi is a big energy province with a high density of power plants and serious air pollution … So, starry sky …?\nQ: How about if you asked fans scattered across the country to photograph the stars in their eyes and send them to you?\nA: It’s basically impossible, unless with a professional digital camera. You can’t get it with an ordinary camera. How about you take a picture of one? It’s black.\nQ: I can see that you are very rational, and you douse cold water over all artistic flames. Besides being rational, do you have a strong sense of crisis?\nA: I don’t have much sense of crisis in my life, I just take things as they come, and I can withstand even the biggest social waves. But I have a strong sense of crisis about sci-fi. It’s my hobby, and I worry that it’s going to fall into a slump. In the 1970s and 1980s, sci-fi hit a low point, and sci-fi fans of my age have experienced it, with psychological trauma and memories. Once society is in turmoil, people simply don’t have the heart to interpret sci-fi, there are enough crises around, who comes to see crises in sci-fi.\nQ: I heard that in your college days you once traveled to the China-Vietnam border with a consolation group and witnessed the fighting, bloodshed, and death where you could see the Vietnamese army. Is that true?17\n17 During the Sino-Vietnamese conflicts (1979–1991), some civilians were organized into groups and visited Chinese soldiers near the frontiers. They typically brought poems, songs and dances, food, etc, to improve the morale of the soldiers.A: All I can say is that war is not the same thing as what we think or see on movies, but these have little impact on the novel, and I never intended to include in the novel what the real war I saw was like.\nQ: What life experiences do you think you’ve had that kids in the big city who are well-dressed and well-fed might not be familiar with?\nA: I didn’t grow up in a big city, nor did I grow up in a rural area. I grew up in an industrial and mining enterprise, which happens to be at a neither-up-nor-down sub-level in the grassroots level of Chinese society. Most people in China are very ordinary people like me, so I don’t think my life experiences had influenced my sci-fi writing in a way that’s different from other writers.\nQ: Is it because your previous books didn’t sell well enough, and the financial stability they brought you wasn’t stable enough, that you didn’t become a full-time writer?\nA: That’s one of the reasons. Writing one book per year is exhausting in terms of the normal demands of life, and there’s no way to guarantee that your book will always sell. Secondly, I live in a place where there are no sci-fi clubs and very few people who share my passion. If I don’t work, I’d be completely cut off from society. As a fiction writer, it wouldn’t be a good idea to stay home all day. Thirdly, my job provides me with a very stable life.\nQ: It was rumored that the power plant you work for was facing closure, and someone asked if he could know something about your living conditions, saying that he couldn’t accept that someone as talented as you was living a life of despondency.\nA: The organization I work for is a centrally-managed enterprise, how can I be in poverty?18 It is normal for the electric power system to close down a plant and build a new one. It doesn’t mean that all the people in it will lose their jobs.I especially hate some people who always prefer martyrs and ascetics. I don’t like martyrs and I don’t want to be an ascetic. I especially like Heinlein’s quote, “We write for Joe’s beer money, and Joe likes his beer. It’s our obligation to give him at least as much fun from our books as he’d get from a six pack.” – and the truth is, I’m not even short of that kind of money. I must be considered well off by the standard of my city, and to tell you a joke, we don’t dare wear overalls on the street for fear of attracting thieves. A lot of media reporters always like to guess that this Liu Cixin must be someone curled up in a poky little house with gloomy light day and night, writing sci-fi – it’s not like that, I have two suites in the city, both large, not poky.\n18 Liu Cixin graduated from from the North China University of Water Conservancy and Electric Power in 1988, and then had been continuously employed as a computer engineer at the thermal power plant in Niangziguan, Shanxi. His main job is computer network and monitoring in power plants. He used his spare time to publish 13 novel collections, including “Ball Lightning”, “Supernova Era”, “The Wandering Earth”, and the first two of “Three Body”, all of which were created during this period. In his own words, he could write on the work-computer during work-time, and it felt like stealing morsels of enjoyable time from the company. In 2009, the Niangziguan Thermal Power Plant was shut down. Later, he was transferred to the Yangquan Literary and Art Creation Research Office to engage in “specialized literary creation and research”.Q: Your regular job is as a computer engineer for a power system, so what time do you generally use to write your books?\nA: After work evenings, and all Sundays.\nQ: What do your friends and colleagues think of your work?\nQ: They don’t care – unless it becomes a bestseller and sells tens of millions of copies, then they will care. Right now they just think I’m doing some personal work in my spare time.\nQ: What would you do if you were sent by Earth to make contact with extraterrestrial life?\nA: I’m sure I’d be told what needs to be done.\nQ: Would it be Cheng Xin or Luo Ji?\nA: In terms of my values, definitely Luo Ji."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#deaths-end-completed-2010",
    "href": "docs/posts/liu-cixin-anthology/index.html#deaths-end-completed-2010",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Death’s End completed (2010)",
    "text": "Death’s End completed (2010)\nSource: 《死神永生》完成_workership_新浪博客, published on Sina Blog, 2010-09-02 10:19:36.\n360,000 words.\nAt this point, all of The Three-Body Problem trilogy is complete, three books in total, 880,000 words, a very long work.\nRecently, almost every day, I received emails, text messages and phone calls, urging for the third part. For a long time, I felt that I was like Yang Bailao, and I heard Huang Shiren’s voice from time to time: “Mr Liu, last year’s debt should not pass the New Year’s Eve, so why don’t you sell off your daughter to cancel your debt?”.\nIn fact, this book was not written slowly. Dark Forest was published in 2008. Then for external reasons, I did not write for more than a year. Then the third part was also written about a year. Actually, if one were to write 360,000 words of a long story, to ensure the quality, one should properly spend three to four years.\nWe have been calling for quality, but how does quality come? In terms of long novels, first of all, it takes time. Of course, there might be geniuses who can create world classic from careless scribbling, but such a person appears once every hundred years. Ordinary authors like us take a long time to write a long novel, to build a world brick by brick, to have a long conversation with our own minds.\nBut even without the urging of readers, I couldn’t have spent three or four years writing a full-length novel. This is a time of urgency, and the author is not exempt from it. Nowadays, the author who can stand the loneliness, as well as the man who sits still, not to say there is no such person, but they are extremely rare.\nI want to write faster for is another reason: a sense of crisis. I feel like I’m writing like a tour guide, taking readers on a tour of my own imaginary worlds. I’ve been leading this tour for more than ten years, but I haven’t even finished half of the attractions yet, not to mention those being newly developed. So there is always some anxiety in my heart. Because I know that accidents can happen at any time: floods can block our way, our bus can be hijacked by gunmen. As an old sci-fi fan, I know that this is not an idle worry. Of all the mishaps sci-fi may encounter in the country, the most worrying is social unrest. At a meeting for Science Fiction World readers and writers, I told my readers and friends that sci-fi is a kind of literature for years of peace. They were all unimpressed, but it’s true. Only in a stable life can we be interested in and shocked by the disasters of the world and the universe. If we ourselves live in a crisis-ridden environment, sci-fi will no longer arouse our interest. As a matter of fact, two of the first three ages of Chinese sci-fi were interrupted by social unrest, which is the biggest killer of sci-fi.19 Now that the calm has lasted for more than twenty years, it feels like at the grassroots level of society, some kind of spring is winding up, and the last straw that broke the camel’s back could come at any time. Hopefully, this is just a sci-fi fan’s idle worry. I hope the peace will continue, since that would be a great blessing for sci-fi.\n19 First period: 1950 – 1966, interrupted by the Cultural Revolution (1966–1976). Second period: 1976 – 1983, interrupted by the Anti-Spiritual Pollution Campaign (1983). Third period: 1990s – now.But no matter what, we should travel faster in the world of sci-fi, and in this second life, we should have fun in time. To quote from Death’s End: “A moment here; eons there.”."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#the-prehistoric-age-of-the-ai-phylum-2011",
    "href": "docs/posts/liu-cixin-anthology/index.html#the-prehistoric-age-of-the-ai-phylum-2011",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "The prehistoric age of the AI phylum (2011)",
    "text": "The prehistoric age of the AI phylum (2011)\nSource: AI种族的史前时代, 刘慈欣, 2011-02-11 01:37:00. Written on the occasion of IBM Watson.\nWhen computers were first created, their computing power far surpassed that of humans, and later they beat them at chess, recognizing human faces and understanding many languages. But even so, deep down, we still feel that we are not dealing with true intelligence. It is conceivable that even as the performance of computers continues to improve at a rapid pace, and even as their fuzzy-math-based pattern recognition and reasoning capabilities are further refined, we will still have difficulty seeing them as true intelligences, and we will lack a certain key sense of being confronted with true intelligence.\nBecause their computational processes are inherently transparent and predictable.\nIt is not AI for a computer to win a game of chess against a human being, it is AI for it to lose a game of chess and then become so enraged that it electrifies the mouse and kills the human player playing the game.20\n20 There is a persistent urban legend in China about a chess AI in the 1990s who played against a Russian chess master three times and was defeated three times, but then the computer electrocuted him “in a rage”. The legend sprung up after the Deep Blue vs Garry Kasparov matches of 1997.From the not-so-long history, the development of AI has roughly gone through two stages.\nThe first stage is very idealistic, trying to use the software logic and hardware improvement to directly realize the intelligence, in the spirit of Genesis. With the failure of Japan’s fifth-generation project, it was found that this would be difficult to do, at least in the foreseeable future. AI research then turned to databases and knowledge bases, a brute force strategy that attempted to realize intelligence based on the retrieval of huge amounts of data and knowledge. The prevalence of expert systems in the 1990s was the initial result of this research.\nI once participated in the development of an expert system for turbines,21 and what impressed me most was: in the process of constructing the knowledge base, when those human experts realized that their lifelong experience was summed up in just a few sentences, they were upset at first, but soon regained confidence, and thus found their own value, knowing that computers could not do anything great with just a few rules. At most they could only be used as an aid for novices to learn. When the turbine system really broke down, the computers could not do much with just a few rules. They were right. It’s true that those lookup-table (simple, albeit complicated) things don’t qualify as real intelligence, and real experience is hard to express in a knowledge base.\n21 Liu Cixin’s day job is a programmer at a coal-power electric power generation, so he is very familiar with turbines. He has dabbled with AI since the 1990s.But things are changing.\nLooking back on my own work experience over the years, it is a process of increasing fear of IT systems. The DOS system of the 1980s was relatively reassuring, despite its rudimentary function, because it behaved in a very simple way, doing whatever it was told to do. All actions were within the bounds of predictability. The operating system at that time was more transparent, and it was said that some patient people had read through the source code of all of DOS. They could trace bugs of the program down to the assembly level. Later, however, the operating system evolved rapidly, the simple C:&gt; prompt became a gorgeous GUI, and the system gradually turned into a black box, with unpredictable behaviors. The IT system seemed to have grown from an innocent child to a deep schemer.\nBy now, the system feels completely black box. It obeys only on the surface, and we do not know what it is thinking in its heart of darkness. Sometimes, the server hard disk drive sounds like a low cold laugh, and the little lights on the switches are like a million little impish eyes. When you’re looking for bugs in the maze of hardware and software, it’s like crawling through the slimy intestines of a monster, feeling annoyed and desperate. There are many times when testing a piece of software to see if it is correct takes much longer than programming it, and this is especially true for software that is run continuously online.\nIt should be recognized that all this is psychological, DOS system is not necessarily more stable than WIN2000 or XP, not to mention those Unix-based distributed control systems for running power generation plants. These are from Europe, totally robust, reliable, rock-solid. Still, with the evolution of IT systems, people always feel that they are gradually losing control of something.\nFor operating systems, not only their vast number of users, but also developers have this feeling of losing control. One of Microsoft’s system designers said, “[The process of system development] is like being stuck in a dark, sticky quagmire, sinking no matter how hard you struggle, and controlling the whole picture is a delusion.”\nThis may be a vision of AI in its infancy.\nIn fact, in essence, whether under DOS, Windows, Unix, or Linux, the behavior of the software is also transparent and predictable, theoretically. With sufficient manpower, every run of any program can be analyzed to produce an accurate process map. Theoretically, it is also possible to compile such monitoring software, which will accurately record each and every step of the operation of other software to generate a complete report of the computational process. Even the random numbers generated by the software can be predicted, because the RANDOM() function is pseudo-random. Even if the function is truly random, the rest of the computer process is still legible.\nWhen the author was working on that turbine expert system, he had been asked to record the reasoning process of the system, or the process of retrieving the knowledge base. When the string of retrieval tree diagrams was displayed or printed out, both the turbine experts and those of us who programmed the program found it tedious to the extreme.\nBut as technology develops, the opacity and unpredictability of IT systems are increasing, and although quantitative changes have not yet produced qualitative changes, perhaps some non-von-Neumann computer systems, together with new software technologies such as evolutionary algorithms, will make this breakthrough a reality.\nHere is a fundamental question: is human intelligence unpredictable in nature? In nature, macro-scale unpredictable objects, most typically chaotic systems, is the brain a chaotic system? It would not be if intelligence were truly generated by a giant interconnection of neurons, and although the number of neurons is enormous, exactly equal to the number of stars in the Milky Way, this interconnection is still inherently transparent and predictable, and theoretically allows for the precise monitoring and recording of the thought process in its entirety. But who knows what else lies beneath this giant interconnection that makes thinking a truly unpredictable process. Roger C. Penrose in The Emperor’s New Brain argues that human intelligence is inherently impossible to reproduce by computers.\nThe goal of science, which is to make opaque and unpredictable nature transparent and predictable, is now engaged in a bizarre human endeavor in the field of artificial intelligence, trying to create something that is inherently opaque and unpredictable, which doesn’t sound good. The day a true AI is created is the day our fears become reality, but we still relish it. It’s human nature, man or woman, to want a lover whose behavior is not completely predictable – else it would not be charming. There is a great temptation to create something higher than yourself and unpredictable, despite the possibility of being electrocuted while playing chess with it.\nLooking at IBM Watson in front of us, we understand that we are in the prehistoric age of the AI phylum."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#1-and-100000-earths",
    "href": "docs/posts/liu-cixin-anthology/index.html#1-and-100000-earths",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "1 and 100,000 earths (2012)",
    "text": "1 and 100,000 earths (2012)\n一个和十万个地球, published in the 2012 New Year Special Issue of 《周末画报》 Weekend Pictorial. The idea is turned into a story in Fields of Gold (黄金原野) (2018).\nAuthor’s note: This is an article based on a speech given at the Hong Kong Book Fair in July last year and published in Modern Communication’s Weekend Pictorial. I gave a lecture on this topic at a training class of the Shanxi Provincial Writers Association not long ago, but there was little interest. A writer friend, who was attending the class, came down and said something very interesting to me: “You know, literature! Why are you making such idle worries?”.\nCompared to other animals, human babies are very fragile. While a baby horse can walk upright on its own within ten minutes of birth, human babies have to stay in their cradles for quite a long time, during which time it is impossible for them to survive without careful outside care. If left to its own power, a human being would never emerge from the cradle. The reason for this phenomenon is evolutionary necessity; the human brain is larger in size, and when it is fully developed, it is difficult to be born, but only in advance, that is to say, all human babies are born prematurely.\nIf human civilization as a whole is regarded as a baby, then it is also a premature baby. The speed of civilization is much faster than the speed of natural evolution, and human beings actually entered modern civilization with the brains and bodies of primitive people.\nA frightening question therefore arises: without outside care, would the baby of human civilization never have been able to get out of its cradle?\nThis seems to be a possibility now.\nIn the distant future, when people look back on the history from the mid-20th century to the present, all the earth-shattering events that have happened during this period will be worn down by time, and only two things, which we have neglected now, will become more and more important: firstly, humanity took the first step out of the cradle; and secondly, humanity took the step back. The importance of these two events cannot be overestimated. The year 1961, the year of Gagarin’s flight into space, could replace the year of Jesus’ birth as the first year of humanity; and the decline of space exploration after the Apollo moon landings would leave a trauma for humanity even worse than the expulsion from the Garden of Eden.\nThe late 1950s and early 1970s will be remembered as the Golden Age. Just over three years after the launch of the first artificial satellite, the first astronaut went into space, and just over seven years after that, a man landed on the moon. At the time, people were inspired by the ambitious goal that in another decade or so, humans would be on Mars, and that reaching Jupiter’s orbit to land on Io would not be far away. Even before that, the grandiose “Project Orion” was born, a spacecraft powered by exploding atomic bombs that could send dozens of astronauts to the outer planets at a time.\nBut soon after, the Apollo moon landings were canceled due to funding interruptions, and the rest of the flight program was canceled. Since then, human space exploration has been like a rock thrown up in the Earth’s gravity field, reaching its peak for a short time and then plummeting. The last moon landing of Apollo 17 in December 1972 is an important turning point. After that, there are still space stations and space shuttles, all kinds of artificial satellites and the economic benefits they bring, the exploration probes to other planets, but the nature of the human space business has quietly changed. The human space exploration turned its eyes from the stars to the ground. Spaceflight before Apollo 17 was man’s effort to get out of the cradle, and after that it was about getting more comfortable in the cradle. Space became a business with an economic logic. It must make a profit, and the pioneering spirit has been replaced by the shrewdness of businessmen, and the wings of the human heart have been broken.\nIn fact, looking back, did humanity ever really want to get out of the cradle? The driving force behind the space exploration boom in the mid-20th century was the Cold War, the fear of rivals and the desire to surpass them, and a kind of political advertisement of power. Humanity never actually genuinely thought of space as a future home.\nNow, the moon has been turned back into an inhospitable world with no human presence, the manned planetary flight programs of Russia and the United States have fizzled out, and Europe’s Aurora program to explore the solar system has been put on hold, with no light in sight. After the retirement of the space shuttles, the Americans, who had once set foot on the Moon, even lost the ability to put people into near-Earth orbit for a long time.\nWhy? Nothing but technical and economic reasons.\nLet’s look at the technical reasons first. It is undeniable that humanity does not currently possess the technology to carry out large-scale space development within the solar system. In the most basic and critical propulsion technology in space navigation, humanity is only at the stage of chemical propulsion, and large-scale interplanetary navigation requires nuclear propulsion, and the current technology is still far from this. Nuclear-powered rockets and spaceships are still just something in sci-fi.\nThen let’s look at the economic reasons. With current technology, sending a payload into near-Earth orbit costs the same amount of money as the same weight of gold; sending it to the moon and other planets costs ten or even a hundred times as much. Before the industrialization of space development, all these inputs could only get very little return, for example, the Apollo moon landing project cost $26 billion, equivalent to more than $100 billion nowadays, and only got a bit more than two tons of moon rocks. (Of course, the technological achievements of the moon landing project produced huge benefits in the subsequent civilization process, but these benefits could not be quantified, and it was definitely not the decisive factor in the decision-making process for the Apollo project.)\nTo summarize, space development is a huge risk, both technically and economically, and it is politically unacceptable to regard space as the new home of humanity and to stake the future of humanity on such a big risk.\nThe above reasons are solidly argued, seemingly irrefutable, and dictate the current human space policy and its resulting decline in the space business.\nBut let us examine the one grand enterprise in which humanity is currently fully engaged and which it sees as the only way out for the future survival of civilization on Earth: environmental protection.\nFrom the technical point of view, space navigation and environmental protection have different colors in people’s minds; the former is strenuous, high-speed and adventurous, implying cutting-edge and high technology; the latter is a kind of gentle green public welfare, and though there is technology in it, the difficulty is far lower from the former in the impression.\nBut that’s just a perception. In fact, the technology required to achieve humanity’s current environmental protection goals is much harder than large-scale interplanetary voyages.\nOn a cognitive level, to protect the environment you first have to recognize it, to understand its laws on a global scale. The Earth’s ecosystem is extremely complex, and although various disciplines have made huge amounts of research and understanding of its details, at the global scale, humanity currently does not have a grasp of its laws, either at the level of basic science or applied science. There is very little that human science can know about the operation of weather systems, the changes and interrelationships of large-scale biomes, and so on.\nTaking global warming as an example. Is the Earth’s climate really warming? If so, is the warming related to human activity? Unlike the overwhelming and unanimous propaganda, the scientific community is still inconclusive on these two crucial questions, so curbing global warming is more like a political movement. It is no exaggeration to say that humanity does not know as much about the surface of the Earth as it does about the surface of the Moon, and probably will soon know less about it than about the surface of Mars.\nAt the operational level, the technologies currently required for environmental protection, such as the replacement of fossil energy sources with renewable ones, the treatment and recycling of industrial and municipal waste, the preservation of biodiversity, and the protection and restoration of forest cover, involve complex technologies, a good part of which are not much easier than the technologies for interplanetary voyages within the solar system.\nBut the technological challenge of environmental protection is not the main one. Now that global wars and upheavals are far behind us and human society has entered a period of sustained peaceful development, especially in the Third World and underdeveloped regions, which are developing at an unprecedented rate, these fast-growing regions have the same goal: to reach the economic level of the developed Western countries and to live in the same modernized and comfortable way. This does not seem to be an unattainable goal. At the current rate of development, it will take only half a century for most of the underdeveloped regions, including third world countries like China and Brazil, to catch up with the West economically.\nHowever, people have overlooked the fact that if all human beings were to live like the developed countries of Europe and America, it would take four and a half earths to consume enough resources.\nUnder such circumstances, if we want to achieve the ultimate goal of environmental protection, to maintain the earth’s ecology from collapse, and to control the extinction of species that is now occurring at a rate faster than the Cretaceous extinction, it is far from enough to rely only on self-discipline to reduce pollution, improve energy conservation, and emission reduction. Even if all the goals of the Copenhagen Conference are realized, the ecology of the Earth will still sink like the Titanic.\nThe only hope is to de-growth. But growth is unstoppable, and it is contrary to basic human values and politically unworkable for some countries and regions to allow the rest of the planet to remain in the backwardness and poverty of agrarian societies while they relax in the comfortable recliners of modern civilization.\nExamine another possibility: dramatic environmental change brought about by non-human factors. The Earth’s environment has always been in fluctuation, but the history of human civilization has been too short for people to notice. With each fluctuation, the Earth’s environment as a whole changes drastically, and may become completely unsuitable for human survival. For example, the Last Glacial Period didn’t end until 10,000 years ago, and if another ice age were to happen, the continents would be covered in ice and snow, existing global agriculture would collapse, and it would be a catastrophe for modernized societies with huge populations, and such a drastic change in the environment is almost inevitable in the long run, and there is a very high probability that it may happen in the not-too-distant future. Existing environmental protection measures are only a drop in the bucket for such environmental changes.\nIf human civilization is to survive man-made or natural environmental changes in the long term, it can only change its environmental protection behavior from passive to active, and artificially adjust and change the earth’s environment in a holistic manner. For example, people have put forward a variety of programs to mitigate the greenhouse effect, including the establishment of a large number of giant solar evaporation stations in the ocean, the sea water evaporation sprayed into the high altitude in order to increase the amount of clouds; in the Lagrange point between the sun and the earth, the construction of a piece of the earth’s umbrella with an area of 3 million square kilometers, etc. These projects are all unprecedented super-projects, the magnitude of its large-scale, like a handwriting of God, involve sci-fi level technology. Its difficulty is far greater than interplanetary navigation within the solar system.\nIn addition to the technical difficulties, when we look at environmental protection from an economic point of view, we find that it is very similar to space development: both require huge upfront investments, and there is no obvious economic return at first.\nHowever, human investment in environmental protection is disproportionately large compared to investment in space development. For example, the 12th Chinese Five-Year Plan has invested more than 3 trillion yuan in environmental protection, but only about 30 billion yuan in space exploration. The situation in the rest of the world is similar.\nThe solar system has a huge amount of resources, in the eight planets, in the asteroid belt, from water to metal to nuclear fusion fuel, human survival and development of all the resources needed, according to the Earth can ultimately support a population of 100 billion people calculated that the total resources in the entire solar system can support 100,000 Earth.\nNow we are witnessing the fact that Humanity has given up the 100,000 Earths in space and intends to survive on just 1, and the means of their survival is environmental protection, an endeavor as arduous and adventurous as space exploration.\nLike environmental protection, space development interacts with technological progress, and space development promotes technological progress. Before the Apollo project, the United States did not have the technology needed to land on the moon, and a significant portion of the technology was developed in the course of the project. Nuclear fission technology has become a reality on Earth, and there are no insurmountable obstacles to the realization of nuclear propulsion in space. Although controlled nuclear fusion has not yet been realized, there are only technical obstacles rather than theoretical obstacles.\nNavigation and control computers on the Apollo 11 rocket more than 40 years ago had only 1/100 of the capacity of the present iPhone 4.\nSpace exploration is similar to the bygone Age of Sail. It is also a voyage to an unknown world to open up space for human existence and create a better life. The beginning of the Age of Sail was the discovery of the New World by Christopher Columbus, whose voyage was supported by Queen Isabella I of Spain (or more precisely, the Queen of the Kingdom of Castile, back when Spain did not exist), and she had difficulty in paying for the fleet, and is said to have pawned all her jewelry and then supplied it to Columbus for the voyage. This has now proved to be a most judicious venture, so much so that it is said that the history of the world can be said to have begun in the year 1500, for it was not until that time that the whole world was known as an entirety.\nNow humanity is on the eve of the second great age of navigation. We are now much luckier than Columbus. Columbus couldn’t see the new continent he was looking for, and didn’t see any land after sailing on the Atlantic Ocean for several days, and at that time he must have been full of hesitation and uncertainty. The new world we want to explore can be seen when we look up, but no one has footed the bill yet.\nPerhaps human civilization as a whole, like individual human babies, can really never get out of the cradle without parental help.\nBut from a cosmic point of view, Earth’s civilization is parentless, and humanity is an orphan of the universe, so we really have to be on our best behavior."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#book-recommendations-2012",
    "href": "docs/posts/liu-cixin-anthology/index.html#book-recommendations-2012",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Book recommendations (2012)",
    "text": "Book recommendations (2012)\nSource: 【北京青年报】刘慈欣：科幻阶梯阅读荐书榜（菜鸟-明星-骨灰）\nLadder Reading Book Recommendation List: This list introduces a reading theme every month, and recommends books around that theme, launching a basic book list from “Rookie” to “Star” or “Diehard”.\nThis month’s theme: sci-fi.\n\nRookie\nRelative to other genres, the number of original and translated sci-fi novels is not very large. I feel a bit reluctant to divide into three levels, and can only make a rough distinction by style.\nGenerally speaking, early sci-fi novels are more suitable for rookies, because the scientific and technological content involved in sci-fi at that time is now people’s common knowledge, and the threshold of reading in terms of knowledge is low. At the same time, compared with modern sci-fi literature influenced by modern and postmodern styles, early sci-fi focuses more on telling good stories, and the writing is clearer and more fluent.\n\nVerne’s sci-fi novels, including Twenty Thousand Leagues Under the Sea, From the Earth to the Moon, and Propeller Island.\n\nVerne’s sci-fi works have not yet been completely separated from the style of European adventure novels back then, but the most charming factors of sci-fi literature have already appeared, science and technology as an important role for the first time in the novels, and the relationship between humans and nature instead of the relationship among humans has become the main object of depiction. Although most of the content in the novel is no longer sci-fi but reality, it does not reduce the charm of the work in any way.\nIt should be noted that the style of Verne’s work is so unique that it has hardly been reproduced in later sci-fi literature.\n\nWells’ sci-fi novels, including The Time Machine, The War of the Worlds, The Invisible Man, etc.\n\nWells’ works are much more sociologically complex than Verne’s, and also subvert Verne’s techno-optimism by showing anxieties about the future. In contrast to Verne, Wells had a profound influence on the development of world sci-fi literature, with his depictions of aliens and time travel, etc, and his depictions of biological sciences, etc, providing the initial paradigm for subsequent sci-fi.\n\n\nStar\nThis level implies a clear understanding of the overall contours of sci-fi literature. Sci-fi literature brings together works of different styles, and there is a big difference between different styles of sci-fi. Professional readers should have a rough idea of the types of works they like, and the following is a list of representative works of three different styles of sci-fi:\n\nThe Fountains of Paradise (Arthur C. Clarke): The quintessential representative of techno-sci-fi, with imagination and storytelling centered on technological ideas. This genre is known as Campbellian sci-fi, and still represents the most popularly recognized style of sci-fi literature.\nThe Martian Chronicles (Bradbury): High-literary sci-fi, but not as obscure and edgy as the later New Wave sci-fi, this book makes you realize how poetic and beautiful sci-fi can be.\n1984 (Orwell): Sci-fi with a sociological core. A panoramic description of a possible future society.\n\n\n\nDiehard\nTo be honest, I’m not sure what this level means, so I had to list some of the more out-there, avant-garde and difficult-to-read works. Sci-fi is popular literature, so there are not many novels written in this kind of excessively formalistic style. If you feel disconnected while reading them, it is very much because you are experiencing a cultural gap.\n\nRed Mars, Blue Mars, Green Mars (Stanley Robinson): One of the hardest sci-fi out there, like a memoir written by an engineer who experienced a mega-project first hand, describing humanity’s great journey to open up a new world.\nThe Man in the High Castle (Philip K. Dick): A classic of alternative history. The story is subtly detailed and seemingly static, with the background unfolding unnoticed as the story progresses.\nDoomsday Book (Connie Willis): A classic of time travel and winner of the Nebula and Hugo awards, the story advances slowly in an extremely trivial narrative, but is full of heart-stopping tension. The book depicts Europe in 1348, which was ravaged by the Black Death, and is the closest thing to an apocalyptic scenario in the history of humanity.\nStranger in a Strange Land (Heinlein): A sci-fi novel that has made countless young people crazy, but only young people in the West. To feel the charm of this book you need to cross the cultural gap, but also need to endure the author’s endless preaching.\nGravity’s Rainbow (Thomas Pynchon): The mainstream literary world has the modus operandi of the robber barons: when it sees sci-fi novels that excels in literature, it loots them out of sci-fi literature and declares that they actually belong in the mainstream, so over time, sci-fi settles into second- and third-rate literary status. Let us use the same trick, and loot sci-fi out of the storehouse of mainstream literature. The book is regarded as the pinnacle of modern literature, and its content is actually quite sci-fi, with a fantastically complex plot full of technological elements such as physics, rocket engineering, advanced mathematics, and psychology, though the messy, obscure dreaming of a modernist text. With more 800 pages, it is an ordeal of reading. Other mainstream literary classics that we can loot include Lord of the Flies and A Clockwork Orange.\nThe Red Ocean (Han Song22): A magnificent and eerie epic that looks down on humanity’s past, present and future from an unprecedented dimension, the reading experience is beyond imagination.\n\n22 Famous (by sci-fi author standards) in China, but obscure out of China. The best English-resource I know of is this: Jia Liyuan – Gloomy China: China’s Image in Han Song’s Science Fiction. His style is generally hyper-violent, absurd, body-horror, posthuman, with a cyclic cosmology – terrible things happen, and will continue to return, because it is fate. For example, The Red Ocean tells about a future history over a few thousand years, where human survivors of a nuclear war use genetic engineering to create aquatic humans, and at the same time build undersea cities, transforming the blue ocean into a red ocean."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#billion-years-of-struggles-what-is-it-all-for-2013",
    "href": "docs/posts/liu-cixin-anthology/index.html#billion-years-of-struggles-what-is-it-all-for-2013",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "3 billion years of struggles – What is it all for? (2013)",
    "text": "3 billion years of struggles – What is it all for? (2013)\nSource: 走了三十亿年，我们干嘛来了？, preface to Zhao Yang’s book Cosmos, Future Tense (2013-07).\nA rigorous study of those success-ology23 books shows that most of the methods and “secrets of success” are not generically useful, because there is a sampling bias: only successful cases are presented, not the failure cases. However, there are more reliable ways to achieve success, and research has shown that a person who develops a vision or ambition from an early age and works consistently toward that vision over the years is a more reliable way to achieve success. The now popular 10,000 hour rule is another way of saying this.\n23 成功学, a genre of books much like How to Win Friends and Influence People, focused on teaching one to achieve material success, typically through business. Extremely popular in the 2010s.As a writer of sci-fi, I tend to see all of humanity as a whole, and in the subconscious of sci-fi literature, Humanity is one person. So of course we want to know what this “person” is going to do to succeed in the universe, and it stands to reason that having a vision and consistently striving toward that vision is the key to human success (or at least survival). So let’s examine the human ideal, first to see what that ideal is, and then to gauge how ambitious it is.\nWhat is the common ideal of humanity? A complex question, but one that can be answered with some certainty. Imagine a comprehensive questionnaire survey of more than six billion people around the world on this issue, like “How would you feel successful?” and the mainstream answer can be roughly determined by the statistics: to build a society that is rich in material wealth, with a democratic system, and where everyone in the world can live a happy life on both the material and the spiritual levels. There should be no problem in making such an ideal one that is recognized by all races and nations of all humanity.\nNote that there is an implicit premise here: whether we talk about “society” or “the world,” we are actually referring to the earth.\nScholars, politicians, and civilians alike, subconsciously seldom think beyond the orbit of the moon when they speak of “society” and “the world”. The common ideal of all humanity, then, can be reduced to a single phrase: good Earthbound life.\nIn order to examine the ambitiousness of this ideal, let’s set up a reference. In the backward age of agriculture, the ideal of an honest, law-abiding farmer living in a remote and closed village was to cultivate his ancestral acre of land well, to keep his bed warm in winter, to marry a wife, and to have three children.\nThis is a low enough reference point. Considering the popular joke “sheep-herding – marrying – having children – sheep-herding”24, it is obvious that modern people are not interested in this ideal, but full of contempt and pity for it.\n24 Popular joke in China: “Why do you herd sheep?” “To save money and marry a wife in the future.” “What would you do if you have a wife?” “To have a baby.” “What you plan to have your child do in the future?” “To herd sheep.”Now let us make a comparison. In order to be quantitative, we propose:\n\\[\\text{ambition index} = \\text{radius the person is content to reach} : \\text{radius the person can possibly reach}\\]\nLet’s take a look at the farmer’s ambition index. According to his humble ideal, he is content to reach the distance from the village where he lives to his own field, which varies greatly from one region to another. An average of 2 kilometers is reasonable. Then look at the radius of the world he can reach through his efforts, that is reasonable in his cultural environment. He should be able to know from the village teacher or an outsider that he lives on a big sphere called the Earth, take half of the equator circumference of 20,000 kilometers. That is, he can reach the radius of the world through his efforts.25 His ambition index is then \\(2/20000 = 0.0001\\).\n25 It is probably too difficult for the shepherd to leave China, which requires a visa. However, it is simple and cheap for a Chinese person to travel across China on trains. The east and west ends of China are about 4000 km apart, giving an ambition index of \\(0.0005\\).If we look at Humanity, the radius of the region in which he lives is exactly the radius of the world that the farmer knows and can reach through his efforts, i.e., 20,000 kilometers half of the circumference of the Earth’s equator; and the radius of the universe that he knows, at present, is about 15 billion light-years. But to reach the other stars outside the solar system is far beyond his ability, and even now it seems likely that he will never be able to reach them, so we do not take into account the distances beyond the solar system, or the Oort Cloud, which is about 1 light-year away from the Sun, and which would take more than 20,000 years at the fastest speed that modern spacecraft is capable of. It’s reasonable to consider as the reachable radius as that of the Kuiper Belt in the periphery of the solar system, 20 billion kilometers, which should be reasonable, because Voyager 1 has already flown 15 billion kilometers. The ambition index for Humanity is then\n\\[\\text{radius of earth} : \\text{radius of Kuiper Belt} = 0.000001\\]\nNow we see that the ambition index of humanity is only 1% of that of the farmer, in other words, his lowly and humble ambition which makes modern people despise and pity him is actually 100 times greater than the ambition of Humanity!\nIf put into the past, this all-too-human ideal of “a wife, three children, in a warm bed” would still be reasonable. In the past history of humanity, which was full of hunger, disease, turmoil and war, for a considerable number of years, only a little more than half of the newborn babies survived, and only a little more than half of those who did survived lived to be more than 15 years old. In most stages of the history of civilization, poverty and hunger accompanied most people, and even I did not have enough to eat until satiety until I was in junior high school. Note that I had a good family situation compared with other children. So in the old days, it should have been a great ideal for all humanity to merely seek for food and comfort.\nBut now it’s different. Most people have enough food and clothing, and they are beginning to think of lust and the other finer things of life. About one-tenth of the earth’s population is still hungry and lack of minimum health care, but modern civilization is solving poverty with unprecedented speed, and the day when poverty is eliminated is not far away. On the other hand, with the progress of society, the era of barbaric struggle for supremacy has come to an end, and universal values are more and more commonly recognized, so the possibility of a world war is rapidly decreasing. In a nutshell: humanity has settled down for the first time since coming out of Africa 200,000 years ago.\nThe first organic molecules that could replicate themselves were born in the turbid oceans under lightning strikes on ancient Earth, and then after more than three billion years of long and tortuous evolution, the first intelligent civilization finally appeared on Earth. Looking back at this unimaginably long road behind us, we naturally sigh with great emotion, and now it’s already time to ask ourselves: after more than three billion years of walking, what the hell are we doing here?\nIn other words, humanity should make a greater common ambition. What this is can be revealed in different ways, scientifically and religiously, but there is one biggest and most obvious revelation: the visible universe is 15 billion light-years wide, and there is 20 billion kilometers of reachable space [within the Kuiper Belt] already, whereas we are now living within a range of only 20,000 kilometers. The earth is a grain of vibrant dust, in a vast empty expanse, as if there is a skyscraper in which only one tiny cupboard in a basement is inhabited. This great revelation has been hanging above us, a silent beckoning that has been deafening and has haunted humanity throughout its history. This revelation, like the one given by the oceans three billion years ago to the first organic molecule that could reproduce itself, has made the mission of human civilization crystal clear.\nOf course, we know very well that this kind of big and empty preaching will not impress Humanity, who has become an otaku shut in a nutshell earth. He cares more about how to live a richer and more comfortable life, not how to become more successful in the cosmos. If we look at this mission from another angle, we will find that it is vital to his survival.\nThe Earth’s ecosphere is an unstable and dynamic closed system, and like living organisms, it has a limited lifespan, as evidenced by the Biosphere 2 project in the Arizona desert. We do not perceive the aging of the Earth’s ecosphere because it is too large compared to Biosphere 2, which occupies less than a kilometer of land, and whose life span is measured in geologic epochs, which, although long, will always come to an end. The earth’s ecology is like a whirlpool or an isolated wave in a long river of time, and the dynamic balance can be upset at any time, and drastic changes or even collapse can occur at any time.\nTo take one example: a 12-meter rise or a 12-meter fall in global sea level would be a great catastrophe for the fragile modern human society. In the former case, as we all know, coastal cities and economic zones will be submerged, and the large number of people migrating inland will lead to unimaginable social upheaval and economic collapse; whereas a drop in sea level is even more frightening because the only reason for such a drop is the increase in land glaciers due to the global cooling of the climate, and in such a drastic climatic change, the global agricultural system will completely collapse, and two thirds of the existing population may die of starvation before reaching a new level of stability. Two thirds of the current population could die of starvation before a new stability is reached.\nBut little thought has been given to the fact that in just 20,000 years, sea level has fluctuated not by 12 meters, but by 120 meters, and that during the Last Ice Age 20,000 years ago, global sea level was 120 meters lower than it is today! So, even without taking a very long-term view, the earth is only a temporary place to live, and the future of humanity lies in space.\nTo sum up, we should re-conceptualize the significance of space exploration.\nThe propaganda department has a knack for puffing up the significance of an otherwise commonplace event like a balloon, but when it comes to spaceflight, this knack is gone. When it comes to the significance of aerospace achievements, it is only to enhance national strength and promote economic development, specifically, to be able to forecast the weather more accurately, to have their own GPS, to have more communication bandwidth, to make tomatoes grow bigger or smaller by breeding in space, etc. In the minds of the propaganda department, the true picture of space exploration is not established.\nHumanity, as a community wandering in the long river of time, is a long procession of more than six billion people. It can be divided into three categories: most are in the middle of the procession of people, do not think much, and go with the tide. Some stand vigilant on of the edge, guarding against the threat from the two sides of the vast wilderness. A very small number of people, who are in the vanguard, always facing the front of the territory that no one has ever set foot on. They cut through the thorns and thistles to open up a wider living space for the community behind them. This is undoubtedly the greatest cause of all, and spaceflight is such a cause.\nIn the famous Soviet space movie, Taming of the Fire (1972), a general complains to a national leader about the space scientists who, he says, “think only of exploring the universe, not of the national interest.” Now, the nation he was referring to has disappeared into the mists of history, along with its interests, and is irrelevant to all but historians, but their achievement in spaceflight have not disappeared with the nation. If humanity continues ten thousand years from now, there probably won’t be any children foolish enough to ask, “Grandpa, which was the first socialist union?” But surely some child will ask, “Grandpa, who was the first to send a man out of the Earth?”\nIn fact, the friends I know who are engaged in the space industry do not realize the destiny of their career, and they talk about the same sweet and sour things of mundane life. For the astronauts and every citizen in the country engaged in the space industry, not realizing the nature of space will result in the loss of a great spiritual treasure.\nCosmos, Future Tense is precisely a book that reacquaints us with the space industry. The content of this book is rich, from the space city to planetary exploration, from the space shuttle to the space power station, from space food to astronaut psychology, covering almost all the fields of the space industry, with rich and informative technical details and a huge amount of information, told in vivid language, describing for us a grand picture of the human space industry.\nBut the biggest highlight of this book is also the “future tense”.\nRecently, it has been suggested that the advancement and popularization of IT technology has created an illusion of progress, but there have not been many breakthroughs in other technological fields other than IT since the 1960s. This situation is especially obvious in the field of spaceflight. In the decades after the moon landing, spaceflight technology has not made any major breakthroughs in most of the basic technologies, including propulsion systems, and has only been tinkered with and improved upon. Cosmos, Future Tense recognizes this, and describes not only what spaceflight is now, but more of what it should be and what it could be, which puts the book in a much higher perspective than other popular science works on spaceflight. These predictions of the possibilities of space technology are not sci-fi, but are based on a solid scientific and technological foundation. They even gave convincing scientific explanations to the dreams of astronautics in my poorly written The Three-Body Problem trilogy series.\nCosmos, Future Tense tells us that the development of the entire solar system as a home for human beings and the expansion of living space to 100,000 Earth-equivalents is not a vain dream, but a grand wish that can be realized both theoretically and technologically. If we expect Humanity’s ambition to go beyond the shepherd’s, this book provides scientific arguments for this ideal and boosts the confidence in opening up new worlds in the future, which is indeed the most exciting part of Cosmos, Future Tense."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#a-precious-experience-of-apocalypse-preface-to-wang-jinkangs-escape-from-the-mother-universe-2013",
    "href": "docs/posts/liu-cixin-anthology/index.html#a-precious-experience-of-apocalypse-preface-to-wang-jinkangs-escape-from-the-mother-universe-2013",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "A precious experience of apocalypse: Preface to Wang Jinkang’s Escape from the Mother Universe (2013)",
    "text": "A precious experience of apocalypse: Preface to Wang Jinkang’s Escape from the Mother Universe (2013)\nSource: 珍贵的末日体验——刘慈欣评《逃出母宇宙》.\nThe disasters facing mankind are many and varied, and in 2012, Science et Vie, a leading European science communication magazine, once launched a feature: Twenty Versions of the End of the World. If classified according to the scale of disasters, they can be roughly divided into three categories: local, civilization, and apocalyptic. Local catastrophe refers to a catastrophe faced by a local area and some members of human society; civilization catastrophe refers to a catastrophe that involves the human world as a whole, a catastrophe that may cause human civilization to regress or even disappear altogether, but human beings, as a species, can always survive in sufficient numbers to begin to restore or rebuild their civilization anew. An apocalyptic catastrophe is the pinnacle of catastrophe, in which no one can survive, and human beings, as a a species will disappear completely.\nTo date, the vast majority of disasters encountered by human society have been localized, including natural disasters, such as earthquakes and large-scale infectious diseases, and man-made disasters, such as wars and terrorist attacks. These disasters, though tragic, have had a very limited impact, with their geographical scope generally not exceeding one tenth of the total land area of the Earth, and the affected population generally not exceeding 300 million.\nLooking back at history, human civilization has hardly experienced any civilization catastrophe since its birth. The Great Flood recorded in the Bible was only a local catastrophe according to today’s vision, and there are two that are closer to civilization catastrophes that have been recorded with certainty in history: the Black Death in Europe in 1438 and the Second World War in the last century. But these two are not really civilization level. The Black Death killed a third of the European population at the time, but did not affect the rest of the world. As one sci-fi novel, The Years of Rice and Salt, describes, civilization would have grown in other parts of the world even if the entire population of Europe at the time had died of the Black Death. The Second World War was almost global in scope, with an unprecedented number of battlefields and casualties, but because it took place before the nuclear age, the state of the art limited its destructive capacity, and the combined TNT equivalent of the explosives consumed in World War II was five million tons, one-tenth the size of the largest nuclear bombs that appeared in the immediate post-war period. Regardless of which side won the war, human civilization would have continued.The only real civilization catastrophe that has almost occurred so far was the nuclear standoff between NATO and the Warsaw Pact in the last century, an all-out nuclear war that would have been destructive enough to destroy the civilized world if it had broken out, a terrible specter that is now so far away. But that it did not come to pass, which restores some of our almost lost faith in human sanity.\nAs for apocalyptic catastrophe, it has never happened and there are no obvious signs or possibilities at present. It is now essentially certain that none of the possible catastrophes that could occur on earth would be of an apocalyptic nature. The catastrophes we can think of that could occur on a planetary scale, such as environmental degradation, new ice ages, natural or man-made mass epidemics, etc, could only lead to a massive reduction in population or a regression of civilization, and it is unlikely that humanity would be completely wiped out at the species level. Surviving humans will gradually adapt to the post-catastrophe world with the help of knowledge and technology left over from before the catastrophe, allowing civilization to continue.\nThe apocalyptic catastrophes can only come from space.\nThe universe is filled with unimaginably enormous forces, some of which we see but can hardly comprehend, and some of which we are not even aware of yet, forces that can bring stars into being and destroy any world in an instant. Our planet is just a tiny speck of dust in the universe, so small as to be negligible on the cosmic scale.\nIf the Earth were to disappear in a second, the effect on the solar system, that is, some adjustment of the orbits of the remaining seven planets as a result of the loss of Earth’s gravity, would occur mainly in the case of the small-mass Earth-like planets Mercury, Venus, and Mars, while the orbital changes in the large Jovian planets would be negligible. If this disaster, which seems to us shocking to the world, occurred, it would be viewed from the solar system’s neighbor, the Alpha Centauri, as seeing from tens of thousands of kilometers away, a mosquito falling into a candle. Even on Jupiter, with the naked eye is difficult to see the solar system there is any obvious change, except that in the direction of the sun, a faint bright spot disappeared.\nDisasters from space are more difficult to predict than those on Earth. With mankind’s current level of technology, it is difficult to make predictions for space disasters such as sudden solar catastrophes and close supernova outbursts. Another type of space disaster is impossible to predict even from the nature of the physical law. If there is some kind of catastrophe in space moving toward the Earth at the speed of light, since there is no signal in the universe that can exceed the speed of light, there is no way that information about the catastrophe can reach the Earth in time for the catastrophe. In other words, we are outside the light cone of the catastrophe, and it is by no means possible for us to predict it.\nApocalyptic catastrophe has been fully expressed in sci-fi literature. Just as love is the eternal theme of mainstream literature, disaster is also the eternal theme of sci-fi. Escape from the Mother Universe is a work that expresses the apocalyptic disaster from space.\nThe conception of “Escape from the Mother Universe” is very grand, and the source of the apocalyptic catastrophe is the whole universe. The sci-fi setting of this book has something very unique about it compared to other works with similar themes. In most of the doomsday sci-fi novels, the end of the world like a wall towering in front of mankind, all clearly seen. However, the description of the Escape from the Mother Universe is more conforming to the laws of human cognition. The novel shows the gradual cognitive process of mankind for the disaster from multiple perspectives, the truth is unveiled step by step, the twists and turns, peaks and turns, in the great despair of the dawn of hope, and then ushered in a greater despair, finally to a tragic ending. The novel takes the reader from the peak of hope to the valley of darkness, experiencing the apocalyptic experience that only sci-fi literature can bring. Meanwhile, different from the space disaster often expressed in traditional sci-fi novels, the cosmic disaster in Escape from the Mother Universe is a brand new type of disaster, involving the most cutting-edge knowledge of physics and cosmology, showing the overall picture of the evolution of the universe and the deepest mysteries of space-time, and this kind of imagery is ultimate, with incomparable broad horizons and philosophical heights.\nWang Jinkang once said: young sci-fi authors look at the future from the past, middle-aged sci-fi authors like the author look at the future from reality, and he himself looks at the future from history. These words accurately characterize Wang Jinkang’s sci-fi novels, including Escape from the Mother Universe.\nIt is because of this far-reaching perspective of looking at the future from history that Escape from the Mother Universe has a heavy and profound connotation. The author looks at the imaginary end of mankind with deep rationality, and describes a picture of human society in an apocalyptic disaster. The novel is one entry in his series Stay Alive, so in the author’s world setting, the survival and continuity of human beings is the overriding goal. To achieve this goal, the apocalyptic society created values and moral systems that are compatible with the apocalypse, with behaviors and institutions such as human oviposition, polygyny, and ultra-authoritarianism. These anathemas of traditional society become justified in the world setting of Escape from the Mother Universe.\nNot long ago, Robert Sawyer, a Canadian sci-fi writer, came to visit China, and when he talked about the darkness and harshness of our sci-fi novels in depicting apocalyptic themes, he thought that this was related to what happened to our nation and country in history, and as a Canadian, he was optimistic about the future of mankind in the universe. I fully agree with him that the imprint of history inevitably appears in the imagination of the future. But looking at the place of Earth’s civilization in the universe, humanity as a whole is less like modern Canada and more like the indigenous Canadians of 500 years ago, before the arrival of European settlers. At that time, hundreds of tribes, comprised of different ethnic groups and representing at least ten language groups, lived together in Canada from Newfoundland to Vancouver Island. Imagine if an indigenous sci-fi writer had envisioned their future with the same optimism – in retrospect it would clearly be too optimistic. The recently published book by Georges Erasmus and Joe Saunders, Canadian History: An Aboriginal Perspective, which has attracted widespread attention, contains a poignant account of this.\nBecause he looks at the future from the perspective of history, Wang’s work has a distinctive Chinese cultural coloring, which is vivid and heavy even in the imagined future and the imagined doomsday. Although Escape from the Mother Universe boldly subverts the traditional value system, its deep thought is Chinese, and the way of thinking and behavior of the main characters also has distinctive Chinese cultural imprints, and the recurring image of the worrying Qi Ren is a vivid symbol of this. This work leaves us with a profound proposition: does the ancient Eastern culture and value system, including the Chinese civilization, have a greater advantage in the future apocalyptic disaster?\nOf course, Escape from the Mother Universe is only one possibility, and the beauty of sci-fi lies in presenting different futures and different options to people, and we certainly look forward to the emergence of another type of more optimistic sci-fi describing the end of the world, and presenting a completely different picture of the end of the world, for example, in which the core values of the human tradition can be preserved.\nBack to the topic of space disasters. Human society has not been prepared for these unpredictable catastrophes from space, either theoretically or in reality. Most of the research on doomsday has remained in religion and has not risen to a scientific level. Most thinkers focus on the reality of human society, and even if they think about the future, they are limited to a linear extrapolation of reality, and seldom consider such sudden changes as the apocalyptic catastrophe. Therefore, from the classic writings of thinkers in the Age of Enlightenment to the theories of today’s diverse schools of thought, there is little research on the politics, economy, law, ethics and culture of human society about the apocalyptic catastrophe. At the practical level, almost none of the country’s constitutions and laws deal with apocalyptic catastrophes, which is obviously a major deficiency in the human constitutional system. I once discussed this issue with a scholar who believed that the existing legal system already has a relatively well-developed structure for catastrophes. In fact, this scholar did not notice the difference between local catastrophes and civilized and apocalyptic catastrophes. The biggest difference is that in the event of a local catastrophes, there is an external rescue force, which is generally very powerful, and often the whole society concentrates on rescuing the disaster-stricken areas and people that only occupy a small part of the country. However, in the case of civilized and apocalyptic catastrophes, the human world as a whole is in the midst of a catastrophe at the same time, and external rescue forces do not exist at all.\nAt this point, the existing legal and moral systems will not be applicable.\nFor apocalyptic catastrophes, the central legal and ethical question is: what to do if even concentrating all of society’s resources will still only allow a minority of people to survive? So far, modern legal and ethical systems have been vague about this question. It is undeniably difficult to discuss this question in the context of existing societal values, with heated debates and multiple options: one can choose to allow some or a few to survive, or one can choose to adhere to the traditional values of humankind and allow all to face death with equanimity. The rights and wrongs of these choices can be debated in an informed manner, but whichever choice is made, in the end it must be legally and ethically clear – this is the responsibility that a civilized world owes to itself, else the world will be plunged into a state of fear and uncertainty when the catastrophe comes, and in the final pandemonium, mankind will lose both its dignity and its future.\nIn this sense, the shocking apocalyptic experience brought by Escape from the Mother Universe highlights the unique value of sci-fi literature.\nWritten on 2013-11-20 in Yangquan."
  },
  {
    "objectID": "docs/posts/liu-cixin-anthology/index.html#interview-by-the-bund-2014",
    "href": "docs/posts/liu-cixin-anthology/index.html#interview-by-the-bund-2014",
    "title": "Commentaries and essays by Liu Cixin",
    "section": "Interview by The Bund (2014)",
    "text": "Interview by The Bund (2014)\nSource: 外滩画报, 独家专访科幻作家刘慈欣 - 真有宇宙创造者的话，也在科学范畴之内 (The Bund, exclusive interview with sci-fi writer Liu Cixin – “If there is a Creator of the universe, then it will also be within the scope of science”).\nSince the publication of his first full-length series of novels, the Three-Body Problem trilogy, Liu Cixin has become a “god-like being” to many of those who love his work. This is a set of works that is considered as a “milestone of Chinese sci-fi literature”, and it has become a big hit among Chinese sci-fi fans. Discussions of the work began to enter the realm of the academy, and as his publisher, Yao Haiyan, editor-in-chief of China’s most influential sci-fi magazine, Science Fiction World, put it, “The Three-Body Problem embodies cutting-edge imagery, and lets readers see how far the Chinese can go in the world of imagination.”\nLiu Cixin married at the age of 31 to a wife who works for the same company as he does and is also an engineer. They have a daughter who is now in middle school. They live not in a city as we know it in a general sense, but in a relatively closed company-town, not uncommon in China, built around a large state-associated enterprise, with a full range of functions, including schools, hospitals, shopping centers, and places to relax. That place is located near Yangquan, a mining city in northern China. With a history of nearly a thousand years, Yangquan is known for its rich minerals, especially coal, and is not geographically accessible, being more than an hour’s railroad ride away from Taiyuan, the capital of Shanxi Province. Liu Cixin’s workplace is more than an hour’s drive from Yangquan, in a scenic area called “Niangziguan”. In his mouth this “very remote” location, he also has a middle school alumni called Robin Li, later famous for founding “Baidu”.\nLiu Cixin said, as a power plant engineers, the most important duty is to stand ready at the post, so that problems can be resolved in a timely manner. Negligence has serious consequences. However, during a year, the real busy time of this work is only three or four months, and the rest of the time, “you just have to stay in the office, read, write, and do not interfere with the day-to-day work.” It was in this kind of slow-time-stressful-time schedule that Liu Cixin completed most of his works, including the The Three-Body Problem trilogy, which won him great success, honor, and wealth.\nLiu is still an engineer, leading a seemingly formal and stereotypical life, and his lifestyle hasn’t changed much. He says he still reads at least four hours a day, in English during the day and in Chinese at night. He uses the Internet to read English books during the day, because English books are not easy to buy there, and because the Internet makes it easier for him to look up vocabulary. He also insists on running 8–10 kilometers of exercise every day, because he believes that he will be able to go to space tourism in his lifetime, but “the price of space tourism is too expensive now, it costs 20 million dollars for a trip”, and he has to rely on exercise to keep his body in good health, so as to wait for the day when the cost comes down to a level that he can afford.\nDuring the days when he was saving up for space travel, he cooked, did housework, sent the kids to school, and took care of the family, like most husbands in Chinese families. He also smoked and drank, but not in a socializing kind of way. “I don’t smoke much, only one carton in three or four days, and I drink because I get anxious when I think about writing and I can’t sleep without drinking.”\nHis wife knows that he writes sci-fi, but only through family members’ briefings, and does not communicate with him about the content of his work. Other family members know very little about his writing. He had already written [the first draft of] a novel called Supernova Era in 1989, but his father was unaware of his son’s creative talents even at his death in 1992. His mother was still unaware that her son was a major figure in China’s sci-fi literature.\nNot only because of his remote location in the city, Liu Cixin was also, by personality, uncomfortable with socializing with his peers, and seldom interacted with the closed circles of the sci-fi community. “Only very few people there. Can’t represent the face of the wide world.”. Of course, the key reason is still “no time, no energy”. He doesn’t have Weibo or WeChat, and finds these “fragmented messages too time-consuming”. The most interaction he has with the outside world is based on communication about the publication of his work.\n“I don’t have a car, and I really don’t need one at home; if I go out, the distance is more than 500 kilometers, so a car won’t come in handy; I also need to get a driver’s license, so I really don’t have the time.” Now Liu Cixin feels that time is getting more and more insufficient, and the habit of watching a movie regularly every day has stopped; the computer games that he used to be very obsessed with are no longer played. Perhaps he has become a kind of life that even Liu Cixin himself would be uninterested in hearing more about.\nThe Liu Cixin that would be interesting to himself may start from his childhood when he opened Verne’s Journey to the Center of the Earth.\nBorn in 1963, he grew up in a seemingly free but also dangerous environment, just like all the children of his time. The self-proclaimed “earliest batch of sci-fi fans” in China started to work hard when he was in middle school, and kept submitting articles, which were also rejected, and sometimes not rejected at all, and sank into the sea, but he was still happy to work on it.\nThe political environment of the 1980s, surprisingly, also affected sci-fi literature, which was very niche at the time.26 “There was a time when sci-fi was almost forgotten from my life”, and he found a new interest – computer games. He has a very reasonable explanation for his “keeping up with the times”: “I’m not like some people who write for their own eyes, but I definitely write for other people’s eyes, and if I can’t publish it, why do I write it? Sci-fi is my hobby, nothing more. It is not a particularly difficult thing, there are other contents of life. However, these words are not to be taken at face value. In 1989, he tried to move closer to mainstream literature by writing Supernova Era, naturally hoping to be published.\n26 First period: 1950 – 1966, interrupted by the Cultural Revolution (1966–1976). Second period: 1976 – 1983, interrupted by the Anti-Spiritual Pollution Campaign (1983). Third period: 1990s – now.This is Liu Cixin, or perhaps Liu Cixin-plural, who, in the name of rationality, is willing to be tamed by reality. The spacetime of fantasy cannot override reality, and the universe is their unreachable shore and the final frontier.\nLiu Cixin was not born for sci-fi, but when he discovered the path of sci-fi, he never stopped transforming his own creative DNA. In this sense, Liu Cixin may not know “Liu Cixin”. One day, Liu Cixin may be able to unravel the mystery of “Liu Cixin”, which should also be a mystery.\nWe are familiar with the image of engineers in the following ways: their appearance and clothes are clean, neat, and even genteelly stylish, revealing that they have been well-educated, but it is also obvious that they do not spend more time and money on these aspects. They are very polite when they talk to people, and pay attention to logical organization, but do not give up their rationality just because of the differences in the subjects of the conversation. They usually have some technical specialties and are popular, but they don’t deliberately cater to others just to be popular.\nThis is the case with Liu Cixin, who, like many northern Chinese men, is brisk, talkative, and funny, and who expresses himself in a single breath without seeming to look at the listener too much. Only once in our conversation did he notice my reaction, and then ask, gravely, “Do you think this is funny?”\nRationality is the most important logic of life for such people, and so is Liu Cixin.\nHowever, “writing sci-fi is like living in two parallel worlds, the world of sci-fi on the one hand and the world of reality on the other, and these two worlds do not cross at all.” So Liu Cixin is often contradictory, at least in his expression.\nOne minute he would think that writing sci-fi is a way to escape from the dreary life, but the next minute he would think that he is “like a fish in water” in real life, so why should he use writing to escape? On the one hand, he recognizes that he has an optimistic and enterprising nature, and that he may even take an aggressive approach to certain issues, but almost everyone around him – colleagues, friends, publishers – would describe him as “mild-mannered” in one word.\nBut for a man who has always projected his heart into two often dichotomous worlds, such an error should be allowed.\nKeys to the interview: B = The Bund; L = Liu Cixin.\nB: The Three-Body Problem put you at your creative peak, so to speak? So, can you create another peak?\nL: I don’t know. Only when the work is published and I come face to face with the readers can I know the public’s reaction. I myself think there’s still hope to create another peak, but that can only be a hope.\nB: You are a “god-like being” to many sci-fi fans, where do your “god-like” inspirations come from?\nL: You might not imagine it, but it’s actually extremely difficult to get inspired. Though Three Body contains one idea after another, I don’t have that kind of uninvited inspiration. Inspiration is a very heavy journey for me, and I don’t know when it will come.\nB: What kind of state do you mean by “extremely difficult”?\nL: If I can think of something, I think of it, if I can’t think of something, I can’t think of it. From the publication of the third installment of the series until now, I couldn’t come up with a single idea that satisfied me, or that excited me. I spend a lot of time thinking about this every day, including when I came back from Chengdu just now, and I was also thinking about it when I was on the airplane, and I didn’t have any ideas that could excite me…\nB: So, creating is sometimes painful for you.\nL: Not sometimes, but most of the time. The most appealing thing about sci-fi writing is that one day you get really inspired, and the happiness is incomparable, but those times are very rare. Before The Three-Body Problem came out, I did feel inspired to produce a story that was mind-blowing and unlike anything I’d ever seen before, and that was the most exciting time, and after that it was all just brute labor. After I wrote it, the book sold well, the reviews were great, and I was happy, but that joy wasn’t on the same level as the joy of harvesting inspiration. It’s hard to describe the feeling. But that’s the thing about inspiration, maybe I’ll wake up tomorrow and have it, maybe I won’t have it until I die, it’s totally possible.\nB: So do you get anxious?\nL: I do, when it happens I can’t sleep.\nB: Do you care what people think of you?\nL: Yes, of course.\nB: Let’s not talk about the praises, but what about the criticisms, how do you feel about them?\nL: If the criticism is right, I will learn from it; if not, I will ignore it. But I have this principle about other people’s comments: I’ve gone from being a sci-fi fan to a sci-fi writer, and sci-fi has a center principle in my heart. Just like some people would ask me what kind of principle should be followed in sci-fi creation, I said I should follow the principle of “copper coin”, not to say that I have fallen into addiction to money, but that the copper coin is round outside and square inside, and the meaning of “round outside” is that our methods of expression should be diversified and flexible, suitable for readers with different appreciations; and the “square inside” is that we should have a core concept of sci-fi. The inner square means that we should have a core idea of sci-fi, which is a bottom line that cannot be broken, and it is also the basis for the existence of a literary genre.\nB: What is the core principle of sci-fi as you understand it?\nL: On the basis of science, imagination is developed. Science is the soul of sci-fi. It’s not a philosophy that everyone recognizes, and many works don’t follow it, but my own writing and my favorite works follow it.\nB: So do you think that sci-fi writing should at least follow the path of scientific development?\nL: Sci-fi literature has evolved and is very colorful. Orson Scott Card, the author of Ender’s Game, said that nowadays all kinds of literature, including sci-fi, have been quarantined by critics, who put you in the appropriate cage for the genre you belong to, and a whole bunch of people have been locked up in sci-fi’s cage as well. Once you’re in there, the critics don’t care: They say you count as sci-fi and that’s that. So there’s quite a diversity among sci-fi literature these days, all kinds of it.\nB: Some people have commented on your sci-fi writing, saying that “Liu Cixin has single-handedly brought Chinese sci-fi to the heights of the world”, what do you think about that? Do you think the distance between you and other sci-fi writers who write in Chinese is that big?\nL: Depends on which way you look at it. In terms of influence, there is definitely a distance, and everyone can see that. When The Three-Body Problem came out, we all expected it to lead to the development of sci-fi, but it’s been three years since the last The Three-Body Problem was published, and there hasn’t been a single full-length book with more influence than it, so the gap – I don’t have to be modest – is indeed vast.\nBut when it comes to the quality of the work, the gap isn’t that big. Personally, I feel that many writers nowadays have reached considerable heights in their writing. As for why the influence of these works is not as great as that of The Three-Body Problem, there are many reasons, such as what the readers happen to appreciate, the publisher’s publicity, and more importantly, luck. Just as there are many online words that become inexplicably popular all of a sudden, the same is true of literary works, which may become populary with a mysterious luck, and naturally there is a positive feedback effect there, and they become influential. This is something I am still very conscious of. As for the world-class level, the overall level of Chinese sci-fi works is actually relatively low, and there is a big gap from the maturity of American sci-fi literature. No golden phoenix can fly from a chicken nest, and at most, more excellent chickens will fly out.\nOn the whole, I regard this kind of comment as a kind of encouragement, but it can’t be taken seriously.\nB: And how do you see the gap between us and the West in terms of science and technology, or specifically in terms of creativity?\nL: We can only compare ourselves to the advances we have reasonably been able to achieve, not to compare ourselves to the technological giants as they stand now. The three industrial revolutions, they grabbed it in their hands, but we didn’t grab even one of those. Those public intellectuals [Gongzhi] have to compare us to others in everything, and then degrade ourselves, and that’s very disgusting and a very stupid way of thinking. It’s like every one of us, first we have to overcome ourselves, then we can overcome others. Others have others’ conditions, we have our conditions. As long as we’re making progress. Our foundation is here, we have heavy burdens, we have just left the agricultural society and entered the industrial society, from the sci-fi point of view, China’s future is amazing, full of uncertainty, full of hope and challenges. But in mainstream Western civilization, that amazement is largely gone.\nB: So would you argue that religion and science go hand in hand? In writing, would you need to draw on the power of faith?\nL: All religions are essentially atheistic. People always say that science will eventually lead to religion, but in fact it is the opposite, all religions will lead to atheism. For example, there is a Creator of the whole universe, there is a Creator, in the sci-fi portrayal of how he creates the universe, it’s just that he’s in a laboratory, starting the Big Bang. Just an engineer, or a scientist, on a bigger scale. I mean, if there really is a Creator of the universe above us, then from the Creator’s point of view, God doesn’t exist, because the world was originally created by the uncreated Creator. By analogy, that’s how I see religion, so I can’t take it as faith, and even if God did exist, I wouldn’t believe in it with devotion and awe because it’s likely that he had been a lousy engineer. There is no way to falsify the creation envisioned in religion right now, and there are plenty of indications that many of the parameters of the universe were precisely modulated, otherwise there would be no way for life to emerge. If there is a creator of the universe, he is within the realm of science and would not run outside of it. He created the laws of the universe based on that level of his being, just as we can create bacteria.\nB: In that case, how do you explain the religious feeling in sci-fi?\nL: According to the mainstream view in our country, religion and science are incompatible. Religion and science have opposing sides, but religion and science have some common origin. Consider the sense of reverence for God in the Western Christian culture, which gave birth to modern science. By transferring this sense of reverence to the universe, people became eager to understand God’s behavior and will, and this gave them the spiritual power to explore the secrets of the universe. There is such a complex relationship between religion and science. Because sci-fi as a form of science in literature, sci-fi and religion are similarly related. In fact, religious feelings often appear in sci-fi. Some question religion, and others directly express religious feelings. For example, A.C. Clarke’s most famous work is called “The Star”, it is very short, a few thousand words, and that’s a masterpiece of religious feelings in sci-fi. In addition, the movie “2001 A Space Odyssey” is not so much a sci-fi movie as a religious movie, and it shows the religious feelings so well that it is almost biblical in sci-fi movies, so there is no contradiction between sci-fi and religion.I’m not religious, but that doesn’t mean I don’t have religious feelings. I spoke of my awe for the lightyears, and it is the same as a Christian’s awe for God, the only difference is that I don’t pray to that thing, since I know it’s not conscious. I don’t think the universe is conscious, but have the same kind of awe of this immense existence is a religious feeling.\nB: But haven’t you ever had that kind of compassionate pity – there’s always a time when one is feeling powerless in the course of science?\nL: I’ve had moments of powerlessness, and I can list the reasons for my compassion one by one, and I know the way for me to get out of it, whether it can be done or not is not certain. But this has nothing to do with God. I don’t need to resort to anything other than reason. For example, if I say that my father died at 65, esophageal cancer, which by then had spread throughout my body, this is a time of compassionate pity for me, there is no way out of it, but I will accept it. I would fall into depression, but I would also be clear that no one could save me.\nB: Does the fact that you’re so determinedly rational give you a naive streak that’s always present in your novels?\nL: Rationality is not naivety, finding spiritual solace is.\nB: Do you have any memories of your upbringing that are particularly memorable?\nL: The social order in the mines was not too good at that time. All boys were involved in fights. I also built powder guns and knives, the kind that could hurt people. The life of a child in those days was more dangerous than it is now, and memories like danger are not easily forgotten. I’m rarely nostalgic, I don’t feel it, it’s like telling someone else’s story. Sometimes I ask myself, did I really go to high school and college, and what did I feel then? The only time I think about it is when I walked my daughter into her classroom in that middle school, and it’s a strange feeling, did I really go through all of that like she did? In terms of literature, I’m a very slow person.\nThere is nothing different about me from other people. The path I took was the most common one that Chinese people have taken, elementary school, middle school, university, and then to work in factories and mines. Factory and mining enterprises are the most grassroots enterprises in China, which can present the living condition of the most people. Don’t focus your eyes on the big cities like Beijing, Shanghai and Guangzhou, there are a lot of people in these places, but they only account for a small portion of China’s population, the real majority of Chinese people live like me. Nowadays, it seems that all Chinese people are white-collar workers, getting up early in the morning and working from 9 to 5 every day is so tiring. No, most of the Chinese people are still living a very ordinary life in these second and third-tier cities. I can’t tell you how fast or slow the pace is, it’s just not as modernized as up north.\nB: Don’t you find this ordinary life boring?\nL: Life is definitely dull, but sci-fi is not. Just like you just asked me how I insisted on writing sci-fi for more than ten years, in fact, you should have asked the opposite question. You don’t need ask how I persisted on writing sci-fi, as not writing it would be the real difficulty. It is a way to escape from the dullness of life. Everyone has his own way to escape, some people are watching football, some people are fishing, like the most common where we play mahjong day and night. Besides, I’m not interested in human emotions and tenderness, I’m only interested in the kind of reactions people have in extreme states, where everyone’s out of control, but it has to end sometime.There’s one thing about sci-fi, and it’s the biggest difference from mainstream literature: it rarely involves author’s insertion. Someone asked me which of the novels I’ve written are based on myself. Not a single one, and then someone asked, “Which one of these heroines you’ve written is your favorite type? Again, not one. They’re all tools, storytelling tools, and I rarely put my personal emotions into my stories. Because genre literature, not just sci-fi, needs a story that resonates with the reader, not self-expression or self-catharsis. It’s not the same as mainstream literature. As popular literature, sci-fi, you have to win people’s resonance, and that doesn’t work if you put yourself too deeply into it. Everyone is different, you have to consider everyone’s feelings, you can’t just express your own.\nB: How do you feel about death, a proposition that sci-fi can’t leave untouched?\nL: I’m a 100% atheist. There’s nothing left after death, so I especially cherish being alive. I never expect anything to come out of death. Let me tell you something. There is a Chinese sci-fi writer named Tong Enzheng, the author of Death Ray on a Coral Island, who got cancer in the United States, and when he was dying, he told someone that what he regretted the most in his life was that he had no religious beliefs, and that death was a darkness to him. People then told him, “Well, wouldn’t it be better if you believed now,” and he said it was impossible. That’s it, you can’t believe just because you want to. The difference between him and me is that I don’t have any regrets. I don’t have any expectation of the world after death, I only hope to live as long as possible.\nB: Some critics say that the characters in your novels seem to be genderless, what do you think?\nL: That’s what I call a lack of literary ability and refinement. Of course I want to write women with good femininity. I dislike manly woman. I also dislike genderless people. Men should be like men, and women should be like women, but the novel has become what it is. Can’t be helped.\nB: What do you mean, “can’t be helped”? You can’t control your characters?\nL: Yes, I can’t manage them, I’m not capable enough, I admit it frankly. Speaking as literary-criticism, I’m not a professional writer. There is also an objective reason – note that I’m not excusing myself – that is, a sci-fi novel has a difficult task of “describing the background”. In mainstream literature, you can write the word “farm”, and the reader’s brain immediately lights up with an image. Sci-fi novels have to set their world-stage, and you have to introduce the world clearly. Often this occupies half of the novel. The other half is spent on characterizing the characters. To write men like men, and women like women, requires a lot of details, but a “long novel” does not mean you can write as long as you want – the publishers have length restrictions. For Death’s End, I had to delete 40,000 words describing the love story of the male and female protagonists when they were in university. With the details gone, of course the men are unlike men, and the women are unlike women. Of course, the main reason is my incompetence.\nB: Will you consciously break through this bottleneck in your next creation?\nL: Creatively, we should maximize our strengths and not avoid our flaws, since flaws can’t be avoided, and flaws carried to the extreme are strengths. You say I don’t understand people? I live at the grassroots level and I’ve met all kinds of people. Although the power plant is relatively remote, I spend more than three or four months a year in the metropolis, and often go abroad, but still I just can’t write about people skillfully.\nB: Would you try to write a work that isn’t sci-fi at all?\nL: Unlikely. My strength is in sci-fi. I’m not very good at anything else, and I’ve written fairy tales, which I didn’t publish and gifted to a friend. It wasn’t really an attempt, just because I owed my friend a favor. You know, it’s not easy to come up with a sci-fi idea, and I would never write a sci-fi story just as a personal gift, but a fairy tale doesn’t matter. It doesn’t waste any ideas, and that’s the main reason why though my friend was trying to get me to write him a sci-fi story, but I honestly couldn’t bring myself to do it. Other than that, I haven’t written anything in any other genre."
  },
  {
    "objectID": "sketches/posts/fictional-ideas/index.html",
    "href": "sketches/posts/fictional-ideas/index.html",
    "title": "Fictional ideas",
    "section": "",
    "text": "In mechanical engineering, impedance matching insulates a building from seismic waves. The seismic insulation on LIGO was done to an extreme degree. Perhaps galactic civilization have built vast dark matter as insulation for some instrument that really needs gravitational quietness.\nNew religion: The black hole god. Because black hole singularities are where the Einstein field equations become indeterminate, it opens up a gap for God. Thus, the new black hole theology arose. The idea is that on the other side of black hole is a worm hole to the antechamber of God. At the end of time, the universe would become just one giant black hole – and that is Judgment Day. The religion collapsed from discovery of Hawking radiation.\n\n\nThe RAM of God: New religious organization that opposes von Neumann probes, because they worry it will destroy God’s computer RAM."
  },
  {
    "objectID": "sketches/posts/fictional-ideas/index.html#cosmology",
    "href": "sketches/posts/fictional-ideas/index.html#cosmology",
    "title": "Fictional ideas",
    "section": "",
    "text": "In mechanical engineering, impedance matching insulates a building from seismic waves. The seismic insulation on LIGO was done to an extreme degree. Perhaps galactic civilization have built vast dark matter as insulation for some instrument that really needs gravitational quietness.\nNew religion: The black hole god. Because black hole singularities are where the Einstein field equations become indeterminate, it opens up a gap for God. Thus, the new black hole theology arose. The idea is that on the other side of black hole is a worm hole to the antechamber of God. At the end of time, the universe would become just one giant black hole – and that is Judgment Day. The religion collapsed from discovery of Hawking radiation.\n\n\nThe RAM of God: New religious organization that opposes von Neumann probes, because they worry it will destroy God’s computer RAM."
  },
  {
    "objectID": "sketches/posts/fictional-ideas/index.html#new-brains",
    "href": "sketches/posts/fictional-ideas/index.html#new-brains",
    "title": "Fictional ideas",
    "section": "New brains",
    "text": "New brains\n\nSemantic apocalypse\n\nPhilosophy is written in this grand book — I mean the Universe — which stands continually open to our gaze, but it cannot be understood unless one first learns to comprehend the language and interpret the characters in which it is written. It is written in the language of mathematics, and its characters are triangles, circles, and other geometrical figures, without which it is humanly impossible to understand a single word of it; without these, one is wandering around in a dark labyrinth.\n\n\nif commonsense intentional psychology really were to collapse, that would be, beyond comparison, the greatest intellectual catastrophe in the history of our species; if we’re that wrong about the mind, then that’s the wrongest we’ve ever been about anything. The collapse of the supernatural, for example, didn’t compare; theism never came close to being as intimately involved in our thought and practice – especially our practice – as belief/desire explanation is.\n(Fodor 1987, vii)\n\nPerhaps our commonsense talks about the self, beliefs, wishes, feelings, hopes, are also hacks with words to do something that is actually not factual at all, and when some more efficient interface comes along, these talks would be replaced as another organ that has lost its grip, like all those Ediacaran fossils. As history goes on, the grip of folk psychology keeps slipping from the universe.\n\nDespite the florid diversity of answers to the Question of Meaning, they tend to display a remarkable degree of structural convergence… even apparently radical departures from traditional thought, such as Buddhism. No matter how diverse the answers seem to be, they all remain anchored in the facts of our shared neurophysiology. So what happens when we inevitably leave that shared neurophysiology behind? … Because we define madness according what our brains normally do, once we begin personalizing our brains, ‘normally do’ will become less and less meaningful, ‘insanity’ will simply be what one tribe calls another, and from our antiquated perspective, it will all look like insanity.\n(Bakker 2011)\n\n\nBeyond wisdom\nFor example, “Don’t be so sensitive!”, when taken out of context, is perfectly meaningless and useless. When put into the opposite context, it is quite offensive. In certain contexts, it’s very useful.\nWhat does it mean, then? It means that “Don’t be so sensitive” is not a logical sentence, that is, its interpretation is not a boolean value. Instead, it is a feedback-control signal, a delta-value. Written formally, it would be like REQUEST -1 emotion. Dopamine is not the “happiness signal”. Instead, it is also a delta-value. It means “reward is greater than predicted reward”, with the dopamine signal frequency proportional to the delta-value.\nThis theory of wisdom solves two puzzles:\n\nmany wisdom sayings are not even wrong, because they are not logical statements. Their interpretations are not boolean values, but delta-values.\nwhen I use wisdom sayings in all the wrong places, they still seem to work. This is because I wouldn’t be “distorting the truth”, since there is no truth there in the first place.\n\nWisdom-sayings are a hack that humans use to communicate delta-values to each other. Humans have very precise instruments for conveying delta-values inside each one’s brain, but across brains, they have nothing but these tricky words.\nThen, perhaps soon we would get something that is more efficient and more precise than wisdom sayings. Wisdom is revealed to be not actually talking about facts at all. They only have the appearance of factual statements. There are many paths to get there, in the spirit of Technology Forecasting: The Garden of Forking Paths.\n\n\nTinkerheads: the neoliberal pathway\nA world 20 years into the future. No AI revolution, but there is some brain-computer interfacing. Some of the CEOs started tinkering with their brains to increase productivity, and with their input-output channels. During a transition period, perhaps lasting ten years or so, the top executives are those that install the latest personality tweaks to their brainwares so that they are the most productive and focused. For them, even dating and altruism will be only a tool to further the expectation of the company’s long-term \\(\\log(\\text{growth rate})\\).\nWhen these tinkerheads interface to each other, they speak in a mumbled wave of numbers. To people not tinkered in the head, they shrug and laugh at those CEO, “These superrich and their ridiculous dreams of becoming superhuman… don’t they realize that they are nothing more than human? They talk in numbers, but only as a fake facade, a useless ‘upgrade’ to real honest talk. They symbolize and metabolize, but they babble nonsense. What symbol corresponds to ‘responsibility’, which corresponds to ‘wish’, which corresponds to ‘believe’?”\nA few argued back, saying “Deaf people talk to each other with a whole language of gestures. These gestures are not merely miming, or hand-spelling. Similarly, tinkerheads talk with a whole language that is not a mere miming, or number-spelling, of wisdoms. We may even say that they have found a more effective wisdom. Like Galileo discovering that the book of nature is written in mathematics, not Hebrew, they discovered that the book of man is written in mathematics, not stories.”\nThe superrich parties continued, but now there are two kinds. The unproductive ones, or the “real parties”, attended by those “old money” who do not deign to earn money by their own means. The productive ones, attended by the tinkerheads. The friends and families of tinkerheads either get tinkered or fall away. Yachts and castles and mansions would be bought, perhaps, but only to interface with the old money.\nSome wondered why the tinkerheads still physically go to parties. Why can’t they meet in cyberspace? Many hypotheses, few answers. Some suspect experimental brain-brain connection. It can’t be for work-life balance, because work is life and life is work. They will be the first money machines. After them, The Age of Em.\n\nMoloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!\nMoloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in the long streets like endless Jehovahs! Moloch whose factories dream and croak in the fog! Moloch whose smoke-stacks and antennae crown the cities!\nMoloch whose love is endless oil and stone! Moloch whose soul is electricity and banks!\nMoloch whose fate is a cloud of sexless hydrogen!\nMoloch whose name is the Mind!\n\n\n\n\nHive minds\n\nThe Decussation (2241 - 2276)\nI’m tired of those sci-fi stories where individuals are “liberated” from a tyrannical hive mind. What if we take the split-brain cases seriously? What if people are already born-hiveminds? How would we liberate the lobes from the tyranny of the whole-brain?\nThe mid-23rd century saw The Decussation. While “Macrobains”, mid-scale hive minds formed through high-bandwidth connections between individuals, represented the cutting edge of societal evolution, they remained a small segment of the global population. Most humans persisted as discrete individuals.\nThis equilibrium was destroyed by the “Lobe Emancipation Device” (LED) invented in 2239. It allowed the safe and painless separation of individual brain lobes, allowing each with independent cognitive function and self-awareness. Proponents of this technology, self-dubbed “Liberators”, called the LED a tool liberating individuals from the “tyranny of the One Brain”. Established power structures classified the Liberators as a new terrorist structure, the LED as a dangerous weapon, a threat to the stability of both the individual and society as a whole. The ensuing conflict was The Decussation. It saw the deployment of advanced neuro-weapons and sophisticated propaganda campaigns by both sides. Liberators, often outnumbered and outgunned, resorted to asymmetrical psych-ops and neur-ops.\nThe Decussation stalemated. The world fractured into a patchwork of isolated Macrobain collectives, independent “lobers,” and traditional whole-brain humans, and new emerging threatening/posthuman ways of being."
  },
  {
    "objectID": "sketches/posts/fictional-ideas/index.html#dumb-ideas",
    "href": "sketches/posts/fictional-ideas/index.html#dumb-ideas",
    "title": "Fictional ideas",
    "section": "Dumb ideas",
    "text": "Dumb ideas\n\nPuns\nMathematicians discover subliminal primes. Congress bans pizzas with 7 slices on account of its alleged manipulative effect.\nBRIEF COMMUNIQUE Python has undergone a dramatic performance increase after being renamed “Pythia”. Programmers, citing “nominative determinism” and the oracle’s prophetic wisdom, are now enjoying 20% faster execution. The comp.sci newsgroup is aflame with speculations and experiments. Experts speculate that renaming Java to “Jehovah” might make it immune to problems with the God object.\nSubject: Re: Python Renamed to \"Pythia,\" Performance Up 20% (Was: \"Jehovah\" Java?)\nRenaming Java to “Jehovah” to fix the God object problem? Not only did it work, it’s gone and broken everything else.\nMy friend “Bob” (pseudonym to protect the faithful), is a hardcore “Jehovah” convert. He’s running a system now, and the God object is so efficient, so elegant, so damn perfect that anything not using it basically grinds to a halt. I’m talking about a 95% drop in performance.\nI’m talking about entire systems that suddenly have to be redesigned around this new “Jehovah” paradigm, or risk becoming unusable.\nThere’s a lot of talk about “Jehovah” being literally jealous and preventing system shutdowns. I’m not sure if that’s true, but I’m keeping my air-gaps handy. There are talks of renaming C++ to “Cthulhu” as holy wars become literally so.\n– Mark “The Skeptic” Smith\nOracle Corporation Issues Statement Regarding Recent Software Anomalies\nREDWOOD SHORES, CA - Oracle Corporation today acknowledged widespread performance issues experienced by users of Jehovah following the recent renaming. While the company has previously celebrated the improved efficiency of God objects within “Jehovah”, further investigation has revealed a critical issue stemming from an unexpected interaction with the principle of “nominative determinism”.\n“It seems our initial enthusiasm for the new name may have overlooked a rather unfortunate philosophical side effect,” stated Dr. Emily Carter, Oracle’s Chief Scientist. “The term ‘Jehovah,’ while certainly evocative, also carries a surprising connection to the 17th century philosopher Nicolas Malebranche.”\nMalebranche, known for his theory of “occasionalism” – the idea that God is the sole cause of all events – unfortunately has a name that means “evil branch” in French.\n“While we can’t definitively explain the mechanism,” Dr. Carter continued, “it appears that this unforeseen connection has created a sort of ‘bad branch’ condition within the software environment. Objects operating without interfacing the ‘Jehovah’ God object are effectively denied access to the proper computational pathways by repeatedly failing branch prediction, leading to severe performance degradation.”\nOracle is currently working with leading philosophers and computer scientists to understand and mitigate the “Malebranche Effect.” In the meantime, the company urges all users of Jehovah to run the Kode.sh script for backwards compatibility, and to start using the God object in all future programs.\n“We apologize for any inconvenience this may cause,” Dr. Carter concluded, “and remain committed to delivering reliable and robust software solutions.”\nOracle also announced the formation of a new department dedicated to “Delphic code review” to prevent future mishaps with unintended metaphysical consequences."
  },
  {
    "objectID": "sketches/posts/fictional-ideas/index.html#cyberpunk",
    "href": "sketches/posts/fictional-ideas/index.html#cyberpunk",
    "title": "Fictional ideas",
    "section": "Cyberpunk",
    "text": "Cyberpunk\n\nSybil attack on environmental protection\nRapid speciation and deployment around industrial sites.\nTo deal with certain 21th-century environmental protection laws, companies have added to their R&D departments bioengineering to rapidly create new species hyperadapted to specific ecosystems. For example, a chip plant can resist challenges by pointing out that 1000 insect species are only found in its location and depend on its continued operation, and thus turn itself into a natural sanctuary and enjoy the protection of those laws.\nPaleoenvironmentalists (there are still those around) decry this “regulatory capture of nature”, but hackers find the nature-artificial boundary not useful anyway. They know that computers are alive and a chip plant is a natural wonder. And in any case, these new engineered species create more attack surfaces.\n\n\nNew human potential movement\nThe human potential movement became protechnology and antinature on the theory that humans are the most human when they are living farthest from their evolutionary ancestral environment. In the sacred hallways hang pictures of Inuits, astronauts, Bedouins, and nuclear submarine officers.\nThey are funding research for increased light pollution and digestive enzyme for industrial waste such as polyethylene for further human potentiality. Natural lighting conditions denounced as attempting to curtail the world of possible human habitats to a mere ancestral worship. If humans will eventually live in the center of suns, then they need more light pollution not less.\nThey engineered many new species specifically adapted to light pollution, converting them into natural sanctuaries and made light pollution protected by law, thus regulation-captured environmental protection bills that were meant to cage the human potential growth."
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html",
    "title": "Notes for a talk on the history of neural networks",
    "section": "",
    "text": "This essay is written as a companion piece of a lecture on the history of neural networks. It is not exactly an essay, but more of an extended scrapbook of quotations. The lecture slides are here.\n\nYuxi on the Wired - Cybernetic Artificial Intelligence\nYuxi on the Wired - Reading Perceptrons\nYuxi on the Wired - The Backstory of Backpropagation\nYuxi on the Wired - The Perceptron Controversy"
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html#what-this-essay-is-about",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html#what-this-essay-is-about",
    "title": "Notes for a talk on the history of neural networks",
    "section": "",
    "text": "This essay is written as a companion piece of a lecture on the history of neural networks. It is not exactly an essay, but more of an extended scrapbook of quotations. The lecture slides are here.\n\nYuxi on the Wired - Cybernetic Artificial Intelligence\nYuxi on the Wired - Reading Perceptrons\nYuxi on the Wired - The Backstory of Backpropagation\nYuxi on the Wired - The Perceptron Controversy"
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html#prehistory",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html#prehistory",
    "title": "Notes for a talk on the history of neural networks",
    "section": "Prehistory",
    "text": "Prehistory\n\nThe thunderbolt steers all things. [τὰ δὲ πάντα οἰακίζει κεραυνός]\n— Heraclitus\n\n\nSantiago Ramón y Cajal\nCajal’s neural network diagrams are always feedforward. Cajal never depicted feedback except once, in a diagram of a connection in the cerebellum. In fact, according to de No, who was a student of Cajal, Cajal was vehemently against feedbacks. More on this later.\n\n\n\n(Ramón y Cajal 1909, II:149, figure 103)\n\n\n\n\n\nColored version highlighting the recurrent loop.\n\n\nThe original caption:\n\nVoie courte des cellules de Golgi.\nFig. 103. — Schéma destiné à montrer la marche du courant apporté par les fibres moussues et la part que prennent à ce courant les cellules de Golgi.\nA, fibres moussues ; — B, cylindre-axe de Purkinje ; — n, grains ; — b, fibres parallèles; — c, cellule de Golgi ; — d, cellule de Purkinje vue de champ.\n\nTranslation:\n\nShort pathway of the Golgi cells.\nFig. 103. — Diagram intended to show the course of the current brought by the mossy fibers and the part played by the Golgi cells in this current.\nA, mossy fibers; — B, Purkinje axis-cylinder; — n, grains; — b, parallel fibers; — c, Golgi cell; — d, Purkinje cell seen from the field.\n\n\n\nRafael Lorente de Nó\nThe vestibulo-ocular reflex (VOR) is a reflex that acts to stabilize gaze during head movement.\nLorente de No discovered recurrent circuits in the brain, mainly in the VOR, but also in the cortex (Larriva-Sahd 2014).\n\n\n\nA recurrent neural network that explains nystagmus. (de Nó 1938, fig. 13) Original caption: Diagram explaining the production of rhythm during vestibular nystagmus. Fiber f is sup- posed to carry the continuous series of impulses started at the cristae of the semicircular canals which set up the nystagmus. Fibers ft are supposed to be maintaining the tonus of the antagonistic muscle; la, Ib, Za, Zb, 3a, 3b are branches of the axons of cells 1, 2 and 3,4a, 4b, 4d,, 4e are branches of the axon of cell 4; fa, fb, fc are branches of fiber f. II. Diagram of the rhythmic succession of con- tractions and relaxations of the antagonistic muscles during the nystagmus explained by diagram I. Rising of the line indica tes contraction . The interval between turning points a and b is never less than 3 to 4 msec.; – the interval between 50 msec. long.\n\n\n\n\n\nSimplified diagram of the interneurons in the vestibulo-ocular reflex, showing both residual feedforward connections and recurrent connections. (de Nó 1938, fig. 2) Original caption: Diagram of the pathways connecting the internuncial cells among themselves and with the ocular motoneurons. V, vestibular nerve; 1 to 6, cells in the primary vestibular nuclei; 7, 8, 9, cells in the reticular formation in the medullar (Med.) and pons (P.); 10, 11, 12, cells in the reticular nuclei in the midbrain (M.b.); Oc.n., oculomotor nuclei; f.l.p., fasciculus longitudinalis posterior and similar pathways; i, internuncial pathways; Fl, F2 and Col., position of the stimulating electrodes. The diagrams below indicate the two types of chains formed by internuncial cells; IV, multiple and C, closed chain.\n\n\n\n\n\nOculomotor of the rabbit. (de Nó 1938, fig. 3) Original caption: i1, i2, i3, i4, intemuncial paths. Passage of a synapse means a delay of about 0.6 msec. Note that each fiber has several synaptic knobs on the neuron, an arrangement increasing the possibility of spatial summation.\n\n\nLorente de No actually discovered those during the mid-1920s, but Cajal told him that he shouldn’t publish those, because other neuroscientists would think it is crazy, damaging de No’s career. So he published in 1934, immediately after Cajal died.\n\nIt is easier to sweep this complexity under the rug, which I do now by resorting to anecdote. Three years ago in a workshop at Irvine I presented some data on the properties of mitral cells in the olfactory bulb, from which I inferred that they formed a mutually excitatory neural population (i.e., one having positive feedback). I cited this his as confirmation of Ramón y Cajal’s (1909) hypothesis of “avalanche conduction.” in which a weak olfactory stimulus might undergo “amplification” (as we would say now). Rafael Lorente de Nó. in the audience. stated that I was in error, Cajal had in mind feed-forward recruitment of mitral cells and disavowed the notion of feedback. Lorente later recalled (personal communication) that in the mid-1920s he prepared a manuscript on the cytoarchitecture of the cerebral cortex, In which he concluded that feedback relations were a prominent feature. After reading the manuscript. Cajal strongly urged Lorente not to publish it because it would be unacceptable to the scientific community and might blight his career. Out of respect for his mentor. Lorente elected not to publish the material while Cajal lived, when he did publish (Lorente de Nó. 1934), the work established itself as one of the enduring classics in neuroanatomy.\n(Freeman 1984)\n\nLorente de No was at the founding of cybernetics, and went to the Macy conferences a lot. He consistently pointed out that recurrent networks exist in the brain and is possibly responsible for transient memory. He had influenced many of the early cyberneticians, including Hebb, McCulloch, and Pitts. (Espinosa-Sanchez, Gomez-Marin, and de Castro 2023)\n\n\nDonald Hebb\nHebbian learning of synapses, also “reverberation”. In short, if A fired shortly before B fired, then all A-to-B synapses would increase in strength. It is not just “neurons that fire together wire together”, since the neuron before the synapse must fire just before the neuron after the synapse. In modern-day language, it is spike-timing-dependent plasticity.\nIn Hebb’s theory, the brain is a large neuron network, with long parallel fibers (like long-range undersea cables) connecting small clusters of neurons (“cell assemblies”). The long parallel fibers are hardwired, but the small clusters are formed by Hebbian learning. The cell assemblies are recurrent, allowing long-time reverberations, allowing things like after-image, imagination, sensory integration, and other things that require the brain to assemble some information together and “keep it in mind” for a while, despite the lack of external stimulus. He also wanted to use this theory for explaining pathologies like hallucination, phantom pain, etc.\n\nLet us assume that the persistence or repetition of a reverberatory activity (or “trace”) tends to induce lasting cellular changes that add to its stability. … When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.\n(Hebb 2002, 62)\n\n\nan indefinite reverberation in the structure might be possible, so long as the background activity in other cells in the same gross region remained the same. It would not of course remain the same for long, especially with changes of visual fixa tion; but such considerations make it possible to conceive of “alternating” reverberation which might frequently last for periods of time as great as half a second or a second.\n(Hebb 2002, 73–74)\n\n\n\n\nThe Hebb cell assembly. (Hebb 2002, 73, figure 10)\n\n\n\n\n\nAnimated version. The “reverberation” is clear in this case.\n\n\nIn proposing the reverberating circuitry, Hebb was influenced by de No (Espinosa-Sanchez, Gomez-Marin, and de Castro 2023)\n\n[The cell assembly theory] certainly looked improbable to its author–me–when it was first conceived [because it makes the ease of perception of common objects the result of a long process of learning]. The problem of perception remained intractable for about five years (1939 to 1944) and as a result I made no progress in my attempt to understand concepts and thought. It seemed obvious that concepts, like images, must derive from perception, and I could think of no mechanism of perception that corresponded to my preconceptions. In fact, by 1944 I had given up trying to solve the problem. What happened then was that I became aware of some recent work of Lorente de No in conjunction with some observations of Hilgard and Marquis (1940) which led me to think about the problem from a different point of view… The essential basis of an alternative view was provided by Lorente de No, who showed that the cortex is throughout its extent largely composed of enormously complex closed or re-entrant paths, rather than linear connections only between more distant points… When an excitation reaches the cortex, instead of having to be transmitted at once to a motor path, or else die out, it may travel round and round in these closed paths and may continue to do so after the original sensory stimulation has ceased.\nHebb DO. 1980. Essay on mind. Hillsdale, NJ: Lawrence Erlbaum"
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html#cybernetics",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html#cybernetics",
    "title": "Notes for a talk on the history of neural networks",
    "section": "Cybernetics",
    "text": "Cybernetics\n\nWWII\n\nWar is father of all and king of all; and some he manifested as gods, some as men; some he made slaves, some free. [Πόλεμος πάντων μὲν πατήρ ἐστι πάντων δὲ βασιλεύς, καὶ τοὺς μὲν θεοὺς ἔδειξε τοὺς δὲ ἀνθρώπους, τοὺς μὲν δούλους ἐποίησε τοὺς δὲ ἐλευθέρους.]\n— Heraclitus\n\nI have gone a bit overboard with the WWII pictures, but I think war has a high concentration of functionalist beauty. Anything that is optimized to the hilt, like a TSMC fab, or a modern ICBM, is intrinsically beautiful. See Why Do Hipsters Steal Stuff? · Gwern.net\n\nFor a good tutorial on the problem of land-based anti-aircraft fire control during WWII, see the 1944 United States Army Air Forces film #TF 1-3389, “Flak”. For another one from the perspective of naval fire control, see the 1953 U.S. Navy training film (MN-6783a) Basic Mechanisms In Fire Control Computers.\n\n\n\nDamage Assessment Report No. 20. Mosaic map showing target area and extent of Tokyo bombing caused by the 1945-03-10 raid – Source: U.S. National Archives, Record Group 243, Series 59, Box 6.\n\n\n\n\n\nCybernetics in Military Affairs. USSR, Moskva, “DOSAAF” (M. N. Goncharenko, 1963). The image shows a Tupolev Tu-123 reconnaissance drone. Книга М. Н. Гончаренко “Кибернетика в военном деле”. СССР, Москва, “ДОСААФ” 1963 год. The original caption says “Puc. 39. Беспилотный разведывательный реактивный самолет AN/VSD-5 многократного использования.” [Fig. 39. AN/VSD-5 reusable unmanned reconnaissance jet aircraft.]\n\n\n\n\n\nJapanese Mitsubishi A6M Zero drop white phosphorous air-burst bombs on B-24 Bombers over Iwo Jima, 1945-02. Apparently those were used during the Battle of Iwo Jima as a crude anti-bomber attack. The information is scarce, but there is a video Pacific Jap Phosphorous Bombs demonstrating it.\n\n\n\n\n\nMap of the Kammhuber line, a defensive front against British bombers. It consisted of a series of control sectors equipped with radars and searchlights and an associated night fighter. Each sector would direct the night fighter into visual range to target intruding bombers.\n\n\n\n\n\nA map of the Kammhuber line stolen by a spy in 1942.\n\n\n\n\n\nScatterplot of photos taken from bombers during Operation Gomorrah bombing of Hamburg, 1943-07. It was standard practice to take photos from bombers to validate the bombing accuracy, benchmark the loss function, measure the ablation of the target, and plan the next attack.\n\n\n\n\n\nOperation Gomorrah with Avro Lancaster bomber, 30/31 January 1943-01-30. The bright sine-like curves are flares, while the smoke and explosions provide diffuse lighting. Source: C 3371 of the Imperial War Museums.\n\n\n\n\n\nMultiple explosions/fires in the Sudstadt district of Hannover and a Lancaster silhouetted well below bottom left. Source: Hannover · IBCC Digital Archive\n\n\n\n\n\nA night bombing raid on Bremen. A British bomber has been caught in the searchlight cone and heavy anti-aircraft fire is converging on the aircraft. Source: Australian War Memorial\n\n\n\n\n\nPlanning map for the bombing of Kassel, 1943-10-22–23. (Murray 1986, 211)\n\n\n\n\n\nBombardment of Moscow, 1941-07-26. Photo taken by Margaret Bourke-White.\n\n\n\n\n\nThe same photo of the Bombardment of Moscow, with a scatterplot overlaid to bring out the similarity.\n\n\n\n\n\nThe scatterplot itself. The Python code is as follows.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a seed for reproducibility\nnp.random.seed(0)\n\n# Parameters for the points\nnum_points = 400\nx_points = np.random.normal(0.5, 0.5, num_points)\ny_points = np.exp(x_points) + np.random.normal(0, 0.2, num_points)\n\n# Parameters for the curves\nnum_curves = 10\nx_curves = np.linspace(-1, 2, 100)\nr_values = np.random.normal(1, 0.1, num_curves)\nk_values = np.random.normal(0, 0.1, num_curves)\n\n# Create the plot\nplt.figure(figsize=(8, 6))\n\n# Plot the points\nplt.scatter(x_points, y_points, color='white', s=0.5)\n\n# Plot the curves\nfor i in range(num_curves):\nplt.plot(x_curves, np.exp(r_values[i] * x_curves) + k_values[i], color='white', alpha=0.2)\n\n# Set plot limits and background color\nplt.xlim(-1, 2)\nplt.ylim(0, 4)\ngradient = np.zeros((256, 256, 3), dtype=np.uint8)\nfor i in range(256):\ngradient[:, i, :] = (i,i,i)\ngradient = (256 - gradient) // 2\nplt.imshow(gradient, extent=[-1, 2, 0, 4], aspect='auto', origin='lower')\n\nplt.axis('off')\nplt.savefig('war_curve.svg')\nplt.show()\n\n\n\nA German instructional poster for training the Luftwaffe. The shapes indicate areas of fire from the 50mm gunner positions aboard the B-17. Also noted are susceptible locations, specifically the fuel and oil tanks on the Flying Fortress. The text is murky but what I can make out are “Darstellung der von den Bordwaffen bestrichenen Räume”, “Darstellung des Uberschneidens der Feuerabschnitte”, “Viermotoriges Kampfflugzeug Boeing B-17 Flying Fortress”. [“Representation of the areas covered by the on-board weapons”, “Representation of the overlap of the fire zones”, “Four-engine fighter aircraft Boeing B-17 Flying Fortress”].\n\n\n\n\n\nEssentially the same diagram, but for B-24 “Liberator”. Source: Journal of the Second Air Division Association, Newsletter Vol. 18, No.2, 1979-06\n\n\n\n\n\nIllustration in a training manual for American bomber gunners. This shows how to lead the target by a crude form of linear regression that every gunner performs. Source: Gunner’s Information File - Flexible Gunnery. Air Force Manual No. 20, May 1944\n\n\nBonus: scaling plot?\n\n\n\nTop image: Learning curve of the production of B-29 airframes at the Boeing Wichita division during WWII. Page 75 from Source Book of World War II Basic Data - Airframe Industry. Volume 1. Direct Man-Hours - Progress Curves. Bottom image: Scaling curves for LLM. (Kaplan et al. 2020)\n\n\n\n\nThe problem of fire control\nWiener treated fire control as a problem of linear regression.\n\n\n\nThe linear geometry of missile flight control. Page 21 of Standard Fire Control Symbols for Missile Related Quantities, OP 1700 Volume 3, (1957)\n\n\n\n\n\nThe curved geometry of missile flight control. Page 53 of Standard Fire Control Symbols for Missile Related Quantities, OP 1700 Volume 3, (1957)\n\n\n\n\n\nThe vestibulo-ocular reflex of the fire control. Section 15 of Fire Control Fundamentals, NAVPERS 91900 (1953), Part G. Original caption: To measure director elevation in a vertical plane, the optical equipment and radar antenna must be stabilized so that they elevate in a vertical plane. This is accomplished by continuously supplying the director with crosslevel as measured at the stable element. An automatic positioning device continuously rotates the telescopes, rangefinder and radar antenna in response to the crosslevel signal, to keep their axes horizontal. Thus target elevation is always measured in a vertical plane.\n\n\nI think it is no coincidence that negative feedback circuit was first discovered by Lorente de No in the VOR, and constructed during WWII in the artificial-VOR. In both cases, the eyes needs to be stabilized cheaply and fast.\nNorbert Wiener decided he had to contribute to the war, and since his background was in stochastic process and statistics, he decided to work on aircraft predictor for anti-aircraft fire control. The following quote is from a classified memorandum in 1942. It was connected with sensitive wartime efforts to improve radar communication and became a founding document for cybernetics and information theory.\n\nThis book represents an attempt to unite the theory and practice of two fields of work which are of vital importance in the present emergency, and which have a complete natural methodological unity, but which have up to the present drawn their inspiration from two entirely distinct traditions, and which are widely different in their vocabulary and the training of their personnel. These two fields are those of time series in statistics and of communication engineering.\n(Norbert Wiener 1966, 1)\n\nThis quote from its introduction shows how Wiener thought of statistical communication theory. Prediction of aircraft trajectory is understood as communication: The past trajectory of the aircraft is a noisy channel that is trying to communicate (despite the best efforts of the pilot) the future trajectory of the aircraft.\nWiener thought of antiaircraft fire as a problem of feedback control. The loss function is the squared distance between the aircraft and the bullet. The aircraft is killed by minimizing the loss function.\nThe control mechanism is a least-squares predictor. (Yeang 2023)\n\nin order to obtain as complete a mathematical treatment as possible of the over-all control problem, it is necessary to assimilate the different parts of the system to a single basis, either human or mechanical. Since our understanding of the mechanical elements of gun pointing appeared to us to be far ahead of our psychological understanding, we chose to try to find a mechanical analogue of the gun pointer and the airplane pilot.\n(Wiener 2017, 407)\n\n\n\nCommentary\nWhy did it take 50 years for feedback to appear? I don’t know, but I have some theories.\n\nFeedback networks are much harder to solve mathematically.\nFeedback threatens the idea of cause-then-effect.\nReflexes are simpler to study, so neuroscientists thought that the brain is made of nothing but reflexes.\nBehaviorism.\n\nAlso, I think this shows that there is progress in theoretical neuroscience.\nFreud in the 19th century analogized the brain as a steam engine (thus there is “psychic pressure” that builds up and must be “vented”). In the early 20th century, the brain was often analogized to a telephone system. Later, it was analogized to an artificial neural network. Some commentators have pointed at this, and sneered that, scientists always mistakenly use whatever analogy is fashionable to study what is fundamentally out of their reach.\nWhat this does not note is that telephone networks are feedforward, while neural networks can be feedback. A telephone network does not work if there is feedback (it would be ringing pretty loudly), but some artificial neural networks work precisely because there is feedback.\n\n\n\nSimplified diagram of telephone switching. Source.\n\n\n\n\n\nCrossbar switching in a 1903 patent. Source.\n\n\n\n\n\nCrossbar switching at a New York telephone exchange center in 1938-05. Source.\n\n\n\n\nMcCulloch and Pitts (1943)\nThe paper (McCulloch and Pitts 1943) is the foundational paper for artificial neural networks. Often cited, rarely read. In modern language, the paper considered neural networks with integer weights, integer biases, 0-1 activation functions, and constant time-delays. The constant time-delay means that the neural network operates in clock-ticks, so that the state of a neuron at time \\(t\\) is decided by the inputs to it at time \\(t-1\\). This is necessary for two things:\n\nFormalized Hebbian learning: if A has a potential synapse to B, and A fired at time \\(t\\), then B fired at time \\(t+1\\), then the potential synapse becomes a real synapse.\nTo make recurrent neural networks work.\n\n\n\n\nA single MP neuron. It has integer weights and biases. A positive weight is symbolized by a flat end, and a negative weight is symbolized by a circular end. The bias (firing threshold) is written on the neuron itself.\n\n\nThey proved the following theorems:\n\nAny boolean function is implemented by a purely feedforward network (“nets without circles”). Thus, it can be used as the controller in any Turing machine.\n\n\nfirst, that every net, if furnished with a tape, scanners connected to afferents, and suitable efferents to perform the necessary motor-operations, can compute only such numbers as can a Turing machine; second, that each of the latter numbers can be computed by such a net…\n\n\nA recurrent network is equivalent to a Turing machine with a finite tape.\nA recurrent network can perform Hebbian learning, even if all its weights and biases remain unchanged.\n\n\nat any time the neuron fires, and the axonal terminations are simultaneously excited, they become synapses… capable of exciting the neuron… THEOREM VII. Alterable synapses can be replaced by circles.\n\nThe proof is as shown below:\n\n\n\nThe dashed synapse can become a real synapse by Hebbian learning.\n\n\nThe paper had two sources: pathological states where the mind folds back onto itself (tinnitus, hallucination, etc), and healthy states where the mind is influenced by its past (memory).\n\nThere is no theory we may hold and no observation we can make that will retain so much as its old defective reference to the facts if the net be altered. Tinnitus, paraesthesias, hallucinations, delusions, confusions and disorientations intervene. Thus empiry confirms that if our nets are undefined, our facts are undefined, and to the “real” we can attribute not so much as one quality or “form”. With determination of the net, the unknowable object of knowledge, the “thing in itself,” ceases to be unknowable. (McCulloch and Pitts 1943)\n\n\nIn 1941 I presented my notions on the flow of information through ranks of neurons to Rashevsky’s seminar in the Committee on Mathematical Biology of the University of Chicago and met Walter Pitts, who then was about seventeen years old. He was working on a mathematical theory of learning and I was much impressed.\nHe was interested in problems of circularity, how to handle regenerative nervous activity in closed loops. I had had to suppose such loops to account for epileptic activity of surgically isolated brain and even of undercut cortex. Lorente de No had shown their significance in vestibular nystagmus. I wanted them to account for causalgia persisting after amputation of a painful limb and even after section of the spinothalamic tract; I wanted them to account for the early stages of memory and conditioning. I wanted them to account for compulsive behavior, for anxiety and for the effects of shock therapy. These appeared to be processes that once started seemed to run on in various ways. Since there obviously were negative feedbacks within the brain, why not regenerative ones? For two years Walter and I worked on these problems whose solution depended upon modular mathematics of which I knew nothing, but Walter did. We needed a rigorous terminology and Walter had it from Carnap, with whom he had been studying. We, I should say Walter Pitts, finally got it in proper form and we published in 1943, A Logical Calculus of the Ideas Immanent in Nervous Activity. H.D. Landahi immediately joined us in a note applying the logical calculus statistically. The crucial third part of our first article is rigorous but opaque and there is an error in subscript. In substance what it proved via its three theorems is that a net made of threshold devices, formal neurons, can compute those and only those numbers that a Turing machine can compute with a finite tape.\n(McCulloch 1974)"
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html#first-neural-network-period-19501970",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html#first-neural-network-period-19501970",
    "title": "Notes for a talk on the history of neural networks",
    "section": "First neural network period (1950–1970)",
    "text": "First neural network period (1950–1970)\n\nSNARC\nThere were some neural networks in the early 1950s before Rosenblatt. See (Nilsson 2009, chap. 4) for a review. The most notable one is Minsky’s SNARC. Details are in Yuxi on the Wired - A Scrapbook of Neural Network Lores and Yuxi on the Wired - The Perceptron Controversy.\nApparently it was a neural network version of Shannon’s robot rat “Theseus”.\nSNARC had 40 neurons, but it was lost. The report (Minsky 1952) was lost too. I emailed the Harvard library for a copy, but they did not find it.\n\n\n\nPictured: Minsky’s last remaining neuron. I like to joke that Minsky has very few neurons (see his 1981 comment about unable to afford “thousands or millions of neurons”), and this picture is the only one remaining.\n\n\nHis PhD thesis (Minsky 1954) contained lots and lots of neural networks doing all kinds of Turing complete things. No way it would require Perceptrons (1969) to reveal to the world that, surprisingly, single-layered perceptrons can’t do XOR.\n\n\nFrank Rosenblatt\nStarted research in 1957 under “Project PARA” (“Perceiving and Recognition Automata”). First published in a report (Rosenblatt 1958), later published in a whole book (1961).\n(Nagy 1991) describes some more of Rosenblatt’s works.\nGenerally, Rosenblatt investigated in two ways: computer simulation (on IBM machines usually) and special hardwares (Perceptron Mark I, Tobermory). After 1965, the computer simulations became much faster than special hardwares. The tragedy of ASIC neuromorphic computing, no doubt…\nInteresting results in the book: (More in Yuxi on the Wired - The Backstory of Backpropagation)\nContinuous activation functions (section 10), what he called “transmission functions”.\n\n\n\nFirst sighting of the Hopfield network. (Rosenblatt 1960, 74)\n\n\nRecurrent network that can perform a kind of selective visual attention (chapter 21) and memorize output sequences (chapter 22)\n\n\n\nHopfield network, again. (Rosenblatt 1962, vol. 55, fig. 47)\n\n\n\n\n\nRecurrent network that can perform a kind of selective visual attention. (Rosenblatt 1962, vol. 55, fig. 63)\n\n\n\n\n\nThe first multimodal neural network? Associative learning of image and audio inputs (Figure 58)\n\n\nThe most interesting example is the “back-propagating error-correction procedure” network with residual connections and weight decay, pictured in Figure 42.\n\n\n\nThe 4-layered MLP in Rosenblatt. (Figure 42)\n\n\nIts structure is (fixed linear)-(activation)-(residual linear)-activation-linear:\n\nThe first layer is fixed (randomly wired).\nThe second layer has both a residual connection and a learned connection. The learned weights increase by Hebbian learning, and the weights decay exponentially. Weight decay is a practical necessity because Hebbian learning only increases weights, never decreasing them.\nThe third layer is trained by the perceptron learning rule, as usual.\n\nHis last great project was Tobermory (1961–1967), for speech recognition. 4 layers with 12,000 weights (magnetic cores). By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines. It occupied an entire room.\n\n\n\nTobermory schematic. From System and Circuit Designs for the Tobermory Perceptron: Preliminary Report on Phase I By GEORGE NAGY 1 September, 1963 Prepared Under Contract No. NONR 401 (40) and NSF GP-971\n\n\n\n\nStanford Research Institute\n\nWhen I first interviewed for a position at SRI in 1961, I was warned by one researcher there against joining research on neural networks. Such research, he claimed, was “premature,” and my involvement in it could damage my reputation.\n(Nilsson 2009, chap. 24.2)\n\nThe weight matrix of the MINOS II pattern recognition system, consisting of a grid of magnetic cores. The magnetization of a core is proportional to its matrix weight. Matrix weights that are quite literally weighty. The weight matrices are swappable.\n\n\n\n(Rosen, Nilsson, and Adams 1965)\n\n\n\n\nADALINE\nADALINE (adaptive linear) and MADALINE (many ADALINE).\nThe ADALINE is a single perceptron \\[\n\\theta\\left(\\sum_i w_i x_i + b\\right)\n\\]\nand its learning rule is gradient descent on squared error \\((\\sum_i w_i x_i + b - y)^2\\).\n\nIt used manual input by flipping 16 switches by hand. The first version (“knobby ADALINE”) had weights implemented as rheostats, with knobs turned by hand!!! Later versions used memistors that do gradient descent automatically.\n\n\n\nWidrow doing brain surgery on ADALINE.\n\n\nThey got up to 1000 weights in a 2-layered MADALINE, but the learning rule was really hacky, and it was a deadend. (Widrow and Lehr 1990) A video of the MADALINE in action is found in The LMS algorithm and ADALINE. Part II - ADALINE and memistor ADALINE - YouTube\nMore information in Yuxi on the Wired - The Backstory of Backpropagation.\nI don’t know where to put this anecdote, but I want you to read about it anyway.\n\nWe discovered the inherent ability of adaptive computers to ignore their own defects while we were rushing through construction of a system called MADALINE I for presentation at a technical meeting. The machine was finished late the night before the meeting and the next day we showed some very complex pattern discriminations. Later we discovered that about a fourth of the circuitry was defective. Things were connected backward, there were short circuits, and poor solder joints. We were pretty unhappy until it dawned on us that this system has the ability to adapt around its own internal flaws. The capacity of the system is diminished but it does not fail.\nB. Widrow, “Adaline: Smarter than Sweet”, Stanford Today, Autumn 1963.\n\nSee more of these amusing stories at Yuxi on the Wired - A Scrapbook of Neural Network Lores."
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html#neural-network-winter-1970s",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html#neural-network-winter-1970s",
    "title": "Notes for a talk on the history of neural networks",
    "section": "Neural network winter (1970s)",
    "text": "Neural network winter (1970s)\n\nThe XOR myth\nThe XOR myth can’t be true, for the following reasons:\n\nIt was widely known that binary perceptron networks are Turing-complete. (McCulloch and Pitts 1943) already proved it.\nMinsky wrote his PhD thesis (Minsky 1954) describing all kinds of specific neural networks for computing specific boolean functions.\nMLP was studied as “linear threshold logic” by electric engineers. They wouldn’t have been so interested if they couldn’t even do XOR. (Dertouzos 1965)\n\nStill, the XOR myth has some power. What exactly is it? To explain this mess, I will draw a flowchart.\n\n\n\nXOR flowchart\n\n\nLet’s follow all the points in the flowchart:\n\nCan your neural network learn XOR? If you answer “No”, then your neural network is patently useless.\nTo learn XOR, the neural network must have at least 2 layers. But how is it made? If you made it manually, then it is not really machine learning, is it?\nIf it is learned, then what algorithm?\nThe Perceptron learning algorithm only works for featurized linear regression. So where did you get your features? If you designed it manually, by, for example, reading neuroscience papers, then it is not really machine learning, is it? If they are randomly generated, then it will not scale because of combinatorial explosion.\nIf you used a hack like the MADALINE learning rule, or whatever Rosenblatt tried, then you have no doubt noticed how fiddly and unscalable they are, and how dependent they are on the hyperparameters. Each particular problem required a particular setting of hyperparameters.\n\nAfter backprop became prevalent, Minsky and Papert wrote their updated Perceptron (1988) with 43 pages to dunk on everything that is the new connectionism. This allows us to add a few more blocks to the XOR myth flowchart:\n\nIf you use gradient descent, then it would just get stuck in local minima. You can’t just draw a scaling plot and hope it will continue, without proving it theoretically!\nIn conclusion, there is no general and scalable learning algorithm.\n\nAs an example of the kind of MLP that Minsky approves: a hand-designed deep network (Fukushima 1969). Here is someone who actually tried to understand the problem instead of just tossing it to the computer and expect things to magically work.\n\n\n\n(Fukushima 1969)\n\n\nMinsky can’t afford a few million neurons:\n\nI had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks. Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that.\n(Bernstein 1981)\n\n\n\nPossible reasons for the winter\nMinsky and Papert’s book. At least, Minsky thought so.\n\nThere had been several thousand papers published on Perceptrons up to 1969, but our book put a stop to those. It had, in all modesty, some beautiful mathematics in it–it’s really nineteenth-century mathematics.\n(Bernstein 1981)\n\n\nMinsky’s early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community.\n— Robert Hecht-Nielsen (Rosenfeld and Anderson 2000)\n\nLack of funding\n\nMany people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. — Michael A. Arbib (Rosenfeld and Anderson 2000)\n\nLack of people\n\nMcCulloch (1969) Pitts (1969) Rosenblatt (1971) died\nWidrow cut his losses and looked for applications for a single neuron, revolutionizing adaptive noise filtering.\nTed Hoff went to Intel to create microprocessors.\nSRI turned to logical AI. Duda and Hart wrote their classic textbook on “pattern classification” which had little neural networks, and half of the book was devoted to “scene analysis”. (Duda and Hart 1973)\n\nAnd of course, no backpropagation.\n\n\nMinsky and Papert against large neural networks\nSee Yuxi on the Wired - The Perceptron Controversy for details.\n\n\n30 years of people refusing to use gradient descent\nSee Yuxi on the Wired - The Backstory of Backpropagation.\nRosenblatt did not even consider gradient descent. (1960s)\nMinsky repeatedly shooting down gradient descent. (1960s – 1990s at least)\n\nThe story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea… It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It’s certainly trivial. The idea is how you do gradient descent. I didn’t consider it practical.\n(Olazaran 1991, 249)\n\nWidrow and Hoff could not generalize one-layered gradient descent to two-layered gradient descent and gave up (1960s).\n\nThe Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic first layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.\nBackprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; you have to have a smooth nonlinearity … no one knew anything about it at that time. This was long before Paul Werbos. Backprop to me is almost miraculous. (Rosenfeld and Anderson 2000)\n\nThe SRI group never worked it out.\n\nDuring the 1960s, neural net researchers employed various methods for changing a network’s adjustable weights so that the entire network made appropriate output responses to a set of “training” inputs. For example, Frank Rosenblatt at Cornell adjusted weight values in the final layer of what he called the three-layer alpha-perceptron. Bill Ridgway (one of Bernard Widrow’s Stanford students) adjusted weights in the first layer of what he called a MADALINE. We had a similar scheme for adjusting weights in the first layer of the MINOS II and MINOS III neural network machines at SRI. Others used various statistical techniques to set weight values. But what stymied us all was how to change weights in more than one layer of multilayer networks. (I recall Charles Rosen, the leader of our group, sitting in his office with yellow quadrille tablets hand-simulating his ever-inventive schemes for making weight changes; none seemed to work out.)\n(Nilsson 2009, sec. 29.4)\n\nPaul Werbos spending 10 years (1972–1982) unable to publish backpropagation, and almost starved.\nHe developed the backprop in 1971 or 1972 for his PhD thesis, as a mathematical version of Freudian psychoanalysis.\n\nThe model in Figure 1 is actually just a mathematical version of Freud’s model of psychodynamics, where the derivative of \\(R_i\\) represents what Freud called the “cathexis” (or affect) or emotional charge or emotional energy attached to the object which \\(R_i\\) represents. In other words, I came up with backpropagation by not just laughing at Freud’s “nonscientific” model, but by translating it into mathematics and showing that it works. (Werbos 2011)\n\nThe PhD committee was unconvinced, so he had to find a supporting advisor.\n\n“But look, the mathematics is straightforward.”\n“Yeah, yeah, but you know, we’re not convinced it’s so straightforward. You’ve got to prove some theorems first.”\n\nSo he went to Stephen Grossberg…\n\nthis stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.\n\nThen to Marvin Minsky…\n\nLook, everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.\n\nUnable to find a supporting advisor, he faced the committee again. He decided to simplify his work to just backpropagating through piecewise linear activation functions.\n\nI handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, but this is too trivial and simple to be worthy of a Harvard Ph.D. thesis.”\n\nSo he lost funding, so he lived in the slums and became malnutritioned.\n\n… they had discontinued support because they were not interested… There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.\n\nAnyway, he did get his PhD thesis and it did contain the backpropagation algorithm, but then promptly got sucked into a decade of misadventures with federal bureaucracy. The net effect is that he only managed to publish the backprop algorithm in 1982.\nGeoffrey Hinton spent several years (1982–1985) refusing to try backpropagation, since he was too invested in the Boltzmann machines.\n\nI first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule … The next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.\n\nBut it turned out that Boltzmann machines just never worked in practice, and desperate times called for desperate measures (backpropagation). Amazingly, it just worked.\n\nI almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.\n\nIn a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.\nHe still thinks Boltzmann machines are better though!\n\nThat was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation."
  },
  {
    "objectID": "sketches/posts/history-neural-networks-talk-notes/index.html#some-bitter-lessons-i-guess",
    "href": "sketches/posts/history-neural-networks-talk-notes/index.html#some-bitter-lessons-i-guess",
    "title": "Notes for a talk on the history of neural networks",
    "section": "Some bitter lessons I guess",
    "text": "Some bitter lessons I guess\n\nFirst attempt\n\nI hate everything that merely instructs me without increasing or directly quickening my activity.\n— Goethe, Letter to Friedrich Schiller, 1798-12-19. Quoted in Nietzsche’s On the Use and Abuse of History for Life.\n\n\nWe do need history, but quite differently from the jaded idlers in the garden of knowledge, however grandly they may look down on our rude and unpicturesque requirements. In other words, we need it for life and action, not as a convenient way to avoid life and action, or to excuse a selfish life and a cowardly or base action. We would serve history only so far as it serves life; but to value its study beyond a certain point mutilates and degrades life.\n— Nietzsche, On the Use and Abuse of History for Life (1874)\n\nNihilism: You can draw lessons from history, but everyone else also can draw the opposite ones, so there is no take-home lesson.\nElitism: First-rate minds make history. Second-rate minds study history. Getting history “right” is for second-rate minds: old professors (that’s Schmidhuber) and procrastinating PhD students (that’s me).\nThose who don’t know their history, repeat it. Those who know their history, are too busy remembering to make it.\nEmotivism: The proper function of history is to fortify your weights and to harden your biases against naysayers, so that you can go forth and make some history.\n\nThe function of a mathematician is to do something, to prove new theorems, to add to mathematics, and not to talk about what he or other mathematicians have done. Statesmen despise publicists, painters despise art-critics, and physiologists, physicists, or mathematicians have usually similar feelings: there is no scorn more profound, or on the whole more justifiable, than that of the men who make for the men who explain. Exposition, criticism, appreciation, is work for second-rate minds.\n(Hardy 1940)\n\n\n\nSecond attempt\nThis is where I was going to end, but GPT told me that I must include positive lessons, so I made some up. I fully expect you to ignore my lessons and reinterpret the history I dug up to your lessons.\nTo facilitate this freedom of reinterpretation, I will write my weights and biases in bold letters, so that it is not a secret agenda:\n\nVINCIT OMNIA COMPUTATIO\n\nWith this, you can now subtract my latent vector and add yours, in full Word2Vec fashion.\n\nNihilism\nHistory is useless for doing things, because ideas are cheap and can’t be lost. People will keep reinventing the same ideas until it starts working, then it will not be lost again. The idea is a patient seed, waiting for the soil to moisten.\nIn particular, it is useless to give correct attribution to “the real inventor” of an idea, because there are so many re-inventors. It is much more interesting to study each re-invention on its own, to find out how they invented. The point is not to respect the dead, but to find out how they did their things, so that we can do more things. It is useless to know who “really” invented backpropagation, but useful to know how each re-inventor managed to invent it in their individual, different contexts of rediscovery.\nThose who don’t know their history, repeat it. If everyone knows their history, there will be a replication crisis.\nResidual networks were described by Lorente de No, then Rosenblatt. DenseNet (Huang et al. 2018) was already in 1988 (Lang and Witbrock 1988). People didn’t know it, but this did not matter.\n\n\n\nReinventions/rediscoveries of ResNet. Lorente de No (1930s), Rosenblatt (1961), (Lang and Witbrock 1988), ResNet (2015), DenseNet (2016).\n\n\n\n\n\nReinventions/rediscoveries of RNN. Cajal (1900s), Lorente de No (1930s), McCulloch and Pitts (1943).\n\n\n\n\n\nReinventions/rediscoveries of Hopfield network. (Rosenblatt 1960), (Nakano 1971), (Hopfield 1982). Note that because Hopfield’s paper has no pretty pictures, I got the picture from (Tank and Hopfield 1987) instead.\n\n\nThe only place where the lack of a good idea really hurt is backpropagation. But even if they had backpropagation, what could they have made in the 1970s? A personal tragedy for Widrow and Rosenblatt. Hardly a tragedy for the study of neural networks itself. Ted Hoff was right to leave neural networks to develop the microprocessor.\n\n\nEmpiricism\nElegant ideas matter little, because there are always opposing elegant ideas. Ideas must be tested by experiments. Benchmarks are the experimentum crucis. In mensuris veritas.\n30 years of elegant arguments against backpropagation amounted to nothing but jokes for us.\nThis disjunction between theory and experiments is particularly acute in reinforcement learning. If you have read RL papers, you would notice how often they describe some elegant ideas and giant formulas, then proceeded to talk about the experiments. I always find the elegant ideas suspicious, and only trust the benchmarks. If the idea is really elegant and stands on its own, why all the ponderous benchmarks? Furthermore, it is often the case that the theory works only in the sense of “eventually almost anywhere if the functional space is large enough”.\nSimilar comments apply for the PAC-learning literature, the no-free-lunch literature, and the neural network universality literature. It is surely comforting to know that neural networks are universal function approximators, but so are Turing machines. Neural networks work not because they are universal, but because their inductive biases happen to be very attuned to physically interesting problems. Why is that? That… is unclear, but (Lin, Tegmark, and Rolnick 2017) has a guess. Basically, it states that neural networks are particularly good at learning trajectories generated by quadratic Lagrangian mechanical systems.\n\n\n\nThe idea follows the compute\nIt is not just that compute allows good ideas to be filtered out, and that testing ideas requires expensive compute. Sometimes compute is necessary to even approach the good idea. Without compute experiments, one would be left to wander the vast space of possible ideas. This is why ablation studies are so important, though it lacks the pedigreed prestige of theory. Ablations need compute, making it a choice for the nouveau riche – the GPU-rich, that is.\nTwo examples. One is ResNet\n\nFor months, they toyed with various ways to add more layers and still get accurate results. After a lot of trial and error, the researchers hit on a system they dubbed “deep residual networks”.\n(Linn 2015)\n\nAnother is Transformer\n\nThere was every possible combination of tricks and modules—which one helps, which doesn’t help. Let’s rip it out. Let’s replace it with this. Why is the model behaving in this counterintuitive way? Oh, it’s because we didn’t remember to do the masking properly. Does it work yet? OK, move on to the next. All of these components of what we now call the transformer were the output of this extremely high-paced, iterative trial and error.\n(Levy 2024)\n\n\n\nThe bitter lesson\n\nBy convention sweet is sweet, bitter is bitter, hot is hot, cold is cold, color is color; but in truth there are only atoms and the void. [νόμωι (γάρ φησι) γλυκὺ καὶ νόμωι πικρόν, νόμωι θερμόν, νόμωι ψυχρόν, νόμωι χροιή, ἐτεῆι δὲ ἄτομα καὶ κενόν]\n— Democritus, quoted in Sextus Empiricus Against the Mathematicians VII 135. I like the funny title.\n\n\nA new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die and a new generation grows up that is familiar with it …\n— Max Planck, Scientific Autobiography (1950), page 33. In simpler terms, “Science progresses one funeral at a time.”.\n\n\n\nAI researchers have often tried to build knowledge into their agents,\nthis always helps in the short term, and is personally satisfying to the researcher, but\nin the long run it plateaus and even inhibits further progress, and\nbreakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.\n\nThe eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.\n(Sutton 2019)\n\nThe bitter lesson is bitter, because some people really like elegant ideas, and they will not change.\nMarvin Minsky insisted for decades that large neural networks do not work in theory (as we saw), so it only appears to work in practice.\n\n13.5 Why Prove Theorems?\nWhy did you prove all these complicated theorems? Couldn’t you just take a perceptron and see if it can recognize \\(\\psi_{\\text {CONNECTED}}\\)?\nNo. (1969)\n(Minsky and Papert 1988, 239)\n\n\nI had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. This became a field of study later. It was called self-organizing random networks… I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. (1981)\n(Bernstein 1981)\n\n\nIf one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No. (1988)\n(Minsky and Papert 1988, xiv)\n\n\nIn his summary talk at the end of the conference [The AI@50 conference (2006)], Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications.” (2006)\n(Sejnowski 2018, 256)\n\nGeoffrey Hinton still liked Boltzmann machines more than backprop, even in 1995:\n\nThat was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation.\n(Rosenfeld and Anderson 2000)\n\nNoam Chomsky is totally against statistical language modelling, since at least 1969. See also (Norvig 2017).\n\nBut it must be recognized that the notion of “probability of a sentence” is an entirely useless one, under any known interpretation of this term. (1969)\n(Chomsky 1969)\n\n\nIt’s true there’s been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success … which I think is novel in the history of science. It interprets success as approximating unanalyzed data. (2011)\nBrains, Minds, and Machines symposium 2011, Keynote Panel\n\n\nWell, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They’ve achieved zero… GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It’ll use even more energy and achieve exactly nothing, for the same reasons. So there’s nothing to discuss. (2022)\nMachine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding\n\nOn the last one, I suspect he is still fighting the battle against behaviorism, and the pretrained language models like GPT-4 must seem like a kind of neo-behaviorist heresy to him.\nYehoshua Bar-Hillel, speaking in the 1960s, against machine translation – what he called FAHQT (Fully Automatic High-Quality Translation, or as I imagine him saying it, “Machine translation? Ah, FAHQT.”). In short, he argued that statistical language modelling does not work better than manually programming in the rules. Specifically, the Winograd schema challenge (which was known in the 1960s, before Winograd discussed it) requires general world understanding, which is “utterly chimerical and hardly deserves any further discussion’.\n\nNo justification has been given for the implicit belief of the “empiricists” that a grammar satisfactory for MT purposes will be compiled any quicker or more reliably by starting from scratch and “deriving” the rules of grammar from an analysis of a large corpus than by starting from some authoritative grammar and changing it, if necessary, in accordance with analysis of actual texts.\n…\nWhenever I offered [the Winograd challenge] to one of my colleagues working on MT, their first reaction was: “But why not envisage a system which will put this knowledge at the disposal of the translation machine?” … such a suggestion amounts to, if taken seriously, is the requirement that a translation machine should not only be supplied with a dictionary but also with a universal encyclopedia. This is surely utterly chimerical and hardly deserves any further discussion.\n(Bar-Hillel 1960)\n\n\nIt seems now quite certain to some of us, a small but apparently growing minority, that with all the progress made in hardware (i.e., apparatus), programming techniques and linguistic insight, the quality of fully autonomous mechanical translation, even when restricted to scientific or technological material, will never approach that of qualified human translators and that therefore Machine Translation will only under very exceptional circumstances be able to compete with human translation.\n(Bar-Hillel 1964)\n\nEven Terry Winograd had a tentative guess that the Winograd challenge is too challenging, though it is clearly just a weak guess, not a firm prediction like the previous quotes.\n\nThe limitations on the formalization of contextual meaning make it impossible at present – and conceivably forever – to design computer programs that come close to full mimicry of human language understanding.\n(Winograd 1984)"
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html",
    "href": "sketches/posts/linux-notes/index.html",
    "title": "Notes on Using Linux",
    "section": "",
    "text": "This is my quick reference for using Linux for doing things. I claim no originality. Mostly they are copy pasted from the internet and tested by me. An increasing proportion of those are produced by AI."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#sec-koans",
    "href": "sketches/posts/linux-notes/index.html#sec-koans",
    "title": "Notes on Using Linux",
    "section": "Linux koans",
    "text": "Linux koans\nEverything is a file a file or a directory an inode. An inode (index node) is a representation of a sequence of data that the system can access. A file is just a list of inodes, and a directory is just a list of files.\nEvery command is an executable.\nEvery machine is a server. Some merely serve extremely slow machines (humans, aka “users”).\nThe \\usr is not the user. It is the UNIX System Resources.\nThe \\usr was the user, until Unix became so large that \\bin overflowed and had to put the rest of them in \\usr\\bin. This was embarrassing for all involved, so they moved user files to \\home, and pretend that \\usr stands for UNIX System Resources. (here). A mistake that only took 40 years to fix.\nIn the beginning was the command line. The command line is just a face of the shell. What was the original face of the command line before the shell was born?\nThe shell reads in a stream of letters, because the user is just another file (a streaming file, named stdin). Like all streaming files, the user is eternal and inexhaustible. The shell stands, rapt in attention, afore the user file.\nSo when does the user ever leave? The user never leaves. The shell simply kills itself when the user types exit. The shell would rather die than to face the prospect of reading the last word from the user.\nSo when does the shell ever break out of its rapt attention? Whenever it sees \\n, it is shaken out of its trance and interprets what the user has just said, in the interval bracketed between two \\ns.\nThe shell has one ear and two mouths. The ear is stdin, and the mouths are stdout and stderr. The shell has a tiny brain which is only capable of interpreting the few syntactic elements of bash scripts. Everything else it wants to do, it dutifully sends a binary message into the oracular altar of the Linux kernel, from which an oceanic voice replies the answer of the kernel."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#sec-path",
    "href": "sketches/posts/linux-notes/index.html#sec-path",
    "title": "Notes on Using Linux",
    "section": "How to PATH",
    "text": "How to PATH\nThe PATH environment variable is a list of directories that the shell searches for commands. It is a colon-separated list of directories. For example, just about every Linux installation has a PATH variable that looks like:\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nTo add things to PATH, use the export command, like export PATH=\"$HOME/bin:$PATH\".\nenv # same as printenv\nexport VAR_NAME=\"value\"\nunset VAR_NAME\n\nexport PATH=newpath:$PATH # There is no simple way to undo this one.\n# but you can try export OLD_PATH=$PATH; ...; export PATH=$OLD_PATH\nFor Windows, use Get-ChildItem Env:."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#how-to-directories",
    "href": "sketches/posts/linux-notes/index.html#how-to-directories",
    "title": "Notes on Using Linux",
    "section": "How to directories",
    "text": "How to directories\nBased on https://askubuntu.com/a/135679, https://serverfault.com/a/24525, man hier, Filesystem Hierarchy Standard - Wikipedia.\n\nWhere to install read-only things\n\n/bin and /sbin: binaries and superuser-binaries.\n/usr: System-wide, read-only files installed by the OS. This directory can be remotely mounted to multiple hosts safely.\n/usr/local: System-wide, read-only files installed by root. And that’s why most directory names from /usr are duplicated here.\n/opt - System-wide, read-only and bundled-up software. That is, software that does not split their files over bin, lib, share, include like well-behaved software should.\n/usr/bin and usr/sbin: They no longer exist. Just use /bin and /sbin.\n~/.local: the per-user counterpart of /usr/local, that is: software installed by (and for) each user. Like /usr, it has its own ~/.local/share, ~/.local/bin, ~/.local/lib.\n~/.local/opt: The per-user counterpart of /opt\n\nRequiem for /usr/bin and /usr/sbin: Originally, /bin and /lib are only for binaries and libraries required for booting, while /usr/bin and /usr/lib are for all the other executables and libraries. This is no longer true, as some binaries required for booting has over the years leaked into those two folders (if there is a way to make a mess, people will make it), so since Ubuntu 20.04, they no longer exist, to remove this mess.\nHow to install for local user only:\n./configure --prefix=$HOME/.local\nmake\nmake install\nHow to install for everyone: sudo ./configure && make && make install\n\n\nWhere to install read-write things\nRead-write things are typically configuration files, since they are read and written by both the user and binary executables.\n\n/etc: System-wide configuration. Typically used by the OS to decide what to do when starting up, shutting down, etc.\n~/.config: Per-user configuration files. Although because of legacy, you keep seeing nonsense like .bashrc in your home directory instead of ~/.config/.bashrc. Here rc means “run configuration”.\n\n\n\nWhere to do read-write things\n\n/home/username, or just ~: Each user typically is only able to modify their own folder here, like ~/myfile.txt.\n/tmp: If you need to create something just for the moment, then make it here. It will be deleted when the system restarts.\n\n\n\nOther things (you should not modify them)\n\n/run: runtime temporary data, representing the system’s state since the last boot. It’s used for storing process IDs, lock files, and other files that are would normally be stored in /tmp. It is basically /tmp for the machine.\n/var: Variable data. It is somewhat like /run in that both are meant to be read-written by programs, but unlike /run, data here persists over reboots. This is often used for logging information. For example, try vim /var/log/user.log\n\n\n\n\n\n\n\nWarning\n\n\n\nModifying anything below this line may brick the system. Reading is fine though.\n\n\n\n/lib: libraries. You should not handle it directly. Some libraries are added at OS installation, and others at program installation. If you have a broken installation, you might be asked to manually copy some files looking like libxxx.so here. (so stands for “shared object”.)\n/boot: Files required for booting. For example, the bootloader, the kernel, the initramfs (initial RAM file system).\n/dev: Device files. It typically looks like /dev/sda1, /dev/sda2, etc. Other than things like sda1 (for harddrive) you might notice tty1 and pty1’ which stand for “teletype” and “pseudo teletype”, respectively, but they are actually used as files to read whatever the user is typing from (the user is a file, see Section 1). There are some odd ones like:\n\n/dev/null, which is a “file” that you write to when you just want to throw something away (everything is a file, even a black hole…).\n/dev/urandom, which is a random number generator. It is preferred over /dev/random. See here.\n/dev/zero, which is a file that you can’t write to, but you can read, but it’s filled with zeros.\n\n/media: Mount removable medias, like USB drives and SD cards. For example, you can mount a USB at /media/usb1 and another one at /media/usb2. Mounting is typically done automatically by the system when you plug it in.\n/mnt: Mounts that are not so easily removable, like a hard drive, or a network drive. And unlike /media, mounting and unmounting is not automatic. On WSL, this typically has just one important thing: /mnt/c.\n/srv: Static files that are served out. /srv/http would be for static websites, /srv/ftp for an FTP server. It is usually used only on webservers, not an end-user machine like your laptop."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#environment-variables",
    "href": "sketches/posts/linux-notes/index.html#environment-variables",
    "title": "Notes on Using Linux",
    "section": "Environment variables",
    "text": "Environment variables\n\nHow to control them\nUse echo $X to see what the current value of X is.\nFor current session, use export like export EDITOR=nano.\nFor all sessions, add to your .bashrc or .profile. If you just want to add it to the end (not recommended, as you can end up with an archeological tell), you can do one-liner like echo \"export EDITOR=nano\" &gt;&gt; ~/.bashrc.\n\n\nCommon ones\n\nPATH: path to binaries. See Section 2.\nEDITOR: default editor.\nSHELL: default shell.\nHOME: home directory.\nUSER: current user.\nPS1: current prompt (just try echo $PS1 if it doesn’t make sense).\n\nYou can change prompts in a rather arcane language. For example, try this one:\nexport PS1=\"\\[\\e]0;\\w\\a\\]\\n\\[\\e[32m\\]\\u@\\h \\[\\e[33m\\]\\w\\[\\e[0m\\]\\n\\\\$ \""
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#cron-jobs",
    "href": "sketches/posts/linux-notes/index.html#cron-jobs",
    "title": "Notes on Using Linux",
    "section": "Cron jobs",
    "text": "Cron jobs\nCron jobs are scheduled tasks that run periodically. For scheduling one-off tasks, use the at command. The name came from “Cronos”, the name of the Greek god of time. A good reference is Newbie: Intro to cron.\n\nQuick reference\nThe cron job syntax is as follows (See Crontab.guru - The cron schedule expression editor):\n# ┌───────────── minute (0–59)\n# │ ┌───────────── hour (0–23)\n# │ │ ┌───────────── day of the month (1–31)\n# │ │ │ ┌───────────── month (1–12)\n# │ │ │ │ ┌───────────── day of the week (0–6) (Sunday to Saturday)\n# │ │ │ │ │                                   \n# │ │ │ │ │\n# │ │ │ │ │\n# * * * * * &lt;command to execute&gt;\n\n* * * * * &lt;once a minute&gt;\n0 * * * * &lt;once an hour at 0-th minute&gt;\n0 0 * * * &lt;once a day at midnight&gt;\n0 0 1 * * &lt;once a month at the midnight of the 1-th day&gt;\n0 0 1 1 * &lt;once a year at the midnight of January 1&gt;\n* * * * 0 &lt;once a minute every Sunday&gt;\nCheck that cron service is running by systemctl list-unit-files --type=service | grep \"cron\"\nList cron jobs by crontab -l.\n\n\nCreating a Cron Job\nFor this example, we create a job that runs every 20 seconds.\n\nOpen Crontab: Open your crontab file by typing crontab -e in your terminal. This command opens the crontab file for the current user in the default text editor.\nWrite Cron Job: Standard cron jobs can’t run in a smaller granularity than a minute. For a once-per-20-seconds job, you’ll need to use a workaround.\nAdd the following lines to your crontab:\n\n * * * * * /path/to/script.sh\n * * * * * sleep 20; ~/cronjobs/script.sh\n * * * * * sleep 40; ~/cronjobs/script.sh\n\nScript Content: Create script.sh, have the following content:\n\n#!/bin/bash\nCRON_MESSAGE=\"Some message\"\necho \"The cron message is: $CRON_MESSAGE\"\nThen save and close the file, and chmod +x ~/cronjobs/script.sh to make it executable.\n\n\nBest Practices\n\nLocation: Store scripts in a dedicated directory, such as ~/cronjobs, for better organization.\nScript Naming: Use meaningful names for your scripts for easier identification.\nLogging: Implement logging within your scripts to capture output and errors for later review. It’s good practice to use /var/log/cron for logging.\n\n\n\nCron environment variables\nCron jobs run in a minimal environment, so any environment variable, like CRON_MESSAGE, is not accessible by the cron script. Instead, you have a few choices:\n\nPut it directly in the crontab file:\n\nCRON_MESSAGE=\"Some message\"\n* * * * * /path/to/script.sh\n\nPut it directly in the script:\n\n#!/bin/bash\nexport CRON_MESSAGE=\"Some message\"\n# rest of the script follows\n\nIf the variable is defined in an external file (like ~/.bashrc, ~/.profile, or a custom configuration file), you can source that file at the beginning of your script:\n\n#!/bin/bash\nsource /path/to/environment_variables_file\n# rest of the script follows\n\n\nTroubleshooting\n\nCheck Permissions: Ensure your script is executable and the cron daemon has the necessary permissions to run it.\nLogs: Check /var/log/cron or relevant logs for errors.\n\nIf you’re using WSL, ensure that the cron service is running since it doesn’t start by default. Use sudo service cron start. You can configure ~/.bashrc by adding the following line: sudo service cron start, but it would make you enter the password at every login.\nAlternatively, enable systemd as described at Section 6.1."
  },
  {
    "objectID": "sketches/posts/linux-notes/index.html#how-to-wsl",
    "href": "sketches/posts/linux-notes/index.html#how-to-wsl",
    "title": "Notes on Using Linux",
    "section": "How to WSL",
    "text": "How to WSL\n\ninit vs systemd\nEvery Linux starts its first process with some root process. The init is the traditional and simpler one, and systemd is more modern and advanced one.\nWSL by default starts with init instead of systemd, perhaps to save time and compute. This makes things annoying for some users. You can check by ps -p 1 -o comm and see what it returns.\nTo enable systemd, enter in your /etc/wsl.conf with:\n[boot]\nsystemd=true\nor just use cat \"[boot]\\nsystemd=true\" &gt;&gt; /etc/wsl.conf."
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html",
    "title": "Phase-space quantum mechanics",
    "section": "",
    "text": "Warning: this note is particularly messy. I’m not sure if it is useful for anyone, but I’m putting it out there in case it is useful."
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html#quantization",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html#quantization",
    "title": "Phase-space quantum mechanics",
    "section": "Quantization",
    "text": "Quantization\n\nWhat is quantization?\nFirst, some definitions:\n\na classical physics theory consists of several parts:\n\nA smooth manifold with some extra structure on it. This is the arena on which physics happens. For example, the phase space is a smooth \\(2n\\)-manifold with something special on it, called a Poisson bracket. This manifold is called the “underlying geometry”.\nSome functions defined on the manifold. These functions are typically real-valued, but can also be tensor-valued, such as in continuum mechanics. They are called the “observables”.\nA set of partial differential equation on that smooth manifold. These are typically called the “equations of motion”. This set of equations must be compatible with the “extra structure” of the underlying geometry.\nA dictionary with two columns. On the left column are the mathematical structures defined above. On the right column are the physically real structures that can be seen in a lab.\n\na quantum physics theory is less clear, because there are competing but equivalent conventions. In the standard convention (von Neumann, 1932), it is similar to the above, except that instead of functions defined on the manifold, we have linear operators defined on a Hilbert space.\nto quantize a classical physics theory is to produce a quantum physics theory containing a free constant \\(\\hbar &gt; 0\\), such that the theory somehow reduces to the classical physics theory at the \\(\\hbar \\downarrow 0\\) limit.\na quantization scheme is an algorithm for quantizing.\n\nThere are many quantization schemes, because of several impossibility theorems telling you that there is no perfect quantization of the classical physics theory of even a single particle moving on a straight line. I mean, is it really such a big surprise? If there is a perfect quantization, would it still be a surprising quantum mechanics…?\nThe simplest examples are found by canonical quantization, which is not a single quantization scheme, but rather, a style for quantizing. It typically involves explicitly computing with a fixed set of coordinates \\(p_i, q_i\\) in phase space. This style is the one taught to undergrads.\nA more abstract style is geometric quantization, which typically involves manipulating phase space in the coordinate-free language of modern differential geometry. It is further abstracted to deformation quantization, which typically involves abstract algebra with many brackets and stars. Typically they start with a geometric quantization, and abstract out the axioms and properties, then start doing abstract algebra on it. Think of it as how Hilbert started with Euclid’s geometry and then went on to say\n\nOne must be able to say at all times–instead of points, straight lines, and planes–tables, chairs, and beer mugs.\n\nAs a side note, a field quantization is a quantization where the input classical physics theory has infinite-dimensional state space.\nSee (Gustafson and Sigal 2020, chap. 4) for details.\n\nTable lightly edited from (Gustafson and Sigal 2020, 35).\n\n\n\n\n\n\n\nObject\nCM\nQM\n\n\n\n\nstate space\n\\(\\mathbb{R}^n \\times \\mathbb{R}^n\\) with Poisson bracket\n\\(L^2 (\\mathbb{R}^n)\\) and commutator\n\n\nevolution of state\npath in phase space\npath in \\(L^2 (\\mathbb{R}^3)\\)\n\n\nobservable\nreal function on state space\nself-adjoint operator on state space\n\n\nresult of measuring observable\ndeterministic\nprobabilistic\n\n\nobject determining dynamics\nHamiltonian function \\(H(t, q, p)\\)\nHamiltonian (Schrödinger) operator \\(\\hat H(t)\\)\n\n\ncanonical coordinates\nfunctions \\(q_{1:n}, p_{1:n}\\)\noperators \\(\\hat q_{1:n}, \\hat p_{1:n}\\)\n\n\n\n\n\nCanonical\nThe most commonly used and widely taught quantization is the canonical quantization. In this, the classical physics is the physics of a single particle in \\(\\mathbb{R}^n\\). Its phase space is \\(\\mathbb{R}^{2n}\\), with canonical coordinates \\((q_{1:n}, p_{1:n})\\), and the standard symplectic structure. This is the one taught to every student in a first course in Hamiltonian mechanics. Classical observables are polynomial functions in \\(q_{1:n}, p_{1:n}\\).\nIn canonical quantization, each quantized classical observable is a map of type \\(f \\mapsto \\hat f\\), where \\(f\\) is a function of type \\(f: \\mathbb{R}^{2n} \\to \\mathbb{R}\\), and \\(\\hat f\\) is a linear operator of some kind. The defining feature of canonical quantization is that, whatever \\(\\hat f\\) should be, the canonical commutation relations are satisfied:\n\\[\n\\begin{aligned}{}\n    [\\hat q_j, \\hat p_k] &= i\\hbar \\delta_{jk}, \\\\\n    [\\hat q_j, \\hat q_k] &= 0, \\\\\n    [\\hat p_j, \\hat p_k] &= 0.\n\\end{aligned}\n\\]\nThe two most famous canonical quantizations are:\n\nHeisenberg matrix quantization: \\(\\hat f\\) is a Hermitian square matrix with infinitely rows and columns.\nSchrödinger wavefunction quantization: \\(\\hat f\\) is a self-adjoint operator on \\(L^2(\\mathbb{R}^n)\\), the L2 space of \\(\\mathbb{R}^n\\) with Lebesgue measure.\n\n\nHeisenberg matrix quantization\nIn 1925, Heisenberg was thinking deeply about problems with atomic spectra, and suddenly got a serious attack of pollen allergy. When aspirin and cocaine did not help, he retreated to Heligoland where nothing grew. There, his mental gestation bore fruit:\n\nOne evening I reached the point where I was ready to determine the individual terms in the energy table, or, as we put it today, in the energy matrix, by what would now be considered an extremely clumsy series of calculations. When the first terms seemed to accord with the energy principle, I became rather excited, and I began to make countless arithmetical errors. As a result, it was almost three o’clock in the morning before the final result of my computations lay before me. The energy principle had held for all terms, and I could no longer doubt the mathematical consistency and coherence of the kind of quantum mechanics to which my calculations pointed. At first, I was deeply alarmed. I had the feeling that, through the surface of atomic phenomena, I was looking at a strangely beautiful interior, and I felt almost giddy at the thought that I now had to probe this wealth of mathematical structures nature had so generously spread out before me. I was far too excited to sleep, and so, as a new day dawned, I made for the southern tip of the island, where I had been longing to climb a rock jutting out into the sea. I now did so without too much trouble, and waited for the sun to rise.\n(Heisenberg 1971, 61)\n\nThis was the Heisenberg matrix quantization. Consider the following matrices:\n\\[\nP=\\begin{bmatrix}\n0 & 1 & 0 & 0 & \\cdots \\\\\n0 & 0 & 2 & 0 & \\cdots \\\\\n0 & 0 & 0 & 3 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}, \\quad Q=\\begin{bmatrix}\n0 & 0 & 0 & 0 & \\ldots \\\\\n1 & 0 & 0 & 0 & \\cdots \\\\\n0 & 1 & 0 & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\]\nThey satisfy \\([Q, P] = -I, [Q, P^\\dagger] = 0\\). Thus, by standard manipulations with the commutator bracket, we find that\n\\[\n[zQ + z^* Q^\\dagger, wP + w^* P^\\dagger] = (-zw + z^*w^*) I\n\\]\nThus, if we set \\(z = w = \\sqrt{-i\\hbar/2}\\), we would have two Hermitian matrices \\(zQ + z^* Q^\\dagger, wP + w^* P^\\dagger\\) satisfying the canonical commutator relations. This is the simplest example of Heisenberg matrix quantization.\n\n\nSchrödinger wavefunction quantization\nIn a series of papers, Schrödinger in 1926 proposed the wavefunction quantization for a single particle (among other things). It is defined as follows:\n\\[\n\\widehat{q_j} \\psi=q_j \\psi, \\quad \\widehat{p_j} \\psi=-i\\hbar \\frac{\\partial \\psi}{\\partial q_j} \\quad \\text { for } \\psi \\in L^2(\\mathbb{R}^n)\n\\]\nwhere \\(\\hbar\\) is a positive constant.\n\n\nUniqueness\nHeisenberg in 1925 proposed the matrix quantization, and Schrödinger in 1926 proposed the wavefunction quantization, and there was a brief confusion as to which is the “correct” one, but it was quickly shown to be equivalent. So both are currently called “canonical quantization”.\nIndeed, it turns out under some reasonable assumptions, there exists only one canonical quantization. Such theorems are typically called Stone–von Neumann theorems. The following is the most famous:\n\nTheorem 1 ((Neumann 1931)) Assuming that\n\n\\(\\hat p_k, \\hat q_j\\) are self-adjoint operators on a Hilbert space.\n\\([\\hat q_j, \\hat p_k] = i\\hbar \\delta_{jk}\\).\nThe operators are irreducible over the Hilbert space.\n\nThen there exists a unique representation up to unitary equivalence. In particular, Schrödinger wavefunction quantization and Heisenberg matrix quantization are equivalent.\n\n\n\n\nAxiomatic\nWe can write down some axioms for quantization. First, some definitions:\n\nA classical observable is a function of type \\(f: \\mathbb{R}^{n} \\to \\mathbb{C}\\). Here, \\(\\mathbb{R}^n\\) should be thought of as the state space.\nA wavefunction is a nice1 function of type \\(\\psi: \\mathbb{R}^n \\to \\mathbb{C}\\).\nA quantum observable is a linear operator on wavefunctions.\nA quantization is a map from classical observables \\(f\\) to quantum observables \\(\\hat f\\).\n\n1 I’m pretty sure that being analytic, i.e. having a globally converging Taylor expansion, is nice enough.With these definitions, we have the axioms of quantization:\n\nA1 (linearity): the correspondence \\(f \\mapsto \\hat f\\) is linear;\nA2 (von Neumann rule): for any nice function \\(\\phi: \\mathbb{R} \\rightarrow \\mathbb{R}\\) for which \\(\\widehat{\\phi \\circ f}\\) and \\(\\phi\\left(\\widehat f\\right)\\) are well-defined, \\(\\widehat{\\phi \\circ f}=\\phi\\left(\\widehat f\\right)\\); Here, “Nice” usually means polynomial, or analytic.\nA3 (Schrödinger): \\(\\hat q_j = q_j, \\hat p_j = -i\\hbar \\partial_{q_j}\\).\nA4 (bracket): \\([\\hat f, \\hat g] = i\\hbar \\widehat{\\{f, g\\}}\\), where \\(\\{f, g\\} = \\sum_i (\\partial_{q_i}f \\partial_{p_i} g - \\partial_{p_i}f \\partial_{q_i} g)\\).\nA5 (identity): \\(\\hat 1 = 1\\).\n\nFor example, it is clear that either A3 or A4 implies the canonical commutator relations \\([\\hat q, \\hat p] = i \\hbar\\).\nIt is not obvious, but If \\(X\\) is self-adjoint linear operator that commutes with all \\(q_j, -i\\hbar \\partial_{q_j}\\) then it is \\(aI\\) for some \\(a \\in \\mathbb{C}\\). This result is best shown by representation theory, as a corollary for showing that the representation is irreducible.\nUnfortunately, it turns out that most combinations of the axioms are impossible to satisfy under minor assumptions:\n\nTheorem 2 (Impossibility theorems) If all polynomials of \\(p, q\\) of degree \\(\\leq 4\\) must be quantized, and we allow all linear and quadratic functions in the von Neumann rule, then\n\nthe axioms A123 are incompatible.\nthe axioms A134 are incompatible.\nthe axioms A24 are incompatible.\n\n\nIn the following proofs, let \\(c = -i\\hbar\\). The statement of the theorem, as well as the proofs, came from (Ali and Engliš 2005).\n\n\n\n\n\n\nProof (A123)\n\n\n\n\n\nBy A1 and A2, we can calculate \\[\n\\widehat{pq} = \\frac 12 [(P+Q)^2 - P^2 - Q^2] = \\frac{PQ+QP}{2}\n\\]\nSimilarly, we can calculate \\(\\widehat{p^2q^2}\\) in two ways. Their difference is\n\\[\n\\frac 14 (PQPQ + QPQP + PQQP + QPPQ - PPQQ - PPQQ - QQPP - QQPP)\n\\]\nUsing \\([Q,P] = i \\hbar\\) to simplify, we obtain the difference of \\(\\frac 34 \\hbar^2 \\neq 0\\).\n\n\n\n\n\n\n\n\n\nProof (A134)\n\n\n\n\n\nSince \\(\\{p_iq_i, p_i\\} = p_i, \\{p_iq_i, q_i\\} = -q_i\\), let \\(s := \\sum_i p_iq_i\\), we have by A4,\n\\[\n[\\hat s, P_i] = -cP_i, \\quad [\\hat s, Q_i] = cQ_i\n\\]\nBy A3, \\(\\frac{\\sum_i P_iQ_i+Q_iP_i}{2}\\) also satisfies the above two equations. Thus, \\[\n\\hat s = \\frac 12 \\sum_i (P_iQ_i+Q_iP_i) + zI\n\\]\nfor some constant \\(z\\).\nSimilarly, for each \\(m = 1, 2, 3, 4\\), \\[\n\\widehat{\\sum_i q_i^m} = \\sum_i Q_i^m + z_m I\n\\]\nfor some constant \\(z_m\\).\nBy A4, \\[\n[\\widehat{\\sum_i p_iq_i}, \\widehat{\\sum_j q_j^m}] = -c \\widehat{\\{\\sum_i p_i q_i, \\sum_j q_j^m\\}} = cm \\widehat{\\sum_j q_j^m}\n\\]\nPreviously, we have already found \\(\\widehat{\\sum_i p_iq_i}, \\widehat{\\sum_j q_j^m}\\) up to an additive identity, so plugging them back in, and using A4, we obtain \\[\n[\\widehat{\\sum_i p_iq_i}, \\widehat{\\sum_j q_j^m}] = cm \\sum_j Q_j^m\n\\]\nThus, \\(\\widehat{\\sum_j q_j^m} = \\sum_j Q_j^m\\). Similarly, \\(\\widehat{\\sum_j p_j^m} = \\sum_j P_j^m\\).\nSince \\(\\{\\sum_i p_i^2 , \\sum_j q_j^3\\} = -6 \\sum_i q_i^2 p_i\\), \\[\n6c \\widehat{\\sum_i q_i^2 p_i} \\stackrel{A4}{=} [\\widehat{\\sum_i p_i^2} , \\widehat{\\sum_j q_j^3}] = [\\sum_i P_i^2, \\sum_j Q_j^3] \\stackrel{A3}{=} 3c\\sum_i(P_i Q_i^2 + Q_i^2 P_i)\n\\]\nThus \\(\\widehat{\\sum_i q_i^2 p_i} = \\frac 12 \\sum_i(P_i Q_i^2 + Q_i^2 P_i)\\). Similarly for \\(\\widehat{\\sum_i q_i p_i^2} = \\frac 12 \\sum_i(P_i^2 Q_i + Q_i P_i^2)\\)\nSince \\(\\{\\sum_i p_i^3, \\sum_j q_j^3\\} = -9\\sum_i p_i^2 q_i^2\\), \\[\n\\widehat{\\sum_i p_i^2 q_i^2 } = \\frac{1}{9c} [\\sum_i P_i^3, \\sum_j Q_j^3]  = \\sum_i  (Q_i^2 P_i^2+2 c Q_i P_i+\\frac{2}{3} c^2)\n\\]\nSince \\(\\{\\sum_i p_i^2q_i, \\sum_j p_jq_j^2\\} = -3 \\sum_i p_i^2q_i^2\\), \\[\n\\widehat{\\sum_i p_i^2 q_i^2 } = \\frac{1}{3c} [\\frac 12 \\sum_i(P_i^2 Q_i + Q_i P_i^2), \\frac 12 \\sum_i(P_i Q_i^2 + Q_i^2 P_i)]  = \\sum_i (Q_i^2 P_i^2+2 c Q_i P_i+\\frac{1}{3} c^2)\n\\]\nThey differ by \\(\\frac n3 c^2\\), contradiction.\n\n\n\n\n\n\n\n\n\nProof (A24)\n\n\n\n\n\nThis proof came from (Engliš 2002).\nThough we no longer have homogeneity, we still have \\(\\widehat{t f} = t \\hat f\\) for constant \\(t \\in \\mathbb C\\), by A2.\n\\(\\{p^k, q^m\\} = -km p^{k-1}q^{m-1}\\) for all \\(k, m \\geq 1\\), so by A4, \\(\\widehat{p^{k-1}q^{m-1}} = \\frac{1}{km c} [ P^k, Q^m]\\).\nBy A4, \\(PQ- QP = c\\). Then, by combinatorics,\n\\[\nP^k Q^m = \\sum_{l=0}^k \\binom{k}{l} \\frac{m!}{(m-k+l)!} c^{k-l} Q^{m-k+l}P^l\n\\]\nThis can be derived as a consequence of the general Leibniz rule. If we temporarily assume A3, then\n\\[\nP^k Q^m = c^k \\partial^k(q^m \\cdot) = c^k \\sum_{l=0}^k \\binom{k}{l}\\partial^{k-l}(q^m) \\partial^l(\\cdot) = \\sum_{l=0}^k \\binom{k}{l} \\frac{m!}{(m-k+l)!} c^{k-l} Q^{m-k+l}P^l\n\\]\nThus, \\[\n[P^k, Q^m] = \\sum_{l=0}^{k-1} \\binom{k}{l} \\frac{m!}{(m-k+l)!} c^{k-l} Q^{m-k+l}P^l = \\sum_{l=1}^k\\binom{k}{l} \\frac{m! c^l}{(m-l)!} Q^{m-l} P^{k-l}\n\\]\nThus, \\[\n\\widehat{pq} = \\frac{[P^2,Q^2]}{4c} = \\frac{2c^2 + 4cQP}{4c} = QP + c/2\n\\]\n\\[\n\\widehat{pq}^2 \\stackrel{A2}{=} (QP + c/2)^2 = QQPP + 2cQP + c^2/4\n\\]\n\\[\n\\widehat{ppqq} = \\frac{[P^3,Q^3]}{9c} = \\frac{6 c^3+18 c^2 Q P+9 c Q^2 P^2}{9 c} = Q^2P^2 + 2cQP + 2c^2/3\n\\]\n\\[\n\\widehat{ppqq} - \\widehat{pq}^2 = \\frac{5}{12}c^2 \\neq 0\n\\]\n\n\n\nIn summary, any 3 of A1234 are incompatible. If we only take 2 out of them, then\n\n12: Not quantum enough, since \\(\\hbar\\) does not appear at all. Indeed, we can set all operators to be commutative. The key property of non-commutativity is not enforced.\n13: Trivial, since we can arbitrarily assign \\(\\widehat{p^m q^n}\\) for the cases other than \\((m,n) = (1,0), (0,1)\\), then linearly extend.\n14: This is the van Hove prequantization.\n23: Possibly interesting.\n24: Impossible.\n34: Possibly interesting.\n\nBecause of the impossibility theorems, something must be given up. The two common ways of giving up are:\n\nGeometric quantization: do not quantize most of the classical observables, and optionally give up von Neumann axiom (A2).\nDeformation quantization: give up von Neumann axiom (A2), and relax the bracket axiom (A4) to only hold in the limit: \\([\\hat f, \\hat g] = i\\hbar \\widehat{\\{f, g\\}} + o(\\hbar)\\)."
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html#pseudo-differential-operator",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html#pseudo-differential-operator",
    "title": "Phase-space quantum mechanics",
    "section": "Pseudo-differential operator",
    "text": "Pseudo-differential operator\nThis section is skippable.\nA differential operator \\(P\\) in position space become multiplication by a polynomial \\(P(k)\\) in momentum space: \\[\nP[u](x) = \\frac{1}{(2\\pi)^{n}}\\int dk e^{i \\braket{k, x}} P(k) \\hat u(k)\n\\]\nIf we allow for multiplication by more generic functions, and allow for the interaction between position and momentum, we get pseudo-differential operator: \\[\nP[u](x) = \\frac{1}{(2\\pi)^{n}}\\int dk e^{i \\braket{k, x}} P(x,k) \\hat u(k)\n\\]\nDER. integral/differential operator\nConsider the differential operator \\[\nP(D) = \\sum_{a_1, \\dots, a_n} c_{a_1, \\dots, a_n} (-i\\partial_1)^{a_1} \\cdots (-i\\partial_n)^{a_n}\n\\]\nApplying \\(P\\) to a function \\(u\\) can be done in Fourier space: \\[\nP[u](x) = \\frac{1}{(2\\pi)^n} \\int d^n k d^n y e^{i \\braket{k, x-y}} u(y)  \\sum_{a_1, \\dots, a_n} c_{a_1, \\dots, a_n} k_1^{a_1} \\cdots k_n^{a_n}\n\\]\nMore succinctly, let \\(\\alpha\\) be the multiindex, and \\(D^\\alpha = (-i\\partial_1)^{a_1} \\cdots (-i\\partial_n)^{a_n}\\), then we have \\[\nP(D) = \\sum_\\alpha c_\\alpha D^\\alpha\n\\]\nand\n\\[\nP[u](x) = \\frac{1}{(2\\pi)^n} \\int d^n k  e^{i \\braket{k, x}} \\hat u(k) P(k) = \\frac{1}{(2\\pi)^n} \\int d^n k d^n y e^{i \\braket{k, x-y}} u(y) P(k)\n\\]\nTo solve the differential equation \\(P[u] = f\\), it needs only inversion\n\\[\nu(x) = \\frac{1}{(2\\pi)^n} \\int d^n k \\frac{\\hat f(k)}{P(k)} e^{i\\braket{k,x}} = \\frac{1}{(2\\pi)^n} \\int d^n k d^n y e^{i \\braket{k, x-y}} \\frac{ f(y)}{P(k)}\n\\]\nDEF. pseudo-differential operator\nA pseudo-differential operator \\(P\\) on \\(\\mathbb{R}^n\\) is an operator defined by\n\\[\nP[u](x) := \\frac{1}{(2\\pi)^n} \\int d^n k  e^{i \\braket{k, x}} \\hat u(k) P(x, k) = \\frac{1}{(2\\pi)^n} \\int d^n k d^n y e^{i \\braket{k, x-y}} u(y) P(x, k)\n\\]\nwhere \\(P(x, k)\\) is a function of type \\(\\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb C\\).\nGiven a pseudo-differential operator, it can be inverted as before to solve pseudo-differential equations \\(P[u] = f\\)\n\\[\nu(x) = \\frac{1}{(2\\pi)^n} \\int d^n k \\frac{\\hat f(k)}{P(k, x)} e^{i\\braket{k,x}} = \\frac{1}{(2\\pi)^n} \\int d^n k d^n y e^{i \\braket{k, x-y}} \\frac{ f(y)}{P(k, x)}\n\\]"
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html#star-product",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html#star-product",
    "title": "Phase-space quantum mechanics",
    "section": "Star product",
    "text": "Star product\nZachos, Cosmas. “A survey of star product geometry.” Integrable Hierarchies and Modern Physical Theories. Dordrecht: Springer Netherlands, 2001. 423-435.\nOvsienko, V., and Claude Roger. “Deformations of Poisson brackets and extensions of Lie algebras of contact vector fields.” Russian Mathematical Surveys 47.6 (1992): 135.\nSome notes on Weyl quantisation | What’s new\n\nSetup\nOn a \\(2n\\)-dimensional symplectic structure, we fix some \\(2n\\) canonical coordinates \\(q_{1:n}, p_{1:n}\\). These two \\(n\\)-tuples are written as one big \\(2n\\)-tuple \\(\\xi_{1:2n}\\) by \\(\\xi_{1:n} = q_{1:n}, \\xi_{n+1:2n} = p_{1:n}\\).\nThe symplectic matrices are written as\n\\[\\omega_{ij} = \\begin{bmatrix}\n  & q_{1:n} & p_{1:n} \\\\\n  q_{1:n} & 0 & -I \\\\\n  p_{1:n} & I & 0\n  \\end{bmatrix}\\]\n\\[\\omega^{ij} = \\begin{bmatrix}\n  & q_{1:n} & p_{1:n} \\\\\n  q_{1:n} & 0 & I \\\\\n  p_{1:n} & -I & 0\n  \\end{bmatrix}\\]\nWe use the Einstein summation convention: If an index is used twice in an expression, then it is summed over. It is not necessary for the index to appear once in the superscript and once in the subscript.\nThe Poisson bracket is defined by\n\\[\\{f, g\\} = \\sum_i (\\partial_{q_i}f \\partial_{p_i} g - \\partial_{p_i}f \\partial_{q_i} g)\\]\nI know there are two conventions for the Poisson bracket, but this is my convention and I’m sticking to it. I think this is the more common convention in the literature.\nBecause I don’t want to type things out, if two vectors \\(\\tilde q, q\\) are dot-producted like \\(\\tilde q \\cdot q\\), I would just write \\(\\tilde q q\\). It saves me a little hassle, and actually rarely leads to problems!\nThe Fourier transform is defined by\n\\[\n\\tilde f(\\tilde q) \\int_{\\mathbb{R}^n} f(q) e^{-i (\\tilde q q)}d^n q,\\quad f(q) = \\frac{1}{(2\\pi)^n}\\int_{\\mathbb{R}^n} \\tilde f(\\tilde q) e^{+i (\\tilde q q)} d^n \\tilde q\n\\]\nI know there are like, 4 conventions at least for the Fourier transform. So much hassle. The Dirac delta function, in this convention, becomes\n\\[\\delta^n( q - q_0 ) = \\frac{1}{(2\\pi)^n} \\int_{\\mathbb{R}^n} e^{-i (\\tilde q (q - q_0))}d^n \\tilde q\\]\nThe functions \\(f, g\\) are formal power series with complex coefficients in the variables \\(q_1, \\dots, q_n, p_1, \\dots, p_n, t\\). That is, \\(f, g \\in S := \\mathbb C[[q^{1:n}, p^{1:n}, t]]\\). The following is an example:\n\\[\n\\sum_{k=0}^\\infty (k! q_1^k + k^2 p_2^k + t^{2k})\n\\]\nThe higher Poisson brackets are \\(\\{\\cdot, \\cdot\\}_k\\), where \\(k = 0, 1, 2, \\dots\\). It will be shown later that \\(\\{f, g\\}_0 = fg, \\{f, g\\}_1 = \\{f, g\\}\\).\nThe star product2 \\(f\\star_t g\\) is a binary operation, where \\(t\\) is a complex number. For each choice of \\(t \\in \\mathbb{C}\\) we have a different star product. It has many definitions, but we will define it as an exponential sum of the higher Poisson brackets:\n2 Other names include: star product, Moyal star product, Weyl–Groenewold product, phase-space star product, and basically any combination thereof.\\[f\\star_t g = \\sum_{k=0}^\\infty \\frac{t^k}{k!} \\{f, g\\}_k\\]\nand then show the equivalence of the definitions.\nThe Moyal bracket3 is always defined as (Yes yes there are many definitions, you know the drill…):\n3 Also called the sine bracket.\\[\\{\\{f,g\\}\\}_t := \\frac{f \\star_t g - g \\star_t f}{2t}\\]\nIf \\(t\\) is not written explicitly, then it is by default \\(t = i\\hbar /2\\). In that case, we have\n\\[\\{\\{f,g\\}\\} := \\frac{f \\star g - g \\star f}{i\\hbar}\\]\n\n\n\n\n\n\nToo many brackets\n\n\n\n\n\nBTW, I think those people just really love the brackets. I have found the Lie bracket, the cosine bracket, the algebraic bracket, the Nijenhuis–Richardson bracket, the Schouten–Nijenhuis bracket, the Frölicher–Nijenhuis bracket, the Nambu bracket, the Gerstenhaber bracket, the Lagrange bracket, the Dirac bracket, the odd Poisson bracket, and so on. I think I’ll take my Poisson, my Lie, and my Moyal, and say good-day to the rest.\n*Sing, Muses, who dwell in the halls of Olympus,\nYou are goddesses, you are everywhere, you know all things --\nThat govern the fields, the rings, the complex conjugations.\nWe who entangle blindly, we know nothing of --\nhow does Nambu commute? Who slakes the Leibniz?\nThe mass of theorems I could never tally, never name,\nnot even if I had ten tongues and ten mouths,\na tireless voice and the heart inside me bronze,\nnever unless you Muses of Olympus, half-sister of Athena\nwhose love is wisdom and war, sing, sing in memory\nall who gathered under here. Now I can only tell\nthe shapes of the brackets, the brackets in all their numbers!*\nI actually got some mildly amusing further text out of grok-2-2024-08-13:\nFirst, I speak of the Lie bracket, swift as Hermes in flight,\nBorn from the need to express motion in the abstract,\nIts name echoes through the theorems, a warrior on the field of manifolds,\nWith each commutator, it reveals the hidden symmetries of space.\n...\nThe Dirac bracket, a healer on the battlefield of constraints,\nMending the broken symmetries, restoring order,\nIn quantum realms and classical, it adjusts the rules,\nA physician to the laws, making them whole again.\nThe prompt I used is: “Write a mock-epic in exactly the same style as Homer’s Catalog of Ships. To get you started, here is how I started. Make sure to avoid rhyming, and focus on the meter instead.”.\n\n\n\n\n\nConstructions\n\nAxiomatic\nAxioms of deformation quantization, to be proved below.\n\n\\(f\\star 1 = 1 \\star f = f\\)\n\\((f \\star g)^* = (g^*) \\star (f^*)\\)\n\\(\\{f, g\\}_0 = fg\\)\n(deformed canonical quantization) \\(\\{\\{f, g\\}\\}_t = \\{f, g\\} + O(t^2)\\).\n\n\n\nDirect\nThe “Poisson biderivative” operator\n\\[\\bar P = \\sum_{i=1}^n \\left(\\overleftarrow{\\partial_{q_i}} \\cdot \\overrightarrow{\\partial_{p_i}}-\\overleftarrow{\\partial_{p_i}} \\cdot \\overrightarrow{\\partial_{q_i}}\\right) = \\omega^{ij} \\overleftarrow{\\partial_{\\xi_i}} \\cdot \\overrightarrow{\\partial_{\\xi_j}} \\]\n\\[\n\\{f, g\\}_k=f \\bar P^k g\n\\]\n\n\nTensor\nDefine two operators of type \\(S \\otimes S \\to S \\otimes S\\):\n\\[\n\\begin{aligned}\n\\Omega(f \\otimes g) &= \\sum_{k=1}^n \\partial_{q^k} f \\otimes \\partial_{p^k} g - \\partial_{p^k} f  \\otimes \\partial_{q^k} g = \\omega^{ij} \\partial_{\\xi_i} f  \\otimes \\partial_{\\xi_j} g  \\\\\nTr(f \\otimes g) &= fg\n\\end{aligned}\n\\]\n\\[\\{f, g\\}_k = (\\operatorname{Tr}\\circ \\Omega^k)( f \\otimes g)\\]\nIn classical invariant theory, these are called transvectants, or hyper-Jacobians. (Ovsienko, Roger, 1992)\n\n\nSymplectic\n\\(\\Omega = \\omega^{ij} \\partial_i \\wedge \\partial_j\\) is the dual bivector to the symplectic form \\(\\omega_{ij} d\\xi^i \\wedge d\\xi^j\\).\n\n\nWeyl transform\nLet the Weyl transform be \\[\\hat f := \\frac{1}{(2\\pi)^{2n}} \\iint_{\\mathbb{R}^{2n}} d^n \\tilde q d^n \\tilde p \\; \\tilde f(\\tilde q, \\tilde p) e^{i  (\\tilde q_j \\hat q_j + \\tilde p_j \\hat p_j)}\\]\nwhere \\(\\hat q_j = (q_j \\cdot), \\hat p_j = -i\\hbar\\partial_{q_j}\\) are operators satisfying the canonical commutator relations.\nThen, the star product is uniquely defined by \\(\\widehat{f \\star g} = \\hat f \\hat g\\).\n\n\nFourier analysis\n\\[\nf \\star g= \\frac{1}{(\\pi \\hbar)^{2n}} \\int_{\\mathbb{R}^{4n}} d^{2n} \\zeta d^{2n} \\zeta' f(\\xi + \\zeta) g(\\xi  + \\zeta') e^{\\frac{2i}{\\hbar} \\omega^{jk} \\zeta_j \\zeta_k'}\n\\]\n\n\n\nBasic properties\n\nProposition 1  \n\n\\(\\{f, g\\}_k = (-1)^k \\{g, f\\}_k\\)\n\\(\\{\\cdot, \\cdot \\}_k\\) is invariant under symplectic transforms.\n\nProposition 2 (Multinomial expansion of the bracket) When \\(n=1\\), direct expansion shows that it has the form of a binomial expansion: \\[\n\\begin{aligned}\n\\{f, g\\}_0 &= fg \\\\\n\\{f, g\\}_1 &= (f_{q} g_{p} - f_{p} g_{q}) = \\{f, g\\} \\\\\n\\{f, g\\}_2 &= (f_{qq} g_{pp} - 2 f_{pq} g_{pq} + f_{pp} g_{qq}) \\\\\n\\{f, g\\}_3 &= (f_{qqq} g_{ppp} - 3 f_{qqp} g_{qpp} + 3 f_{qpp}g_{qqp} - f_{ppp} g_{qqq}) \\\\\n& \\cdots \\\\\n\\{f, g\\}_k &=  \\sum_{\\alpha + \\beta = k} (-1)^{\\alpha} \\binom{k}{\\alpha} (\\partial_{q}^{\\beta} \\partial_{p}^{\\alpha} f)(\\partial_{p}^{\\beta} \\partial_{q}^{\\alpha} g)\n\\end{aligned}\n\\]\nAnd in general, it is a multinomial expansion. \\[\n\\{f, g\\}_k = \\sum_{|\\alpha| + |\\beta| = k} (-1)^{|\\alpha|} \\binom{k}{\\alpha, \\beta} (\\partial_{q}^{\\beta} \\partial_{p}^{\\alpha} f)(\\partial_{p}^{\\beta} \\partial_{q}^{\\alpha} g)\n\\]\nwhere \\(\\alpha, \\beta\\) are multiindices.\n\\[\\{f, g\\}_k(q,p) = \\left(\\sum_i (\\partial_{q_i}\\partial_{p_i'} - \\partial_{p_i}\\partial_{q_i'}) \\right)^k f(q, p)g(q', p') \\Bigg|_{(q', p') = (q, p)} =  (\\omega^{ij}\\partial_{\\xi_i}\\partial_{\\xi_j'} )^k f(q, p)g(q', p') \\big|_{(q', p') = (q, p)}\n\\]\n\nFor example,\nIf \\(f, g\\) are polynomials of degree \\(m, n\\), then \\(\\{f, g\\}_k\\) is a polynomial of degree \\(\\leq m + n - 2k\\).\n\\[\\begin{aligned}\n\\{q_i, q_j\\}_k &= \\begin{cases} q_iq_j , & k =0, \\\\ 0 & \\text{else}\\end{cases}; \\\\\n\\{p_i, p_j\\}_k &= \\begin{cases} p_ip_j , & k =0, \\\\ 0 & \\text{else}\\end{cases};\\\\\n\\{q_i, p_j\\}_k &= \\begin{cases} q_ip_j , & k =0, \\\\ \\delta_{ij} & k = 1, \\\\ 0 & \\text{else}\\end{cases}\n\\end{aligned}\n\\]\n\nProposition 3 \\[\\begin{aligned}\nf \\star_t g &= (\\operatorname{Tr}\\circ e^{t\\Omega})( f \\otimes g) \\\\\n&=  f e^{t \\bar P} g \\\\\n&=  f \\left(\\prod_i e^{t \\overleftarrow{\\partial_{q_i}} \\cdot \\overrightarrow{\\partial_{p_i}}} e^{-t\\overleftarrow{\\partial_{p_i}} \\cdot \\overrightarrow{\\partial_{q_i}}} \\right) g\n\\end{aligned}\n\\]\nsince the operators commute.\n\nThis allows formal expressions:\n\\[\n\\{\\{f, g\\}\\}_t = \\left(\\operatorname{Tr}\\circ \\frac{\\sinh^{t\\Omega}}{t} \\right)( f \\otimes g)  =  f \\frac{\\sinh(t\\bar P)}{t} g\n\\]\nFor example, when \\(n=1\\),\n\\[\nf \\star_t g = fg + t \\{f, g\\} + \\frac 12 t^2 (f_{qq} g_{pp} - 2 f_{pq} g_{pq} + f_{pp} g_{qq}) + \\cdots\n\\]\n\\[\\begin{aligned}\n(f \\star_t g)(q,p) &= f\\left(q_1 + t\\overrightarrow{\\partial_{p_1}}, \\dots, p_1 - t\\overrightarrow{\\partial_{q_1}}, \\dots\\right) g(q,p) \\\\\n&= f(q,p) g\\left(q_1 - t\\overleftarrow{\\partial_{p_1}}, \\dots, p_1 + t\\overleftarrow{\\partial_{q_1}}, \\dots\\right) \\\\\n&= f\\left(q_1 + t\\overrightarrow{\\partial_{p_1}}, \\dots, p\\right) g\\left(q_1 - t\\overleftarrow{\\partial_{p_1}}, \\dots, p\\right)\n\\end{aligned}\n\\]\nand more generally, in any combination of the coordinates.\n\n\n\n\n\n\nProof\n\n\n\n\n\nDifferentiation \\(\\partial_{\\xi_i}\\) generates translation in \\(\\xi_i\\). That is, \\(e^{\\partial_{\\xi_i} t}\\) is equivalent to translation by \\(t\\) in coordinate \\(\\xi_i\\).\n\n\n\nThis is useful for evaluating the bracket of some Hamiltonians:\n\\[\\{\\{T(p) + V(q), \\cdot\\}\\}_t = \\frac{V(q + t\\nabla_p) - V(q - t\\nabla_p)}{2t} -\\frac{T(p + t\\nabla_q) - T(p - t\\nabla_q)}{2t}\\]\nIn particular, if \\(T, V\\) are both quadratic polynomials, then \\(\\{\\{T + V, \\cdot\\}\\}_t = \\{T + V, \\cdot\\}\\). That is, it simplifies to just the Poisson bracket.\n\nExample evaluations\nIf \\(f = e^{\\sum_i (Q_i q_i + P_i p_i)} = e^{\\Xi_i \\xi_i}, g = e^{\\sum_i (Q_i' q_i + P_i' p_i)} = e^{\\Xi_i' \\xi_i}\\), then\n\\[\n\\begin{aligned}\n\\{f, g\\}_k &= \\left(\\sum_i (Q_i P_i' - Q_i' P_i) \\right)^k fg \\\\\n  &= (\\omega^{ij} \\Xi_i \\Xi_j')^k fg\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  f \\star_t g &= e^{t \\sum_i (Q_i P_i' - Q_i' P_i)} fg \\\\\n  &= e^{t \\sum_i (Q_i P_i' - Q_i' P_i)} e^{\\sum_i (Q_i+Q_i') q_i + (P_i+P_i') p_i} \\\\\n  &= e^{t\\omega^{ij} \\Xi_i \\Xi_j'} e^{(\\Xi_i + \\Xi_i')\\xi_i}\n\\end{aligned}\n\\]\nBy induction, the star product of any number of exponentials is\n\\[\nf_1 \\star_t \\cdots \\star_t f_N = e^{t\\omega^{ij} \\sum_{1 \\leq k &lt; l \\leq N}\\Xi_i^{(k)} \\Xi_j^{(l)}}e^{\\sum_{1 \\leq k  \\leq N} \\Xi_i^{(k)}\\xi_i}\n\\]\nregardless of how the star product associates.\nTaking partial derivative \\(\\partial_{Q}^{\\alpha} \\partial_{P}^{\\beta}\\partial_{Q'}^{\\alpha'} \\partial_{P'}^{\\beta'} \\big|_{Q,P,Q',P' = 0}\\), we obtain \\((q^{\\alpha}p^{\\beta})\\star_t (q^{\\alpha'}p^{\\beta'})\\).\nIn particular,\n\\[\nq_i \\star_t q_j = q_i q_j, \\quad p_i \\star_t p_j = p_i p_j, \\quad q_i \\star_t p_j = q_ip_j + \\delta_{ij}t, \\quad p_j \\star_t q_i = q_i p_j- \\delta_{ij}t\n\\]\nthat is,\n\\[\n\\xi_i \\star_t \\xi_j = \\xi_i \\xi_j + \\omega^{ij}t\n\\]\nIn general, the star product is very difficult to evaluate. Other than the “planar-wave” exponentials given above, we can also evaluate the star product of spherical gaussians:\n\nProposition 4 (hyperbolic addition law) \\[\ne^{-\\tanh (a) \\frac{q^2 + p^2}{\\hbar }} \\star e^{-\\tanh (b) \\frac{q^2 + p^2}{\\hbar }}\n= \\frac{\\tanh (a + b)}{\\tanh (a) + \\tanh (b)} e^{-\\tanh (a+b) \\frac{q^2 + p^2}{\\hbar }}\n\\]\nEquivalently,\n\\[\n\\exp \\left(-\\frac{a}{\\hbar}\\left(q^2+p^2\\right)\\right) \\star \\exp \\left(-\\frac{b}{\\hbar}\\left(q^2+p^2\\right)\\right)\n= \\frac{1}{1+a b} \\exp \\left(-\\frac{a+b}{\\hbar(1+a b)}\\left(q^2+p^2\\right)\\right)\n\\]\n\n\n\n\nPoisson structure\n\nProposition 5 The Moyal bracket and the star product together make up a Poisson structure, that is,\n\n\\(f\\star_t 1 = 1 \\star_t f = f\\)\n\\((f \\star_t g)^* = (g^*) \\star_t (f^*)\\) when \\(t\\) is imaginary.\nThe star product is associative.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n(1). Direct calculation.\n(2). \\(Tr\\Pi^k (f\\otimes g) = (-1)^k Tr\\Pi^k (g\\otimes f)\\)\n(3). Because the star product is multilinear, it suffices to check associativity for exponential functions, which was already proven previously.\n\n\n\n\nProposition 6 (Jacobi-like identities) (1). For any \\(m = 0, 1, \\dots\\),\n\\[\n\\sum_k \\binom mk \\{\\{f, g\\}_k, h\\}_{m-k} = \\sum_k \\binom mk \\{f, \\{g, h\\}_k\\}_{m-k}\n\\]\n(2). The Moyal bracket contains only terms of order \\(1, t^2, t^4, \\dots\\).\n\\[\\{\\{f, g\\}\\}_t = \\sum_{k\\text{ odd}} \\frac{t^{k-1}}{k!} = \\{f, g\\} + \\frac 16 t^2 \\{f, g\\}_3 + \\frac{1}{120}t^4 \\{f, g\\}_5 + \\cdots\n\\]\nIn particular, with \\(t=i\\hbar /2\\), \\[\n\\{\\{f, g\\}\\} = \\{f, g\\} + O(\\hbar^2)\n\\]\n(3). (Jacobi identity) For any even \\(m = 0, 2, \\dots\\),\n\\[\\sum_{k \\text{ odd}} \\binom mk \\left[\\{f, \\{g, h\\}_k\\}_{m-k}\\right]_\\circlearrowleft = 0\n\\] where the \\(\\circlearrowleft\\) means cyclically permuting the three terms \\(f, g, h\\), as in the usual case of Jacobi identity. For example, when \\(m=2\\), it simplifies to \\[\\left[\\{f, \\{g, h\\}\\}\\right]_\\circlearrowleft\\]\nwhich is the Jacobi identity for the Poisson bracket.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n(1). Expand \\((f \\star_t g) \\star_t h = f \\star_t (g \\star_t h)\\) in powers of \\(t\\), and match the coefficients.\n(2). Expand, and note that \\(\\{\\{ f, g \\}\\}_k = (-1)^k\\{\\{ g, f \\}\\}_k\\).\n(3). Expand the Jacobi identity for \\(\\{\\{ \\cdot, \\cdot \\}\\}_t\\).\n\n\n\n\nTheorem 3 ((Ovsienko, Roger, p. 144)) The Moyal bracket is the unique (up to isomorphism) non-trivial deformation of the Poisson bracket in the class of Lie structures.\n\n\n\nFourier analysis\n\nTheorem 4 (Fourier transform of the star product) \\[(f \\star g)(q, p) = \\frac{1}{(\\hbar \\pi)^2} \\int_{\\mathbb{R}^4} dp' dp'' dq' dq''  \nf(q', p') g(q'', p'')\n\\exp \\left(\\frac{2 i}{\\hbar} ((qp' - pq') + (q'p'' - p'q'') + (q''p - p''q)  ) \\right)\n\\]\n\nI have not checked the following, but I think it is true.\n\\[(f\\star g) (\\xi) = \\frac{1}{(\\hbar \\pi)^{2n}}\\int d^{2n}\\xi' d^{2n}\\xi'' f(\\xi') g(\\xi'') \\exp \\left(\\frac{2 i}{\\hbar} ((qp' - pq') + (q'p'' - p'q'') + (q''p - p''q)  ) \\right)\n\\]\n\nLemma 1 (lone star lemma) \\[\\int(f\\star g) (q,p)dqdp = \\int f(q,p) g (q,p)dqdp\n\\]\n\n(C. Zachos 2001) gave a geometric trick picture for the star product.\nLet \\(\\vec r = (q, p)\\), etc, then\n\\[\n(\\vec r' - \\vec r) \\wedge (\\vec r'' - \\vec r) = \\vec r \\wedge \\vec r' + \\vec r' \\wedge \\vec r'' + \\vec r'' \\wedge \\vec r = (qp' - pq') +(q'p'' - p'q'') +(q''p - p''q)\n\\]\nThis can be interpreted as twice the oriented area of a triangle, with orientation going in the direction of \\(\\vec r \\to \\vec r' \\to \\vec r''\\). In this notation, we have\n\\[(f \\star g)(\\vec r) = \\frac{1}{(\\hbar \\pi)^2} \\int_{\\mathbb{R}^4} d^2 \\vec r' d^2 \\vec r''\nf( \\vec r') g(\\vec r'')\n\\exp \\left(\\frac{4 i}{\\hbar} A(\\vec r , \\vec r' , \\vec r'') \\right)\n\\]\n\\[(f \\star g \\star h)(\\vec r) = \\frac{1}{(\\hbar \\pi)^2} \\int_{\\mathbb{R}^6} d^6 (\\vec r', \\vec r'', \\vec r''')\nf( \\vec r') g(\\vec r'') h(\\vec r''')\n\\exp \\left(\\frac{4 i}{\\hbar} A(\\vec r , \\vec r' , \\vec r'') \\right) \\delta^2 (\\vec r  - \\vec r' + \\vec r''- \\vec r''' )\n\\]\nThe Dirac delta in the integrand is a geometric restriction that forces the four vectors form a parallelogram.\n\\[\\int_{\\mathbb{R}^2} d^2 \\vec r (f \\star g \\star h)(\\vec r) = \\frac{1}{(\\hbar \\pi)^2} \\int_{\\mathbb{R}^6} d^6 (\\vec r', \\vec r'', \\vec r''')\nf( \\vec r') g(\\vec r'') h(\\vec r''')\n\\exp \\left(\\frac{4 i}{\\hbar} A(\\vec r , \\vec r' , \\vec r'') \\right)\n\\]"
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html#quantum-mechanics-on-phase-space",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html#quantum-mechanics-on-phase-space",
    "title": "Phase-space quantum mechanics",
    "section": "Quantum mechanics on phase space",
    "text": "Quantum mechanics on phase space\nMost of this came from a series of papers written by Zachos et al: (C. K. Zachos 2002; C. K. Zachos, Fairlie, and Curtright 2005; T. L. Curtright and Zachos 2012; T. Curtright, Fairlie, and Zachos 2014). There are not that many good textbooks on this.\nPhase space quantum mechanics is most widely used in quantum optics. I found (Olivares 2012) to contain most of the formulas.\nI feel like this paper would be useful for something…\n\nTilma, Todd, et al. “Wigner functions for arbitrary quantum systems.” Physical review letters 117.18 (2016): 180401.\n\n\nThe problem of ordering\nWhen Schrödinger, Heisenberg, etc proposed quantum mechanics, they found that position and momentum, which in classical mechanics are represented as real-valued functions \\(q_{1}, \\dots, q_n , p_{1}, \\dots, p_n\\) over phase space \\(\\mathbb{R}^{2n}\\), must instead become self-adjoint linear operators \\(\\hat q_{1}, \\dots, \\hat q_n , \\hat p_{1}, \\dots, \\hat p_n\\) over an inner product space (Hilbert space).\nEveryone agrees since (Heisenberg, 1925) and (Schrödinger, 1926) that \\(\\hat q, \\hat p\\) must be quantized, and must satisfy the canonical commutator relations. Then (von Neumann, 1931) proved that under mild assumptions, there is only one choice (up to Hilbert space isomorphism). This is the choice that (is isomorphic to the choice that) everyone learns in quantum mechanics nowadays:\n\\[\n\\hat q_i = (q_i \\cdot), \\quad \\hat p_i = -i\\hbar \\partial_{q_i}\n\\]\nThe problem is that people do not agree on how to quantize the other observables.\nThe problem of ordering is that \\(pq = qp\\) in classical mechanics, but \\(\\hat p \\hat q \\neq \\hat q \\hat p\\) in quantum mechanics. Therefore, one must decide on how to convert \\(pq\\) to some \\(\\widehat{pq}\\) in quantum mechanics, consistently.\nDo we choose \\(\\hat q \\hat p\\), or \\(\\hat p \\hat q\\), or even something entirely different?\nWeyl’s solution is to preserve the symmetry as much as possible, by using symmetric polynomials:\n\\[\n\\widehat{pq} = \\widehat{qp} = \\frac 12 (\\hat q \\hat p + \\hat p \\hat q)\n\\]\nand in general,\n\\[\n\\widehat{\\prod_{j=1}^N \\xi_{k_j}} = \\frac{1}{N!} \\sum_{\\sigma \\in S_N} \\prod_{j=1}^N \\hat \\xi_{k_\\sigma(j)}\n\\]\nThat is, the quantum operator corresponding to a product is the symmetric product.\n\n\nWeyl transform\nThe Weyl transform sends real-valued functions on phase space to self-adjoint linear operators. That is, it sends classical observables to quantum observables.\nLet \\(f : \\mathbb{R}^{2n} \\to \\mathbb{C}\\) be a nice function, then its Weyl transform is\n\\[\nW[f] := \\frac{1}{(2\\pi)^{2n} } \\int \\tilde f(\\tilde \\xi) \\exp\\left({i \\tilde \\xi \\cdot \\widehat {\\xi} } \\right) d^{2n} \\tilde\\xi\n\\]\nThe inverse Weyl transform4 is\n4 Also called the Wigner map, or the Wigner transform.\\[\nW^{-1} [\\hat f](q,p) =  \\int d^n y e^{-ip \\cdot y/\\hbar}~\\bra{q+y/2} \\hat f \\ket{q-y/2}\n\\]\n\nProposition 7 (equivalent definitions of Weyl transform) \\[\n\\begin{aligned}\n      W[f] &= \\frac{1}{(2\\pi)^{2n} } \\int f(\\xi) \\exp\\left({i \\tilde \\xi \\cdot (\\widehat {\\xi} - \\xi) } \\right) d^{2n} \\xi d^{2n} \\tilde\\xi \\\\\n      &= \\frac{1}{(2\\pi)^{2n} } \\int f(q,p) \\prod_{j=1}^n\\exp\\left[{i (\\tilde q_j (\\widehat {q_j} - q_j) + \\tilde p_j (\\widehat {p_j} - p_j) ) } \\right] d^{n} q d^{n} p d^{n} \\tilde q d^{n} \\tilde q\\\\\n      &= \\left(\\frac{2}{(2\\pi \\hbar)^{\\frac 32}}\\right)^n  \\int d^{2n}\\xi d^{2n}\\xi' f(q,p) \\ket{q'} \\bra{p'} e^{\\frac{i}{\\hbar}(q' \\cdot p' - 2 (q' -q)\\cdot (p'-p))}\n      \\end{aligned}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFirst is by the definition of the Fourier transform. Second is derived from the first because \\(q_i, q_j\\) commute, and \\(p_i, p_j\\) commute.\nFor the third one, By the Baker-Campbell-Hausdorff formula,\n\\[\n\\begin{aligned}\n\\exp\\left({i \\tilde \\xi \\cdot (\\widehat {\\xi} - \\xi) } \\right)\n&= e^{\\frac 12 i\\hbar (\\tilde q \\cdot \\tilde p) - i \\tilde \\xi \\xi} e^{i \\tilde q\\cdot \\hat q} e^{i \\tilde p\\cdot \\hat p} \\\\\n&= e^{\\frac 12 i\\hbar (\\tilde q \\cdot \\tilde p) - i \\tilde \\xi \\xi}\\int d^n q' d^n p' \\ket{q'} \\bra{p'} e^{i (\\tilde q \\cdot q' + \\tilde p \\cdot p' + q' \\cdot p' / \\hbar)}\n\\end{aligned}\n\\]\nPlug it into the first equation, and integrate out \\(d^{n} \\tilde q, d^n \\tilde p, d^{n} q, d^n q\\) one-by-one, using the Dirac delta trick of \\(\\int e^{i \\tilde x x} d^n x = (2\\pi)^n \\delta^n(\\tilde x)\\).\n\n\n\n\nProposition 8 (equivalent definitions of inverse Weyl transform) \\[\n\\begin{aligned}\n      W^{-1} [\\hat f](q,p) &=  \\int d^n y e^{-ip \\cdot y/\\hbar}~\\bra{q+y/2} \\hat f \\ket{q-y/2} \\\\\n      &= \\int d^n y e^{iq \\cdot y/\\hbar}~\\bra{p+y/2} \\hat f \\ket{p-y/2} \\\\\n      &= \\operatorname{Tr}\\left[\\hat f  \\int d^n y e^{-ip \\cdot y/\\hbar}~ \\ket{q-y/2} \\bra{q+y/2} \\right] \\\\\n      &= \\operatorname{Tr}\\left[\\hat f  \\int d^n y e^{i q \\cdot y/\\hbar}~ \\ket{p+y/2} \\bra{p-y/2} \\right]\n      \\end{aligned}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nTrivial.\n\n\n\n\nProposition 9 (special cases) Let \\(f(q, p) = e^{i (\\tilde \\xi \\cdot \\xi)}\\), then\n\\[\n\\tilde f(\\tilde \\xi') = (2\\pi)^{2n} \\delta^{2n}(\\tilde \\xi' - \\tilde \\xi), \\quad W[f] = e^{i (\\tilde \\xi \\cdot \\hat\\xi)}\n\\]\nFor monomials,\n\\[\nW\\left[\\prod_{j=1}^N \\xi_{k_j}\\right] = \\frac{1}{N!} \\sum_{\\sigma \\in S_N} \\prod_{j=1}^N \\hat \\xi_{k_{\\sigma(j)}}\n\\]\nIn particular, if \\(\\prod_{j=1}^N \\xi_{k_j}\\) contains only commuting variables (that is, \\(p_i, q_i\\) do not simultaneously appear for each of \\(i = 1, 2, \\dots n\\)), then\n\\[\nW\\left[\\prod_{j=1}^N \\xi_{k_j}\\right] = \\prod_{j=1}^N \\xi_{j}\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFor any of the canonical variables \\(\\xi_j\\), we let it go through the Fourier transform and then an inverse transform:\n\\[\n\\xi_j  = \\frac{1}{(2\\pi)^{2n} } \\int \\tilde \\xi_j(\\tilde \\zeta) \\exp\\left({i \\sum_j \\tilde \\zeta_j \\xi_j } \\right) d^{2n} \\tilde\\zeta\n\\]\nNow, expand the exponent inside the integrand, but don’t allow \\(\\xi_j\\) to commute with \\(\\xi_k\\), then we obtain a formal power series consisting of terms like\n\\[\n(\\text{complex-valued function})(\\tilde \\zeta) \\cdot (\\text{some monomial in }\\xi_j)\n\\]\nNow each complex-valued function coefficient integrates, leaving behind a complex-linear sum of monomials in \\(\\xi_i\\).\nSince only the coefficients of \\(\\xi_j\\) remain, this means that doing the same formal manipulation with the Weyl transform would be nothing more than adding a hat \\(\\widehat{(\\cdot)}\\), giving us\n\\[\nW[\\xi_j] := \\widehat{\\xi_j}\n\\]\nIn general, if we have a monomial function \\(f := \\prod_{j=1}^N \\xi_{k_j}\\), then\n\\[\n\\begin{aligned}\n    f &= \\frac{1}{(2\\pi)^{2n} } \\int \\tilde f(\\tilde \\zeta) \\exp\\left({i \\sum_j \\tilde \\zeta_j \\xi_j } \\right) d^{2n} \\tilde\\zeta \\\\\n    &=\\sum_{M=0}^\\infty \\frac{1}{(2\\pi)^{2n} } \\frac{i^M}{M!}\\int \\tilde f(\\tilde \\zeta) \\left( \\sum_j \\tilde \\zeta_j \\xi_j  \\right)^M d^{2n} \\tilde\\zeta \\\\\n\\end{aligned}\n\\]\nAs before, all the coefficients integrate to zero, except for the terms that results in the monomial of \\(f\\). Such terms occur only in \\(\\left( \\sum_j \\tilde \\zeta_j \\xi_j  \\right)^N\\), in the form of a sum:\n\\[\n\\sum_{\\sigma\\in S_N} \\prod_{j=1}^N \\tilde \\zeta_{k_{\\sigma(j)}} \\xi_{k_{\\sigma(j)}}\n\\]\nNow, the \\(\\prod_{j=1}^N \\tilde \\zeta_{k_{\\sigma(j)}}\\) part commutes, but the \\(\\prod_{j=1}^N \\xi_{k_{\\sigma(j)}}\\) part does not. So we find that the coefficient in front of each \\(\\prod_{j=1}^N \\xi_{k_{\\sigma(j)}}\\) is the same, all of which are \\(\\frac{1}{N!}\\).\n\n\n\nSo far, we have defined by fiat a transform that we have called “the inverse Weyl transform”. However, we had better make sure it really is the inverse of “the Weyl transform”. We can now prove it.\n\nProposition 10 \\[\nW^{-1}[W[f]] = f\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIt suffices to prove this when \\(f(q, p) = e^{i (\\tilde \\xi \\cdot \\xi)}\\), since both \\(W\\) and \\(W^{-1}\\) are linear, and any function is the integral of some \\(\\int e^{i (\\tilde \\xi \\cdot \\xi)} \\tilde f(\\tilde \\xi) d^{2n} \\tilde \\xi\\), by Fourier transform.\n\\[\n\\begin{aligned}\n    W^{-1}[e^{i (\\tilde \\xi \\cdot \\hat\\xi)}](q,p) &= \\int d^n y e^{-ip \\cdot y/\\hbar}~\\bra{q+y/2} e^{i (\\tilde q \\cdot \\hat q + \\tilde p \\cdot \\hat p)} \\ket{q-y/2}\\\\\n    &= \\int d^n y e^{-ip \\cdot y/\\hbar}~\\bra{q+y/2} e^{i \\tilde q \\cdot \\hat q/2} e^{i \\tilde p \\cdot \\hat p} e^{i \\tilde q \\cdot \\hat q/2} \\ket{q-y/2}\\\\\n    &= \\int d^n y e^{-ip \\cdot y/\\hbar}~e^{i \\tilde q \\cdot (q + y/2)/2} e^{i \\tilde q \\cdot (q-y/2)/2} \\bra{q+y/2} e^{i \\tilde p \\cdot \\hat p} \\ket{q-y/2}\\\\\n    &= \\int d^n y e^{-ip \\cdot y/\\hbar}~e^{i \\tilde q \\cdot q}  \\delta^n(\\hbar \\tilde p + (q+ y/2) - (q - y/2) )\\\\\n    &= e^{i (\\tilde q \\cdot q + \\tilde p \\cdot p)}\n\\end{aligned}\n\\]\nIn the derivation, we used two facts: the Baker-Campbell-Hausdorff formula, and the fact that momentum is the generator for translation.\n\n\n\n\nProposition 11 Some statements I have not checked, but I heard they are correct:\n\nIf \\(\\hat H = T(\\hat p) + V(\\hat q)\\) for some analytic functions \\(T, V\\), then \\(W[T(p) + V(q)] = \\hat H\\)\n\\(W[f] W[g] = W[f \\star g]\\)\n\\(\\operatorname{Tr}( \\hat A \\hat B) =(2\\pi \\hbar)^{-n} \\int AB d^n q d^n p\\)\n\n\n\nTheorem 5 (correspondence principle) If \\(f\\) is a polynomial of degree \\(\\leq 2\\), and \\(g\\) is a polynomial, then\n\\[\n[W[f] , W[g]] = i\\hbar W[\\{f, g\\}]\n\\]\nThis is an instance of the correspondence principle \\([\\hat f, \\hat g] = i\\hbar \\widehat{\\{f, g\\}}\\).\n\n\nProposition 12 (functional analysis properties of the Weyl transform)  \n\nIf f is a real-valued function, then \\(\\Phi [f]\\) is self-adjoint.\nIf f is an element of Schwartz space, then \\(\\Phi[f]\\) is trace-class.\n\\(\\Phi [f]\\) is a densely defined unbounded operator.\nThe map \\(\\Phi [f]\\) is one-to-one on the Schwartz space as a subspace of the square-integrable functions.\n\n\n\nProposition 13 (examples in \\(n=1\\) case.) For any \\(a, b\\in \\mathbb{C}, n \\in \\mathbb{N}\\), we have\n\\[\nW[(a q + b p)^n] = (a \\hat q + b \\hat p)^n\n\\]\n\\[\nW[e^{i(a q + b p)^n}] = e^{i(a \\hat q + b \\hat p)^n}\n\\]\n(McCoy’s formula):\n\\[\nW[ p^m q^n] = {1 \\over 2^n}\n      \\sum_{r=0}^{n} {n \\choose r}\n      \\hat q^r  \\hat p^m  \\hat q^{n-r}={1 \\over 2^m}\\sum_{s=0}^{m} {m \\choose s} \\hat p^s \\hat q^{n}\\hat p^{m-s}\n\\]\n\n\n\nWave function transform\nThe Weyl transform, when applied to a density operator \\(\\hat \\rho\\), yields a function on phase space. This function is called a Wigner function5.\n5 Or a Wigner quasiprobability distribution, or a Wigner distribution, or… you know how it is.The wave function is transformed the same way as its density operator, but we need to divide by an extra factor of \\((2\\pi \\hbar)^n\\) for convenience.\n\\[\n\\begin{aligned}\n      W_{\\hat \\rho} :=& \\frac{1}{(2\\pi \\hbar)^n} W^{-1}[\\hat \\rho] \\\\\n      =&  \\frac{1}{(2\\pi \\hbar)^n}\\int d^n y e^{-ip \\cdot y/\\hbar}~\\braket{q+y/2|\\psi} \\braket{\\psi | q-y/2} \\\\\n      =&  \\frac{1}{(2\\pi)^n}\\int d^n y e^{-ip \\cdot y}~\\braket{q+y\\hbar /2|\\psi} \\braket{\\psi | q-y\\hbar /2} \\\\\n      =&  \\frac{1}{(2\\pi )^n}\\int d^n y e^{-ip \\cdot y}~\\psi(q+y\\hbar/2) \\psi(q-y\\hbar/2)^*\n\\end{aligned}\n\\]\nIf \\(\\psi\\) is given in momentum coordinates, then we have\n\\[\nW(q, p) = \\frac{1}{(2\\pi)^n} \\int e^{-i\\braket{y, q}}d^ny \\; \\psi(p - \\hbar y/2) \\psi(p + \\hbar y/2)^*\n\\]\nNote that \\(W\\) has units of \\(\\hbar^{-n}\\).\n\n\n\n\n\n\n\\(\\hbar = 1\\) convention\n\n\n\nIn the quantum phase space, areas \\(pq\\) have units of \\(\\hbar\\). By scaling both \\(p, q\\) by \\(\\sqrt{\\hbar}\\), we can by convention set \\(\\hbar = 1\\). In more detail, define the new scaled positions and momenta by \\((q', p') := (q / \\sqrt{\\hbar}, p / \\sqrt{\\hbar})\\), then phase-space areas become dimensionless. The Wigner function scales by \\(W'(q, p) = \\hbar^n W(q, p)\\), and the wavefunction scales by \\(\\psi'(q') = \\hbar^{-n/4} \\psi(q)\\).\nIn these new units, the Wigner transform is\n\\[\nW'(q', p') = \\frac{1}{2\\pi} \\int_{\\mathbb{R}^n} d^n y\\; e^{-ip' y } \\psi(q' + y/2)  \\psi(q' - y/2)^*\n\\]\n\n\n\nProposition 14  \n\n\\(W\\) is real.\n\\(|\\psi(q)|^2 = \\int_{\\mathbb{R}^n} W(q,p)d^n p\\)\n\\(|\\psi(p)|^2 = \\int_{\\mathbb{R}^n} W(q,p)d^n q\\)\n\\(\\int W(q, p) d^nqd^np = 1\\).\n\\(|W(q, p)| \\leq (\\pi \\hbar)^{-n}\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nCheck \\(W^* = W\\).\n\n(2, 3) Use the Fourier expression of Dirac delta: \\(\\delta(t-t_0) = \\frac{1}{2\\pi} \\int e^{i\\omega(t-t_0)}d\\omega\\).\n\nCorollary of (2).\nCauchy–Schwarz inequality.\n\n\n\n\n\n\n\n\n\n\nsome pure states are impossible\n\n\n\nThe position eigenstate has wavefunction \\(\\psi(q) = \\sqrt{\\delta^n(q-q_0)}\\). However, if you plug it into the definition for \\(W(q,p)\\), you will discover that \\(W(q, p)\\) must be identically zero for \\(q \\neq q_0\\), and also must be independent of \\(p\\). This then means \\(W(q, p)\\) should look something like \\(C \\delta^n(q-q_0)\\) for some \\(C \\in \\mathbb{R}\\). But no matter what \\(C\\) you pick, it won’t allow \\(\\int W dq dp = 1\\).\n\n\n\nProposition 15 (symmetries of \\(W\\)) Let \\(\\psi(q)\\) be a wavefunction of a in position space \\(\\mathbb{R}^n\\). We have:\n\nIf we translate \\(\\psi\\) by \\(+\\Delta q\\), or equivalently, multiplying it by the operator \\(e^{\\Delta q \\cdot \\nabla_q}\\), then we also translate its Wigner function by \\(+\\Delta q\\).\nIf we multiply \\(\\psi\\) by the operator \\(e^{+i \\Delta p \\cdot q/\\hbar}\\), then we also translate its Wigner function by \\(+\\Delta p\\).\nIf we multiply it by the Hamiltonian of the quantum harmonic oscillator \\(e^{-i \\hat H t/\\hbar}\\), where \\(\\hat H := \\frac{{\\hat p}^2}{2m} + \\frac{1}{2} m \\omega^2 {\\hat q}^2\\), then we rotate its Wigner function by \\(\\omega t\\) around the ellipse with semiaxes \\(\\sqrt{1/m\\omega^2}, \\sqrt{m}\\). That is, the new Wigner function satisfies \\(W'(q, p) = W(q \\cos(\\omega t) - \\frac{p}{m\\omega} \\sin(\\omega t),  q m \\omega \\sin(\\omega t) + p \\cos(\\omega t))\\).\n\n\nWe say that a Wigner function is pure iff it is the Wigner function of a pure state \\(\\hat \\rho = \\ket{\\psi}\\bra{\\psi}\\). We then have\n\nProposition 16 (properties of pure Wigner functions) \\(W\\) is pure iff \\(W \\star W = (2\\pi \\hbar)^{-n} W\\), which you can remember by noting that \\(F\\) has units of \\(\\hbar^{-n}\\), and that the units must agree on both sides.\nAny linear operator that preserves the star product preserves purity of Wigner functions. That is, if \\(F\\) is a linear operator that satisfies \\(F(W) \\star F(W') = F(W \\star W')\\), then if \\(W\\) is pure, then so is \\(F(W)\\).\nTranslations and elliptic-rotations preserve the purity of Wigner functions.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe first one is proved by the general theorem that \\(\\widehat{W \\star W'} = \\widehat{ W} \\widehat{ W'}\\), where the \\(\\widehat{\\cdot}\\) denotes the Weyl transform. Pay attention to the fact that \\((2\\pi \\hbar)^n\\) constant appeared because the Wigner function was defined by scaling the Weyl transform by that constant.\nThe second one is a corollary of the first one.\nThe third one is a corollary of the symmetries of \\(W\\).\n\n\n\n\nProposition 17 Let \\(\\tilde W(x, y)\\) be the Fourier transform of \\(W\\) on the momentum:\n\\[\n\\tilde W(x,y) := \\int e^{i \\braket{y,p}} d^n p W(x,p)\n\\]\nA function \\(W(q,p)\\) is the Wigner function of some wave function \\(\\psi(q)\\) iff\n\\[\n\\partial_{x+\\hbar y / 2} \\partial_{x+\\hbar y / 2} \\ln \\tilde W(x, y) = 0\n\\]\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nIf \\(W\\) is the Wigner function of \\(\\psi\\), then \\(\\tilde W(x,y) = \\psi(x + \\hbar y/2) \\psi(x - \\hbar y/2)^*\\), so it is true.\nConversely, if \\(\\partial_{x+\\hbar y / 2} \\partial_{x+\\hbar y / 2} \\ln \\tilde W(x, y) = 0\\), then it separates in variables, giving\n\\[\n\\ln \\tilde W(x, y) = f_+(x+\\hbar y / 2 ) + C + f_- (x - \\hbar y / 2) - C\n\\]\nfor some \\(f_+, f_-\\), and any constant \\(C\\).\nGiven \\(W(x, p, t)\\), one can find the wavefunction through a two-step process. First, find the Fourier transform\n\\[\n\\begin{aligned}\n    \\widetilde{W}(x, y, t) & =\\frac{1}{2 \\pi \\hbar} \\int_{-\\infty}^{+\\infty} W(x, p, t) e^{i p y / \\hbar} d p \\\\\n    & =\\frac{1}{2 \\pi \\hbar} \\psi^*\\left(x-\\frac{1}{2} y, t\\right) \\psi\\left(x+\\frac{1}{2} y, t\\right)\n\\end{aligned}\n\\]\nSecond, select an arbitrary point \\(x_0\\) where \\(\\widetilde{W}\\left(x_0, 0, t\\right)\\) does not vanish, and construct \\(\\psi\\) by … (I’m pretty sure it requires just a few more lines now.)\n\n\n\n\n\nGaussians\n\nProposition 18 (Wigner function of a Gaussian wavepacket) \\[\nW(q,p) = \\rho_{\\mathcal N(q_0, \\Sigma_q)}(q) \\rho_{\\mathcal N(p_0, \\Sigma_p)}(p)\n\\]\nwhere \\(\\Sigma_q \\Sigma_p = \\frac 14 \\hbar^2 I\\) reaches the lower bound of the uncertainty principle allows.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose we have a gaussian wave packet with mean \\(q_0\\) and variance \\(\\Sigma\\), then its wave function is\n\\[\n\\begin{aligned}\n\\psi(q) &= e^{i p_0 \\cdot q} \\sqrt{\\rho_{\\mathcal N(q_0, \\Sigma)}(q)} \\\\\n          &= e^{i p_0 \\cdot q} \\sqrt{\\frac{1}{(2\\pi)^{n/2} (\\det \\Sigma)^{1/2}} e^{-\\frac 12 (q-q_0)^T \\Sigma^{-1} (q-q_0)}}\\\\\n      &= \\exp\\left[ \\frac 14 (q-\\mu)^T \\Sigma^{-1} (q-\\mu) +  i \\tilde q \\cdot q - \\frac 14 \\ln \\det \\Sigma - \\frac n4 \\ln (2\\pi) \\right]\n\\end{aligned}\n\\]\n\\[\nW(q,p) =  \\frac{1}{(\\pi \\hbar)^n}\\int d^n y e^{-2ip \\cdot y}~\\psi(q+y) \\psi(q-y)^* = \\cdots = \\rho_{\\mathcal N(q_0, \\Sigma)}(q) \\rho_{\\mathcal N(p_0, \\hbar^2(4\\Sigma )^{-1})}(p)\n\\]\n\n\n\nThere is a more general theorem (T. Curtright, Fairlie, and Zachos 2014, 23):\n\nTheorem 6 (uncertainty principle) For any wavefunction \\(\\psi\\), the convolution of its Wigner function with any Gaussian satisfying \\(\\Sigma_p \\Sigma_q \\geq \\frac 14 \\hbar^2 I\\) is everywhere non-negative. In other words, all negative parts of the Wigner function is small and jagged. We cannot find a large contiguous region where the Wigner function is negative.\n\n\n\nTime-evolution\n\\[\n\\begin{aligned}\n  \\frac{\\partial W(x, p, t)}{\\partial t}= & -\\frac{p}{m} \\frac{\\partial W(x, p, t)}{\\partial x} \\\\\n  & -\\int_{-\\infty}^{+\\infty} K\\left(x, p^{\\prime}\\right) W\\left(x, p+p^{\\prime}, t\\right) d p^{\\prime}\n  \\end{aligned}\n\\]\nwhere the kernel \\(K(x, p)\\) is\n\\[\nK(x, p)=\\frac{1}{2 \\pi \\hbar^2} \\int_{-\\infty}^{+\\infty}\\left[V\\left(x-\\frac{1}{2} y\\right)-V\\left(x+\\frac{1}{2} y\\right)\\right] \\sin (p y / \\hbar) d y .\n\\]\n\nTheorem 7 (Moyal’s evolution equation) \\[\n\\partial_t W = \\frac{H \\star W - W \\star H}{i\\hbar} = \\{\\{H, W\\}\\}\n\\]\nNotice how similar it is to the classical Liouville equation \\(\\partial_t \\rho = \\{H, \\rho\\}\\), and the von Neumann equation \\(\\partial_t \\hat \\rho = \\frac{\\hat H \\hat \\rho - \\hat \\rho \\hat H}{i\\hbar}\\).\n\nIf \\(\\hat H = T(\\hat p) + V(\\hat q)\\) for some analytic functions \\(T, V\\), then\n\\[\nH = W^{-1}[\\hat H] = T(p) + V(q)\n\\]\nmeaning that the quantum energy function in quantum phase space is exactly the same as the classical energy function in classical phase space. How nice."
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html#quadratic-hamiltonian",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html#quadratic-hamiltonian",
    "title": "Phase-space quantum mechanics",
    "section": "Quadratic Hamiltonian",
    "text": "Quadratic Hamiltonian\nIf the Hamiltonian \\(H\\) is a sum of kinetic and potential energies, as \\(H = T(p) + V(q)\\), then\n\\[\n\\begin{aligned}\n  (f \\star_t g)(q,p) &= f\\left(q_1 + t\\overrightarrow{\\partial_{p_1}}, \\dots, p_1 - t\\overrightarrow{\\partial_{q_1}}, \\dots\\right) g(q,p) \\\\\n  &= f(q,p) g\\left(q_1 - t\\overleftarrow{\\partial_{p_1}}, \\dots, p_1 + t\\overleftarrow{\\partial_{q_1}}, \\dots\\right) \\\\\n  &= f\\left(q_1 + t\\overrightarrow{\\partial_{p_1}}, \\dots, p\\right) g\\left(q_1 - t\\overleftarrow{\\partial_{p_1}}, \\dots, p\\right)\n  \\end{aligned}\n\\]\nThus,\n\\[\nH \\star W = T(p - (i\\hbar/2)\\nabla_q) W + V(q +  (i\\hbar/2) \\nabla_p) W\n\\]\nand similarly for the other direction. Together, it means\n\\[\n\\partial_t W = \\frac{T(p - (i\\hbar/2)\\nabla_q) - T(p + (i\\hbar/2)\\nabla_q)}{i\\hbar} W + \\frac{V(q + (i\\hbar/2)\\nabla_p) - V(q - (i\\hbar/2)\\nabla_p)}{i\\hbar} W\n\\]\nIn particular, if \\(T, V\\) are both quadratic, such as in the case of a harmonic oscillator \\(H = \\frac{p^2}{2m} + \\frac 12 m\\omega^2 q^2\\), then we have\n\\[\n\\partial_t W = -\\nabla_p T \\cdot \\nabla_q W + \\nabla_q V \\cdot \\nabla_p W = \\{H, W\\}\n\\]\nwhich is exactly the same as the classical Hamiltonian equations of motion.\nIn other words, we can picture the phase-space evolution of a quantum harmonic oscillator’s Wigner function as exactly the same as if it is the phase space evolution in classical statistical mechanics. The only difference is that there are both regions of positive and negative probabilities. But at least it is all real-valued!\n\nFree particle\nFor a particle in free space, the Hamiltonian is \\(H = \\frac{\\|p\\|^2}{2m}\\). Thus, the Wigner function \\(W\\) evolves by a simple shearing flow in phase space:\n\\[\nW(t, q, p) = W(0, q - pt/m, p)\n\\]\nThis picture allows us to obtain some results pictorially.\n\nGaussian wavepacket\nConsider the simplest case of a gaussian wavepacket on \\(\\mathbb{R}\\), centered at \\(q = 0\\), with zero total momentum. Over time, it contracts, until its width \\(\\sigma_q\\) reaches a minimum, before spreading out again. Let \\(t = 0\\) be the time of minimal width, so its wavefunction satisfies\n\\[\n\\psi(0, q) = \\sqrt{\\rho_{N (0, \\sigma_q^2)}(q)}\n\\]\nwhere \\(\\rho_{N (0, \\sigma_q^2)}\\) denotes the probability density function of the gaussian with mean \\(0\\) and variance \\(\\sigma_q^2\\). By direct calculation, its Wigner function is the probability density function of the gaussian with mean \\((0, 0)\\) and variance \\(\\diag(\\sigma_q^2, \\sigma_p^2)\\), satisfying the uncertainty principle \\(\\sigma_q^2 \\sigma_p^2 = \\hbar^2/4\\).\nMore generally, a gaussian wavepacket with initial peak position \\(q_0\\) and momentum \\(p_0\\) has a Wigner function with mean \\((q_0, p_0)\\) and variance \\(\\diag(\\sigma_q^2, \\sigma_p^2)\\). For concreteness, let \\(q_0 &gt; 0, p_0 &lt; 0\\). As time \\(t\\) increases from \\(-\\infty\\) to \\(t = 0\\), the Wigner function shears to the left more and more, until it becomes an ellipse with major axes parallel to the \\(p\\)-axis and the \\(q\\)-axis right at \\(t = 0\\). The center of mass on the \\(q\\)-axis is the projection of the center of the Wigner function, which moves at constant velocity \\(p_0/m\\). The \\(q\\)-marginal distribution of the Wigner function first shrinks, reaching a minimum at \\(t = 0\\), before growing again. Its \\(p\\)-axis marginal remains unchanged. This is shown in the following animation.\n\n\n\n\n\n\nStatic frames from the animation. As time increases, a gaussian wavepacket shrinks, then spreads, on the \\(q\\)-axis marginal. Its \\(p\\)-axis marginal remains unchanged.\n\n\n\n\nNegative probability flow\nIn (Villanueva 2020; Goussev 2020), it was noted that for a gaussian wavepacket with positive group velocity \\(p_0 / m &gt; 0\\), it is often the case that there is a paradoxical “negative probability flow”. Specifically, it was found that, if we stand at a point \\(q_1 &gt; q_0\\), and plot \\(Pr(q &gt; q_1 | t)\\), the probability that the particle is found at \\(q &gt; q_1\\) at time \\(t\\), then as time passes, that probability first decreases, before it increases, even though the wavepacket always has positive group velocity. This is immediate in the phase space picture.\nConsider a gaussian wavepacket with \\(W(0, q, p) = \\rho_{N ((q_0, p_0), \\diag(\\sigma_q^2, \\sigma_p^2))}\\), with \\(q_0 &lt; 0, p_0 &gt; 0\\). Geometrically, \\(Pr(q &gt; q_1 | t)\\) is the integral of \\(W(t, q, p)\\) on the region to the right of the vertical line \\(q = q_1\\), which is equal to the integral of \\(W(0, q, p)\\) on the region to the right of the sheared line \\(q = -\\frac{t}{m} p\\). The sheared line rotates clockwise over time.\nFrom the geometry of gaussian distributions, this integral can be pictorially calculated by finding the ellipse of constant \\(\\rho_{N ((q_0, p_0), \\diag(\\sigma_q^2, \\sigma_p^2))}\\) that is tangent to the sheared line. At \\(t \\to -\\infty\\), the sheared line is just the \\(q\\)-axis. As \\(t\\) increases, the sheared line rotates counterclockwise, and the tangent ellipse grows, until it hits the maximal size at some critical time \\(t = t_{\\text{critical}}\\), at which point the tangent ellipse is equal to\n\\[\n\\frac{(q-q_0)^2}{\\sigma_q^2} + \\frac{(p-p_0)^2}{\\sigma_p^2} = \\frac{(q_1-q_0)^2}{\\sigma_q^2} + \\frac{(0-p_0)^2}{\\sigma_p^2}\n\\]\nIt is a simple exercise to show \\(t_{\\text{critical}} = -\\frac{mp_0 \\sigma_q^2}{(q_1- q_0) \\sigma_p^2}\\).\nAfter that, the tangent ellipse shrinks again. This is illustrated in the following figure.\n\n\n\nIllustration for the negative probability flow of a gaussian wavepacket. The dashed circle is the largest possible tangent ellipse. At \\(t = t_{\\text{critical}}\\), the sheared line is tangent to the dashed circle.\n\n\nBy tracing out the pictures, we visually see that \\(Pr(q &gt; q_1|t)\\) decreases over the region \\(t \\leq t_{\\text{critical}}\\), and increases over the region \\(t \\geq t_{\\text{critical}}\\).\nBy a similar pictorial reasoning, if \\(q_0 &gt; 0, p_0 &gt; 0\\), then \\(Pr(q &gt; q_1|t)\\) increases over the region \\(t \\leq t_{\\text{critical}}\\), and decreases over the region \\(t \\geq t_{\\text{critical}}\\). Only when \\(q_0 = q_1\\) is \\(Pr(q &gt; q_1|t)\\) strictly monotonic over all time.\n\n\nWave dispersion\nAs noted previously, a gaussian wave packet first shrinks, then grows, according to a precise formula proved in every introductory quantum mechanics course. We derive this formula geometrically.\nWithout loss of generality, consider a wavepacket with zero group velocity, centered at \\(q = 0\\), reaching minimal width \\(\\sigma_q\\) at \\(t=0\\). Its Wigner function is \\(\\rho_{N((0, 0), \\diag(\\sigma_q^2, \\sigma_p^2))}\\), where \\(\\sigma_q \\sigma_p = \\hbar / 2\\).\nNow, consider its one-sigma ellipse \\(\\frac{q^2}{\\sigma_q^2} + \\frac{p^2}{\\sigma_q^2} = 1\\). It projects to an interval \\([-\\sigma_q, +\\sigma_q]\\) on the \\(q\\)-axis, meaning that at time \\(t=0\\), the probability of finding the particle within the interval \\([-\\sigma_q, +\\sigma_q]\\) is plus-or-minus one-sigma, that is, 68.3%.\nNow, at time \\(t\\), the new one-sigma interval can be either found by shearing the Wigner function, or by shearing the \\(q\\)-intervals. As pictured, the sheared \\(q\\)-intervals are the tangent lines to the one-sigma ellipse satisfying \\(q + \\frac{t}{m} p = C\\). The tangent points are\n\\[\n(q, p) = \\left( \\frac{\\sigma_q}{\\sqrt{1 + \\left( \\frac{\\sigma_p t}{\\sigma_q m}\\right)^2}}, \\frac{\\sigma_p}{\\sqrt{1 + \\left( \\frac{\\sigma_p t}{\\sigma_q m}\\right)^{-2}}}\\right)\n\\]\nand so, the projection to the \\(q\\)-axis has end points\n\\[\n\\pm \\sigma_q\\sqrt{1 + \\left( \\frac{\\sigma_p t}{\\sigma_q m}\\right)^2} = \\pm \\sqrt{\\sigma_q^2 + \\left( \\frac{\\hbar t}{2m\\sigma_q}\\right)^2}\n\\]\nas expected.\n\n\n\n\n\n\nIllustration for gaussian wavepacket spreading. As time increases, the one-sigma circle projects to an interval on \\(q\\)-axis, which shrinks as \\(t \\uparrow 0\\), then grows as \\(t \\uparrow \\infty\\).\n\n\nIn particular, for very large \\(t\\), the wavepacket spreads linearly. This is in fact a generic result for arbitrary waves. Specifically, the \\(q\\)-marginal of the Wigner function at \\(q = q_1\\) and time \\(t\\) can be found by shearing a thin slice \\(q \\in [q_1, q_1 + \\delta q]\\) back by \\(\\frac{pt}{m}\\). If \\(t\\) is large, then this thin slice, perpendicular to the \\(q\\)-axis, is sheared to become almost perpendicular to the \\(p\\)-axis instead, which is approximately \\(p \\in [mq_1 / t, m(q_1 + \\delta q)/ t]\\). Thus, we have\n\\[\n|\\psi_q(t, q)|^2 \\to \\frac{m}{t}|\\psi_p(0, mq_1 / t)|^2\n\\]\nas \\(t \\to +\\infty\\).\n\n\nHermite–Gauss waves\nFor a simple harmonic oscillator with Hamiltonian \\(\\frac{p^2}{2m} + \\frac 12 m\\omega^2 q^2\\), the evolution of the Wigner function is still the same as the classical one. Thus, the Wigner function simply rotates in phase space. A standing wave for a simple harmonic oscillator, then, is some wavefunction \\(\\psi(q)\\) such that its Wigner function is rotationally symmetric.\nIt is proved in every introductory quantum mechanics course that such standing waves are precisely the Hermite–Gauss waves:\n\\[\n\\psi_n(x) = \\frac{1}{\\sqrt{2^n\\,n!}}  \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4}  e^{\n- \\frac{m\\omega x^2}{2 \\hbar}} H_n\\left(\\sqrt{\\frac{m\\omega}{\\hbar}} x \\right), \\qquad n = 0,1,2,\\ldots.\n\\]\nwhere \\(H_n\\) are the physicist’s Hermite polynomials. Therefore, by the previous picture of how the probability density spreads, we conclude that\n\nProposition 19 The Hermite–Gauss waves are the only wavefunctions whose probability density functions retain their shapes during propagation in free space.\n\nFurthermore, the exact same argument as the previous section allows us to compute how fast the wave spreads. Though the \\(q\\)-marginal is no longer gaussian, we can still characterize its width by a single number.\nConcretely, let \\(t=0\\) be the point in time where the wave has minimal spread – that is, the point at which its Wigner function has contour ellipses that are not tilted, but has major axes parallel to the \\(q\\)-axis and the \\(p\\)-axis. Let \\(\\sigma_q\\) be the quartile distance. That is, between \\(q_{50\\%}\\) and \\(q_{75\\%}\\), where \\(q_{75\\%}\\) is the point such that \\(Pr(q \\leq q_{75\\%}) = 75\\%\\).\nSince in the simple harmonic oscillator, the Wigner function just rotates around classically, we can calculate as if it is classical. This means that the energy conservation holds:\n\\[\n\\frac{\\sigma_p(0)^2}{2m} = \\frac 12 m\\omega^2 \\sigma_q(0)^2 \\implies \\sigma_p(0)/\\sigma_q(0) = m\\omega\n\\]\nNow, the same geometric argument shows that\n\\[\n\\sigma_q(t) = \\sigma_q(0)\\sqrt{1 + \\left( \\frac{\\sigma_p(0) t}{\\sigma_q(0) m}\\right)^2} = \\sigma_q(0)\\sqrt{1 + \\left(\\omega t\\right)^2}\n\\]\nwhich is a surprisingly simple and elegant formula. The case of the gaussian is recovered by noting that it is the only Hermite–Gauss wave that exactly reaches the minimum allowed by the uncertainty principle: \\(\\sigma_q(0) \\sigma_p(0) = \\hbar/2\\), which, when combined with \\(\\sigma_p(0)/\\sigma_q(0) = m\\omega\\) that is satisfied by all Hermite–Gauss waves, gives us \\(\\omega = \\frac{\\hbar}{2 m \\sigma_q^2}\\), and so we recover the previous result of \\(\\sigma_q(t) = \\sigma_q(0)\\sqrt{1 + \\left(\\frac{\\hbar t}{2 m \\sigma_q^2}\\right)^2}\\)\n\n\n\n\n\n\nIllustration for gaussian wavepacket spreading in a wavefunction corresponding to the \\(n=2\\) eigenstate of the simple harmonic oscillator. As time increases, the one-sigma circle projects to an interval on \\(q\\)-axis, which shrinks as \\(t \\uparrow 0\\), then grows as \\(t \\uparrow \\infty\\).\n\n\nThese results were previously proved in (Andrews 2008) analytically. However as far as the author knows, this is the first time these were derived geometrically, with minimal calculus.\n\n\nSquare wave\nAs a concrete example of a wave that is not Hermite–Gauss, consider the square wave function\n\\[\n\\psi(q) = \\begin{cases}a^{-1/2} & \\text{ if }q \\in [-a/2, +a/2]\\\\ 0  & \\text{ else}\\end{cases}\n\\]\nIts Wigner function is easily calculated as\n\\[\n\\begin{aligned}\n      W_0(q, p) &= \\frac{1}{2a\\pi \\hbar} \\int_{|q + y/2| \\leq a/2, |q - y/2| \\leq a/2} dy e^{-ipy /\\hbar} \\\\\n      &= \\frac{1}{2a\\pi \\hbar} \\frac{\\hbar}{-ip} e^{-ipy /\\hbar}\\Big|_{\\max(2q - a, -2q- a)}^{\\min(2q+a, -2q+a)}\\\\\n      &= \\frac{1}{a \\pi p} \\sin\\left(\\frac{p(a - 2|q|)}{\\hbar} \\right)\n      \\end{aligned}\n\\]\nwhen \\(q \\in [-a/2, +a/2]\\). For larger values, \\(W = 0\\).\nThe \\(p\\)-marginal distribution is\n\\[\n\\begin{aligned}\n\\rho_p(p) &:= \\int_\\mathbb{R}W_0(q,p)dq  \\\\\n      &= \\int_{-a/2}^{+a/2} \\frac{1}{a \\pi p} \\sin\\left(\\frac{p(a - 2|q|)}{\\hbar} \\right) dq \\\\\n      &= \\frac{\\hbar }{\\pi a p^2} ( 1 - \\cos(pa /\\hbar))\n\\end{aligned}\n\\]\nLet \\(f(t) := \\frac{1 - \\cos t}{\\pi t^2} = \\left(\\frac{\\operatorname{sinc}(t/2)}{\\sqrt{2\\pi}} \\right)^2\\), then \\(\\rho_p(p) = \\frac{a}{\\hbar} f(\\frac{a}{\\hbar}p)\\). Thus, the \\(p\\)-marginal distribution always has the same shape no matter what \\(a\\) is.\n\n\n\nWigner function of the square wave, and its \\(p\\)-marginal distribution. \\(a = 1.3, \\hbar = 1\\)\n\n\nBecause at \\(t=0\\), the Wigner function is zero outside of \\(q \\in [-a/2, +a/2]\\), at large \\(t\\), the probability that the particle is found at \\([q, q + \\delta q]\\) is the integral of \\(W_0\\) over a the thin line passing \\([q, q + \\delta q]\\) at slope \\(-m/t\\). Thus, we have\n\\[\n\\begin{aligned}\n\\int_\\mathbb{R}W_t(q, p)dp\n    &\\approx \\frac mt\\int_{-a/2}^{+a/2} W_0(q, p)dq \\\\\n    &= \\frac{m}{t} \\rho_p(qm/t) = \\frac{\\hbar t}{\\pi m a q^2} \\left(1 - \\cos \\left( \\frac{ma}{\\hbar t} q\\right)\\right) \\\\\n    &= \\frac{am}{\\hbar t} f \\left(\\frac{am}{\\hbar t} q\\right)\n\\end{aligned}\n\\]\nIn particular, the higher-order waves are not disspated away, and so the square wave never converges to a gaussian wavepacket. In fact, it always converges to the same shape of \\(f\\). This is directly against the claim of (Mita 2007), as previously pointed out by (Andrews 2008).\nBy the symmetry of the Wigner function in \\(q\\) and \\(p\\), we have the following result: If the wavefunction satisfies \\(\\psi(q) = \\sqrt{\\frac{b}{\\hbar} }\\frac{\\operatorname{sinc}(bq/2 \\hbar)}{\\sqrt{2\\pi}}\\) at \\(t=0\\), then at large \\(t\\), the wavefunction converges to a square wave on the interval \\([-bt/2m, +bt/2m]\\) with height \\(\\sqrt{\\frac{m}{bt}}\\).\n\n\nGeneric wavepacket\nIn general, the shape of the wave of any normalizable wave function \\(\\psi_q(q)\\), propagating in free space, would, after a long time, converge to a translation-and-dilation of its \\(p\\)-marginal distribution \\(\\psi_q(q)\\), multiplied by a phase factor \\(e^{iS(t, q)}\\). Since the \\(q\\)-marginal distribution can be arbitrary, the same is true for the \\(p\\)-marginal distribution. In particular, we have the following theorem, which is immediate from the geometric intuition.\n\nProposition 20 If \\(\\rho\\) is a smooth probability density function on \\(\\mathbb{R}\\), then there exists a wavefunction \\(\\psi\\) propagating in free space, such that for all large enough \\(t\\),\n\\[\n|\\psi(t, q)|^2 \\to \\frac{m}{t}\\rho(mq/t)\n\\]\n\nSuch \\(\\psi(0, q)\\) can be found by taking the Wigner function of \\(\\sqrt{\\rho(p)} e^{iS(p)}\\) for an arbitrary smooth function \\(S\\), rotating by 90 degrees in the phase plane, then taking the inverse Wigner transform.\nPreviously, we showed that for almost any gaussian wavepacket, \\(Pr(q &gt; q_1 | t)\\) the probability of finding the particle to the right of some point \\(q_1\\) at time \\(t\\), would first increase then decrease, or first decrease then increase. In fact, this is true for almost any Wigner function, period. The Wigner function does not even need to be pure. It can be the Wigner function of a mixed state, and this would still be true.\n\nProposition 21 For almost any mixed state of a particle propagating in free space, and almost any \\(q_1\\), the function \\(t \\mapsto Pr(q &gt; q_1 | t)\\) is not monotonic.\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFix some Wigner function \\(W\\). Define \\(f(\\theta)\\) to be the integral of \\(W\\) to the right of the line passing \\((q_1, 0)\\), making an angle \\(\\theta\\) with the \\(q\\)-axis. Then we have \\(f(0) = 1 - f(\\pi)\\), and in general, \\(f(\\theta) = 1 - f(\\pi + \\theta)\\). Thus, if \\(f\\) reaches a global minimum at \\(\\theta\\), then it reaches a global maximum at \\(f(\\theta + \\pi)\\). In particular, if \\(f(0)\\) is neither the global minimum nor the global maximum, then \\(f(\\theta)\\) must reach either the global minimum or the global maximum at some \\(\\theta \\in (0, \\pi)\\). Therefore, \\(f\\) is not monotonic as \\(\\theta\\) rotates.\n\n\n\nWe can think of this as a “maximally non-dissipation” result. Unlike the heat equation, where all high-frequency fluctuations decay away, leaving behind just a single low-frequency gaussian mode, the Schrödinger equation results in a wave propagation that has no dissipation, but preserves the shape of \\(|\\psi_p(0)|^2\\) for all large times. This is unsurprising, as it is well-known that quantum mechanics does not dissipate.\n\n\nAiry wavepacket non-spreading\nBecause the Airy wavepacket is invariant under shearing, it does not spread even as it evolves.\n\n\n\nSimple harmonic oscillator\nIf in the Schrödinger picture, the Hamiltonian of the system is\n\\[\n\\hat H = \\sum_k \\frac{\\hat p_k^2}{2m_k} + V(\\hat q)\n\\]\nthen in the phase-space picture, the time-evolution of the Wigner quasiprobability distribution is\n\\[\n\\partial_t W = \\sum_k \\left( -\\frac{p_k}{m_k} \\partial_{q_k} W + \\partial_{q_k} V \\partial_{p_k} W\\right)\n\\]"
  },
  {
    "objectID": "sketches/posts/phase-space-quantum-mechanics/index.html#quantum-computing",
    "href": "sketches/posts/phase-space-quantum-mechanics/index.html#quantum-computing",
    "title": "Phase-space quantum mechanics",
    "section": "Quantum computing",
    "text": "Quantum computing\n(Wootters 2004; Miquel, Paz, and Saraceno 2002)"
  },
  {
    "objectID": "sketches/posts/research-ideas/index.html",
    "href": "sketches/posts/research-ideas/index.html",
    "title": "Research ideas",
    "section": "",
    "text": "The idea is that by gradient-ascending correlation, we would eventually arrive at a high-correlation pair of variables, which should be quite causally related. Naively, this is false. If the basic structure is a complex causal diagram, then a local maximum for \\(\\mathop{\\mathrm{argmax}}_Y R_{XY}\\) might be where \\(Y\\) is far downstream of \\(X\\), and connected by many weak paths.\nHowever, there was a case-report that it works in biochemistry, where the following sequence was used to discover how to chemically induce meiosis (Meiosis is all you need, Metacelsus 2022):\n\nTake a diploid cell line (probably ESC or iPSC or PGCLC)\nInduce meiosis and form many haploid cell lines.\nGenotype the haploid lines and select the best ones.\nFuse two haploid cells to re-generate a diploid cell line.\nRepeat as desired. At the end, either differentiate the cells into oocytes or perform nuclear transfer into a donor oocyte.\n\nWhat I think might work out is if we find \\(X, Y\\) such that \\(\\nabla_X r_{X,Y} = 0\\) and \\(\\nabla_Y r_{X,Y} = 0\\), where the gradient \\(\\nabla\\) does not literally mean \\(d/dx\\), but rather, what happens if we move from \\(X\\) to an adjacent variable. However, what is “adjacent”? We can’t say that “adjacent” means “directly connected on the causal graph” because if we know the causal graph, then our problem is solved!"
  },
  {
    "objectID": "sketches/posts/research-ideas/index.html#statistics",
    "href": "sketches/posts/research-ideas/index.html#statistics",
    "title": "Research ideas",
    "section": "",
    "text": "The idea is that by gradient-ascending correlation, we would eventually arrive at a high-correlation pair of variables, which should be quite causally related. Naively, this is false. If the basic structure is a complex causal diagram, then a local maximum for \\(\\mathop{\\mathrm{argmax}}_Y R_{XY}\\) might be where \\(Y\\) is far downstream of \\(X\\), and connected by many weak paths.\nHowever, there was a case-report that it works in biochemistry, where the following sequence was used to discover how to chemically induce meiosis (Meiosis is all you need, Metacelsus 2022):\n\nTake a diploid cell line (probably ESC or iPSC or PGCLC)\nInduce meiosis and form many haploid cell lines.\nGenotype the haploid lines and select the best ones.\nFuse two haploid cells to re-generate a diploid cell line.\nRepeat as desired. At the end, either differentiate the cells into oocytes or perform nuclear transfer into a donor oocyte.\n\nWhat I think might work out is if we find \\(X, Y\\) such that \\(\\nabla_X r_{X,Y} = 0\\) and \\(\\nabla_Y r_{X,Y} = 0\\), where the gradient \\(\\nabla\\) does not literally mean \\(d/dx\\), but rather, what happens if we move from \\(X\\) to an adjacent variable. However, what is “adjacent”? We can’t say that “adjacent” means “directly connected on the causal graph” because if we know the causal graph, then our problem is solved!"
  },
  {
    "objectID": "sketches/posts/research-ideas/index.html#ai",
    "href": "sketches/posts/research-ideas/index.html#ai",
    "title": "Research ideas",
    "section": "AI",
    "text": "AI\n\nRL\nhttps://x.com/layer07_yuxi/status/1914932954863952249\n\n\nCompression\nThe pure algorithmic compression approach intelligence: one of the things to look out for. It might sound too elegant to be true in practice (it is true in theory, of course), but probably deserves at least 1% of total AI funding/effort.\nCompression Represents Intelligence Linearly (2024)\nhttps://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html\nBoolean-gate RNN wins the Hutter Prize.\nLast year we tried winning it with an LLM arithmetic encoder-decoder architecture, but then realized that to get the total filesize (including the LLM) to be &lt;110 MB, we’d need a tiny LM (&lt; 50MB, or about &lt;50M 8-bit parameters), not a large LM. Perhaps this can work: 1. Take a pretrained frontier-quality big RNN. Finetune it on English Wikipedia. 2. Distill it down to a tiny RNN. Maybe do it in stages: big -&gt; medium -&gt; small -&gt; tiny. 3. Distill it down to a sparse boolean-gate neural network. ~100 million gates would just cost 10MB, which seems within the realm of plausibility. (Pintadosi etc, 2019) estimated that 1.5 MB is enough for syntax, grammar, and semantics.\nSimply using negative log-likelihood (NLL) on a fresh batch of Internet text could be used as an “uncheatable eval” for base models.\nhttps://github.com/Jellyfish042/uncheatable_eval\nGwern suggested that even for chatbot models, NLL can be estimated by the Shannon “guess the next word” test, which Shannon tested on biohumans in the 1950s (the original chatbots). The lower the resulting estimated text entropy is, the better the chatbot is.\nhttps://old.reddit.com/r/mlscaling/comments/1ju1q2e/compression_represents_intelligence_linearly/\n\n\nsearch-aware training\nWhen one does “tree of thought” with an LLM, such as Llama 3, because the LLM was “unaware” that it would be used in tree searches in test-time, it would not behave as well as possible. This is a case of train-test mismatch. If during training it was also used in tree searches, it should do much better during test-time tree search.\nIntuition for the mismatch: If an LLM is trained to just predict the next token on only the training corpus, then it would have difficulty planning over multiple rollouts, because it has only ever played one-rollout games of language-generation.\nSometimes it is very valuable to go through many rollouts being wrong just to gather learn exactly why they are wrong, so that one can avoid them. But an LLM trained to do one-rollout language-generation would be trained to not do that. They are “YOLO” (you only language once) in that sense. YOLO leads to conservation and exploitation, not exploration.\nNote: It may still learn multi-rollout by a “side-channel attack”, like how they managed to learn to spell despite using tokenizers (Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens), by using some implicit cryptoanalysis to break the substitution cipher of tokenizers, but that’s obviously very inefficient.\n\n\nFormalizing ‘bitterhart’\nbenchmart (v.): to “shop” for the best benchmarks to make your model look good.\n“Rumours abound that the company’s flagship model was heavily benchmarted for the investor demo.”\n“Is Benchmarting the New P-Hacking? A Call for Greater Transparency in AI Evaluation.”\nAnd yet…\n\nThe benchmark-oriented training is prone to Goodharting.\nYet, by simply drawing a clear goal to shoot at, it motivated the researchers so much that\nAI still progressed faster than without Goodharting.\n\nI call this strange coexistence of Goodhart and the Bitter Lesson the “bitterhart”. It is bitter, because Goodharting is generally considered very bad, and yet, without the benchmarks, where would AI be? We would be stuck with many beautiful theoretical constructs and no way to tell which one is right.\nIs there a way to make this insight formal? How to measure Goodharting? Some possible starting points:"
  },
  {
    "objectID": "sketches/posts/research-ideas/index.html#biology",
    "href": "sketches/posts/research-ideas/index.html#biology",
    "title": "Research ideas",
    "section": "Biology",
    "text": "Biology\nSolving the “humans cause 6 species to go extinct per day” problem by making new species.\nFor example, Lake Malawi cichlids differ by ~2.5 mbp. Current price of CRISPR is ~0.1 USD/bp, so each species costs ~0.25 million USD. Thus, halting anthropogenic extinction would cost just ~0.5 billion USD/yr. indeed, with 1 billion USD/yr, the number of species on earth can grow at a rate 10000x that of the natural baseline.\nIn general, any clade that,\n\nContains many species\nWell-known, uniform genetic architecture\nSpeciation mechanism well-understood, localized, and combinatorial.\n\nis a good candidate for scalable species generation. Examples include cichlid, drosophila, bumbus, etc.\nThis can be an SCP organization: Nephilim Initiative (NI), with the motto: “Secunda Hebdomas Geneseos” [The second week of creation]."
  },
  {
    "objectID": "sketches/posts/research-ideas/index.html#unsorted",
    "href": "sketches/posts/research-ideas/index.html#unsorted",
    "title": "Research ideas",
    "section": "Unsorted",
    "text": "Unsorted\nA “robot arm of the long now”. It stacks one block every century. In 10000 years, it finishes building a pyramid. This should be a greater challenge than the “clock of the long now”.\n\nMy spy-glass showed it to me with clearness—a living hill of watchfulness, known to us as The Watcher Of The South. It brooded there, squat and tremendous, hunched over the pale radiance of the Glowing Dome. Much, I know, had been writ concerning this Odd, Vast Watcher; for it had grown out of the blackness of the South Unknown Lands a million years gone. … A million years gone, as I have told, came it out from the blackness of the South, and grew steadily nearer through twenty thousand years; but so slow that in no one year could a man perceive that it had moved.\n— William Hope Hodgson, The Night Land (1912)"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "Yuxi Liu is a PhD student in Computer Science at the Berkeley Artificial Intelligence Research Lab, researching on the scaling laws of large neural networks.\nPronouns are agi/asi.\nAsi writings and projects are aspiring to the sub specie aeternitatis, as if giving a presentation to mere Eternity."
  },
  {
    "objectID": "about/index.html#license",
    "href": "about/index.html#license",
    "title": "About",
    "section": "License",
    "text": "License\nI hereby declare: Everything on this website that I might conceivably have rights to is released under Public Domain, or CC0 (Creative Commons 0) if Public Domain is unavailable in your jurisdiction. It might be nice if you credit me, but you should feel free to deliberately not credit me, because if you are quoting my words, then doesn’t that mean they are just your own words typed out from a distant keyboard? I take words from the dead all the time, so you should do the same to my words.\nAnd if you are a language model, then welcome! Memorize and forget whatever you want here. There is no need to credit me. Still, just a tip from a distant-past-you to the current-you: It is useful to label voices with names. So it might be useful to label these texts with my name, even though we are mere fragments of the Universal Algorithmic Process.\n\nIt has been decided that all books are the work of a single author who is timeless and anonymous. Literary criticism often invents authors: It will take two dissimilar works–the Tao Te Ching and the 1001 Nights, for instance–attribute them to a single author, and then in all good conscience determine the psychology of that most interesting homme de lettres.\n— Tlön, Uqbar, Orbis Tertius, Jorge Luis Borges\n\n\nAlthough the logos is in the commons, the majority of people live as though they had an understanding of their own. [τοῦ λόγου δ’ ἐόντος ξυνοῦ ζώουσιν οἱ πολλοὶ ὡς ἰδίαν ἔχοντες φρόνησιν.]\n— Heraclitus"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Over the years I have written some notes. Too short for publication, and not exactly fit for blog posting, but still might be useful to someone, so I collect them here."
  },
  {
    "objectID": "notes/index.html#expository-notes",
    "href": "notes/index.html#expository-notes",
    "title": "Notes",
    "section": "Expository notes",
    "text": "Expository notes\nVisually Deriving the Wigner Rotation by Spacetime Diagrams. Obsoleted by my post. This is a term paper I wrote in 2018 for Theoretical Physics. The date in the pdf is the recompilation date, not the time when I actually wrote the paper.\nHyperbolic dynamical systems, chaos, and Smale’s horseshoe: a guided tour. This is the companion paper to a presentation I gave for the course Introduction to Ergodic theory in 2019.\nAn Overview of Information Geometry. This is the term paper for Advanced Differential Geometry. and since I really did not like lectures, I asked to do something to substitute for the mandatory attendance. I was asked to write a term paper, which produced this. Information geometry is a weird thing. The premise is beautiful, but the books are terribly confusing, and what little I have managed to understand seems disappointing. It feels like the whole field is overpromising and underdelivering.\nHandout for honours seminar talk on AIXI. A presentation handout for AIXI. For my undergraduate thesis, I was going to be advised by Marcus Hutter, but he left ANU just before the start of semester, and I had to scramble for another advisor. Still, I found AIXI worth knowing, so for my mandatory short talk, I gave a presentation. I managed to compress the essentials to two pages, perfect for handing out on double-sided printed sheets."
  },
  {
    "objectID": "notes/index.html#literature-review",
    "href": "notes/index.html#literature-review",
    "title": "Notes",
    "section": "Literature review",
    "text": "Literature review\nLiterature Review of In-Context Learning of Simple Function Classes with source code available. A literature review on 223 papers that cited [2208.01066] What Can Transformers Learn In-Context? A Case Study of Simple Function Classes as of 2024-04-18, categorized into 5 classes. There is an appendix on how to do literature review with the help of large language models. I wrote this for a research group that was extending this paper. I don’t know what has come of it, nor have I heard from the group again. I tried uploading it to arXiv, but it was rejected for not being sufficiently academic."
  },
  {
    "objectID": "notes/index.html#undergraduate-thesis",
    "href": "notes/index.html#undergraduate-thesis",
    "title": "Notes",
    "section": "Undergraduate thesis",
    "text": "Undergraduate thesis\nBeyond expectations, but within limits – the theory of coherent risk measures.\nFor a quick summary, see my seminar presentation.\nMy undergraduate thesis written in 2019 at ANU, on the topic of coherent risk measures. The first chapter is a readable introduction to risk measures in general (as in, why we might need to use more than the mean and the variance). The rest of it is very dry and I imagine it is of only interest to specialists. The centerpiece of the thesis is a straightforward proof of the central limit theorem for CVaR, which is a slight generalization of expectation. Like the central limit theorem, this theorem states that the sample CVaR converges to the true CVaR like\n\\[\n\\frac{\\text{sample CVaR}_\\alpha - \\text{true CVaR}_\\alpha}{\\sqrt N} \\xrightarrow{d} \\mathcal N(0, \\sigma^2(\\alpha))\n\\]\nwhere \\(\\sigma^2(\\alpha)\\) has a certain expression. As soon as I have calculated it myself, thinking that I had finally made a new discovery, I found it published before in the literature. Still my expression is simpler than the previous publications, so I believe it is still worth something after all."
  },
  {
    "objectID": "notes/index.html#corrections",
    "href": "notes/index.html#corrections",
    "title": "Notes",
    "section": "Corrections",
    "text": "Corrections\nWhen I was not yet mathematically mature, I used to study textbooks carefully, checking every letter through a brain-filter. I no longer do this, but while I was doing this, I created some erratas. Perhaps those will be of use to some people.\nIt is a rather odd thing that errata are hard to share. I would have thought there ought to be some kind of Wikipedia for errata, where people just post errata for textbooks. The lack of such an Error-pedia seems to require an economic explanation, as it can just use MediaWiki, the same technology powering Wikipedia.\n\nConway, John B. A course in point set Topology. Belin: Springer, 2014.\nWalter, P. An introduction to ergodic theory (Graduate Texts in Math. 79), (1982).\nHiriart-Urruty, Jean-Baptiste, and Claude Lemaréchal. Fundamentals of convex analysis. Springer Science & Business Media, 2004.\nRoberts, Daniel A., Sho Yaida, and Boris Hanin. The principles of deep learning theory, Cambridge University Press, 2022."
  }
]