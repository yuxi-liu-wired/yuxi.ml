<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2024-07-04">
<meta name="description" content="How to think like a classical statistical-mechanic, with many examples from gas theory, biology, probability, and information theory. Prerequisites: thermodynamics, calculus, probability, and mathematical maturity.">

<title>Statistical Mechanics – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Statistical Mechanics – Yuxi on the Wired">
<meta property="og:description" content="How to think like a classical statistical-mechanic, with many examples from gas theory, biology, probability, and information theory. Prerequisites: thermodynamics, calculus, probability, and mathematical maturity.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/statistical-mechanics/figure/banner/banner.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="768">
<meta property="og:image:width" content="1232">
<meta property="og:image:alt" content="A bricolage representing modern statistical mechanics in all its multifaceted chaotic order. Containing numbers (information), particles in a box, Jupiter's great red eye, RNA hairpins, bacteria with wavy flagella. Made in Ideogram V1, with prompt 'An abstract conceptual illustration of a brutalist structure, with sharp angles and a minimalistic design, made of concrete, steel balls, and rebars. The structure appears to be made up of particles in a box, jamming together, creating bits and numbers in the realm of 0-1. The design also features an RNA hairpin, with eddies upon eddies upon eddies, evoking the image of Jupiter's red spot. The random walk of swimming bacteria with wavy flagella is depicted, adding to the complexity of the design. The illustration is presented on a white background, with high contrast and a monochromatic color scheme, reminiscent of a flat style vector SVG art., illustration, conceptual art'.">
<meta name="twitter:title" content="Statistical Mechanics – Yuxi on the Wired">
<meta name="twitter:description" content="How to think like a classical statistical-mechanic, with many examples from gas theory, biology, probability, and information theory. Prerequisites: thermodynamics, calculus, probability, and mathematical maturity.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/statistical-mechanics/figure/banner/banner.png">
<meta name="twitter:image-height" content="768">
<meta name="twitter:image-width" content="1232">
<meta name="twitter:image:alt" content="A bricolage representing modern statistical mechanics in all its multifaceted chaotic order. Containing numbers (information), particles in a box, Jupiter's great red eye, RNA hairpins, bacteria with wavy flagella. Made in Ideogram V1, with prompt 'An abstract conceptual illustration of a brutalist structure, with sharp angles and a minimalistic design, made of concrete, steel balls, and rebars. The structure appears to be made up of particles in a box, jamming together, creating bits and numbers in the realm of 0-1. The design also features an RNA hairpin, with eddies upon eddies upon eddies, evoking the image of Jupiter's red spot. The random walk of swimming bacteria with wavy flagella is depicted, adding to the complexity of the design. The illustration is presented on a white background, with high contrast and a monochromatic color scheme, reminiscent of a flat style vector SVG art., illustration, conceptual art'.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Statistical Mechanics</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          How to think like a classical statistical-mechanic, with many examples from gas theory, biology, probability, and information theory. Prerequisites: thermodynamics, calculus, probability, and mathematical maturity.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">math</div>
                <div class="quarto-category">physics</div>
                <div class="quarto-category">probability</div>
                <div class="quarto-category">statistics</div>
                <div class="quarto-category">biology</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 4, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">March 4, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#what-this-essay-contains" id="toc-what-this-essay-contains" class="nav-link" data-scroll-target="#what-this-essay-contains">What this essay contains</a></li>
  <li><a href="#quick-reference" id="toc-quick-reference" class="nav-link" data-scroll-target="#quick-reference">Quick reference</a></li>
  <li><a href="#further-readings" id="toc-further-readings" class="nav-link" data-scroll-target="#further-readings">Further readings</a></li>
  </ul></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#philosophical-comments" id="toc-philosophical-comments" class="nav-link" data-scroll-target="#philosophical-comments">Philosophical comments</a></li>
  <li><a href="#principles-of-statistical-mechanics" id="toc-principles-of-statistical-mechanics" class="nav-link" data-scroll-target="#principles-of-statistical-mechanics">Principles of statistical mechanics</a></li>
  <li><a href="#sec-differential-entropy-ill-defined" id="toc-sec-differential-entropy-ill-defined" class="nav-link" data-scroll-target="#sec-differential-entropy-ill-defined">Differential entropy depends on coordinates choice</a></li>
  <li><a href="#jaynes-epistemological-interpretation" id="toc-jaynes-epistemological-interpretation" class="nav-link" data-scroll-target="#jaynes-epistemological-interpretation">Jaynes’ epistemological interpretation</a></li>
  </ul></li>
  <li><a href="#mathematical-developments" id="toc-mathematical-developments" class="nav-link" data-scroll-target="#mathematical-developments">Mathematical developments</a>
  <ul class="collapse">
  <li><a href="#fundamental-theorems" id="toc-fundamental-theorems" class="nav-link" data-scroll-target="#fundamental-theorems">Fundamental theorems</a></li>
  <li><a href="#microcanonical-ensembles" id="toc-microcanonical-ensembles" class="nav-link" data-scroll-target="#microcanonical-ensembles">Microcanonical ensembles</a></li>
  <li><a href="#canonical-ensembles" id="toc-canonical-ensembles" class="nav-link" data-scroll-target="#canonical-ensembles">Canonical ensembles</a></li>
  <li><a href="#free-entropies" id="toc-free-entropies" class="nav-link" data-scroll-target="#free-entropies">Free entropies</a></li>
  <li><a href="#the-partition-function" id="toc-the-partition-function" class="nav-link" data-scroll-target="#the-partition-function">The partition function</a></li>
  <li><a href="#conditional-entropies" id="toc-conditional-entropies" class="nav-link" data-scroll-target="#conditional-entropies">Conditional entropies</a></li>
  </ul></li>
  <li><a href="#kinetic-gas-theory" id="toc-kinetic-gas-theory" class="nav-link" data-scroll-target="#kinetic-gas-theory">Kinetic gas theory</a>
  <ul class="collapse">
  <li><a href="#ideal-gas" id="toc-ideal-gas" class="nav-link" data-scroll-target="#ideal-gas">Ideal gas</a></li>
  <li><a href="#ideal-gas-again" id="toc-ideal-gas-again" class="nav-link" data-scroll-target="#ideal-gas-again">Ideal gas (again)</a></li>
  <li><a href="#hard-ball-gas-dilute-gas-limit" id="toc-hard-ball-gas-dilute-gas-limit" class="nav-link" data-scroll-target="#hard-ball-gas-dilute-gas-limit">Hard ball gas (dilute gas limit)</a></li>
  <li><a href="#soft-ball-gas-high-temperature-and-dilute-gas-limit" id="toc-soft-ball-gas-high-temperature-and-dilute-gas-limit" class="nav-link" data-scroll-target="#soft-ball-gas-high-temperature-and-dilute-gas-limit">Soft ball gas (high temperature and dilute gas limit)</a></li>
  </ul></li>
  <li><a href="#other-classical-examples" id="toc-other-classical-examples" class="nav-link" data-scroll-target="#other-classical-examples">Other classical examples</a>
  <ul class="collapse">
  <li><a href="#countably-many-states" id="toc-countably-many-states" class="nav-link" data-scroll-target="#countably-many-states">Countably many states</a></li>
  <li><a href="#fluctuation-by-n-12" id="toc-fluctuation-by-n-12" class="nav-link" data-scroll-target="#fluctuation-by-n-12">Fluctuation by <span class="math inline">\(N^{-1/2}\)</span></a></li>
  <li><a href="#blackbody-radiation" id="toc-blackbody-radiation" class="nav-link" data-scroll-target="#blackbody-radiation">Blackbody radiation</a></li>
  <li><a href="#sec-rubber-band" id="toc-sec-rubber-band" class="nav-link" data-scroll-target="#sec-rubber-band">Rubber bands</a></li>
  </ul></li>
  <li><a href="#combinatorial-examples" id="toc-combinatorial-examples" class="nav-link" data-scroll-target="#combinatorial-examples">Combinatorial examples</a>
  <ul class="collapse">
  <li><a href="#burning-the-library-of-babel" id="toc-burning-the-library-of-babel" class="nav-link" data-scroll-target="#burning-the-library-of-babel">Burning the library of Babel</a></li>
  <li><a href="#multinomials-and-the-chi-squared-test" id="toc-multinomials-and-the-chi-squared-test" class="nav-link" data-scroll-target="#multinomials-and-the-chi-squared-test">Multinomials and the chi-squared test</a></li>
  <li><a href="#sanovs-theorem" id="toc-sanovs-theorem" class="nav-link" data-scroll-target="#sanovs-theorem">Sanov’s theorem</a></li>
  <li><a href="#surface-area-of-high-dimensional-spheres" id="toc-surface-area-of-high-dimensional-spheres" class="nav-link" data-scroll-target="#surface-area-of-high-dimensional-spheres">Surface area of high-dimensional spheres</a></li>
  </ul></li>
  <li><a href="#biological-examples" id="toc-biological-examples" class="nav-link" data-scroll-target="#biological-examples">Biological examples</a>
  <ul class="collapse">
  <li><a href="#how-elastic-is-the-skin-of-red-blood-cell" id="toc-how-elastic-is-the-skin-of-red-blood-cell" class="nav-link" data-scroll-target="#how-elastic-is-the-skin-of-red-blood-cell">How elastic is the skin of red blood cell?</a></li>
  <li><a href="#the-lac-operon" id="toc-the-lac-operon" class="nav-link" data-scroll-target="#the-lac-operon">The <code>lac</code> operon</a></li>
  <li><a href="#sec-rna-hairpin" id="toc-sec-rna-hairpin" class="nav-link" data-scroll-target="#sec-rna-hairpin">Unzipping RNA hairpins</a></li>
  <li><a href="#hungry-hungry-bacteria" id="toc-hungry-hungry-bacteria" class="nav-link" data-scroll-target="#hungry-hungry-bacteria">Hungry hungry bacteria</a></li>
  </ul></li>
  <li><a href="#statistical-field-theory" id="toc-statistical-field-theory" class="nav-link" data-scroll-target="#statistical-field-theory">Statistical field theory</a>
  <ul class="collapse">
  <li><a href="#maximum-caliber" id="toc-maximum-caliber" class="nav-link" data-scroll-target="#maximum-caliber">Maximum caliber</a></li>
  <li><a href="#fluctuation-dissipation-relations" id="toc-fluctuation-dissipation-relations" class="nav-link" data-scroll-target="#fluctuation-dissipation-relations">Fluctuation-dissipation relations</a></li>
  <li><a href="#equality-before-the-law" id="toc-equality-before-the-law" class="nav-link" data-scroll-target="#equality-before-the-law">Equality before the law</a></li>
  <li><a href="#one-dimensional-fdr" id="toc-one-dimensional-fdr" class="nav-link" data-scroll-target="#one-dimensional-fdr">One-dimensional FDR</a></li>
  <li><a href="#sound-waves" id="toc-sound-waves" class="nav-link" data-scroll-target="#sound-waves">Sound waves</a></li>
  </ul></li>
  <li><a href="#metastability" id="toc-metastability" class="nav-link" data-scroll-target="#metastability">Metastability</a>
  <ul class="collapse">
  <li><a href="#common-applications" id="toc-common-applications" class="nav-link" data-scroll-target="#common-applications">Common applications</a></li>
  <li><a href="#uncommon-applications" id="toc-uncommon-applications" class="nav-link" data-scroll-target="#uncommon-applications">Uncommon applications</a></li>
  </ul></li>
  <li><a href="#sec-cft" id="toc-sec-cft" class="nav-link" data-scroll-target="#sec-cft">Crooks fluctuation theorem</a>
  <ul class="collapse">
  <li><a href="#in-a-closed-system-microcanonical" id="toc-in-a-closed-system-microcanonical" class="nav-link" data-scroll-target="#in-a-closed-system-microcanonical">In a closed system (microcanonical)</a></li>
  <li><a href="#in-an-energy-bath-canonical" id="toc-in-an-energy-bath-canonical" class="nav-link" data-scroll-target="#in-an-energy-bath-canonical">In an energy bath (canonical)</a></li>
  <li><a href="#easy-consequences" id="toc-easy-consequences" class="nav-link" data-scroll-target="#easy-consequences">Easy consequences</a></li>
  </ul></li>
  <li><a href="#applications-of-cft" id="toc-applications-of-cft" class="nav-link" data-scroll-target="#applications-of-cft">Applications of CFT</a>
  <ul class="collapse">
  <li><a href="#molecular-machines" id="toc-molecular-machines" class="nav-link" data-scroll-target="#molecular-machines">Molecular machines</a></li>
  <li><a href="#worked-example-bouncing-ball" id="toc-worked-example-bouncing-ball" class="nav-link" data-scroll-target="#worked-example-bouncing-ball">Worked example: bouncing ball</a></li>
  <li><a href="#other-examples" id="toc-other-examples" class="nav-link" data-scroll-target="#other-examples">Other examples</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="introduction" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<section id="what-this-essay-contains" class="level3">
<h3 class="anchored" data-anchor-id="what-this-essay-contains">What this essay contains</h3>
<p>Classical mechanics, electrodynamics, and thermodynamics are all conceptually simple, but settled. Things are less clear with statistical mechanics. To say it kindly, it is a lively field of research. To say it unkindly, it is a field whose very conceptual foundation is in doubt. The main problem is not with quantum mechanics, as quantum statistical mechanics works very well, but with how to handle systems far-from-equilibrium. Fortunately, as long as we stick to the (near-)equilibrium parts, then it is mostly settled, and so this is how this essay is going to be written about. We will avoid quantum because that deserves its own entire essay, and avoid far-from-equilibrium because it is unsettled. What we can present is still a beautifully precise mathematical toolbox with surprisingly wide applications.</p>
<p>Statistical mechanics is technically independent of thermodynamics, but it is strongly related. You can acquire the basic skills in thermodynamics by working through the first half of my previous essay <a href="https://yuxi-liu-wired.github.io/essays/posts/equilibrium-thermoeconomics/"><em>Classical Thermodynamics and Economics</em></a>.</p>
<p>About half of the essay is taken up with calculations. The theoretical core of statistical mechanics is small, and most of the skills are in applying it to actual systems. Therefore, a lot of worked-through examples are necessary. I have tried to make them flow well and skimmable.</p>
<p>The essay contains: entropy, free entropies, partition function, about 12 useful theorems, fluctuation-dissipation relations, maximal caliber, Crooks fluctuation theorem, Jarzynski equality, rubber bands, kinetic gas theory, van der Waals law, blackbody radiation, combinatorics, chi-squared test, large deviation theory, bacteria hunting, unzipping RNA hairpins, Arrhenius equation, martingales, Maxwell’s demon, Laplace’s demon.</p>
<p>It does not contain: quantum statistical mechanics, nonequilibrium statistical mechanics, linear response theory, Onsager reciprocal relations, statistical field theory, phase transitions, stochastic processes, Langevin equation, diffusion theory, Fokker–Planck equation, Feynman–Kac formula, Brownian ratchets.</p>
<p>The prerequisites are thermodynamics, multivariate calculus, probability, combinatorics, and mathematical maturity. It’s good to be familiar with biology and the basics of random walk as well.</p>
</section>
<section id="quick-reference" class="level3">
<h3 class="anchored" data-anchor-id="quick-reference">Quick reference</h3>
<ul>
<li><span class="math inline">\(D_{KL}\)</span>: <a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback–Leibler divergence</a>.</li>
<li><span class="math inline">\(S[\rho]\)</span>: entropy of probability distribution <span class="math inline">\(\rho\)</span>.</li>
<li><span class="math inline">\(S^*\)</span>: maximal entropy under constraints.</li>
<li><span class="math inline">\(f[\rho]\)</span>: Helmholtz free entropy of probability distribution <span class="math inline">\(\rho\)</span>.</li>
<li><span class="math inline">\(f^*_{X|y}\)</span>: maximal Helmholtz free entropy under the constraint that <span class="math inline">\(Y = y\)</span>.</li>
<li><span class="math inline">\(Z\)</span>: the partition function.</li>
<li><span class="math inline">\(\beta\)</span>: inverse temperature.</li>
<li><span class="math inline">\(N\)</span>: number of particles, or some other quantity that can get very large.</li>
<li><span class="math inline">\(n\)</span>: number of dimensions, or some other quantity that is fixed.</li>
<li><span class="math inline">\(F\)</span>: Helmholtz free energy.</li>
<li><span class="math inline">\(\binom{m}{n}\)</span>: <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial coefficient</a>.</li>
<li><span class="math inline">\(\braket{f(x, y) | z}_y\)</span>: The expectation of <span class="math inline">\(f\)</span> where we fix <span class="math inline">\(x\)</span>, let <span class="math inline">\(y\)</span> vary, and conditional on <span class="math inline">\(z\)</span>.</li>
<li><span class="math inline">\(\mathrm{Var}\)</span>: variance.</li>
<li><span class="math inline">\(\int (\cdots) D[x]\)</span>: path integral where <span class="math inline">\(x\)</span> varies over the space of all possible paths.</li>
</ul>
<p>As usual, we set <span class="math inline">\(k_B = 1\)</span>, so that <span class="math inline">\(\beta = 1/T\)</span>, except when we need a numerical answer in SI units.</p>
</section>
<section id="further-readings" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="further-readings">Further readings</h3>
<p>Unfortunately, I never learned statistical mechanics from any textbook. I just understood things gradually on my own after trying to make sense of things. This means I cannot recommend any introductory textbook based on my personal experience.</p>
<ul>
<li>Books I did learn from:
<ul>
<li><span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021</a>)</span> shows how a modern statistical mechanist thinks. It is a rather eclectic book, because modern statistical mechanics is full of weird applications, from music theory to economics.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li><span class="citation" data-cites="nelsonBiologicalPhysicsEnergy2003">(<a href="#ref-nelsonBiologicalPhysicsEnergy2003" role="doc-biblioref">Nelson 2003</a>)</span> teaches basic statistical thermodynamics in the context of biology, and <span class="citation" data-cites="bergRandomWalksBiology1993">(<a href="#ref-bergRandomWalksBiology1993" role="doc-biblioref">Howard C. Berg 1993</a>)</span> teaches random walks.</li>
<li><span class="citation" data-cites="ben-naimFarewellEntropyStatistical2008 feynmanFeynmanLecturesComputation1996">(<a href="#ref-ben-naimFarewellEntropyStatistical2008" role="doc-biblioref">Ben-Naim 2008</a>; <a href="#ref-feynmanFeynmanLecturesComputation1996" role="doc-biblioref">Richard P. Feynman 1996</a>)</span> show how to combine (unify?) statistical mechanics with information theory.</li>
<li><span class="citation" data-cites="jaynesProbabilityTheoryLogic2003">(<a href="#ref-jaynesProbabilityTheoryLogic2003" role="doc-biblioref">Jaynes 2003</a>)</span> gives Jaynes’ entire philosophy of information, one application of which is his theory of why entropy is maximized in statistical mechanics.</li>
<li><span class="citation" data-cites="lemonsTrailBlackbodyRadiation2022">(<a href="#ref-lemonsTrailBlackbodyRadiation2022" role="doc-biblioref">Lemons, Shanahan, and Buchholtz 2022</a>)</span> closely follows the story of how Planck actually derived the blackbody radiation law. Reading it, you almost have the illusion that you too could have discovered what he discovered.</li>
<li><span class="citation" data-cites="penroseFoundationsStatisticalMechanics2005">(<a href="#ref-penroseFoundationsStatisticalMechanics2005" role="doc-biblioref">Penrose 2005</a>)</span> gives an elegant mathematical deduction that brings philosophers and mathematical logicians joy. However, it is not useful for applications.</li>
</ul></li>
<li>Books I did not learn from, but feel obliged to recommend:
<ul>
<li><span class="citation" data-cites="maStatisticalMechanics1985 tolmanPrinciplesStatisticalMechanics1980 feynmanStatisticalMechanicsSet2018 schrodingerStatisticalThermodynamics1989">(<a href="#ref-maStatisticalMechanics1985" role="doc-biblioref">Ma 1985</a>; <a href="#ref-tolmanPrinciplesStatisticalMechanics1980" role="doc-biblioref">Tolman 1980</a>; <a href="#ref-feynmanStatisticalMechanicsSet2018" role="doc-biblioref">Richard P. Feynman 2018</a>; <a href="#ref-schrodingerStatisticalThermodynamics1989" role="doc-biblioref">Schrodinger 1989</a>)</span> are books that apparently every real physicist must read before they die. Like those other “1001 books you must read before you die”, I did not read them.</li>
<li><span class="citation" data-cites="nashElementsStatisticalThermodynamics2006">(<a href="#ref-nashElementsStatisticalThermodynamics2006" role="doc-biblioref">Nash 2006</a>)</span> is a concise introduction for chemistry students.</li>
<li><span class="citation" data-cites="sommerfeldLecturesTheoreticalPhysics1950">(<a href="#ref-sommerfeldLecturesTheoreticalPhysics1950" role="doc-biblioref">Sommerfeld 1950, vol. 5</a>)</span> is by the master, Sommerfeld. If you want to do 19th century style thermodynamics, then it is very good, but otherwise, I don’t know what this book is for.</li>
</ul></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The book has quite many buzzwords like “fractals”, “complexity”, “avalanche”, and “edge of chaos”, buzzy in the 1990s. A joke is that during the 1980s, as the Cold War was winding down, physicists were overproduced and underemployed, and had to find someway to get employed. Thus, they went into economics, social sciences, etc, resulting in the discipline of “econophysics”, the nebulous non-discipline of “complexity studies”, etc.</p></div></div></section>
</section>
<section id="overview" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<section id="philosophical-comments" class="level3">
<h3 class="anchored" data-anchor-id="philosophical-comments">Philosophical comments</h3>
<p>It is fair to say that, although it originated in the 19th century like all other classical fields of physics, statistical mechanics is unsettled.</p>
<p>Trajectory-centric statistical mechanics. In this view, we start with the equations of motion for a physical system, then study statistical properties of individual trajectories, or collections of them. For example, if we have a pendulum hanging in air, being hit by air molecules all the time, we would study the total trajectory <span class="math inline">\((\theta, x_1, y_1, z_1, x_2, y_2, z_2, \dots)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle of the pendulum swing, and <span class="math inline">\((x_i, y_i, z_i)\)</span> is the location of the <span class="math inline">\(i\)</span>-th air molecule. Then we may ask that, over a long enough period, how frequent would the pendulum visit a certain angle range of <span class="math inline">\([\theta_0, \theta_0 + \delta\theta]\)</span>:</p>
<p><span class="math display">\[
Pr(\theta \in [\theta_0, \theta_0 + \delta\theta]) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{+T} 1[\theta \in [\theta_0, \theta_0 + \delta\theta]] dt
\]</span></p>
<p>In the trajectory-centric view, there are the following issues:</p>
<ul>
<li>Problem of ergodicity: When does time-average equal ensemble-average? A system is called “ergodic” iff for almost all starting conditions, the time-average of the trajectory is the ensemble-average over all trajectories.</li>
<li>Problem of entropy: How is entropy defined <em>on a single trajectory</em>?</li>
<li>H-theorem: In what sense, and under what conditions, does entropy increase?</li>
<li>Problem of equilibrium: What does it mean to say that a trajectory is <em>in equilibrium</em>?</li>
<li>Approach to equilibrium: In what sense, and under what conditions, does the trajectory converge to an equilibrium?</li>
<li>Reversibility problem (<em>Umkehreinwand</em>): If individual trajectories are reversible, why does entropy increase instead of decrease?</li>
</ul>
<p>While these philosophical problems are quite diverting, we will avoid them as much as possible, because we will be working with the ensemble-centric equilibrium statistical mechanics. This is the statistical mechanics that every working physicist uses, and this is what we will present. If you are interested in the philosophical issues, read the <a href="https://plato.stanford.edu/entries/statphys-statmech/">Stanford Encyclopedia entry on the <em>Philosophy of Statistical Mechanics</em></a>.</p>
</section>
<section id="principles-of-statistical-mechanics" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="principles-of-statistical-mechanics">Principles of statistical mechanics</h3>
<ul>
<li>A <strong>physical system</strong> is a classical system with a state space, evolving according to some equation of motion.</li>
<li>An <strong>ensemble</strong> of that system is a probability distribution over its state space.</li>
<li>The idea of (ensemble-centric) statistical mechanics is to study the evolution of an entire probability distribution over all possible states.</li>
<li>The <strong>entropy</strong> of a probability distribution <span class="math inline">\(\rho\)</span> is</li>
</ul>
<p><span class="math display">\[S[\rho] := -\int dx\; \rho(x) \ln \rho(x)\]</span></p>
<ul>
<li>Under any constraint, there exists a unique ensemble, named the <strong>equilibrium ensemble</strong>, which maximizes entropy under constraint.</li>
</ul>
<p>Most of the times, the state space is a phase space, and the equation of motion is described by a Hamiltonian function. However, the machinery of statistical mechanics, as given above, is purely mathematical. It can be used to study any problem in probability whatsoever, even those with no physical meaning.</p>
<p>Believe it or not, the above constitutes the entirety of equilibrium statistical mechanics. So far, it is a purely mathematical theory, with no falsifiability (Popperians shouting in the background). To make it falsifiable, we need to add one more assumption, necessarily fuzzy:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><blockquote class="blockquote"><sup>2</sup>&nbsp;
<p>As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.</p>
<p>— Albert Einstein, <em>Address to Prussian Academy of Sciences</em> (1921)</p>
</blockquote>
</div></div><ul>
<li>The equilibrium ensemble is physically meaningful and describes the observable behavior of physical systems.</li>
</ul>
<p>In other words, when a physical system is <em>at equilibrium</em>, then everything observable can be found by studying it <em>as if</em> it has the maximum entropy distribution under constraint.</p>
<p>Of course, just what that “is physically meaningful” means, is another source of endless philosophical arguments. I would trust that you will know what is physically meaningful, and leave it at that, while those who have a taste for philosophy can grapple with the <a href="https://plato.stanford.edu/entries/scientific-underdetermination/">Duhem–Quine thesis</a>.</p>
</section>
<section id="sec-differential-entropy-ill-defined" class="level3">
<h3 class="anchored" data-anchor-id="sec-differential-entropy-ill-defined">Differential entropy depends on coordinates choice</h3>
<p>There is a well-known secret among information theorists: differential entropy is ill-defined.</p>
<p>Consider the uniform distribution on <span class="math inline">\([0, 1]\)</span>. It is the maximal-entropy distribution on <span class="math inline">\([0, 1]\)</span> – relative to the Lebesgue measure. However, why should we pick the Lebesgue measure, and what happens if we don’t?</p>
<p>Suppose we now stretch the <span class="math inline">\([0, 1]\)</span> interval nonlinearly, by <span class="math inline">\(f(x) = x^2\)</span>, then the maximal-entropy distribution relative to <em>that</em> would no longer be the uniform distribution on <span class="math inline">\([0, 1]\)</span>. Instead, it would be the uniform distribution after stretching.</p>
<p>The problem is this: Differential entropy is not coordinate-free. If we change the coordinates, we change the base measure, and the differential entropy changes as well.</p>
<p>To fix this, we need to use the KL-divergence, which is invariant under a change of base measure, as in <span class="math display">\[-D_{KL}(\rho \| \mu) := - \int dx\; \rho(x) \ln\frac{\rho(x)}{\mu(x)}\]</span></p>
<p>In typical situations, we don’t need to worry ourselves with KL-divergence, as we just pick the uniform distribution <span class="math inline">\(\mu\)</span>. When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an <em>improper prior</em>.</p>
<p>In this interpretation, the principle of “maximum entropy distribution under constraint” becomes the principle of “minimal KL-divergence under constraint”, which <em>is</em> Bayesian inference, with exactly the same formulas.</p>
<p>In almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn’t really justify it other than saying that <em>it just works</em>, and suggesting it has something to do with Liouville’s theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: We <em>should use</em> the uniform prior over phase space, because phase space volume has a natural unit of measurement: <span class="math inline">\(h^N\)</span>, where <span class="math inline">\(h\)</span> is Planck’s constant, and <span class="math inline">\(2N\)</span> is the dimension of phase space. As Planck’s constant is a universal constant, independent of where we are in phase space, we should weight all of the phase space equally, resulting in a uniform prior.</p>
</section>
<section id="jaynes-epistemological-interpretation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="jaynes-epistemological-interpretation">Jaynes’ epistemological interpretation</h3>
<p>The question at the foundation of statistical mechanics is: Why maximize entropy? The practical scientist would say, “Because it works.”, and that is well and good, but we will give one possible answer to the why, from the most ardent proponent of maximal entropy theory, <a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">E. T. Jaynes</a>.</p>
<p>According to Jaynes, statistical mechanics is epistemological. That is, probability is not out there in the world, but in here in our minds, and statistical mechanics is nothing more than maximal entropy inference applied to physics. Macroscopic properties are evidences, and the ensemble <span class="math inline">\(\rho\)</span> is the posterior after we incorporate the evidences.</p>
<p>It might be difficult to swallow Jaynes’ interpretation, as it seems obvious that entropy is objective, but epistemology is subjective. How could he explain objective entropy by subjective information? I might make this more palatable by three examples.</p>
<p>In <span class="citation" data-cites="jaynesGibbsParadox1992">(<a href="#ref-jaynesGibbsParadox1992" role="doc-biblioref">Jaynes 1992</a>)</span>, he proposed the following thought experiment: Suppose we have a tank of neon gas, separated in the middle by a slab, as in the <a href="https://en.wikipedia.org/wiki/Gibbs_paradox">Gibbs paradox</a>. If both sides have the same temperature and pressure, then the system has the same entropy even after we remove the slab. But suddenly, Jaynes’ demon tells us that the left side contains neon-20, while the right side contains neon-22, and thus, we were not subject to the Gibbs paradox after all! And so we have just wasted some perfectly good free energy for nothing.</p>
<p>We ask, “Wasted? How could we have wasted anything unless there is a practical way to extract energy? You say they are two distinct gases, but what difference does it make if you simply call one side ‘neon-20’ and the other ‘neon-22’?”</p>
<p>So Jaynes’ demon gives us two membranes, one permeable only to neon-20, while the other only permeable to neon-22. This allows us to put both of them in the middle, and slowly let the two gases diffuse into the middle, extracting mechanical work by the pressure on the two membranes.</p>
<p>The point of the thought-experiment is that the entropy of a system is, in a practical sense, subjective. The tank of gas <em>might as well</em> have maximal entropy if we don’t have the two membranes. But as soon as we have the two membranes, it expands our space of possible actions, and previously “lost” work, suddenly becomes extractable, and the entropy of the world drops.</p>
<p>The following is a more concrete example from optics. Shoot some hard balls through a bumpy region (an analogy of shining a laser light through a bumpy sheet of glass). The balls would be scattered. It would increase the entropy of the system… unless we reflect them via a corner mirror, then the balls would be reflected right back through the bumpy region, and return to the previous zero-entropy state!</p>
<p>Did we violate the second law? Not so, if we think of entropy as a measure of our <em>actionable ignorance</em>. Without a corner mirror, we cannot use the detailed information of the scattered ball beam, and have to treat it as essentially random. However, all the detailed information is still there, in the bumpy region and in the scattered beam, and it takes a corner mirror to “unlock” the information for us.</p>
<p>Thus, with a corner mirror, the scattered beam still has zero entropy, but without it, the scattered beam has positive entropy.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Jaynes_irreversible_scattering.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 5.27</a>)</span></figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>The old adage ‘knowledge is power’ is a very cogent truth, both in human relations and in thermodynamics.</p>
<p>— E. T. Jaynes</p>
</blockquote>
<p>Still, there is a nagging feeling that, even if we have no corner reflector, and so the beam of light truly has increased in entropy <em>relative to us</em>, as long as the corner reflector is theoretically possible, then the entropy of the beam of light is still unchanged <em>in itself</em>. Isn’t this blatant <a href="https://en.wikipedia.org/wiki/Psychologism">psychologism</a>? Haven’t we reduced objective entropy <em>out there</em> to subjective information <em>in here</em>? Surely even if nobody is around to see it, if a tank of gas explodes in a forest, entropy still makes a sound.</p>
<p>Responding to this would entangle us into a whole mess of philosophical arguments about objective vs subjective probability, and observer vs observed phenomena. My quick answer is simply that knowledge isn’t magical, and even nature does not laugh at the difficulty of inference.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> If the entropy of a tank of gas appears high to us, then chances are, it would appear even higher to a car engine, for the car engine has only a few ways to interact with this tank of gas, unlike us, who have all of modern technology. A car engine can only burn up the tank of gas, but we can distill it, extract it, push it through membranes, etc. The tank of gas has higher entropy <em>relative to the car engine</em> – yes, the car engine has an opinion about the world, as much as we do. It has subjective beliefs about everything it can touch and burn, and we can listen to it with the language of math.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Supposedly, Laplace said “Nature laughs at the difficulties of integration.”, but when I tried to hunt it down, all citations led directly to a 1953 essay, which cites an anonymous “mathematician”. I have tried searching for it in French, with no results. I think this is actually a pseudo-Laplace quote, a <a href="https://gwern.net/leprechaun">paper ghost</a>, much like how people kept attributing things to <a href="https://en.wikipedia.org/wiki/Pseudo-Aristotle">pseudo-Aristotle</a>.</p>
<blockquote class="blockquote">
<p>It would take a physicist a long time to work out the problem and he could achieve only an approximation at that. Yet presumably the coin will stop exactly where it should. Some very rapid calculations have to be made before it can do so, and they are, presumably, always accurate. And then, just as I was blushing at what I supposed he must regard as my folly, the mathematician came to my rescue by informing me that Laplace had been puzzled by exactly the same fact. “Nature laughs at the difficulties of integration.” <span class="citation" data-cites="krutchBestTwoWorlds1953">(<a href="#ref-krutchBestTwoWorlds1953" role="doc-biblioref">Krutch 1953, 148</a>)</span></p>
</blockquote>
<p>However, I’m inclined to believe that even nature does not laugh at the difficulties of integration. In fact, one of my hobbies is to “explain” natural laws as ways for nature to avoid really difficult integrations. For example, Newtonian gravity is “explained” by the <a href="https://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation">Barnes–Hut algorithm</a> that allows n-body gravity to be calculated in <span class="math inline">\(O(n \ln n)\)</span> time.</p></div></div><p>What is the payoff of this long detour? I think it is to provide an intuitive feeling of the identity of physics and information. Information is physical, and physics is informational. If you are a physicist, then this allows you to invade into other fields, much like statistical mechanists “invaded” other fields like artificial intelligence and economics. If you are a mathematician or a computer scientist, then this allows you to translate intuition about physical objects into intuition about high-dimensional probability distributions and large combinatorial objects. And if you are Maxwell’s demon, then you won’t listen to me – I would gladly listen to <em>you</em>, since magically transforming information and physics back and forth is your entire reason of existence!</p>
<div class="callout callout-style-default callout-note callout-titled" title="Is entropy unique?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Is entropy unique?
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the above formulation, maximal entropy inference is interpreted as how rational agents can act optimally under limited information. An alternative viewpoint argues that it is not the entropy that is fundamental, but the argmax of <em>something</em> that is fundamental. In this view, if we replaced the entropy function <span class="math inline">\(S[\cdot]\)</span> with a… <em>kentropy</em> function <span class="math inline">\(K[\cdot]\)</span>, such that any scientist who uses reasons about experiments on a system using</p>
<p><span class="math display">\[\rho^* = \mathop{\mathrm{argmax}}_{\rho: \rho\text{ satisfies constraints }C}K[\rho]\]</span></p>
<p>would still satisfy certain axioms of rationality, then <span class="math inline">\(K\)</span> should be as good as <span class="math inline">\(S\)</span>.</p>
<p>In this vein, there is a Shore–Johnson theorem which shows that if a scientist using a certain kentropy function <span class="math inline">\(K\)</span> would end up satisfying these certain axioms of rationality, then</p>
<p><span class="math display">\[\mathop{\mathrm{argmax}}_{\rho: \rho\text{ satisfies constraints }C}S[\rho] = \mathop{\mathrm{argmax}}_{\rho: \rho\text{ satisfies constraints }C}K[\rho]\]</span></p>
<p>In other words, as long as we are doing constrained maximization, the choice of the entropy doesn’t matter. In particular, the standard entropy function <span class="math inline">\(S\)</span> is <em>good enough</em> – any constraint-maximizing rational thinker thinks <em>as if</em> it is doing constraint-maximizing entropy inference, so we <em>might as well</em> use <span class="math inline">\(S\)</span> and stop worrying about alternative ones like <span class="math inline">\(K\)</span>.</p>
<p>IF you are steeped in Bayesian epistemology, this is in the same vein as those Bayes-theological theorems proving that any rational being must use Bayes theorem for updates. <span class="citation" data-cites="pressePrinciplesMaximumEntropy2013">(<a href="#ref-pressePrinciplesMaximumEntropy2013" role="doc-biblioref">Pressé et al. 2013</a>)</span> Other examples include <a href="https://en.wikipedia.org/wiki/Blackwell%27s_informativeness_theorem">Blackwell’s informativeness theorem</a>, <a href="https://en.wikipedia.org/wiki/Aumann's_agreement_theorem">Aumann’s agreement theorem</a>, <a href="https://en.wikipedia.org/wiki/Cox's_theorem">Cox’s theorem</a>, <a href="https://en.wikipedia.org/wiki/Dutch_book_theorems">Dutch book theorems</a>, etc.</p>
</div>
</div>
</section>
</section>
<section id="mathematical-developments" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mathematical-developments">Mathematical developments</h2>
<section id="fundamental-theorems" class="level3">
<h3 class="anchored" data-anchor-id="fundamental-theorems">Fundamental theorems</h3>
<div id="thm-liouville" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Liouville’s theorem)</strong></span> For any phase space and any Hamiltonian over it (which can change with time), phase-space volume is conserved under motion.</p>
<p>For any probability distribution <span class="math inline">\(\rho_0\)</span>, if after time <span class="math inline">\(t\)</span>, it evolves to <span class="math inline">\(\rho_t\)</span>, and a point <span class="math inline">\(x(0)\)</span> evolves to <span class="math inline">\(x(t)\)</span>, then <span class="math inline">\(\rho_0(x(0)) = \rho_t(x(t))\)</span>.</p>
</div>
<p>The proof is found in any textbook, and also <a href="https://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)">Wikipedia</a>. Since it is already simple enough, and I can’t really improve upon it, I won’t.</p>
<div id="cor-conservation-entropy" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 (conservation of entropy)</strong></span> For a Hamiltonian system, with any Hamiltonian (which can change with time), for any probability distribution <span class="math inline">\(\rho\)</span> over its phase space, its entropy is conserved over time.</p>
</div>
<p>In particular, we have the following corollary:</p>
<div id="cor-ensemble-conservation" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2</strong></span> Given any set of constraints, if the Hamiltonian preserves these constraints over time, then any constrained-maximal entropy distribution remains constrained-maximal under time-evolution.</p>
</div>
<p>In most cases, the constraint is of a particular form: the expectation is known. In that case, we have the following theorem:</p>
<div id="thm-constrained-optimization" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (maximal entropy under linear constraints)</strong></span> For the following constrained optimization problem</p>
<p><span class="math display">\[
\begin{cases}
\max_\rho S[\rho] \\
\int A_1(x) \rho(x) &amp;= \bar A_1 \\
\cdots &amp;= \cdots \\
\int A_n(x) \rho(x) &amp;= \bar A_n \\
\end{cases}
\]</span></p>
<p>Consider the following ansatz</p>
<p><span class="math display">\[
\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}
\]</span></p>
<p>where <span class="math inline">\(Z(a_1, \dots, a_n) = \int e^{-\sum_i a_i A_i(x)} dx\)</span>, and <span class="math inline">\(a_1, \dots, a_n\)</span> are chosen such that the constraints <span class="math inline">\(\int A_i(x) \rho(x) = \bar A_i\)</span> are satisfied.</p>
<p><em>If</em> the ansatz exists, then it is the unique solution.</p>
</div>
<p>The ansatz solution is what you get by Lagrangian multipliers. For a refresher, see the <a href="https://yuxi-liu-wired.github.io/essays/posts/analytical-mechanics/index.html#lagranges-devil-at-disneyland"><em>Analytical Mechanics</em>#Lagrange’s devil at Disneyland</a>. The theorem shows that the solution is unique – provided that it exists. Does it exist? Yes, in physics. If it doesn’t exist, then we are clearly not modelling a physically real phenomenon.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In physics, these are “Boltzmann distributions” or “Gibbs distributions”. In statistics, these are <a href="https://en.wikipedia.org/wiki/Exponential_family">exponential families</a>. Because they are everywhere, they have many names.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Define a distribution <span class="math inline">\(\rho\)</span> as given in the statement of the theorem. That is,</p>
<p><span class="math display">\[
\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}
\]</span></p>
<p>etc.</p>
<p>Now, it remains to prove that for any other <span class="math inline">\(\rho'\)</span> that satisfies the constraints, we have <span class="math inline">\(S[\rho] \geq S[\rho']\)</span>.</p>
<p>By routine calculation, for any probability distribution <span class="math inline">\(\rho'\)</span>,</p>
<p><span class="math display">\[
D_{KL}(\rho' \| \rho) = -S[\rho'] + \sum_i a_i \braket{A_i}_{\rho'} + \ln Z(a_1, \dots, a_n)
\]</span></p>
<p>If <span class="math inline">\(\rho'\)</span> satisfies the given constraints, then <span class="math inline">\(D_{KL}(\rho' \| \rho) = -S[\rho'] + \Const\)</span> where the constant does not depend on <span class="math inline">\(\rho'\)</span>, as long as it satisfies the constraints. Therefore, <span class="math inline">\(S[\rho']\)</span> is maximized when <span class="math inline">\(D_{KL}(\rho' \| \rho)\)</span> is minimized, which is exactly <span class="math inline">\(\rho\)</span>.</p>
</div>
</div>
</div>
<p>The following proposition is often used when we want to maximize entropy in a two-step process:</p>
<div id="thm-compound-entropy" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (compound entropy)</strong></span> If <span class="math inline">\(\rho_{X,Y}\)</span> is a probability distribution over two variables <span class="math inline">\((X, Y)\)</span>, then</p>
<p><span class="math display">\[S[\rho_{X,Y}] = S[\rho_Y] + \braket{S[\rho_{X|y}]}_y\]</span></p>
<p>or more succinctly,</p>
<p><span class="math display">\[S_{X,Y} = S_Y + \braket{S_{X|y}}_y\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Notations">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notations
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(\rho_Y\)</span> is the probability distribution over <span class="math inline">\(Y\)</span>, after we integrate/marginalize <span class="math inline">\(X\)</span> away:</p>
<p><span class="math display">\[
\rho_Y(y) := \int \rho_{X,Y}(x,y)dx
\]</span></p>
<p><span class="math inline">\(\rho_{X|y}\)</span> is the conditional probability distribution over <span class="math inline">\(X\)</span>, conditional on <span class="math inline">\(Y=y\)</span>:</p>
<p><span class="math display">\[
\rho_{X|y}(x) := \frac{\rho_{X,Y}(x,y)}{\int \rho_{X,Y}(x,y) dx}
\]</span></p>
<p><span class="math inline">\(\braket{\cdot}_y\)</span> is the expectation over <span class="math inline">\(\rho_Y\)</span>:</p>
<p><span class="math display">\[
\braket{S_{X|y}}_y := \int S_{X|y} \rho_Y(y)dy
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consider a compound system in ensemble <span class="math inline">\(\rho(x, y)\)</span>. Its entropy is</p>
<p><span class="math display">\[S[\rho] = -\int dxdy \; \rho(x, y) \ln \rho(x, y)\]</span></p>
<p>We can take the calculation in two steps:</p>
<p><span class="math display">\[S[\rho] = -\int dxdy \; \rho(x|y)\rho(y) (\ln \rho(x|y) + \ln  \rho(y)) = S[\rho_Y] + \braket{S[\rho_{X|y}]}_y\]</span></p>
</div>
</div>
</div>
<p>Intuitively, what does <span class="math inline">\(S_{X,Y} = S_Y + \braket{S_{X|y}}_y\)</span> mean? It means that the entropy in <span class="math inline">\((X, Y)\)</span> can be decomposed into two parts: the part due to <span class="math inline">\(Y\)</span>, and the part remaining after we know <span class="math inline">\(Y\)</span>, but not yet knowing <span class="math inline">\(X\)</span>. In the language of information theory, the total information in <span class="math inline">\((X, Y)\)</span> is equal to the information in <span class="math inline">\(Y\)</span>, plus the information of <span class="math inline">\(X\)</span> conditional over <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
I(X, Y) = I(Y) + I(X|Y)
\]</span></p>
</section>
<section id="microcanonical-ensembles" class="level3">
<h3 class="anchored" data-anchor-id="microcanonical-ensembles">Microcanonical ensembles</h3>
<p>If the only constraint is the constant-energy constraint <span class="math inline">\(H(x) = E\)</span>, then the maximal entropy distribution is the uniform distribution on the shell of constant energy <span class="math inline">\(H = E\)</span>. It is uniform, because once we enforce <span class="math inline">\(H(x) = E\)</span>, there are no other constraints, and so by <a href="#thm-constrained-optimization" class="quarto-xref">Theorem&nbsp;2</a>, the distribution is uniform.</p>
<p>Thus, we obtain the <strong>microcanonical ensemble</strong>:</p>
<p><span class="math display">\[\rho_E(x) \propto 1_{H(x) = E}\]</span></p>
<p>It is sometimes necessary to deal with the “thickness” of the energy shell. In that case, <span class="math inline">\(\rho_E(x) \propto \delta(H(x) - E)\)</span>, where <span class="math inline">\(\delta\)</span> is the Dirac delta function.</p>
<p>By <a href="#thm-constrained-optimization" class="quarto-xref">Theorem&nbsp;2</a>, the microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy. In particular, if the Hamiltonian does not change over time, then any microcanonical ensemble is preserved over time. In words, if we uniformly “dust” the energy shell of <span class="math inline">\(H(x) = E\)</span> with a cloud of system states, and let all of them evolve over time, then though the dust particles move about, the cloud remains exactly the same.</p>
<p>More generally, we can impose more (in)equality constraints, and still obtain a microcanonical ensemble. For example, consider a ball flying around in an empty room with no gravity. The Hamiltonian is <span class="math inline">\(H(q, p) = \frac{p^2}{2m}\)</span>, and its microcanonical ensemble is <span class="math inline">\(\rho(q, p) \propto \delta(p = \sqrt{2mE})1[p \in \text{the room}]\)</span>. That is, its velocity is on the energy shell, while its position is uniform over the entire room.</p>
<p>If we want to specify the number of particles for each chemical species, then that can be incorporated into the microcanonical ensemble as well. For example, if we want the number of species <span class="math inline">\(i\)</span> be exactly <span class="math inline">\(N_{i0}\)</span>, then we multiply <span class="math inline">\(\rho\)</span> by <span class="math inline">\(1[N_i = N_{i0}]\)</span>.</p>
</section>
<section id="canonical-ensembles" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="canonical-ensembles">Canonical ensembles</h3>
<p>Consider a small cup of liquid in a giant tank of chemical fluids, or a small lump of air in the whole atmosphere. These are all examples of “a small system in contact with a giant system”. In general, if we have a small system connected to a large system, then we typically don’t care about the large system, and only want to study the small system’s ensemble. How do we do that? Rigorously, we would need to first find the microcanonical ensemble for the total compound small–large system, then take an integral over all states of the large system, resulting in an ensemble over just the small system, as in</p>
<p><span class="math display">\[\rho_{\text{small}}(x) = \int \rho_{\text{total}}(x, y) dy\]</span></p>
<p>where <span class="math inline">\(x\)</span> ranges over the states of the small system, and <span class="math inline">\(y\)</span> of the large system.</p>
<p>However, this is difficult to perform in general, because the large system, having so many particles, has a huge state space. We cannot do it in general. However, there is an easy way out. Whenever we have a big system changing only a little bit, we can assume linearity. Whenever we have a function <span class="math inline">\(f(x)\)</span> where <span class="math inline">\(x\)</span> changes only a little bit around <span class="math inline">\(x_0\)</span>, we can assume <span class="math inline">\(f(x) \approx f(x_0) + f'(x_0) (x - x_0)\)</span>. This is the trick that will allow us to solve the problem.</p>
<p>Assuming that the energy of the compound system is extensive, we obtain the canonical ensemble. Assuming that the energy and volume are both extensive, we obtain the grand canonical ensemble, etc. The following table would be very useful</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>extensive constraint</th>
<th>ensemble</th>
<th>free entropy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>none</td>
<td>microcanonical</td>
<td>entropy</td>
</tr>
<tr class="even">
<td>energy</td>
<td>canonical</td>
<td>Helmholtz free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume</td>
<td>?</td>
<td>Gibbs free entropy</td>
</tr>
<tr class="even">
<td>energy, particle count</td>
<td>grand canonical</td>
<td>Landau free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume, particle count</td>
<td>?</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>There are some question marks in the above table, because there are no consensus names for those question marks. What is more surprising is that there is no name for the ensemble of constrained energy and volume. I would have expected something like the “Gibbs ensemble”, but history isn’t nice to us like that. Well, then I will name it first, as the <em>big canonical ensemble</em>. And while we’re at it, let’s fill the last row as well:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>extensive constraint</th>
<th>ensemble</th>
<th>free entropy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>none</td>
<td>microcanonical</td>
<td>entropy</td>
</tr>
<tr class="even">
<td>energy</td>
<td>canonical</td>
<td>Helmholtz free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume</td>
<td>big canonical</td>
<td>Gibbs free entropy</td>
</tr>
<tr class="even">
<td>energy, particle count</td>
<td>grand canonical</td>
<td>Landau free entropy</td>
</tr>
<tr class="odd">
<td>energy, volume, particle count</td>
<td>gross canonical</td>
<td>EVN free energy</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-tip callout-titled" title="Extensivity">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Extensivity
</div>
</div>
<div class="callout-body-container callout-body">
<p>In classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.</p>
<p>This is not always obvious. If we have two galaxies of stars, we can think of each as a “cosmic gas” where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of “the entropy of galaxy 1”, but only “the entropy of galaxy-compound 1-2”.</p>
<p>In statistical mechanics, extensivity means a certain property of each subsystem is unaffected by the state of the other subsystems, and the total is the sum of them. So for example, if <span class="math inline">\(A\)</span> is an extensive property, then it means</p>
<p><span class="math display">\[
A(x_1, \dots, x_n) = A_1(x_1) + \dots + A_n(x_n)
\]</span></p>
<p>Like most textbooks, we assume extensivity by default, although as we noted in <a href="https://yuxi-liu-wired.github.io/essays/posts/equilibrium-thermoeconomics/"><em>Classical Thermodynamics and Economics</em></a>, both classical thermodynamics and statistical mechanics do not require extensivity. We assume extensivity because it is mathematically convenient, and good enough for most applications.</p>
</div>
</div>
<p>In the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble)</p>
<div id="thm-canonical-ensembles" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4</strong></span> If the two systems are in energy-contact, and energy is conserved, and energy is extensive, and the compound system is in a microcanonical ensemble, then the small system is in the <strong>canonical ensemble</strong></p>
<p><span class="math display">\[
\rho(x) \propto e^{-\beta H(x)}
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is the marginal entropy of energy of the large system:</p>
<p><span class="math display">\[\beta := \partial_E S[\rho_{bath, E}]\]</span></p>
<p>Similarly, if the two systems are in energy-and-particle-contact, then the small system is in the <strong>grand canonical ensemble</strong></p>
<p><span class="math display">\[
\rho(x) \propto e^{-(\beta H(x) + (-\beta \mu) N(x))}
\]</span></p>
<p>where <span class="math inline">\(-\beta\mu\)</span> is the marginal entropy of particle of the large system:</p>
<p><span class="math display">\[-\beta\mu := (\partial_N S[\rho_{bath, E, N}])_{E}\]</span></p>
<p>Most generally, if the two systems are in <span class="math inline">\(q_1, \dots, q_m\)</span> contact, and <span class="math inline">\(q_1, \dots, q_m\)</span> are conserved and extensive quantity, then</p>
<p><span class="math display">\[\rho(x) \propto e^{-\sum_i p_i q_i(x)}\]</span></p>
<p>where <span class="math inline">\(p_i = (\partial_{q_i} S[\rho_{bath, q}])_{q}\)</span> is the marginal entropy of <span class="math inline">\(q_i\)</span> of the large system.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We prove the case for the canonical ensemble. The other cases are similar.</p>
<p>Since the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:</p>
<p><span class="math display">\[\max_\rho S[\rho]\]</span></p>
<p>By <a href="#thm-compound-entropy" class="quarto-xref">Theorem&nbsp;3</a>,</p>
<p><span class="math display">\[S = S_{\text{system}}(E_{\text{system}}) + \braket{S_{bath|system}(E_{total} - E_{\text{system}})}_{\text{system}}\]</span></p>
<p>Since the bath is so much larger than the system, we can take just the first term in its Taylor expansion:</p>
<p><span class="math display">\[S_{bath|system}(E_{total} - E_{\text{system}}) = S_{\text{bath}}(E_{total}) - \beta E_{\text{system}}\]</span></p>
<p>where <span class="math inline">\(E_{total}\)</span> is the total energy for the compound system, <span class="math inline">\(\beta = \partial_E S_{\text{bath}}|_{E = E_{total}}\)</span> is the marginal entropy per energy, and <span class="math inline">\(E_{\text{system}}\)</span> is the energy of the system.</p>
<p>This gives us the linearly constrained maximization problem of</p>
<p><span class="math display">\[\max_{\rho_{\text{system}}} (S_{\text{system}} - \beta \braket{E_{\text{system}}}_{\rho_{\text{system}}})\]</span></p>
<p>and we apply Lagrange multipliers to finish the proof.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Extensivity, again">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Extensivity, again
</div>
</div>
<div class="callout-body-container callout-body">
<p>Extensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing <span class="math inline">\(S_{\text{bath}}(E)\)</span>, instead of <span class="math inline">\(S_{\text{bath}}(E, E_{\text{system}})\)</span>, requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.</p>
<p><span class="math inline">\(S_{\text{bath}}\)</span> is the logarithm of the surface area of the energy shell <span class="math inline">\(H_{\text{bath}} = E_{\text{bath}}\)</span>. By extensivity, <span class="math inline">\(H(x_{\text{bath}}, x_{\text{system}}) = H_{\text{bath}}(x_{\text{bath}}) + H_{\text{system}}(x_{\text{system}})\)</span>, so the energy shells of the bath depends on only <span class="math inline">\(E_{\text{bath}}\)</span>, not <span class="math inline">\(E_{\text{system}}\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The proof showed something extra: If the small system is in distribution <span class="math inline">\(\rho\)</span> that does not equal to the equilibrium distribution <span class="math inline">\(\rho_B\)</span>, then the total system’s entropy is</p>
<p><span class="math display">\[S = S_{max} - D_{KL}(\rho \| \rho_B)\]</span></p>
<p>which is related to of Sanov’s theorem and large deviation theory, though I don’t know how to make this precise.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Enthalpic ensemble">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Enthalpic ensemble
</div>
</div>
<div class="callout-body-container callout-body">
<p>What if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is thermally insulating. Notice that in this case, the small system still exchanges energy with the large system via <span class="math inline">\(d\braket{E} = -Pd\braket{V}\)</span>. We don’t have <span class="math inline">\(E = -PdV\)</span>, because the small system might get unlucky. During a moment of weakness, all its particles has abandoned their frontier posts, and the bath has taken advantage of this by encroaching on its land. The system loses volume by <span class="math inline">\(\delta V\)</span>, without earning a compensating <span class="math inline">\(\delta E = P \delta V\)</span>. In short, the thermodynamic equality <span class="math inline">\(E = -PdV\)</span> is inexact in statistical mechanics, and only holds true on the ensemble average.</p>
<p>In this case, because pressure is a constant, we have <span class="math inline">\(d(E + PV) = 0\)</span>, and so we have the enthalpic ensemble <span class="math inline">\(\rho \propto e^{-\beta H}\)</span>, where <span class="math inline">\(H := E + PV\)</span> is the <a href="https://en.wikipedia.org/wiki/Enthalpy">enthalpy</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>Specifically, if you work through the same argument, you would end up with the following constrained maximization problem:</p>
<p><span class="math display">\[
\begin{cases}
\max_{\rho_{\text{system}}} (S_{\text{system}} - \beta \braket{E_{\text{system}}}_{\rho_{\text{system}}} - \beta P \braket{V}) \\
\braket{E_{\text{system}}} + P\braket{V_{\text{system}}} = \Const
\end{cases}
\]</span></p>
<p>yielding the enthalpic ensemble (or the <a href="https://en.wikipedia.org/wiki/Isoenthalpic%E2%80%93isobaric_ensemble">isoenthalpic-isobaric ensemble</a>).</p>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Sorry, I know this is not the Hamiltonian, but we are running out of letters to use.</p></div></div></section>
<section id="free-entropies" class="level3">
<h3 class="anchored" data-anchor-id="free-entropies">Free entropies</h3>
<p>Just like in thermodynamics, it is useful to consider free entropies, which are the convex duals of the entropy:</p>
<ul>
<li>Helmholtz free entropy: <span class="math inline">\(f[\rho] := S[\rho] - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))\)</span>.</li>
<li>Gibbs free entropy: <span class="math inline">\(g[\rho] := S[\rho] - \beta \braket{E} - \beta P \braket{V}\)</span>.</li>
<li>Landau free entropy: <span class="math inline">\(\omega[\rho] := S[\rho] - \beta \braket{E} - \beta (-\mu) \braket{N}\)</span>. Note that the sign of <span class="math inline">\((-\mu)\)</span> is not a typo. It is simply that 19th-century chemists have messed up the sign convention, like how Benjamin Franklin messed up the sign convention of electric charge.</li>
</ul>
<p>Etc. Of those, we would mostly use the Helmholtz free energy, so I will write it down again:</p>
<p><span class="math display">\[
f[\rho] := S[\rho] - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))
\]</span></p>
<div id="thm-chain-rule" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (chain rule for free entropies)</strong></span> <span class="math inline">\(f_X = S_Y + \braket{f_{X|y}}_y\)</span>, and similarly <span class="math inline">\(g_X = S_Y + \braket{g_{X|y}}_y\)</span>, and similarly for all other free entropies.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
  f_X &amp;= S_X - \beta \braket{E}_x  \\
  &amp;= S_Y + \braket{S_{X|y}}_y - \beta \braket{\braket{E}_{x \sim X|y}}_y \\
  &amp;= S_Y + \braket{f_{X|y}}_y
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<p>A common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has at least 4 characterizations. “Muscle memory” in statistical mechanics would allow you to nimbly applying the most suitable one for any occasion.</p>
<div id="thm-canonical-characterization" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (4 characterizations of the canonical ensemble)</strong></span> &nbsp;</p>
<ol type="1">
<li>(total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in energy-contact with an energy bath that satisfies <span class="math inline">\(\partial_E S_{\text{bath}} = \beta\)</span>, under the constraint that <span class="math inline">\(E + E_{\text{bath}}\)</span> is fixed.</li>
<li>(entropy under mean energy constraint) Let <span class="math inline">\(E_0\)</span> be a real number, and let <span class="math inline">\(\beta\)</span> be the unique solution to <span class="math inline">\(\int dx \; e^{-\beta E(x)} = E_0\)</span>. A system maximizes its entropy under constraint <span class="math inline">\(\braket{E} = E_0\)</span> when it assumes the canonical ensemble with <span class="math inline">\(\beta\)</span>.</li>
<li>(Boltzann’s thermodynamic limit argument): Take <span class="math inline">\(N\)</span> copies of a system, and connect them by energy-contacts. Inject the system with total energy <span class="math inline">\(NE_0\)</span>, and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of <span class="math inline">\(N\to \infty\)</span>, the distribution of a single system is the canonical distribution with <span class="math inline">\(\beta\)</span> that is the unique solution to <span class="math inline">\(\int dx \; e^{-\beta E(x)} = E_0\)</span>.</li>
<li>(free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At the optimal distribution <span class="math inline">\(\rho^*\)</span>, the maximal Helmholtz free entropy is <span class="math inline">\(f[\rho^*] = \ln Z\)</span>, where <span class="math inline">\(Z = \int dx \; e^{-\beta E(x)}\)</span> is the partition function.</li>
</ol>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>We already proved this.</li>
<li>Use the Lagrange multiplier.</li>
<li>Isolate one system, and treat the rest as an energy-bath.</li>
<li><span class="math inline">\(f[\rho] = \ln Z - D_{KL}(\rho \| \rho_B)\)</span>.</li>
</ol>
</div>
</div>
</div>
</section>
<section id="the-partition-function" class="level3">
<h3 class="anchored" data-anchor-id="the-partition-function">The partition function</h3>
<p>When the system is in a canonical ensemble, we can define a convenient variable <span class="math inline">\(Z = \int dx\; e^{-\beta E(x)}\)</span> called the <strong>partition function</strong>. As proven in <a href="#thm-canonical-characterization" class="quarto-xref">Theorem&nbsp;6</a>, the partition function is equal to <span class="math inline">\(e^f\)</span>, where <span class="math inline">\(f\)</span> is the Helmholtz free entropy of the canonical ensemble.</p>
<div id="thm-partition-cumulant" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (the partition function is the cumulant generating function of energy)</strong></span> Let a system be in canonical ensemble with inverse temperature <span class="math inline">\(\beta\)</span>, and let <span class="math inline">\(K(t) := \ln \braket{e^{tE}}\)</span> be the <a href="https://en.wikipedia.org/wiki/Cumulant">cumulant generating function</a> of its energy, then <span class="math display">\[K(t) = \ln Z(\beta-t) - \ln Z(\beta)\]</span></p>
<p>In particular, the <span class="math inline">\(n\)</span>-th cumulant of energy is<br>
<span class="math display">\[\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\partial_\beta)^n (\ln Z)\]</span></p>
<p>A similar proposition applies for the other ensembles and their free entropies.</p>
</div>
<p>The proof is by direct computation.</p>
<p>For example, the first two cumulants are the mean and variance:</p>
<p><span class="math display">\[\braket{E} = (-\partial_\beta) (\ln Z), \quad \mathrm{Var}(E) = \partial_\beta^2 (\ln Z)\]</span></p>
<p>Typical systems are made of <span class="math inline">\(N\)</span> particles, where <span class="math inline">\(N\)</span> is large, and that these particles are only weakly interacting. In this case, the total Helmholtz free entropy per particle converges at the thermodynamic limit of <span class="math inline">\(N \to \infty\)</span>:</p>
<p><span class="math display">\[
\lim_N \frac 1N \ln Z \to \bar f_\beta
\]</span></p>
<p>Thus, for large but finite <span class="math inline">\(N\)</span>, we have</p>
<p><span class="math display">\[\braket{E} \approx -N \partial_\beta \bar f_\beta, \quad \mathrm{Var}(E) = N\partial_\beta^2 \bar f_\beta\]</span></p>
<p>In particular, the relative fluctuation scales like <span class="math inline">\(\frac{\sqrt{\mathrm{Var}(E)}}{\braket{E}} \sim N^{-1/2}\)</span>.</p>
</section>
<section id="conditional-entropies" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="conditional-entropies">Conditional entropies</h3>
<p>Given any two random variable <span class="math inline">\(X, Y\)</span>, and an “observable” variable <span class="math inline">\(Y\)</span> that is determined by <span class="math inline">\(X\)</span> by some function <span class="math inline">\(h\)</span>, such that <span class="math inline">\(Y = h(X)\)</span>. If we know <span class="math inline">\(X\)</span>, we would know <span class="math inline">\(Y\)</span>, but it is not so conversely, as multiple <span class="math inline">\(X\)</span> may correspond to the same <span class="math inline">\(Y\)</span>. Typically, we use <span class="math inline">\(Y\)</span> as a “summary statistic” for the more detailed, but more complicated <span class="math inline">\(X\)</span>. For example, we might have multiple particles in a box, such that <span class="math inline">\(X\)</span> is their individual locations, while <span class="math inline">\(Y\)</span> is their center of mass.</p>
<div id="thm-cond-ent" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 (conditional entropy)</strong></span> Given any random variable <span class="math inline">\(X\)</span>, and an “observable” variable <span class="math inline">\(Y\)</span> that is determined by <span class="math inline">\(X\)</span>, and some constraints <span class="math inline">\(c\)</span> on <span class="math inline">\(X\)</span>, if <span class="math inline">\(X\)</span> is the distribution that maximizes entropy under constraints <span class="math inline">\(c\)</span>, with entropy <span class="math inline">\(S_X^*\)</span>, then the observable <span class="math inline">\(Y\)</span> is distributed as</p>
<p><span class="math display">\[\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}, \quad e^{S_X^*} = \int dy\; e^{S_{X|y}^*}\]</span></p>
<p>where <span class="math inline">\(S_{X|y}^*\)</span> is the maximal entropy for <span class="math inline">\(X\)</span> conditional on the same constraints, plus the extra constraint that <span class="math inline">\(Y = y\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>By assumption, <span class="math inline">\(X\)</span> is the unique solution to the constrained optimization problem</p>
<p><span class="math display">\[
\begin{cases}
    \max S_X \\
    \text{constraints on $x$}
\end{cases}
\]</span></p>
<p>By <a href="#thm-compound-entropy" class="quarto-xref">Theorem&nbsp;3</a>, the problem is equivalent to:</p>
<p><span class="math display">\[
\begin{cases}
    \max S_Y + \braket{S_{X|y}}_{y\sim Y} \\
    \text{constraints on $x$}
\end{cases}
\]</span></p>
<p>Now, we can solve the original problem in a two-step process: For each possible observable <span class="math inline">\(y\sim Y\)</span>, we solve an extra-constrained problem:</p>
<p><span class="math display">\[
\begin{cases}
    \max S_{X|y} \\
    \text{original constraints on $x$} \\
    \text{$x$ must be chosen such that the observable $Y = y$}
\end{cases}
\]</span></p>
<p>Then, each such problem gives us a maximal conditional<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> entropy <span class="math inline">\(S_{X|y}^*\)</span>, and we can follow it up by solving for <span class="math inline">\(Y\)</span> with</p>
<p><span class="math display">\[\max\left(S_Y + \braket{S_{X|y}^*}_{y \sim Y}\right)\]</span></p>
<p>Again, the solution is immediate once we see it is just the KL-divergence:</p>
<p><span class="math display">\[S_Y + \braket{S_{X|y}^*}_{y \sim Y} = - \int dy \; \rho_Y(y) \ln\frac{\rho_Y(y)}{e^{S_{X|y}^*}} = \ln Z - D_{KL}(\rho_Y \| \rho_Y^*)\]</span></p>
<p>where</p>
<p><span class="math display">\[Z = \int dy\; e^{S_{X|y}^*}, \quad \rho_Y^*(y) = \frac{e^{S_{X|y}^*}}{Z}\]</span></p>
<p>At the optimal point, the entropy for <span class="math inline">\(X\)</span> is maximized at <span class="math inline">\(S_X^* = \ln Z - 0\)</span>, so <span class="math inline">\(Z = e^{S_X^*}\)</span>.</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;If you’re a pure mathematician, you can formalize this using <a href="https://en.wikipedia.org/wiki/Disintegration_theorem">measure disintegration</a>.</p></div></div><div class="callout callout-style-default callout-note callout-titled" title="deriving the canonical ensemble yet again">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
deriving the canonical ensemble yet again
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a small system with energy states <span class="math inline">\(E_1, E_2, \dots\)</span> and a large bath system, in energy contact. We can set <span class="math inline">\(X\)</span> to be the combined state of the whole system, and <span class="math inline">\(Y\)</span> to be the state of the small system. Once we observe <span class="math inline">\(y\)</span>, we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system: <span class="math display">\[S_{X|y}^* = S_{\text{bath}} = S_{\text{bath}}(E_{total}) - \beta E_y\]</span></p>
<p>Consequently, the distribution of the small system is <span class="math inline">\(\rho_Y(y) \propto e^{-\beta E_y}\)</span>, as we expect.</p>
<p>A similar calculation gives us the grand canonical ensemble, etc.</p>
</div>
</div>
<div id="thm-cond-free-ent" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9 (conditional free entropy)</strong></span> Given any random variable <span class="math inline">\(X\)</span>, and an “observable” variable <span class="math inline">\(Y\)</span> that is determined by <span class="math inline">\(X\)</span>, and some constraints <span class="math inline">\(c\)</span> on <span class="math inline">\(X\)</span>, if <span class="math inline">\(X\)</span> is the distribution that maximizes Helmholtz free entropy under constraints <span class="math inline">\(c\)</span>, with Helmholtz free entropy <span class="math inline">\(f_X^*\)</span>, then the observable <span class="math inline">\(Y\)</span> is distributed as <span class="math display">\[\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}, \quad e^{f_X^*} = \int dy\; e^{f_{X|y}^*}\]</span></p>
<p>where <span class="math inline">\(f_{X|y}^*\)</span> is the maximal Helmholtz free entropy for <span class="math inline">\(X\)</span> conditional on the same constraints, plus the constraint that <span class="math inline">\(Y = y\)</span>.</p>
<p>Similarly for Gibbs free entropy, and all other free entropies.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>First note that <span class="math inline">\(f_X = S_Y + \braket{f_{X|y}}_y\)</span>, then argue in the same way.</p>
</div>
</div>
</div>
</section>
</section>
<section id="kinetic-gas-theory" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="kinetic-gas-theory">Kinetic gas theory</h2>
<p>Kinetic gas theory is <em>the</em> paradigm for pre-1930 statistical mechanics. Boltzmann devoted his best years to kinetic gas theory. The connection between kinetic gas theory and statistical mechanics was so strong that it was often confused as one. Modern statistical mechanics has grown to be so much more than this, so we will only settle for deriving the van der Waals equation. This strikes a balance between triviality (the ideal gas equation could be derived in literally two lines) and complication (Boltzmann’s monumental <em>Lectures on Gas Theory</em> has 500 pages <span class="citation" data-cites="boltzmannLecturesGasTheory2011">(<a href="#ref-boltzmannLecturesGasTheory2011" role="doc-biblioref">Boltzmann 2011</a>)</span>).</p>
<p>To review, the van der Waals gas equation is</p>
<p><span class="math display">\[P = \frac{N/\beta}{V- bN} - \frac{cN^2}{V^2}\]</span></p>
<p>where <span class="math inline">\(b, c\)</span> are real numbers that depend on the precise properties of the gas molecules. The term <span class="math inline">\(V - bN\)</span> accounts for the fact that each gas molecule excludes some volume, so that, as <span class="math inline">\(N\)</span> grows, it corrects for the ideal gas pressure <span class="math inline">\(P_{ideal}\)</span> by <span class="math inline">\(\sim P_{ideal}\frac{bN}{V}\)</span>. The term <span class="math inline">\(\frac{cN^2}{V^2}\)</span> accounts for overall interaction energy between gas molecules. Suppose the interaction is overall attractive, then we would have <span class="math inline">\(c &gt; 0\)</span>, and otherwise <span class="math inline">\(c &lt; 0\)</span>.</p>
<section id="ideal-gas" class="level3">
<h3 class="anchored" data-anchor-id="ideal-gas">Ideal gas</h3>
<p>Consider a tank of ideal gas consisting of <span class="math inline">\(N\)</span> point-masses, flying around in a free space with volume <span class="math inline">\(V\)</span>. The tank of gas has inverse temperature <span class="math inline">\(\beta\)</span>, so its phase-space distribution is</p>
<p><span class="math display">\[
\rho(q_{1:N}, p_{1:N}) = \prod_{i\in 1:N} \rho(q_i, p_i), \quad \rho(q, p) = \underbrace{\frac{1}{V}}_{\text{free space}} \times \underbrace{\frac{e^{-\beta \frac{\|p_i\|^2}{2m}}}{(2\pi m/\beta)^{3/2}}}_{\text{Boltzmann momentum distribution}}
\]</span></p>
<p>The total energy of the gas has no positional term, so it is all due to momentum. Because the momenta coordinates <span class="math inline">\(p_{1,x}, p_{1,y}, \dots, p_{N,y}, p_{N,z}\)</span> do not interact, their kinetic energies simply sum, giving</p>
<p><span class="math display">\[
U = 3N \times \int_{\mathbb{R}}dp\; \frac{p^2}{2m} \frac{e^{-\frac{p^2}{2m/\beta}}}{\sqrt{2\pi m/\beta}} = \frac{3N}{2\beta}
\]</span></p>
<p>This is the same as Boltzmann’s derivation so far. However, although entropy is exactly defined when there are only finitely or countably many possible states, as <span class="math inline">\(\sum_{j \in \mathbb{N}} -p_j \ln p_j\)</span>, this is not so when state space is uncountably large, like <span class="math inline">\(\mathbb{R}^{6N}\)</span>. When Boltzmann encountered the issue, he solved it by discretizing the phase space into <em>arbitrary but small</em> cubes. The effect is that he could rederive the ideal gas laws, but the entropy has an additive constant that depends on the exact choice of the cube size. This was not a problem for Boltzmann, who was trying to found classical thermodynamics upon statistical mechanics, and in classical thermodynamics, entropy <em>does</em> have an indeterminate additive constant.</p>
<p>Later, Planck in his derivation of the blackbody radiation law, used the same trick. Ironically, Planck did not believe in atoms nor quantized light, but he did make the correct assumption that there is a natural unit of measurement for phase space area, which he called <span class="math inline">\(h\)</span>, and which we know as Planck’s constant. <span class="citation" data-cites="duncanConstructingQuantumMechanics2019">(<a href="#ref-duncanConstructingQuantumMechanics2019" role="doc-biblioref">Duncan and Janssen 2019, chap. 2</a>)</span>.</p>
<p>Following Planck, we discretize the phase space into little cubes of size <span class="math inline">\(h^{3N}\)</span>, and continue:</p>
<p><span class="math display">\[
\begin{aligned}
    S &amp;= -\sum_{i \in\text{Little cubes}} p_i \ln p_i \\
    &amp;\approx -\sum_{i \in\text{Little cubes}} (\rho(i) h^{3N}) \ln (\rho(i) h^{3N}) \\
    &amp;\approx -\int_{\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \; \rho(p_{1:N}, q_{1:N}) \ln (\rho(p_{1:N}, q_{1:N}) h^{3N}) \\
    &amp;= -\int_{\mathbb{R}^{6N}} dp_{1:N}dq_{1:N} \; \rho(p_{1:N}, q_{1:N}) \ln \rho(p_{1:N}, q_{1:N}) - 3N \ln h \\
    &amp;= -\underbrace{N\int_{\mathbb{R}^{6}} dpdq \; \rho(p, q) \ln \rho(p, q)}_{\text{non-interacting particles}} - 3N \ln h
\end{aligned}
\]</span></p>
<p>Now, the entropy of a single atom <span class="math inline">\(\int_{\mathbb{R}^{6}} dpdq \; \rho(p, q) \ln \rho(p, q)\)</span> factors again into one position space and three momentum spaces:</p>
<p><span class="math display">\[
\begin{aligned}
    -\int_{\mathbb{R}^{6}} dpdq \; \rho(p, q) \ln \rho(p, q) &amp;= -\int_{\mathbb{R}^3} dq \rho(q) \ln \rho(q) - \sum_{i = x, y, z} \int_{\mathbb{R}} dp_i \ln \rho(p_i) \\
    &amp;= \ln V + 3 \times \underbrace{(\text{entropy of }\mathcal N(0, m/\beta))}_{\text{check Wikipedia}} \\
    &amp;= \ln V + \frac 32 \ln(2\pi m/\beta) + \frac 32 \\
\end{aligned}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Does this remind you of our previous discussion about how <a href="#sec-differential-entropy-ill-defined">differential entropy is ill-defined</a>? Finally that discussion is paying off! The choice of a natural unit of measurement in phase space is <em>equivalent</em> to fixing a natural base measure on phase space, such that differential entropy becomes well-defined.</p>
</div>
</div>
<p>The above is not yet correct, because permuting the atoms does not matter. That is, we have grossly inflated the state space. For example, if <span class="math inline">\(N = 2\)</span>, then we have counted the state <span class="math inline">\((q_1, p_1, q_2, p_2)\)</span>, then <span class="math inline">\((q_2, p_2, q_1, p_1)\)</span>, as if they are different, but they must be counted as the same. We must remove this redundancy by “quotienting out” the permutation group over the particles. The effect is dividing the phase space by <span class="math inline">\(\ln N!\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \frac SN &amp;= \ln V + \frac 32 \ln(2\pi m/\beta) + \frac 32 - 3 \ln h - \underbrace{\frac 1N \ln N!}_{\text{Stirling's approximation}} \\
    &amp;= \ln\left[\frac{V}{N} \left(\frac{2\pi m}{\beta h^2}\right)^{\frac 32}\right] + \frac 52
\end{aligned}
\]</span></p>
<p>giving us the <strong>Sackur–Tetrode formula</strong>:</p>
<p><span class="math display">\[
S(U, V, N) = \ln\left[\frac{V}{N} \left(\frac{4\pi m U}{3N h^2}\right)^{\frac 32}\right] + \frac 52
\]</span></p>
<p>All other thermodynamic quantities can then be derived from this. For example, the pressure is</p>
<p><span class="math display">\[P = \beta^{-1}(\partial_V S)_{U, N} = \frac{1}{\beta V}\]</span></p>
<p>more conventionally written as <span class="math inline">\(PV = \beta^{-1} = Nk_BT\)</span>, the <strong>ideal gas equation</strong>, where we have re-inserted the Boltzmann constant in respect for tradition.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="How Tetrode measured $h$">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Tetrode measured <span class="math inline">\(h\)</span>
</div>
</div>
<div class="callout-body-container callout-body">
<p>In early 1900s, Walther Nernst proposed the third law of thermodynamics. The history is rather messy, but suffice to say that the version we are going to care about says, “At the absolute zero of temperature the entropy of every chemically homogeneous solid or liquid body has a zero value.”. In support, he studied experimentally the thermodynamic properties of many materials at temperatures approaching absolute zero. He had a hydrogen liquefier and could reach around <span class="math inline">\(20 \ut{K}\)</span>.</p>
<p>Working on the assumption that <span class="math inline">\(S = 0\)</span> in any chemical at <span class="math inline">\(T = 0\)</span>, he could measure the entropy of any substance by slowing heating up a substance (or cooling down), measuring its heat capacity at all temperatures, then take an integral:</p>
<p><span class="math display">\[
k_B S = \int \frac{CdT}{T}
\]</span></p>
<p>The low-temperature data for mercury was the most available (mercury was also the substance with which Onnes discovered superconductivity). However, mercury is mostly in a liquid form at low temperatures. Fortunately, the latent heat of vaporization <span class="math inline">\(\Delta L\)</span> can be measured, and then we can get</p>
<p><span class="math display">\[
S_{\text{vapor}} = S_{\text{liquid}} + \frac{\Delta L}{k_BT}
\]</span></p>
<p>Back then, <span class="math inline">\(k_B = \frac{\text{Gas constant}}{\text{Avogadro constant}}\)</span>, and the <span class="math inline">\(S_{\text{liquid}}, \Delta L\)</span> of mercury were all measured, so combining these, Tetrode calculated a value of <span class="math inline">\(h\)</span> that is within <span class="math inline">\(30\%\)</span> of modern measurement. <span class="citation" data-cites="grimus100thAnniversarySackur2013">(<a href="#ref-grimus100thAnniversarySackur2013" role="doc-biblioref">Grimus 2013</a>)</span></p>
</div>
</div>
</section>
<section id="ideal-gas-again" class="level3">
<h3 class="anchored" data-anchor-id="ideal-gas-again">Ideal gas (again)</h3>
<p>We rederive the thermodynamic properties of ideal monoatomic gas via Helmholtz free entropy.</p>
<p><span class="math display">\[Z = \int e^{-\beta E} = \underbrace{\frac{1}{N!}}_{\text{identical particles}} \underbrace{V^N}_{\text{position}} \underbrace{(2\pi m/\beta )^{\frac 32 N}}_{\text{momentum}}\]</span></p>
<p>In typical textbooks, they use the Helmholtz free energy, which is defined as</p>
<p><span class="math display">\[
F = -\beta^{-1} \ln Z = -\beta^{-1} N \left(\ln \frac{V}{N} + \frac 32 \ln \frac{2\pi m}{\beta} + \frac{\ln N}{2N}\right)
\]</span></p>
<p>By the same formula from classical thermodynamics,</p>
<p><span class="math display">\[
d\ln Z = -\braket{E}d\beta + \beta\braket{P} dV \implies
\begin{cases}
    \braket{E}   &amp;= \frac 32 \frac{N}{\beta} \\
    \braket{P}V  &amp;= \frac{N}{\beta}
\end{cases}
\]</span></p>
<p>Notice how the <span class="math inline">\(\ln N!\)</span> part simply does not matter in this case.</p>
</section>
<section id="hard-ball-gas-dilute-gas-limit" class="level3">
<h3 class="anchored" data-anchor-id="hard-ball-gas-dilute-gas-limit">Hard ball gas (dilute gas limit)</h3>
<p>In order to refine the approach, we need to account for two effects.</p>
<ol type="1">
<li>Each particle takes up finite volume, which forces the total volume of positional space to be smaller than <span class="math inline">\(V^N\)</span>.</li>
<li>Particle pairs have interactions, which changes the Boltzmann distribution.</li>
</ol>
<p>The first effect can be modelled by assuming each atom is a hard ball of radius <span class="math inline">\(r\)</span>. The particles still have no interaction <em>except</em> that their positions cannot come closer than <span class="math inline">\(2r\)</span>.</p>
<p>Because there is no potential energy, the Boltzmann distribution on momentum space is the same, and so the Helmholtz free entropy <span class="math inline">\(\ln Z\)</span> still splits into the sum of positional entropy and momentous entropy. The momentum part is still <span class="math inline">\(\frac 32 N \ln\frac{2\pi}{\beta m}\)</span>, as the hard balls do not interfere with each other’s momentum, but the position part is smaller, because the balls mutually exclude each other.</p>
<p>Let <span class="math inline">\(a = 8V_{ball} = \frac{32}{3}\pi r^3\)</span> be a constant for the gas.</p>
<p>To measure the volume of the diminished position space, we can add one hard ball at a time. The first hard ball can take one of <span class="math inline">\(V\)</span> possible positions, as before. The next ball’s center cannot be within <span class="math inline">\(2r\)</span> of the center of the first ball, so its position can only take one of <span class="math inline">\((V - a)\)</span> positions, where <span class="math inline">\(a = 8V_{ball} = \frac{32}{3}\pi r^3\)</span> is a constant that depends on the shape of the hard balls. We continue this argument, obtaining the total volume in position space:</p>
<p><span class="math display">\[V(V- a) \cdots (V - (N-1)a) \approx V^N e^{0 -\frac{a}{V}-2\frac{a}{V} -\dots -(N-1)\frac{a}{V}} \approx V^N\left(1- \frac{N^2 a}{2V} \right)\]</span></p>
<p>This gives us</p>
<p><span class="math display">\[\braket{E} = \frac{3N}{2\beta}, \quad \braket{P}V \approx \frac N\beta \left(1 + \frac{a N}{2V}\right) \approx \frac{N/\beta}{V - \frac a2 N}\]</span></p>
<p>The second equation is the van der Waals equation when the term <span class="math inline">\(c = 0\)</span>, meaning there is neither attraction nor repulsion between particles.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="virial expansion">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
virial expansion
</div>
</div>
<div class="callout-body-container callout-body">
<p>In the above derivation, we are assuming that only pairwise exclusion matters. That is, we ignore the possibility that three or more balls may simultaneously intersecting each other. We can make a more accurate counting argument via the <a href="https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle">inclusion-exclusion principle</a>, which would lead us to a <a href="https://en.wikipedia.org/wiki/Virial_expansion">virial expansion</a> for gas.</p>
<p>Specifically, if the balls <span class="math inline">\(A, B\)</span> are intersecting, which has probability <span class="math inline">\(a/V\)</span>, and <span class="math inline">\(B, C\)</span> are also intersecting, also with probability <span class="math inline">\(a/V\)</span>, then <span class="math inline">\(A, C\)</span> are quite likely to be also intersecting, with probability much higher than <span class="math inline">\(a/V\)</span>. Therefore, if we have excluded the cases where <span class="math inline">\(A, B\)</span> are intersecting by subtracting with <span class="math inline">\(a/V\)</span>, and the cases where <span class="math inline">\(B, C\)</span> are intersecting by subtracting another <span class="math inline">\(a/V\)</span>, then we should be subtracting with something less than <span class="math inline">\(a/V\)</span>. The cluster expansion principle makes this precise. Unfortunately, it requires some difficult combinatorics. The interested reader should study <span class="citation" data-cites="andersenClusterMethodsEquilibrium1977">(<a href="#ref-andersenClusterMethodsEquilibrium1977" role="doc-biblioref">Andersen 1977</a>)</span>.</p>
</div>
</div>
</section>
<section id="soft-ball-gas-high-temperature-and-dilute-gas-limit" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="soft-ball-gas-high-temperature-and-dilute-gas-limit">Soft ball gas (high temperature and dilute gas limit)</h3>
<p>In the above derivation, we got one part of van der Waals equation right – the part where particles take up space. However, we have not yet accounted for the force between particles. We expect that if the particles attract each other, then <span class="math inline">\(P\)</span> should be smaller, and if the particles repel each other, then <span class="math inline">\(P\)</span> should be larger.</p>
<p>Let’s assume the gas is made of balls that has a hard core and a soft aura. That is, they repulse or attract each other at a distance, and when a pair comes too close. We also assume the force law depends only on the distances between particles.</p>
<p>That is, we can write such a system as having a gas potential energy <span class="math inline">\(V(q_1, \dots, q_N) = \sum_{i &lt; j} V(\|q_i - q_j\|)\)</span>. To enforce the hard core, we should have <span class="math inline">\(V(r) = \infty\)</span> when <span class="math inline">\(r \in [0, r_0]\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Lennard-Jones_potential.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Example potential energy field. This is the <a href="https://en.wikipedia.org/wiki/Lennard-Jones_potential">Lennard-Jones potential</a>, with a hard (not perfectly hard, but hard enough!) exclusive core, a soft repelling middle, and an attraction when far away. Figure from <a href="https://commons.wikimedia.org/wiki/File:Graph_of_Lennard-Jones_potential.png">Wikimedia Commons</a></figcaption>
</figure>
</div>
<p>Now, the partition function becomes</p>
<p><span class="math display">\[
Z = \int e^{-\beta\sum_i \frac{p_i^2}{2m} - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dqdp
\]</span></p>
<p>The momentum part is still the same <span class="math inline">\((2\pi/\beta m)^{\frac 32 N}\)</span>, but the position part is more difficult now. Still, we hope it will be close to <span class="math inline">\(V^N\)</span>.</p>
<p>That is, we need to calculate:</p>
<p><span class="math display">\[Z = \underbrace{V^N (2\pi/\beta m)^{\frac 32 N} \frac{1}{V^N}}_{\text{ideal gas}} \int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq\]</span></p>
<p>The integral <span class="math inline">\(\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq\)</span> can be evaluated piece-by-piece: <span class="math display">\[
\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq = \int dq_1 \left(\int dq_2 \; e^{-\beta V(\| q_1 - q_2 \|)} \left(\int dq_3 \; e^{-\beta (V(\| q_1 - q_3 \|) + V(\| q_2 - q_3 \|))} \cdots\right)\right)
\]</span></p>
<p>Because the chamber is so much larger than the molecular force-field, it is basically infinite. So for almost all of <span class="math inline">\(q_1\)</span> (except when it is right at the walls of the chamber), <span class="math inline">\(\int dq_2 \; e^{-\beta V(\| q_1 - q_2 \|)} \approx V - \delta\)</span>, where <span class="math inline">\(\delta\)</span> is some residual volume:</p>
<p><span class="math display">\[\delta := \int_{V} dq_2 \; (1 - e^{-\beta V(\|q_2 \|)})\]</span></p>
<p>Furthermore, because we are dealing with a dilute gas, the higher-order interactions don’t matter (see previous remark about the virial expansion). Therefore, the integral <span class="math display">\[\int_{V} dq_3 \; e^{-\beta (V(\| q_1 - q_3 \|) + V(\| q_2 - q_3 \|))} \approx \int_{V_1 \cup V_2 \cup V_3} dq_3 \; e^{-\beta (V(\| q_1 - q_3 \|) + V(\| q_2 - q_3 \|))} \]</span></p>
<p>where <span class="math inline">\(V_1\)</span> is the “turf” of particle <span class="math inline">\(1\)</span>, and <span class="math inline">\(V_2\)</span> is the turf of particle <span class="math inline">\(2\)</span>, and <span class="math inline">\(V_3\)</span> is the rest of the volume. Because the gas is dilute, we have basically <span class="math inline">\(V_1\)</span> disjoint from <span class="math inline">\(V_2\)</span>, giving us<br>
<span class="math display">\[\approx \sum_{j = 1, 2, 3}\int_{V_j} dq_3\; e^{-\beta V(\| q_j - q_3 \|)} \approx V - 2\delta\]</span></p>
<p>Together, we have <span class="math display">\[\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(\|q_i - q_j\|)} dq  \approx V(V-\delta) \cdots(V - (N-1)\delta)\]</span></p>
<p>Giving us <span class="math display">\[\ln Z \approx \ln Z_{\text{ideal}} - \frac{N^2 \delta}{2V}\]</span></p>
<p>It remains to calculate the residual volume. It has two parts, one due to the hard core and one due to the soft halo: <span class="math display">\[\delta = \int_{\|q_2 \| \leq r_0} dq_2 \; (1 - e^{-\infty}) + \int_{\|q_2 \| &gt; r_0} dq_2 \; (1 - e^{-\beta V(\|q_2\|)})\]</span></p>
<p>The first part is just <span class="math inline">\(a\)</span>, as calculated previously. The second part depends on the exact shape of the potential well. However, when temperature is high, <span class="math inline">\(\beta\)</span> would be very small, so the second part is approximately <span class="math inline">\(\int dq_2 (\beta V)\)</span>, which is a constant times <span class="math inline">\(\beta\)</span>.</p>
<p>Thus, we have <span class="math display">\[\ln Z \approx \ln Z_{\text{ideal}} - \frac{N^2}{V}(a + b \beta)\]</span></p>
<p>for some constants <span class="math inline">\(a, b\)</span>. This gives us the van der Waals equation: <span class="math display">\[\braket{P} V = \frac{N}{\beta} + \frac{N^2}{\beta V}a + \frac{N^2}{V} b\]</span></p>
</section>
</section>
<section id="other-classical-examples" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="other-classical-examples">Other classical examples</h2>
<section id="countably-many-states" class="level3">
<h3 class="anchored" data-anchor-id="countably-many-states">Countably many states</h3>
<p>In a surprising number of applications, we have a single system in an energy bath. The system has finitely many, or countably infinitely many, distinguishable states, each with a definite energy: <span class="math inline">\(E_0 \leq E_1 \leq E_2 \leq \cdots\)</span>. In particular, this covers most of the basic examples from quantum mechanics. In such a system, the probability of being in state <span class="math inline">\(i\)</span> is <span class="math inline">\(p_i = \frac 1Z e^{-\beta E_i}\)</span> where</p>
<p><span class="math display">\[
Z = \sum_i e^{-\beta E_i}
\]</span></p>
<p>Because I don’t like sections that are literally two paragraphs long, I will reformulate this as multinomial regression in mathematical statistics.</p>
<p>In the problem of classification, we observe some vector <span class="math inline">\(\vec X\)</span>, and we need to classify it into one of finitely many states <span class="math inline">\(\{1, 2, \dots\}\)</span>. With multinomial regression, we construct one vector <span class="math inline">\(\vec b_i\)</span> for each possible state <span class="math inline">\(i\)</span>, and then declare that the probability of being in state <span class="math inline">\(k\)</span> is</p>
<p><span class="math display">\[
Pr(i | X) = \frac{e^{-\vec X \cdot \vec b_i}}{Z(X)}, \quad Z(\vec X) = \sum_j e^{-\vec b_j \cdot \vec X}
\]</span></p>
<p>To make the parallel clearer:</p>
<p><span class="math display">\[
\begin{aligned}
\text{log probability} &amp; &amp; \text{observable } &amp; &amp;\text{ feature} &amp; &amp; \text{normalization constant} \\
    \ln p(i | \beta) &amp;=&amp; -\beta &amp; \; &amp; E_i &amp; &amp; - \ln Z \\
    \ln Pr(i | \vec X) &amp;=&amp;    -\vec X     &amp; \cdot &amp; \vec b_i &amp; &amp; - \ln Z
\end{aligned}
\]</span></p>
<p>We can make the analogy exact by adding multiple observables. Specifically, if we solve the following constrained optimization problem</p>
<p><span class="math display">\[
\begin{cases}
\max S \\
\braket{\vec b} = \vec b_0
\end{cases}
\]</span></p>
<p>then the solution is a multinomial classifier, with <span class="math inline">\(\vec X\)</span> playing the role of <span class="math inline">\(\beta\)</span>.</p>
<p>Interpreting the physics as statistics, we can think of <span class="math inline">\(\beta\)</span> as an “observable”. It is as if we are asking the physical system “What state are you in?” but we can only ask a very crude question “What is your energy on average?” Knowing that, we can make a reasonable guess by using the maximal entropy compatible with that answer.</p>
<p>Interpreting the statistics as physics, we can think of the observable <span class="math inline">\(\vec X\)</span> as “entropic forces”, trying to push the system towards the distribution of maximal entropy. At the equilibrium of zero entropic force, we have a multinomial classifier. This is the prototypical idea of <a href="https://en.wikipedia.org/wiki/Energy-based_model">energy-based statistical modelling</a>.</p>
</section>
<section id="fluctuation-by-n-12" class="level3">
<h3 class="anchored" data-anchor-id="fluctuation-by-n-12">Fluctuation by <span class="math inline">\(N^{-1/2}\)</span></h3>
<p>Suppose we have several tanks of oxygen gas that can exchange energy. They are in a microcanonical ensemble. Now, if we measure the total energy in the first tank, we would get a value <span class="math inline">\(E_1\)</span>. We sample it again after a while, and we would get another value. Averaging them, we would get <span class="math inline">\(\braket{E_1}\)</span>, which ought to match the prediction by classical thermodynamics. However, if thermodynamics is the theory of the average, then to go beyond it, statistical mechanics must predict the variance as well.</p>
<p>In <a href="#thm-partition-cumulant" class="quarto-xref">Theorem&nbsp;7</a>, We had already seen that the partition function generates the mean, the variance, and all other terms. Here we expand on this.</p>
<p>Take several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting. The system has total entropy</p>
<p><span class="math display">\[S = \sum_i S_i(E_i, A_i)\]</span></p>
<p>where <span class="math inline">\(A_i\)</span> stand for the other state variables we don’t care about, because they are held constant. Now, there is a single constraint of constant total energy:</p>
<p><span class="math display">\[E = \sum_i E_i\]</span></p>
<p>In the thermodynamical limit, the compound system reaches the maximal entropy state <span class="math inline">\(E_1^*, \dots, E_n^*\)</span>, which solves the following constrained maximization</p>
<p><span class="math display">\[
\begin{cases}
    \max \sum_i S_i(E_i, A_i)\\
    E = \sum_i E_i
\end{cases}
\]</span></p>
<p>By calculus, at the optimal point, all systems satisfy</p>
<p><span class="math display">\[
(\partial_{E_i} S_i)_{A_i} = \beta
\]</span></p>
<p>for some number <span class="math inline">\(\beta\)</span>. This is the <em>zeroth law of thermodynamics</em>.</p>
<p>However, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate. Write the <strong>fluctuation vector</strong> as</p>
<p><span class="math display">\[Y = (\Delta E_1, \dots, \Delta E_n)\]</span></p>
<p>which satisfies the constraint <span class="math inline">\(\sum_i \Delta E_i = 0\)</span>.</p>
<p>Suppose we observe the fluctuation vector to be a certain value <span class="math inline">\(Y = y\)</span>, then by <a href="#thm-cond-ent" class="quarto-xref">Theorem&nbsp;8</a>,</p>
<p><span class="math display">\[\rho_Y(y) \propto e^{S^*_{X|y}}\]</span></p>
<p>where <span class="math inline">\(S^*_{X|y}\)</span> is the entropy of the compound system, given <span class="math inline">\(Y = y\)</span>. For small fluctuations, we do a Taylor expansion:</p>
<p><span class="math display">\[S^*_{X|y} = \sum_i S_i(E_i^*) + \underbrace{(\partial_{E_i} S_i)_{A_i}}_{\text{$=\beta$}} \Delta E_i + \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2 + \cdots\]</span></p>
<p>Since <span class="math inline">\(\sum_i \Delta E_i = 0\)</span> at the equilibrium point,</p>
<p><span class="math display">\[\rho_Y(\Delta E) \propto e^{\sum_i \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2}\]</span></p>
<p>Now, <span class="math inline">\(\partial_E S = \beta\)</span>, and <span class="math inline">\(\partial_E^2 S = -\frac{1}{T^2 C}\)</span> in typical thermodynamic notation, where <span class="math inline">\(C\)</span> is <span class="math inline">\(\partial_T E\)</span>, the heat capacity (holding all other variables <span class="math inline">\(A\)</span> constant), so we have the following equation:</p>
<p><span class="math display">\[\rho_Y(\Delta E) \propto e^{-\sum_i \frac{1}{2T^2 C_{i}} (\Delta E_i)^2}\]</span></p>
<p>with fluctuation on the order <span class="math inline">\(\Delta E_i \sim \sqrt{T^2 C_i}\)</span>. For most substances studied by 19th century physicists, such as gas, that is <span class="math inline">\(\sim \sqrt{N} k_B T\)</span>. If they could measure the energy of gas at <span class="math inline">\(T = 500 \ut{K}\)</span> with precision down to <span class="math inline">\(10^{-3} \ut{J}\)</span>, that would still require a tank of gas with <span class="math inline">\(N = 10^{34} = 10^{10} \ut{mol}\)</span>. If they wanted to study this in oxygen, they would need 0.1 million tonnes of it.</p>
</section>
<section id="blackbody-radiation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="blackbody-radiation">Blackbody radiation</h3>
<p>Planck’s derivation of the blackbody radiation is the first great success of quantum statistical mechanics. We give a brief presentation here that tracks Planck’s original argument.</p>
<p>Consider a hollow cubic box with side lengths <span class="math inline">\(L\)</span>. The box has perfectly reflecting walls. At thermal equilibrium, the box is full of standing electromagnetic waves. Each standing EM wave has form <span class="math inline">\(\vec E(x, y, z, t) = \vec E_0 \sin(\omega t)\sin(\frac{n_x \pi x}{L})\sin(\frac{n_y \pi y}{L})\sin(\frac{n_z \pi z}{L})\)</span>, for some positive integers <span class="math inline">\(n_x, n_y, n_z\)</span>. Each wave has wavevectors <span class="math inline">\(\vec k = (n_x, n_y, n_z) \frac{\pi}{L}\)</span>. If we draw a region of volume <span class="math inline">\(\delta K\)</span> in the space of wavevectors, then the region would contain about <span class="math inline">\(\delta K \frac{L^3}{\pi^3}\)</span> valid wavevectors. Thus, we say that the wavevector space is <span class="math inline">\([0, +\infty)^3\)</span>, and has <strong>density of states</strong> <span class="math inline">\(\frac{L^3}{\pi^3}\)</span>. We can picture it as <span class="math inline">\([0, +\infty)^3\)</span> with a rectangular grid of points being the valid wavevectors, such that the numerical density of such grid points is <span class="math inline">\(\frac{L^3}{\pi^3}\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/density_of_states.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="margin-caption">The rectangular grid of valid wavevectors in the space of possible wavevectors.</figcaption>
</figure>
</div>
<p>At this point, we depart from Planck’s derivation. Instead of considering standing waves in a perfectly reflecting chamber, we consider planar waves in a chamber with periodic boundaries. That is, we imagine that we have opened 6 portals, so that its top wall is “ported” to the bottom, etc. In this case, the planar waves have valid wavevectors <span class="math inline">\(\vec k = (n_x, n_y, n_z) \frac{2\pi}{L}\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Wait, the numerical density of grid points is now just <span class="math inline">\(\frac{L^3}{8\pi^3}\)</span>, which is <span class="math inline">\(1/8\)</span> of what we found previously?</p>
<p>Yes, indeed, but it will work out correctly, because whereas the density of states has dropped to just <span class="math inline">\(1/8\)</span> of previously, the state space has increased <span class="math inline">\(8\times\)</span>, from <span class="math inline">\([0, +\infty)^3\)</span> to <span class="math inline">\(\mathbb{R}^3\)</span>.</p>
</div>
</div>
<p>Now, we need to allow <em>two</em> states at each valid wavevector, to account for polarization.</p>
<p>At this point, we have decomposed the state space into a composition of oscillators. Because there is no interaction between these oscillators,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> it remains to calculate the partition function of each oscillator.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;That is, two photons do not interact, except when the energy levels are so high that you would need a quantum field theorist to know what is going on.</p>
<p>Whereas modern spiritualists talk of electromagnetic fields and quantum vibrations, a century ago they talked of subatomic structures and ether vibrations. Light resembles ghosts and spirits in that they are massless, untouchable, moving very fast, bright, and vaguely associated with good feelings. During the 19th century, the best scientific theory for light, that of ether theory, became the foundation of many spiritualist world systems. <span class="citation" data-cites="aspremPonderingImponderablesOccultism2011">(<a href="#ref-aspremPonderingImponderablesOccultism2011" role="doc-biblioref">Asprem 2011</a>)</span> The connection of electromagnetism with <a href="https://en.wikipedia.org/wiki/Animal_magnetism">animal magnetism</a> did not help.</p></div></div><p>Planck considered an ensemble of <span class="math inline">\(N\)</span> oscillators, all at the same wavevector and polarization. If they have average energy <span class="math inline">\(\braket{E}\)</span>, the question is to find the total entropy for the whole system, which, when divided by <span class="math inline">\(N\)</span>, should yield the entropy of a single oscillator. Here he used the celebrated <em>quantum hypothesis</em>: The energy levels are divided into integer levels of <span class="math inline">\(nh\nu\)</span>, where <span class="math inline">\(n = 0, 1, 2, \dots\)</span>. By the <a href="https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)">stars and bars</a> argument, there are <span class="math inline">\(\binom{N + M-1}{M}\)</span> ways do distribute these energy-quanta between these oscillators, where <span class="math inline">\(M = \frac{N\braket{E}}{h\nu}\)</span>.</p>
<p><span class="math display">\[S = \frac 1N \ln \binom{N+M-1}{M} \underbrace{\approx}_{\text{Stirling}} (1 + a) \ln (1+a) - a \ln a, \quad a = \frac{\braket{E}}{h\nu}\]</span></p>
<p>Given the entropy function, he then matched <span class="math inline">\(\braket{E}\)</span> to temperature <span class="math inline">\(\beta\)</span> by the equality <span class="math inline">\(\beta = \partial_{\braket{E}} S\)</span>, giving</p>
<p><span class="math display">\[
\braket{E} = \left(\frac{h\nu}{e^{\beta h\nu}-1}\right)
\]</span></p>
<p>Now, in any direction <span class="math inline">\(\hat k\)</span>, for any wavelength interval <span class="math inline">\([\lambda, \lambda + d\lambda]\)</span>, and any span of solid angle <span class="math inline">\(d\Omega\)</span>, compute its corresponding wavevector space volume <span class="math inline">\(k dkd\Omega = \frac{4\pi^2}{\lambda^3} d\lambda d\Omega\)</span>, and multiply that by the density of states and <span class="math inline">\(\braket{E}\)</span>, yielding the blackbody radiation law.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/blackbody_radiation_wavevector_space.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">In any direction, wavelength, and span of solid angle, we find the corresponding volume in the space of wavevectors, sum up all the states within that volume, to obtain the blackbody radiation law.</figcaption>
</figure>
</div>
<p>The details are found in any textbook. I will just point out some interesting facts typically passed over in textbooks.</p>
<p>In the above derivation of the blackbody radiation law, the allowed wavevectors <span class="math inline">\(\vec k\)</span> are like a dust cloud in the space of possible wavevectors. The cloud is assumed to be dense and even, so that the number of states inside a chunk of volume <span class="math inline">\(\Delta V\)</span> is roughly <span class="math inline">\(\Delta V \rho\)</span>, where <span class="math inline">\(\rho\)</span> is the <em>average</em> density of states. This only works if <span class="math inline">\(\Delta V \rho \gg 1\)</span>, or in other words, <span class="math inline">\(\frac{\Delta \lambda}{\lambda} \gg \frac{\lambda^3}{L^3}\)</span>. Thus, when the chamber is small, or when temperature is low enough that the spectral peak is close to the zero, then the murky cloud of wavevectors resolves into individual little specks, and we have deviation from blackbody radiation law.</p>
<p>In this limit, the precise shape of the chamber becomes important, since the precise chamber shape has a strong effect on long-wavelength (low-temperature) resonant modes. A tiny cube and a tiny cylinder have different blackbody spectra. See <span class="citation" data-cites="reiserGeometricEffectsBlackbody2013">(<a href="#ref-reiserGeometricEffectsBlackbody2013" role="doc-biblioref">Reiser and Schächter 2013</a>)</span> for a literature review.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/microscopic_blackbody_spectrum.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Red solid line is Planck’s blackbody radiation law. Black dashed line is the analytical prediction for a spherical blackbody.<span class="citation" data-cites="garcia-garciaFinitesizeCorrectionsBlackbody2008">(<a href="#ref-garcia-garciaFinitesizeCorrectionsBlackbody2008" role="doc-biblioref">García-García 2008</a>)</span></figcaption>
</figure>
</div>
<p>According to <a href="https://en.wikipedia.org/wiki/Kirchhoff's_law_of_thermal_radiation">Kirchhoff’s law of thermal radiation</a>, a chunk of matter is exactly as absorptive as it is emissive. A blackbody absorbs all light, and conversely it emits light at the maximal level. A white body absorbs no light, and conversely it does not emit light. This can be understood as a consequence of the second law: If a body emits more light than it absorbs, then it would spontaneously get colder when placed inside a blackbody radiation chamber.</p>
<p>However, much more can be said than this. Not only is it exactly as absorptive as it is emissive, it is as absorptive as it is emissive at <em>any angle</em>, at any wavelength, and any polarization. So for example, if a piece of leaf is not absorptive when viewed from an angle, at the green light wavelength, of clockwise polarization, then it is not emissive under the same angle, wavelength, polarization.</p>
<p>Why is that? The standard argument <span class="citation" data-cites="reifFundamentalsStatisticalThermal1998">(<a href="#ref-reifFundamentalsStatisticalThermal1998" role="doc-biblioref">Reif 1998, chap. 9.15</a>)</span> uses a time-reversal argument, but I like to think of it as yet more instances of protecting the second law. If you look inside a blackbody radiation chamber, you would see a maximal entropy state. Light rushes in all directions equally, at all polarizations equally, and the energy is distributed optimally across the spectrum to maximize entropy (because <span class="math inline">\(\beta\)</span> is constant across the whole spectrum). If we have a material that takes in blue light and outputs green light, then it would spontaneously decrease entropy. Similarly, if it can absorb vertically polarized light to emit diagonally polarized light, it would also spontaneously decrease entropy, etc.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Relativistic gas?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Relativistic gas?
</div>
</div>
<div class="callout-body-container callout-body">
<p>A box full of blackbody radiation is also called a photon gas. The photon gas is sometimes treated as the limit of “ultrarelativistic gas”. Start with the relativistic energy function <span class="math inline">\(E = \sqrt{m^2 c^4 + \|p\|^2 c^2}\)</span>, derive its <a href="https://en.wikipedia.org/wiki/Maxwell-J%C3%BCttner_distribution">Boltzmann distribution</a> <span class="math inline">\(\rho(q, p) \propto e^{-\beta \sqrt{m^2 c^4 + \|p\|^2 c^2}}\)</span>, then take the <span class="math inline">\(m \to 0\)</span> limit. This gives some correct results, such as the <span class="math inline">\(U = 3PV\)</span>.</p>
<p>However, accounting for the entropy of photon gas, as well as deriving the blackbody radiation, hinges critically on the photon quantization <span class="math inline">\(E = h\nu, 2h\nu, \dots\)</span>. I guess it can be done correctly by relativistic quantum mechanics, but it is of course beyond the world of classical mechanics.</p>
</div>
</div>
<p>The point is that the blackbody radiation law is not <em>about</em> a blackbody. Instead, it is about photons in vacuum. We could have taken a perfectly reflecting mirror box (or if you are fancy, a <a href="https://en.wikipedia.org/wiki/3-torus">three-dimensional torus</a>) and injected it with a gas of <span class="math inline">\(400 \ut{nm}\)</span> photons with zero total momentum and angular momentum. Since no conservation laws are constraining, the system will equilibrate to its maximal entropy state, which is the blackbody radiation spectrum. We simply need to wait a few eternities for <a href="https://en.wikipedia.org/wiki/Two-photon_physics">photon-photon interactions</a> to do the job. Thus, the precise material of the chamber does not matter, and charcoal is merely a better catalyst than titanium oxide.</p>
</section>
<section id="sec-rubber-band" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-rubber-band">Rubber bands</h3>
<p>It turns out that rubber bands, and generally things made of long polymers, <em>contract</em> when heated up, instead of expanding. This is the <a href="https://en.wikipedia.org/wiki/Gough%E2%80%93Joule_effect">Gough–Joule effect</a>. Roughly speaking, this is because elasticity in long polymer material (like rubber) is very different from elasticity in short molecule solids (like copper and ice). In rubber, elasticity is an entropic force, while in copper, it is an electrostatic force caused by attraction between molecules.</p>
<p>To model a rubber band, consider a long chain molecule with <span class="math inline">\(N\)</span> joints. Each joint can go forward or backward, with equal energy. Each link between two joints has length <span class="math inline">\(d\)</span>. The total length of the system is <span class="math inline">\(L\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rubber_band.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A sample shape from the ensemble of all rubber band shapes.</figcaption>
</figure>
</div>
<section id="direct-argument-microcanonical" class="level4">
<h4 class="anchored" data-anchor-id="direct-argument-microcanonical">Direct argument (microcanonical)</h4>
<p>The entropy of the system, conditional on <span class="math inline">\(L\)</span>, is</p>
<p><span class="math display">\[S = \ln \binom{N}{\frac{N + L/d}{2}}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="elastic constant">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
elastic constant
</div>
</div>
<div class="callout-body-container callout-body">
<p>The thermodynamic equation for the rubber band is</p>
<p><span class="math display">\[0 = TdS + FdL\]</span></p>
<p>because the internal energy of the rubber band is constant, no matter how the joint turns.</p>
<p>Therefore, the elastic force is</p>
<p><span class="math display">\[F = -T \partial_L S \approx -T \frac{S(L+2d) - S(L)}{2d} \approx \frac{T}{2d }\ln\frac{Nd+L}{Nd - L}\]</span></p>
<p>When <span class="math inline">\(Nd \gg L\)</span>, that is, we have not stretched it close to the breaking point, the elastic force is</p>
<p><span class="math display">\[F \approx \frac{TL}{Nd^2} = k L\]</span></p>
<p>where <span class="math inline">\(k = \frac{T}{Nd^2}\)</span> is the elastic constant, proportional to temperature.</p>
</div>
</div>
<p>Why does the rubber band stiffen when temperature rise? We can interpret it as follows. When we place the rubber band in a chamber of hot air, the air particles would often collide with the links in the rubber band, flipping it. When there are more links going to the right than the left, then the air particles would tend to flip the links to the left, decreasing <span class="math inline">\(L\)</span>, and conversely. The net force is zero only when there are an equal number of links going either way, which is when <span class="math inline">\(L = 0\)</span>.</p>
</section>
<section id="via-free-entropy-canonical" class="level4">
<h4 class="anchored" data-anchor-id="via-free-entropy-canonical">Via free entropy (canonical)</h4>
<p>Because the rubber band has <span class="math inline">\(dE = TdS + FdL\)</span>, the corresponding free entropy is <span class="math inline">\(S - \beta \braket{E} + \beta F \braket{L}\)</span>. Under the canonical distribution, that free entropy is maximized, meaning that <span class="math inline">\(\rho(x) \propto e^{\beta FL(x)}\)</span> where <span class="math inline">\(x\)</span> is a microstate of the rubber band (i.e.&nbsp;the precise position of each link), and <span class="math inline">\(L(x)\)</span> is the corresponding length (macrostate).</p>
<p>The trick of using the free entropy is that it decomposes the entire rubber band into atomic individuals. Like how opening an energy market converts consumers trying to coordinate their energy use into consumers each individually buying and selling energy, and thus simplifying the calculation problem. Like how <a href="https://yuxi-liu-wired.github.io/essays/posts/analytical-mechanics/index.html#lagranges-devil-at-disneyland">Laplace’s devil</a> allows you to calculate the optimal way to schedule your day. Microcanonical ensembles are true, but canonical ensembles are almost just as true, and much easier to use. The idea is that the canonical ensemble and the microcanonical ensemble are essentially the same because the fluctuation is so tiny.</p>
<p>Back to the rubber band. Each individual link in the rubber band now is freed from the collective responsibility of reaching exactly length <span class="math inline">\(L\)</span>. It is now an atomized individual, maximizing its own free entropy <span class="math inline">\(S - \beta \braket{E} + \beta F \braket{L}\)</span>. Let <span class="math inline">\(p\)</span> be its probability of going up, then its free entropy is</p>
<p><span class="math display">\[
\underbrace{-p\ln p - (1-p) \ln(1-p) }_{\text{$S$}} - 0 + \beta F (d p - d(1-p))
\]</span></p>
<p>This is maximized by the Boltzmann distribution <span class="math inline">\(p = \frac{e^{\beta F 2d}}{1+e^{\beta F 2d}}\)</span>, with first two moments</p>
<p><span class="math display">\[
\braket{L} = \frac{e^{\beta F 2d} - 1}{e^{\beta F 2d} + 1} d \approx \beta Fd^2,
\quad \mathbb{V}[L] = p(1-p)d^2 \approx d^2/4
\]</span></p>
<p>The total extension of the rubber band has the first two moments</p>
<p><span class="math display">\[
N\braket{L} \approx \beta F Nd^2,
\quad N\mathbb{V}[L] \approx Nd^2/4 = \frac{1}{4F\beta} N\braket{L}
\]</span></p>
<p>The first equation is the same as the previous one. The second equation tells us the fluctuation in rubber band length when held under constant force and temperature. For typical conditions like <span class="math inline">\(F \sim 1 \ut{N}, T \sim 300 \ut{K}, N\braket{L} \sim 1 \ut{m}\)</span>, the fluctuation is on the order of <span class="math inline">\((0.1\ut{nm})^2\)</span>, about one atom’s diameter. So we see that the difference between the canonical and the microcanonical ensemble are indeed too tiny to speak of.</p>
</section>
</section>
</section>
<section id="combinatorial-examples" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="combinatorial-examples">Combinatorial examples</h2>
<section id="burning-the-library-of-babel" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="burning-the-library-of-babel">Burning the library of Babel</h3>
<blockquote class="blockquote">
<p>The universe (which others call the Library) is composed of an indefinite, perhaps infinite number of hexagonal galleries… always the same: 20 bookshelves, 5 to each side, line four of the hexagon’s six sides… each bookshelf holds 32 books identical in format; each book contains 410 pages; each page, 40 lines; each line, approximately 80 black letters… punctuation is limited to the comma and the period. Those two marks, the space, and the twenty-two letters of the alphabet are the 25 sufficient symbols…</p>
<p><em>The Library of Babel</em>, Jorge Luis Borges</p>
</blockquote>
<p>Like the artist <a href="https://en.wikipedia.org/wiki/M._C._Escher">M. C. Escher</a>, the writer <a href="https://en.wikipedia.org/wiki/Jorge_Luis_Borges">J. L. Borges</a> is a favorite of scientists, for his stories that construct precise worlds like elegant thought experiments. The Library of Babel is a thought experiment on combinatorics and entropy. In the universe, there are only books. Each book contains</p>
<p><span class="math display">\[410 \ut{page} \times 40\ut{line/page} \times 80 \ut{symbol/line} = 1.3\times 10^6\ut{symbol}\]</span></p>
<p>Suppose the books are uniformly random sequences made of 25 symbols, then each symbol contains <span class="math inline">\(\ln 25\)</span> amount of entropy, and each book contains <span class="math inline">\(1.3\times 10^6\ln 25 = 4.2 \times 10^6\)</span>. Now, consider another library of Babel, but this one consists of books filled with white space, so each book has zero entropy. Then, we can take one empty book and “burn” it into a uniformly random book, recovering <span class="math inline">\(4.2 \times 10^6 k_B T\)</span> free energy per book burned. At ambient temperature <span class="math inline">\(300\ut{K}\)</span>, that is just <span class="math inline">\(1.4 \times 10^{-14}\ut{J}\)</span> per book. Book-burning isn’t going to keep the librarians warm.</p>
<p><img src="figure/babel_burning_ideogram.webp" class="img-fluid"></p>
<blockquote class="blockquote">
<p>After having razed the garden and profaned the chalices and altars, the Huns entered the monastery library on horseback and trampled the incomprehensible books and vituperated and burned them, perhaps fearful that the letters concealed blasphemies against their god, which was an iron scimitar. Palimpsests and codices were consumed, but in the heart of the fire, amid the ashes, there remained almost intact the twelfth book of the <em>Civitas Dei</em>, which relates how in Athens Plato taught that, at the centuries’ end, all things will recover their previous state and he in Athens, before the same audience, will teach this same doctrine anew.</p>
<p><em>The Theologians</em>, Jorge Luis Borges</p>
</blockquote>
<p>While it is fanciful to “burn” a book by randomizing its letters, if we take the “information is physical, physics is informational” equivalence seriously, then there must be a way to actually “burn information”. Indeed, there is a way.</p>
<p>We can consider a chamber containing a single ball inside. The chamber has a removable wall, separating it into two parts labelled <code>0</code> and <code>1</code>. If we know which side the ball is in, then we can put a piston into the other side, remove the wall, then extract work by isothermal expansion. Of course, the actual work extractable is random, but if we do this for a large number of chambers, then it is the same as isothermal expansion of an ideal gas to double its volume. By the ideal gas equation, we would extract <span class="math inline">\(k_B T \ln 2\)</span> of mechanical energy per chamber on average, exactly as predicted by the information-burning argument.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/information-burning_engine.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A car driven by burning a tape of zero information, emitting “waste information” in its wake. <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 5.10</a>)</span></figcaption>
</figure>
</div>
<p>In the tradition of <a href="https://en.wikipedia.org/wiki/Steampunk">steampunk</a>, <span class="citation" data-cites="luEngineeringMaxwellDemon2014">(<a href="#ref-luEngineeringMaxwellDemon2014" role="doc-biblioref">Lu, Mandal, and Jarzynski 2014</a>)</span> constructed a purely mechanical model of information-burning. As shown below, we have a belt of paddles that can rotate freely, except when they hit one of the two red rods. The two red rods divide space into two sides: the <code>0</code> side and the <code>1</code> side. One of the red rods has an opening on it, allowing the paddle to go through. All the paddles start on the top in the <code>0</code> side, and slowly move down. As soon as a paddle has moved into the middle, it would be able to be able to freely move between <code>0</code> and <code>1</code> sides.</p>
<p>So, if we blur our vision and look at the average motion of the paddles, we would see an averaged “entropic force” that drives paddles from <code>0</code> side to <code>1</code> side. We can harvest this entropic force by adding a ring with a paddle inside, which can be hit by the paddles, driving it into rotation. The ring would be able to wind up a spring, thus converting information into mechanical work.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/mechanical_maxwell_demon.jpeg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The mechanical information-engine. <span class="citation" data-cites="luEngineeringMaxwellDemon2014">(<a href="#ref-luEngineeringMaxwellDemon2014" role="doc-biblioref">Lu, Mandal, and Jarzynski 2014</a>)</span></figcaption>
</figure>
</div>
<figure class="figure">
<video controls="" width="100%">
<source src="figure/mechanical_maxwell_demon.webm" type="video/webm">
</video>
<figcaption>
Video source: <a href="https://www.youtube.com/watch?v=00TyIShzR6o">Mechanical Maxwell’s Demon in Motion - YouTube</a>.
</figcaption>
</figure>
</section>
<section id="multinomials-and-the-chi-squared-test" class="level3">
<h3 class="anchored" data-anchor-id="multinomials-and-the-chi-squared-test">Multinomials and the chi-squared test</h3>
<p>Let <span class="math inline">\(\vec p := (p_1, \dots, p_k)\)</span> be a discrete probability distribution, and let <span class="math inline">\(N_1, \dots, N_n\)</span> be integers that depend on <span class="math inline">\(N\)</span>, such that</p>
<p><span class="math display">\[\lim_{N \to \infty}\vec N / N = \vec p\]</span></p>
<p>where <span class="math inline">\(\vec N := (N_1, \dots, N_n)\)</span>, then we have</p>
<p><span class="math display">\[\lim_{N \to \infty}\frac 1N \ln \binom{N}{N_1, \dots, N_n} = -\sum_i p_i \ln p_i = S[\rho]\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the discrete probability distribution, and <span class="math inline">\(\binom{N}{N_1, \dots, N_n} = \frac{N!}{N_1! \cdots N_n!}\)</span> is the <a href="https://en.wikipedia.org/wiki/Multinomial_theorem">multinomial coefficient</a>, which counts the number of ways for <span class="math inline">\(N\)</span> labelled balls to go into <span class="math inline">\(n\)</span> labelled boxes, such that the <span class="math inline">\(i\)</span>-th box contains <span class="math inline">\(N_i\)</span> balls.</p>
<p>This can be interpreted as the thermodynamic limit of a lot of balls.</p>
<p>Consider a population of particles, all in energy-contact with an energy bath of <span class="math inline">\(\beta = 1\)</span>. Each particle has <span class="math inline">\(n\)</span> states, with state <span class="math inline">\(i\)</span> having energy <span class="math inline">\(-\ln p_i\)</span>. Thus, at the Boltzmann distribution, each particle is precisely sampled from the categorical distribution <span class="math inline">\(\vec p\)</span>. Clearly, this system has entropy <span class="math inline">\(N\sum_i -p_i \ln p_i\)</span>. This is a “canonical ensemble”.</p>
<p>In contrast, consider the “microcanonical ensemble” (in quotation marks, because the constraint is not on the total energy) of <span class="math inline">\(N\)</span> particles, such that there are exactly <span class="math inline">\(N_i\)</span> particles in state <span class="math inline">\(i\)</span>.</p>
<p>Thinking thermodynamically, we can single out one ball as the system, and regard the other <span class="math inline">\(N\)</span> balls as a bath. Once we calculate the marginal entropies of the bath, we can infer the Boltzmann distribution for the system, and show that it is the same as the Boltzmann distribution in the canonical ensemble.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="non-rigorous part">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
non-rigorous part
</div>
</div>
<div class="callout-body-container callout-body">
<p>We <em>hope</em> that as <span class="math inline">\(N \to \infty\)</span>, all correlations (pairwise, triple-wise, etc) between balls decay fast enough, such that the interaction-entropy between the balls drop to zero, leaving</p>
<p><span class="math display">\[\text{average entropy per ball} = \text{marginal entropy of a ball}\]</span></p>
<p>Justifying this rigorously is generally very difficult, and in fact, the assumption is false at phase transitions, where correlations <em>do not</em> decay fast enough, and so the thermodynamic limit is false. However, it is typically good enough to check that the fluctuations decay as <span class="math inline">\(N^{-1/2}\)</span>, and if so, the thermodynamic limit is true.</p>
</div>
</div>
<p>Suppose we move the ball from box <span class="math inline">\(i\)</span> to box <span class="math inline">\(j\)</span>, then it would force the bath to change all its balls, changing its entropy by</p>
<p><span class="math display">\[
\ln\binom{N}{n_1', \dots, n_k'} -\ln \binom{N}{n_1, \dots, n_k}
\]</span></p>
<p>where <span class="math inline">\(n_i' = n_i + 1, n_j' = n_j - 1\)</span>, and otherwise unchanged. By definition of multinomials, this is <span class="math inline">\(\ln n_j - \ln(n_i+1) \to (\ln p_j - \ln p_i)\)</span> at large enough <span class="math inline">\(N\)</span>.</p>
<p>Let <span class="math inline">\(X\)</span> stand for the total state, including the bath and the singled-out system, and let <span class="math inline">\(Y\)</span> stand for the state of the singled-out system. By the conditional entropy theorem, when the entire system is at the maximal entropy distribution, the distribution of the singled-out system is</p>
<p><span class="math display">\[\rho^*_Y(y) \propto e^{S_{X|y}^*}\]</span></p>
<p>From the previous calculation, we have</p>
<p><span class="math display">\[S_{X|j}^* - S_{X|j}^* = \ln p_j - \ln p_i\]</span></p>
<p>yielding <span class="math inline">\(S_{X|i}^* = S_0 + \ln p_i\)</span> for some constant <span class="math inline">\(S_0\)</span>, and so <span class="math inline">\(\rho^*_Y(y) \propto p_i\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="chi-squared test">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
chi-squared test
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(X\)</span> stand for the state of the entire population of particles. That is, <span class="math inline">\(X\)</span> describes the precise state of each particle. Let <span class="math inline">\(Y\)</span> stand for the vector of <span class="math inline">\(\vec N\)</span>. That is, it counts the number of particles in each state. As typical, at the limit of <span class="math inline">\(N \to \infty\)</span>, we should expect <span class="math inline">\(\frac{N_i}{N} \to p_i\)</span> with certainty (the thermodynamic limit), and the variance should scale as <span class="math inline">\(1/N\)</span> (statistical fluctuation). So, if we observe the particle population many times, and plot all the <span class="math inline">\(\frac{N_i}{N}\)</span> on the <span class="math inline">\(n\)</span>-simplex, we should see an ellipsoidal cloud centered around <span class="math inline">\(\vec p\)</span> with radius <span class="math inline">\(\sim N^{-1/2}\)</span>.</p>
<video controls="" width="100%">
<source src="figure/Sanov_convergence.webm" type="video/webm">
</video>
<p>This is an example of how the microcanonical ensemble and the canonical ensemble become indistinguishable at the large particle limit. Indeed, Boltzmann often used this equation to derive the canonical ensemble from microcanonical ensembles.</p>
<p>We have argued that the cloud should be ellipsoidal and centered around <span class="math inline">\((p_1, \dots, p_n)\)</span> with radius <span class="math inline">\(\sim N^{-1/2}\)</span>. What exactly is its shape? Well, since each particle’s distribution of states is categorical, and the particles are uncorrelated (thanks to the energy market), the mean and covariance of <span class="math inline">\(\vec N= (N_1, \dots, N_n)\)</span> are <span class="math inline">\(\vec p N\)</span> and <span class="math inline">\([\diag(\vec p) - \vec p \vec p^T] N\)</span>. The covariance matrix is not linearly independent, because of the constraint <span class="math inline">\(\sum_i N_i/N = 1\)</span>.</p>
<p>A good trick is worth doing again and again. Like how the microcanonical ensemble is freed up by a free market into a canonical ensemble, here we free up <span class="math inline">\(\vec N\)</span> by <em>adding noise</em>, then conditioning on zero noise:</p>
<p><span class="math display">\[
\vec N / N + z\vec p/\sqrt N \sim \mathcal N(\vec p, \diag(\vec p) / N) \quad z \sim \mathcal N(0, 1)
\]</span></p>
<p>Therefore, the probability density of <span class="math inline">\(\vec N\)</span> satisfies</p>
<p><span class="math display">\[\rho(\vec N / N) \propto \exp\left(-\frac 12 \sum_i \frac{(N_i - Np_i)^2}{Np_i}\right)\]</span></p>
<p>after conditioning on <span class="math inline">\(z = 0\)</span>. Therefore, the distribution of <span class="math inline">\(\sum_i \frac{(N_i - Np_i)^2}{Np_i}\)</span> converges to <span class="math inline">\(\chi^2(n-1)\)</span>. This is the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">chi-squared test</a>.</p>
</div>
</div>
</section>
<section id="sanovs-theorem" class="level3">
<h3 class="anchored" data-anchor-id="sanovs-theorem">Sanov’s theorem</h3>
<p>Typically in statistical mechanics, we study the fluctuation of a macroscopic variable on the order of <span class="math inline">\(N^{-1/2}\)</span>. Large deviation theory studies the fluctuation on the order of <span class="math inline">\(1\)</span>. The prototypical example is Sanov’s theorem.</p>
<p>The problem is as follows: Suppose that we require <span class="math inline">\(\vec N / N \to \vec q\)</span>, where <span class="math inline">\(\vec q\)</span> is some other probability vector, what is the behavior of <span class="math inline">\(Pr(\vec N / N \approx \vec q)\)</span>? Sanov’s theorem states that</p>
<p><span class="math display">\[
Pr(\vec N / N \approx \vec q) \sim e^{-ND_{KL}(\vec q \| \vec p)}
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>By <a href="#thm-cond-free-ent" class="quarto-xref">Theorem&nbsp;9</a>,</p>
<p><span class="math display">\[
\frac 1N \ln Pr(\vec N / N \approx \vec q) = \bar f^*_{|\vec N / N \approx \vec q} - \bar f^*
\]</span></p>
<p>where <span class="math inline">\(\bar f^*\)</span> is the Helmholtz free entropy per particle for the system without constraint on <span class="math inline">\(\vec N\)</span>, and <span class="math inline">\(\bar f^*_{|\vec N / N \approx \vec q}\)</span> is the Helmholtz free entropy per particle, conditional on <span class="math inline">\(\vec N / N \approx \vec q\)</span>.</p>
<p>The constraint of <span class="math inline">\(\vec N / N \approx \vec q\)</span> is a global constraint, but as usual, when <span class="math inline">\(N\)</span> is large, global constraints over all particles becomes local constraints for each particle individually. In this case, the constraint on individual particle is simply that its state is distributed according to <span class="math inline">\(\vec q\)</span>, energy be damned.</p>
<p>Thus, we find that</p>
<p><span class="math display">\[
\frac 1N \ln Pr(\vec N / N \approx \vec q) = \left(\sum_i -q_i \ln q_i - \sum_i q_i E_i\right) - \left(\sum_i -p_i \ln p_i - \sum_i p_i E_i\right) = -D_{KL}(\vec q \| \vec p)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="surface-area-of-high-dimensional-spheres" class="level3">
<h3 class="anchored" data-anchor-id="surface-area-of-high-dimensional-spheres">Surface area of high-dimensional spheres</h3>
<p>Let <span class="math inline">\(\Omega_N\)</span> be the surface area of a sphere of radius <span class="math inline">\(\sqrt N\)</span> in <span class="math inline">\(\mathbb{R}^N\)</span>, then</p>
<p><span class="math display">\[\ln\Omega_N = \frac N2 \ln (2\pi e)-\frac 12 \ln (\pi e) + O(N^{-1})\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(x_1, \dots, x_N\)</span> be sampled IID from <span class="math inline">\(\mathcal N(0, 1)\)</span>, and let <span class="math inline">\(r_N^2 = \sum_i x_i^2\)</span>. By routine calculation, <span class="math inline">\(\braket{r_N^2} = N\)</span>, and <span class="math inline">\(\braket{r_N^4} = 2N + N^2\)</span>. Therefore, <span class="math inline">\(r_N^2\)</span> is approximately distributed as <span class="math inline">\(\mathcal N(N, 2N)\)</span>, and so <span class="math inline">\(r_N\)</span> is approximately distributed as <span class="math inline">\(\mathcal N(\sqrt N, 1/2)\)</span>.</p>
<p>Now consider two distributions on <span class="math inline">\(\mathbb{R}^N\)</span>. The first is a microcanonical ensemble: <span class="math inline">\(x_{1:N}\)</span> is distributed uniformly on the thin energy shell of <span class="math inline">\(r_N^2 \in [N, N + \delta N]\)</span>. The second is a canonical ensemble: each <span class="math inline">\(x_i\)</span> is distributed independently according to <span class="math inline">\(\mathcal N(0, 1)\)</span>.</p>
<p>We can think of them as particles in a potential well of form <span class="math inline">\(V(x) = \frac 12 x^2\)</span>. The first ensemble is the microcanonical ensemble where the total energy is fixed, and the second is the canonical ensemble at temperature <span class="math inline">\(\beta = 1\)</span>.</p>
<p>We can calculate the entropy of the canonical ensemble in two ways. We can calculate it by adding up the entropy of each particle, which are the same since there is no interaction energy between particles. We can also calculate it indirectly, by first sampling a random radius, then a random point from the microcanonical ensemble, then multiplying them together.</p>
<p>Because the canonical ensemble is spherically symmetric, the radius and the direction of the vector <span class="math inline">\(x_{1:N}\)</span> are independent. Therefore,</p>
<p><span class="math display">\[S_{\text{canonical}} = S_{\text{microcanonical}} + S_{\text{radius}}\]</span></p>
<p>or</p>
<p><span class="math display">\[\ln \Omega_N = \underbrace{S_{\text{canonical}}}_{\text{$= N S[\mathcal N(0, 1)]$}} - \underbrace{S_{\text{radius}}}_{\text{$\approx S[\mathcal N(0, 1/2)]$}}\]</span></p>
<p>Because the entropy of <span class="math inline">\(\mathcal N(0, \sigma^2)\)</span> is <span class="math inline">\(\frac 12 \ln(2\pi e \sigma^2)\)</span>, we plug them in and obtain the result.</p>
</div>
</div>
</div>
<p>Again, this is an example of a general pattern: the microcanonical ensemble and the canonical ensemble become indistinguishable when the number of particles goes infinite.</p>
</section>
</section>
<section id="biological-examples" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="biological-examples">Biological examples</h2>
<section id="how-elastic-is-the-skin-of-red-blood-cell" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="how-elastic-is-the-skin-of-red-blood-cell">How elastic is the skin of red blood cell?</h3>
<p><span class="citation" data-cites="discherNewInsightsErythrocyte2000">(<a href="#ref-discherNewInsightsErythrocyte2000" role="doc-biblioref">Discher 2000</a>)</span> measured the elasticity of the red blood cell’s skin. To a good approximation, it is just a 2D spring, following Hooke’s law. He attached a tiny particle to the surface of a red blood cell, and measured its thermal motion.</p>
<p>As usual, in the microscopic world, inertia might as well not exist,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> so the oscillator’s energy is entirely elastic. Let it be of form <span class="math inline">\(\frac 12 k(x^2 + y^2)\)</span>. By equipartition of energy, we would have</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;But just to be sure, I checked the original numbers: It is a particle of diameter <span class="math inline">\(40 \ut{nm}\)</span>, and moving at about <span class="math inline">\(100 \ut{nm/s}\)</span>, so its kinetic energy is <span class="math inline">\(\sim 10^{-33}\ut{J} \sim 10^{-13} k_BT\)</span>.</p></div></div><p><span class="math display">\[\frac 12 k\braket{x^2} = \frac 12 \beta^{-1}\]</span></p>
<p>The data shows that <span class="math inline">\(\braket{x^2} = (35 \ut{nm})^2\)</span> at temperature <span class="math inline">\(T = 310 \ut{K}\)</span>, giving us an effective elastic constant of <span class="math inline">\(k \sim 0.004 \ut{pN/nm}\)</span>.</p>
<p>A red blood cell has diameter <span class="math inline">\(10^4 \ut{nm}\)</span>, so dragging a patch of its skin all across the surface would take only about <span class="math inline">\(40 \ut{pN}\)</span>. This is about the force output of 10 kinesins working in concert. Thus we can say that the skin of red blood cell is very slack.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/red_blood_cell_discher_2000.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">(A): A tiny particle attached to long molecules in the skin of the red blood cell. (B): Photo of a red blood cell with attached nanoparticle. (C): An example <span class="math inline">\((x,y)\)</span> trajectory. <span class="citation" data-cites="discherNewInsightsErythrocyte2000">(<a href="#ref-discherNewInsightsErythrocyte2000" role="doc-biblioref">Discher 2000, fig. 6</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="the-lac-operon" class="level3">
<h3 class="anchored" data-anchor-id="the-lac-operon">The <code>lac</code> operon</h3>
<p>This example is from <span class="citation" data-cites="garciaQuantitativeDissectionSimple2011">(<a href="#ref-garciaQuantitativeDissectionSimple2011" role="doc-biblioref">Garcia and Phillips 2011</a>)</span>.</p>
<p><em>E. coli</em> has two main sources of food: glucose and lactose. It prefers glucose, so when it is in an environment rich in glucose, it would start by metabolizing the glucose until it is mostly exhausted, then switch to metabolizing lactose.</p>
<p>To simplify, let’s consider a single gene, called the <code>lac</code>, which codes for a lactose-digesting enzyme (lactases). In front of <code>lac</code> there is a site where <code>repressor</code> protein can bind to, which stops <code>lac</code> transcription. The gene is <code>on</code> iff the <code>repressor</code> is not bound there.</p>
<p>The <code>repressor</code> protein can bind to either that specific site, of which there is only one on the entire <em>E. coli</em> genome, or any other site on the entire DNA sequence (non-specific binding). Write the binding energy on the specific site be <span class="math inline">\(E_S\)</span>, and the binding energy on the non-specific sites be <span class="math inline">\(E_{NS}\)</span>.</p>
<p>The system is in a delicate balance between energy, which favors specific binding, and entropy, which favors non-specific binding, with <span class="math inline">\(E_S &lt; E_{NS}\)</span> but not <span class="math inline">\(E_S \ll E_{NS}\)</span>. That is, specific binding is favored, but not <em>too</em> favored. This soft-favorism is what allows <code>lac</code> to be controlled. If it is not favored at all, then it would rarely bind. If it is too favored, then it would almost always bind.</p>
<p>Suppose there are <span class="math inline">\(R\)</span> <code>repressors</code> in the bacterium, then when none is binding specifically, the Gibbs free energy is</p>
<p><span class="math display">\[G_{\text{on}} = RE_{NS} - \beta^{-1} \ln \binom{N_{NS}}{R}\]</span></p>
<p>Notice how the entropy term <span class="math inline">\(\ln\binom{N_{NS}}{R}\)</span> assumes that the <code>repressor</code> proteins are indistinguishable, just like in gas theory. If one of them is binding specifically, the Gibbs free energy is</p>
<p><span class="math display">\[G_{\text{off}} = E_S + (R-1)E_{NS} - \beta^{-1} \ln \binom{N_{NS}}{R-1}\]</span></p>
<p>Thus, the probability of <code>on</code> vs <code>off</code> satisfies</p>
<p><span class="math display">\[\frac{p_{\text{on}}}{p_{\text{off}}} = e^{-\beta (G_{\text{on}} - G_{\text{off}})} = \frac{N_{NS} - R + 1}{R} e^{\beta (E_{NS} - E_{S})} \approx \frac{N_{NS}}{R} e^{\beta (E_{NS} - E_{S})}\]</span></p>
<p>giving us</p>
<p><span class="math display">\[p_{\text{on}} = \frac{N_{NS}}{N_{NS} + R e^{-\beta (E_{NS} - E_{S})}}\]</span></p>
</section>
<section id="sec-rna-hairpin" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-rna-hairpin">Unzipping RNA hairpins</h3>
<p>The RNA molecules are polymers made of 4 different “letters” that can pair up as A-U and C-G. A common shape for single-stranded RNA is the “hairpin”, pictured below.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rna_hairpin.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">RNA hairpin. Figure from <a href="https://commons.wikimedia.org/wiki/File:Stem-loop.svg">Wikimedia Commons</a></figcaption>
</figure>
</div>
<p>Since each base pair has about 100 atoms, an RNA hairpin is a large molecule with <span class="math inline">\(\sim 1000\)</span> atoms. This is large enough for statistical mechanics, but not large enough to smooth out thermal fluctuations, which ought to make it interesting. Now, what happens if we pull on the hairpin? This is the experiment done by <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001</a>)</span>. They attached a single RNA hairpin to two beads, and pulled the beads apart very slowly, using force-feedback to keep the force stable within <span class="math inline">\(0.1 \ut{pN}\)</span>. The RNA they used is <code>P5ab</code>, which has about 20 base pairs, each base of length about 5 Angstrom.</p>
<p>If we ignore fluctuation, and treat it by classical thermodynamics, we get <span class="math inline">\(dS = \beta dE - \beta F dx\)</span>, where <span class="math inline">\(F\)</span> is the pulling force, and <span class="math inline">\(x\)</span> is the distance between the two tweezers. Looking at this equation, we immediately see that the problem is best analyzed using the following free entropy</p>
<p><span class="math display">\[
h := \underbrace{S - \beta \braket{E}}_{\text{Helmholtz free entropy}} + \beta F \braket{x}
\]</span></p>
<p>which can be interpreted as “one-dimensional Gibbs free entropy”.</p>
<p>Let <span class="math inline">\(x_0\)</span> be the length of a single RNA unit, <span class="math inline">\(n\)</span> be the number of unzipped RNA base pairs, <span class="math inline">\(2N\)</span> be the total number of RNA bases, and <span class="math inline">\(-E_0\)</span> be the bonding energy of a base pair (assume that the two kinds of base pairs have the same bonding energy).</p>
<p>Because the state of the system is fully determined once we know what <span class="math inline">\(n\)</span> is, the entropy <span class="math inline">\(S\)</span> conditional on <span class="math inline">\(n\)</span> is zero, and so, plugging in the <a href="#thm-cond-free-ent" class="quarto-xref">Theorem&nbsp;9</a>, the probability of the hairpin in state <span class="math inline">\(n\)</span> is: <span class="math display">\[
\rho(n) \propto e^{\beta(2Fx_0 - E_0)n}, \quad 0 \leq 2n \leq 2N
\]</span></p>
<p>a truncated exponential distribution. When <span class="math inline">\(F = E_0/2x_0\)</span>, the bonding force and the pulling force are exactly balanced, and the hairpin is equally likely to be in any state. When the pulling force is larger, then <span class="math inline">\(n\)</span> concentrates on the <span class="math inline">\(n = N\)</span> end.</p>
<p>For reasons they did not explain, and I do not understand (sorry), in the <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001</a>)</span> experiment, the RNA hairpin had <em>no</em> intermediate state. It either fully unfolded, or fully folded. In that case, the hairpin became a two-level system, satisfying</p>
<p><span class="math display">\[
p_{\text{unfolded}} = \frac{e^{\beta( FL - \Delta G )}}{1 + e^{\beta( FL - \Delta G )}}
\]</span></p>
<p>where <span class="math inline">\(L = 18 \ut{nm}\)</span> is the length increase in unfolding, and <span class="math inline">\(\Delta G\)</span> is the increase in Helmholtz free energy during unfolding. This is exactly what they observed.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/liphardt_2001_1.jpeg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">(Left) As the pulling force increases, the RNA hairpin spends more and more time in the unfolded state. In all cases, the RNA is binary, either fully folded or fully unfolded, never in-between. (Right) The probability of being in an unfolded state increases as a logistic function, as predicted. <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>Another interesting finding is that when the pulling force increases slowly, the dissipated energy is small, but when the pulling force increases quickly, the wasted energy is large. In short, we have a form of speed limit second law: fast change requires more entropy production. We will discuss this in much more detail in the section on <a href="#sec-cft">Crooks fluctuation theorem</a>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/liphardt_2001_2.jpeg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption">Trajectories of folding and unfolding cycles. The area between the upper and lower curves is the dissipated energy during one cycle. Fast cycles dissipate more energy. <span class="citation" data-cites="liphardtReversibleUnfoldingSingle2001">(<a href="#ref-liphardtReversibleUnfoldingSingle2001" role="doc-biblioref">Liphardt et al. 2001, fig. 1</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="hungry-hungry-bacteria" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="hungry-hungry-bacteria">Hungry hungry bacteria</h3>
<p>This example came from <span class="citation" data-cites="bergRandomWalksBiology1993">(<a href="#ref-bergRandomWalksBiology1993" role="doc-biblioref">Howard C. Berg 1993, chap. 6</a>)</span>. See <span class="citation" data-cites="bergPhysicsChemoreception1977 bergMotileBehaviorBacteria2000">(<a href="#ref-bergPhysicsChemoreception1977" role="doc-biblioref">H. C. Berg and Purcell 1977</a>; <a href="#ref-bergMotileBehaviorBacteria2000" role="doc-biblioref">Howard C. Berg 2000</a>)</span> for a detailed look at how bacteria optimally forage for food molecules.</p>
<p>Consider a bacteria swimming in water. A typical one, such as <em>E. coli</em>, is roughly a rod with length <span class="math inline">\(a = 10^{-6}m\)</span> and swimming at <span class="math inline">\(v = 2\times 10^{-5} m/s\)</span>. That is, it swims 20 body-lengths a second.</p>
<p>Water has density <span class="math inline">\(\rho = 10^3 kg/m^3\)</span> and viscosity <span class="math inline">\(\eta = 10^{-3} kg/m\cdot s\)</span>. As is well-known, the <a href="https://en.wikipedia.org/wiki/Reynolds_number">Reynold number</a> of the system is</p>
<p><span class="math display">\[Re = \frac{\rho v a}{\eta} = 2 \times 10^{-5}\]</span></p>
<p>meaning that as soon as the bacteria stops powering itself, its motion ceases after coasting for a length of <span class="math inline">\(\approx 0.1 a \cdot Re = 2\times 10^{-12}m\)</span>, less than the length of an atom! Thus, the bacteria lives in an essentially inertia-less world.</p>
<p>Assuming that a bacteria is a sphere, then its swimming dissipates power at</p>
<p><span class="math display">\[P = Fv = 6\pi \eta a v^2 \approx 8\times 10^{-18}W \approx 2000 k_BT/s\]</span></p>
<p>where <span class="math inline">\(T = 300 K\)</span> is the standard temperature for biology. Since each glucose at complete metabolism produces <span class="math inline">\(686 kcal/mol \approx 1000 k_BT\)</span>, the bacteria just needs to eat 2 molecules of glucose per second to power its swimming.</p>
<p>By the FDR, the diffusion constant for the bacteria is</p>
<p><span class="math display">\[D = \frac{k_BT}{6\pi \eta a} = 2 \times 10^{-13} m^2/s\]</span></p>
<p>meaning that within 1 second, the bacteria diffuses a distance of <span class="math inline">\(\sim \sqrt{2 D \Delta t} \sim a\)</span>, about 1 body length. This shows that the swimming motion is 20 times faster than its thermal motion.</p>
<p>However, if we look at the trajectory of a bacterium under the microscope, we would notice its direction jumping around rapidly. This is due to the thermal motion of its orientation on the rotational group <span class="math inline">\(SO(3)\)</span>. Just like how each translational degree of freedom gets <span class="math inline">\(\frac 12 k_BT\)</span> of thermal energy, each rotational degree of freedom gets it as well, although, because the space of rotations is not a vector space, making this precise is tricky.</p>
<p>Still, once we make it precise, the FDR is still the same, gives us <span class="math inline">\(D_{\text{rotational}} = \frac{k_B T}{\gamma_{\text{rotational}}}\)</span>. Assuming the bacterium is still a sphere of radius <span class="math inline">\(a\)</span>, then <span class="math inline">\(\gamma_{\text{rotational}} = 8\pi \eta a^3\)</span>, giving <span class="math inline">\(D_{\text{rotational}} = 0.2 \ut{rad^2/s}\)</span>.</p>
<p>Now, a swimming bacterium does not care about rotation around its velocity axis (“longitude”), but does care about the other two rotational freedoms. The variation in “latitude” angle is the sum:</p>
<p><span class="math display">\[\braket{\theta^2} = 2 \times (2D_{\text{rotational}}\Delta t)\]</span></p>
<p>for small time <span class="math inline">\(\Delta t\)</span>. This is false for large <span class="math inline">\(\Delta t\)</span>, because otherwise we would get <span class="math inline">\(\braket{\theta^2} &gt; \pi^2\)</span>, which is absurd because <span class="math inline">\(\theta \in [0, \pi]\)</span>.</p>
<p>This implies that the bacteria veers by about <span class="math inline">\(\braket{\theta^2} \approx (50^\circ)^2\)</span> after a second of swimming. The actual observation gives <span class="math inline">\(\braket{\theta^2} \approx (30^\circ)^2\)</span>. In particular, the bacteria cannot keep a direction for more than about 3 seconds, as at that point its velocity vector would have diffused by about 90 degrees. As the bacteria has no “vestibular organ”, it essentially “forgets its heading” within a few seconds. Any sense of direction must come from the outside world, such as a chemical gradient.</p>
<p>The standard strategy for <em>E. coli</em> hunting is the “run and tumble” strategy. During the “run” phase, the bacterium keeps swimming forward for 1 – 10 seconds, curving gently due to rotational diffusion. At a random point it stops and tumbles for about 0.1 seconds, changing its direction randomly. Tumbling essentially samples a random direction for the next run, close but not quite uniformly on the sphere. It remembers the average chemical concentration during the first second and last second of each run. If the chemical concentration seem to increase, it would tumble less often (thus going on longer runs). Otherwise, it would tumble more often. The net effect is an extremely simple homing missile that diffuses up the chemical gradient.</p>
<p>There is no point for it to swim more than 10 seconds, because it would have totally lost its heading due to diffusion. There is no point in swimming less than 1 second, because it needs to average the chemical concentration over at least a second to overcome the statistical noise. And so, by simple physics, we have explained an <em>E. coli</em>’s hunting strategy.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/bacterial_swimming_berg_1993.png" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption class="margin-caption">A spherical bacterium is propelled by spinning flagella. The spherical bacterium swims along the <span class="math inline">\(x\)</span> -axis. Its rotational state diffuses along three possible degrees of freedom. Of those, the one around <span class="math inline">\(x\)</span>-axis is irrelevant, and the ones around <span class="math inline">\(y\)</span>- and <span class="math inline">\(z\)</span>-axes are relevant. <span class="citation" data-cites="bergRandomWalksBiology1993">(<a href="#ref-bergRandomWalksBiology1993" role="doc-biblioref">Howard C. Berg 1993, fig. 6.6</a>)</span></figcaption>
</figure>
</div>
<p><span class="citation" data-cites="bergChemotaxisEscherichiaColi1972">(<a href="#ref-bergChemotaxisEscherichiaColi1972" role="doc-biblioref">Howard C. Berg and Brown 1972</a>)</span> tracked the 3D trajectory of <em>E. coli</em> in a liquid that is <span class="math inline">\(3\times\)</span> more viscous than water. They found that the bacterium alternates between swimming and tumbling. During the swimming phase, the angular diffusion has a diffusion constant of <span class="math inline">\(0.06 \ut{rad^2/s}\)</span>, as theoretically predicted.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/bacterial_trajectory_berg_1972.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A single 3D trajectory of a single bacterium, projected into the <span class="math inline">\(xy, yz, xz\)</span> planes. Gently winding runs alternate with the tight tumbles. <span class="citation" data-cites="bergChemotaxisEscherichiaColi1972">(<a href="#ref-bergChemotaxisEscherichiaColi1972" role="doc-biblioref">Howard C. Berg and Brown 1972, fig. 1</a>)</span></figcaption>
</figure>
</div>
</section>
</section>
<section id="statistical-field-theory" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="statistical-field-theory">Statistical field theory</h2>
<section id="maximum-caliber" class="level3">
<h3 class="anchored" data-anchor-id="maximum-caliber">Maximum caliber</h3>
<p>Equilibrium statistical mechanics is typically called a “static” theory: It deals with the ensemble of states, but not with how the states change over time. Maximal caliber statistical mechanics handles trajectories in the most obvious way: Collect all paths into a “path space”, define a measure over path space, then study constrained entropy maximization over path space.</p>
<p><a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">Edward Jaynes</a>, who proposed the idea, called path space entropy “caliber”, so the name “maximum caliber” stuck, even though it is really just standard equilibrium statistical mechanics in path space. In other words, it is just standard statistical field theory, where the fields are of type <span class="math inline">\(\phi: \mathbb{R}\to \mathcal S\)</span>, where <span class="math inline">\(\mathbb{R}\)</span> stand for time, and <span class="math inline">\(\mathcal S\)</span> stand for state space of the system.</p>
<section id="markov-chains" class="level4">
<h4 class="anchored" data-anchor-id="markov-chains">Markov chains</h4>
<p>The simplest example is when time comes in discrete steps. In this case, we can reconstruct Markov chains as a particular kind of maximum caliber distribution.</p>
<p>Consider trajectories of type <span class="math inline">\(\{0, 1, 2, \dots, N\} \to \{1, 2, \dots, m\}\)</span>, and let <span class="math inline">\(s_t\)</span> is the state of timestep <span class="math inline">\(t\)</span>.</p>
<p>If we fix the singleton probability <span class="math inline">\(p_i\)</span> of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and <span class="math inline">\(p_{i\to j} = p_i p_j\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If we fix the singleton probability of each state, then the problem is a constrained maximization problem</p>
<p><span class="math display">\[
\begin{cases}
    \max S \\
    \frac 1N \sum_t 1[s_t = k] = p_k, \quad \forall k = 1, \dots, m
\end{cases}
\]</span></p>
<p>This is the same problem as <span class="math inline">\(N\)</span> balls in a certain constrained microcanonical ensemble. When <span class="math inline">\(N\)</span> is large enough, the discrete “macrostate” <span class="math inline">\(\frac 1N \sum_t 1[s_t = k]\)</span> becomes continuous, and we can use the Lagrange multiplier to obtain</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) = \frac 1Z e^{-\sum_{k=1}^m \lambda_k (\frac 1N \sum_{t=1}^N 1[s_t =k] - p_k)}\]</span></p>
<p>This factors over time <span class="math inline">\(t\)</span>, giving us</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) = \prod_{t=1}^N \rho(s_t)\]</span></p>
<p>with</p>
<p><span class="math display">\[\rho(s_t=k) \propto e^{-\sum_{k=1}^m\frac{\lambda_k}{N}(1[s_t =k] - p_k)} \propto e^{-\frac{\lambda_k}{N}}\]</span></p>
<p>The multiplier <span class="math inline">\(\lambda_k\)</span> can be found by the typical method of solving <span class="math inline">\(p_k = -\partial_{\lambda_k}\ln Z\)</span>, or we can be clever and notice that the constraint implies <span class="math inline">\(\rho(s_t=k) = p_k\)</span>.</p>
</div>
</div>
</div>
<p>Similarly, if we fix the transition probability <span class="math inline">\(p_{i \to j}\)</span> of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it’s still the Markov chain with the same transition probabilities.</p>
<p>And more generally, if we fix <span class="math inline">\(n\)</span>-th order transition probability <span class="math inline">\(p_{i_1, \dots, i_n \to j}\)</span>, then we obtain an <span class="math inline">\(n\)</span>-th order Markov chain model.</p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Similarly as above, the path-space distribution is</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) \propto \prod_{t=1}^{N-1} e^{-\sum_{k, k' \in 1:m} \frac{\lambda_{k, k'}}{N} 1[s_t = k, s_{t+1} = k']} \propto \prod_{t=1}^{N-1} p_{s_t \to s_{t+1}}\]</span></p>
<p>Because the distribution does not specify <span class="math inline">\(s_1\)</span>, it is uniformly distributed on <span class="math inline">\(s_1\)</span>. Otherwise, we can constrain <span class="math inline">\(s_1\)</span> with yet another set of Lagrange multipliers and obtain</p>
<p><span class="math display">\[\rho(s_1, \dots, s_N) \propto \rho(s_1) \times_{t=1}^{N-1} \rho(s_t, s_{t+1})\]</span></p>
<p>Similarly for higher orders.</p>
</div>
</div>
</div>
</section>
<section id="diffusion" class="level4">
<h4 class="anchored" data-anchor-id="diffusion">Diffusion</h4>
<p>In diffusion, we consider paths of type <span class="math inline">\(x: [0, T] \to \mathbb{R}^n\)</span> with <span class="math inline">\(x(0) = 0\)</span>. The path-space entropy</p>
<p><span class="math display">\[
S = - \int \rho(x) \ln \rho(x) D[x]
\]</span></p>
<p>where <span class="math inline">\(D[x]\)</span> means we integrate over path space. Here we see a proper path-integral, which is roughly speaking what happens when we integrate not over <span class="math inline">\(\mathbb{R}^{10}\)</span> or even <span class="math inline">\(\mathbb{R}^{10^{23}}\)</span>, but literally <span class="math inline">\(\mathbb{R}^\infty\)</span>.</p>
<p>As usual, path integrals cannot be done directly, but can be done by first dropping down to <span class="math inline">\(\mathbb{R}^N\)</span>, where <span class="math inline">\(N\)</span> is large but still finite, then hope that the result is still sensible at the <span class="math inline">\(N \to \infty\)</span> limit. If this disturbs you, sorry. It disturbs me too, but it is necessary for all but the most trivial calculations in field theory.</p>
<p>Discretize continuous-time path <span class="math inline">\(x : [0, T] \to \mathbb{R}^n\)</span> into discrete-time path <span class="math inline">\(x: \{0, 1, 2, \dots, N\} \to \mathbb{R}^n\)</span>. Because we can decompose</p>
<p><span class="math display">\[
\rho(x) = \rho(x_0) \rho(x_1 | x_0) \cdots \rho(x_N | x_{0:N-1})
\]</span></p>
<p>the path-space entropy decomposes sequentially:</p>
<p><span class="math display">\[
S = S[x_0] + \braket{S[x_1 | x_0]} + \braket{S[x_2 | x_{0:1}]} + \dots + \braket{S[x_N | x_{0:N-1}]}
\]</span></p>
<p>To prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as</p>
<p><span class="math display">\[(\braket{x_t|x_{0:t-1}}, \braket{x_t^2|x_{0:t-1}}) = (0, \sigma^2), \quad \forall t \in 0:N\]</span></p>
<p>by reasoning backwards from <span class="math inline">\(t = N\)</span> to <span class="math inline">\(t=0\)</span>, we find that the maximal entropy distribution is a white noise:</p>
<p><span class="math display">\[
\rho(x) = \prod_{t\in 0:N} \rho(x_t), \quad \rho(x_t) \propto e^{-\frac{\|x_t\|^2}{2\sigma^2}}
\]</span></p>
<p>If you have studied <a href="https://en.wikipedia.org/wiki/Dynamic_programming">dynamic programming</a> and cybernetics, this should look very similar to the argument by which you derived the <a href="https://en.wikipedia.org/wiki/Linear-quadratic_regulator">LQR</a>.</p>
<p>To keep the path from exploding into white noise, we instead impose the constraints on the sizes of the steps</p>
<p><span class="math display">\[(\braket{x_t - x_{t-1}|x_{0:t-1}}, \braket{\|x_t - x_{t-1}\|^2|x_{0:t-1}}) = (0, \sigma^2), \quad \forall t \in 1:N\]</span></p>
<p>Now, as in dynamic programming, we reason backwards from <span class="math inline">\(t = N\)</span> to <span class="math inline">\(t=0\)</span>, and we find that the maximal entropy distribution is the Brownian motion</p>
<p><span class="math display">\[\rho(x) \propto e^{-\frac{\sum_{t\in 1:N} \| x_t-x_{t-1}\|^2}{2\sigma^2}}\]</span></p>
<p>If we constrain the first <em>and</em> the second moments of each step, and also allow them to be affected by the previous step, as in</p>
<p><span class="math display">\[
\begin{cases}
    \braket{x_t - x_{t-1}|x_{0:t-1}} &amp;= \mu(t, x_{t-1}) \\
    \braket{(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}} &amp;= \Sigma(t, x_{t-1})
\end{cases}, \quad \forall t \in 1:N
\]</span></p>
<p>then, reasoning backwards as before, we would obtain the <a href="https://en.wikipedia.org/wiki/Langevin_equation">Langevin equation</a>.</p>
<p>Other results, such as the <a href="https://en.wikipedia.org/wiki/Green%E2%80%93Kubo_relations">Green–Kubo relations</a>, the <a href="https://en.wikipedia.org/wiki/Onsager_reciprocal_relations">Onsager reciprocal relations</a>, etc, can be similarly derived by imposing the right constraints in path space. <span class="citation" data-cites="hazoglouCommunicationMaximumCaliber2015">(<a href="#ref-hazoglouCommunicationMaximumCaliber2015" role="doc-biblioref">Hazoglou et al. 2015</a>)</span></p>
</section>
</section>
<section id="fluctuation-dissipation-relations" class="level3">
<h3 class="anchored" data-anchor-id="fluctuation-dissipation-relations">Fluctuation-dissipation relations</h3>
<p>Imagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time <span class="math inline">\(t\)</span> is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes <span class="math inline">\(\int_0^t W_s ds\)</span>. This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.</p>
<p>In order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that <span class="math inline">\(\frac 12 m \braket{v^2} = k_BT\)</span> exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the <em>family</em> of FDRs, because there have been so many of those.</p>
<p>Each FDR is a mathematical equation of form</p>
<p><span class="math display">\[
\text{something about fluctuation} = \text{something about dissipation}
\]</span></p>
<p>The prototypical FDR is the <a href="https://en.wikipedia.org/wiki/Einstein_relation_(kinetic_theory)">Einstein relation</a>, to be derived below:</p>
<p><span class="math display">\[
\underbrace{\beta D}_{\text{fluctuation}} = \underbrace{(\gamma\beta)^{-1}}_{\text{dissipation}}
\]</span></p>
<p>where the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.</p>
</section>
<section id="equality-before-the-law" class="level3">
<h3 class="anchored" data-anchor-id="equality-before-the-law">Equality before the law</h3>
<p>Why should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is “equality before the second law”.</p>
<p>If we pause and think about it, then isn’t it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I’m going to answer here. Perhaps calling it an “invisible hand” is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).</p>
<p>Now, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.</p>
<p>Assuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.</p>
<p>For example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.</p>
<p>Now, when physicists use the word “dissipation”, they mean the restoring effect of a system under <em>external</em> dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.</p>
<p>Thus, we see that each FDR manifests as an “equality before the law”:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text{fluctuation} \\
\underbrace{=}_{\text{random walk theory}} &amp;\text{dissipation (of internal fluctuations)} \\
\underbrace{=}_{\text{equality before the law}} &amp;\text{dissipation (of external fluctuations)}
\end{aligned}
\]</span></p>
</section>
<section id="one-dimensional-fdr" class="level3">
<h3 class="anchored" data-anchor-id="one-dimensional-fdr">One-dimensional FDR</h3>
<p>Consider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at <span class="math inline">\(x = 0, t = 0\)</span>, and at each time-step of <span class="math inline">\(\Delta t\)</span>, it moves by <span class="math inline">\(\Delta x\)</span> to the left or the right.</p>
<p>The fluid is at temperature <span class="math inline">\(\beta^{-1}\)</span>, and we pull on the particle at constant force <span class="math inline">\(F\)</span>. We expect that <span class="math inline">\(F = \gamma \braket{v}\)</span>, where <span class="math inline">\(v\)</span> is the ensemble-average velocity of the particle, and <span class="math inline">\(\gamma\)</span> is the viscosity constant.</p>
<p>Now, we let the particle move for a time <span class="math inline">\(t = N\Delta t\)</span>, where <span class="math inline">\(N\)</span> is a large number. The particle would have arrived at some point <span class="math inline">\(x\)</span>, which is a random variable. The particle’s time-averaged velocity is <span class="math inline">\(v = x/t\)</span>.</p>
<p>The number of possible paths that connect <span class="math inline">\((0, 0)\)</span> with <span class="math inline">\((t, x)\)</span> is <span class="math inline">\(\binom{N}{\frac N2 - \frac{x}{2\Delta x}}\)</span>, therefore, the path-space entropy is</p>
<p><span class="math display">\[S_{\text{path}} = \ln \binom{N}{\frac N2 - \frac{x}{2\Delta x}} \approx N \left(\ln 2 - \left(\frac{x}{N\Delta x}\right)^2\right)\]</span></p>
<p>where the approximation is either by Stirling’s approximation, or the binary entropy function.</p>
<p>Because the external force performs work <span class="math inline">\(Fx\)</span>, which is dissipated into the sticky liquid at temperature <span class="math inline">\(\beta\)</span>, we also have</p>
<p><span class="math display">\[S_{\text{work}} = \beta F x\]</span></p>
<p>Because <span class="math inline">\(N\)</span> is large, <span class="math inline">\(\braket{x}\)</span> should be highly concentrated around the point of maximal entropy. That is, we should have</p>
<p><span class="math display">\[
\braket{x} \approx \mathop{\mathrm{argmax}}_x (S_{\text{path}} + S_{\text{work}})
\]</span></p>
<p>The equation on the right is quadratic in <span class="math inline">\(\braket{x}\)</span>, and achieves maximum at <span class="math inline">\(2\braket{x} = \beta FN(\Delta x)^2\)</span>, which simplifies to the Einstein relation</p>
<p><span class="math display">\[\beta D \gamma = 1\]</span></p>
<p>where <span class="math inline">\(D = \frac{\Delta x^2}{2\Delta t}\)</span> is the diffusion coefficient.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can calculate not just the mean <span class="math inline">\(\braket{x}\)</span>, but also its variance <span class="math inline">\(\braket{x^2}\)</span> if we expand <span class="math inline">\(S_{\text{path}} + S_{\text{work}}\)</span> to second order around its maximum, then apply <a href="#thm-cond-ent" class="quarto-xref">Theorem&nbsp;8</a>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="rubber band">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
rubber band
</div>
</div>
<div class="callout-body-container callout-body">
<p>We notice how this is identical to the <a href="#sec-rubber-band">rubber band system</a>. We can think of the shape of a rubber band as a timeless image of the trajectory of in time. Dissipation is identical to the entropic force pulling on the rubber band. Fluctuation is the variance in rubber band length. The dependence of the elastic constant on temperature, <span class="math inline">\(F/L = \frac{T}{Nd^2}\)</span>, is then the FDR for the rubber band:</p>
<p><span class="math display">\[
\beta \left(\frac{d^2}{1/N}\right) (F/L) = 1
\]</span></p>
<p>This is the simplest example of the general idea of <a href="https://en.wikipedia.org/wiki/Wick_rotation">Wick rotation</a>, where a problem in <span class="math inline">\(n\)</span>-dimensional space and <span class="math inline">\(1\)</span>-dimensional time is converted to a problem in <span class="math inline">\((n+1)\)</span>-dimensional space.</p>
</div>
</div>
</section>
<section id="sound-waves" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sound-waves">Sound waves</h3>
<p>Recall that the Helmholtz free entropy of ideal gas is</p>
<p><span class="math display">\[
f = N \left(\ln \frac{V}{N} + \frac 32 \ln \frac{2\pi m}{\beta} + \frac{\ln N}{2N}\right) \underbrace{\approx}_{\text{large $N$}} N \left(\ln \frac{V}{N} + \frac 32 \ln \frac{2\pi m}{\beta}\right)
\]</span></p>
<p>The free entropy density, then, has an elegant formula:</p>
<p><span class="math display">\[
f/V = -\rho \ln \rho + \rho \frac 32 \ln \frac{2\pi m}{\beta}
\]</span></p>
<p>where <span class="math inline">\(\rho = N/V\)</span> is the particle density. Notice that on both the left side and the right side, the individual particles have completely disappeared, leaving behind only a density field <span class="math inline">\(\rho\)</span>, a temperature field <span class="math inline">\(\beta\)</span>, and some constants. That is, we have obtained the <a href="https://en.wikipedia.org/wiki/Continuum_limit">continuum limit</a> of statistical mechanics – a field. Statistical mechanics still works in the limit, where it is called statistical field theory.</p>
<p>We wish to apply the <a href="#thm-cond-free-ent" class="quarto-xref">Theorem&nbsp;9</a> once again. To apply it, we need to construct a field-theoretic observable <span class="math inline">\(Y\)</span>, such that the maximal Helmholtz free entropy conditional on <span class="math inline">\(Y = y\)</span> is <span class="math inline">\(f^*_{X|y}\)</span> has a solution. Then, we can write <span class="math inline">\(Pr_Y(y) \propto e^{f^*_{X|y}}\)</span> – that is, the probability distribution of the <span class="math inline">\(Y\)</span> field being in state <span class="math inline">\(Y=y\)</span> is itself a Boltzmann distribution.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/density_fluctuation_gas.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Density fluctuation of ideal gas. On the right is a certain coarse-grained macrostate <span class="math inline">\(Y=y\)</span>, and on the left is a microstate <span class="math inline">\(x\)</span> that is compatible with the coarse-grained macrostate. Modified from <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 6.8</a>)</span> and <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 6.9</a>)</span></figcaption>
</figure>
</div>
<p>Looking at the picture, it is clear to us that the field-theoretic observable should be the particle density field <span class="math inline">\(\rho\)</span>. And what is <span class="math inline">\(f^*_{X|\rho}\)</span>? It is the total Helmholtz free entropy of the entire tank of gas, given that its particle density is distributed like <span class="math inline">\(\rho\)</span>. As we found previously, it is the integral of the free entropy density:</p>
<p><span class="math display">\[
f^*_{X|\rho} = \int_{\text{Box}} \left(-\rho(x) \ln \rho(x) + \rho(x) \frac 32 \ln \frac{2\pi m}{\beta}\right) dx
\]</span></p>
<p>It is intuitively clear that the most likely density field <span class="math inline">\(\rho^*\)</span> is the uniform density field <span class="math inline">\(\rho^*(x) = \rho_0\)</span>. We can prove this by maximizing <span class="math inline">\(f^*_{X|\rho}\)</span>. First, take its functional derivative:</p>
<p><span class="math display">\[
\begin{cases}
\partial_{\rho} f^*_{X|\rho} &amp;= - \ln \rho - 1 + \frac 32 \ln \frac{2\pi m}{\beta} \\
\partial_{\rho}^2 f^*_{X|\rho} &amp;= -\rho^{-1}
\end{cases}
\]</span></p>
<p>We cannot naively solve <span class="math inline">\(\partial_{\rho} f^*_{X|\rho} = 0\)</span>, because we have the extra constraint that particles cannot be created or destroyed. Let <span class="math inline">\(\rho = \rho_0 + \delta \rho\)</span>, with <span class="math inline">\(\int\delta\rho = 0\)</span>. Plugging it in, we obtain</p>
<p><span class="math display">\[
f^*_{X|\rho} = f^*_{X|\rho_0} - \frac{1}{2\rho_0} \int (\delta \rho)^2dx
\]</span></p>
<p>which tells us that <span class="math inline">\(Pr(\rho_0 + \delta \rho) \propto e^{- \frac{1}{2\rho_0} \int (\delta \rho)^2dx}\)</span>, which is precisely the Boltzmann distribution of a physical system whose energy is</p>
<p><span class="math display">\[
\delta E := \frac{1}{2\rho_0 \beta} \int \delta \rho(x)^2dx \underbrace{=}_{\text{$P = \rho_0/\beta$ by ideal gas law}} \frac 12 P\int \left(\frac{\delta \rho(x)}{\rho_0}\right)^2 dx
\]</span></p>
<p>Now, if you have studied continuum mechanics, this should look very familiar: it is the elastic energy of air. At this point, we are quite close to a statistical field theory of sound waves, but to actually derive sound waves from this, we need to not only know the free entropy <span class="math inline">\(f^*_{X|\rho_0 + \delta \rho}\)</span>, but also know how quickly a perturbation <span class="math inline">\(\delta \rho\)</span> is pulled back to <span class="math inline">\(\delta \rho = 0\)</span>.</p>
<p>Since <span class="math inline">\(\delta E = \frac 12 P\int \left(\frac{\delta \rho(x)}{\rho_0}\right)^2 dx\)</span> looks like the potential energy of a spring <span class="math inline">\(\frac 12 kx^2\)</span>, it is reasonable to guess that we need to add a “kinetic energy” term, giving</p>
<p><span class="math display">\[
\delta E = \frac 12 P\int \left(\frac{\delta \rho(x)}{\rho_0}\right)^2 dx + \frac 12 \int \mu \delta \dot\rho(x)^2 dx
\]</span></p>
<p>At this point, it looks just like a Hamiltonian, and it is more productive to switch to Hamiltonian mechanics. This is a taste of statistical field theory of idea gas and how it reduces to classical field theory of density waves (sound waves). I might write a proper essay about statistical field theory someday.</p>
</section>
</section>
<section id="metastability" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="metastability">Metastability</h2>
<p>Often in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.</p>
<p>The point is that “getting over a potential barrier” is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on <em>equilibrium</em> statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.</p>
<p>We model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than <span class="math inline">\(\Delta E\)</span> of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time <span class="math inline">\(\tau\)</span> (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is <span class="math inline">\(N \tau\)</span>, where <span class="math inline">\(N\)</span> is the number of times we look at the system.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/metastable_potential_well.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Getting over a potential well.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="relaxation time">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
relaxation time
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we look at the oscillator, its state is totally fixed at some <span class="math inline">\((q, p)\)</span>. If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at <span class="math inline">\((q, p)\)</span>. How long must we wait before the oscillator state has reasonably “relaxed” from a pointy distribution to the serene bell-shape of the Boltzmann distribution?</p>
<p>That is beyond the scope. Suffice to say that we should wait a while, not too short, after which the system is close enough to equilibrium. But we cannot wait for too long either, because in the long run, the particle would have escaped the potential well. So there is a time-scale, neither too long nor too short, which we call the relaxation time <span class="math inline">\(\tau\)</span>.</p>
<p>Formalizing all these things requires stochastic calculus, which I might write about later.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Goldilocks and the three chefs">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Goldilocks and the three chefs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Goldilocks, with a rumbling stomach, stumbles upon the house of the three chefs. Each chef is holding a pan in one hand and a bottle of Brownian batter in the other. “Excuse me, might I have some pancakes?”</p>
<p>“Of course!” exclaims the first chef, whose pan holds a small, quivering mound. “It’s just been poured, still brimming with energy!”</p>
<p>Goldilocks frowns. “It hasn’t even reached the edges! This batter needs more time to settle.”</p>
<p>The second chef, whose pan appears curiously empty, sighs. “And what of mine? I’ve given it all the time in the world, allowed it to explore every corner of its potential.” He gestures at the pan, now bereft of batter, a few crumbs clinging to the edges.</p>
<p>“But… there’s nothing left!” exclaims Goldilocks, aghast. “Given too much time, the batter has vanished completely!”</p>
<p>The third chef, whose batter has flowed smoothly to coat the pan, smiles warmly. “Perhaps this will be more to your liking. Given much time, but not too much, it’s achieved perfect consistency.”</p>
<p>“Ah, this is perfect!” exclaims Goldilocks, taking a bite of the fluffy pancake. “It’s had enough time to spread evenly, but not so long that it’s dried out.”</p>
<p>“Indeed, timing is everything. Too brief, and the batter remains confined to its starting point, unable to fulfill its pan-sized destiny. But too long, it would have escaped the edge of the pan to reach its true destiny – on the ground. Not too short, not too long, just right… that is meta-stability.”</p>
<p>— Guest entry written by <code>Gemini-1.5-Pro</code>.</p>
<p><img src="figure/Goldilocks_pancake.jpg" class="img-fluid"></p>
</div>
</div>
</div>
</div>
</div>
<div id="thm-arrhenius" class="theorem">
<p><span class="theorem-title"><strong>Theorem 10 (Arrhenius equation)</strong></span> The escape time for a simple harmonic oscillator in contact with an energy bath scales as <span class="math inline">\(e^{\beta \Delta E}\)</span>, where <span class="math inline">\(\Delta E\)</span> is the energy barrier, and <span class="math inline">\(\beta\)</span> is the temperature of the bath.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let the oscillator have <span class="math inline">\(n\)</span> dimensions, then its energy function is <span class="math inline">\(H = \sum_{i=1}^n \frac{p_i^2}{2m_i} + \frac{k_i q_i^2}{2}\)</span>, where <span class="math inline">\(q_i, p_i\)</span> are the generalized position and momentum, and <span class="math inline">\(m_i, k_i\)</span> are the effective masses and elastic constants. The Boltzmann distribution is <span class="math inline">\(\rho(q, p) = Z^{-1} e^{-\beta H}\)</span>, and the probability that it has enough energy to overcome the barrier is</p>
<p><span class="math display">\[
P = \frac{\int_{H \geq \Delta E} \rho(q, p)dqdp}{\int_{H \geq 0} \rho(q, p)dqdp}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure" aria-labelledby="-caption">
<p><img src="figure/Arrhenius_oscillator_phase_space.jpg" class="img-fluid figure-img"></p>

</figure>
</div>
<p>Notice that the proportionality constant <span class="math inline">\(Z\)</span> is removed. After a change of variables by <span class="math inline">\(x_i = \frac{p_i}{\sqrt{2m_i}}, y_i = \sqrt{\frac{k_i}{2}} q_i\)</span>, we get</p>
<p><span class="math display">\[
P = \frac{\int_{\sum_i x_i^2 + y_i^2 \geq \Delta E} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}{\int_{\sum_i x_i^2 + y_i^2 \geq 0} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}
\]</span></p>
<p>Integrating in spherical coordinates, and simplifying, we get <span class="math inline">\(P = e^{-\beta \Delta E}\)</span>. Thus, the expected time until escape is</p>
<p><span class="math display">\[
T = \braket{N}\tau = \frac{1}{P}\tau = \tau e^{\beta \Delta E}
\]</span></p>
<p>the Arrhenius equation.</p>
<p>The same calculation, for a system held in contact with an energy-and-volume bath, gives us <span class="math inline">\(T = \tau e^{\beta \Delta G}\)</span>, where <span class="math inline">\(\Delta G\)</span> is the Gibbs free energy barrier.</p>
</div>
</div>
</div>
<div id="-caption" class="margin-figure-caption column-margin callout-32-contents callout-collapse collapse callout-margin-content">Phase space diagram of a particle in a potential well. As long as it remains within the potential well, it behaves like a simple harmonic oscillator constantly impacted by thermal noise, but if it ever falls within the shaded region, it would escape the potential well. If we observe it within the well, we would turn away and wait for relaxation time  until its phase space distribution thermalizes, then we make another observation. This continues until we observe its escape.</div><p>The argument given above for the Arrhenius equation is quite generic. It only assumes there is a system that is stuck in some kind of potential well, and is held at a constant temperature somehow. There is no requirement for the system to be an actual particle in an actual well. The “particle” can very well be the 100-dimensional configuration of a protein during folding, or even the simultaneous position of <span class="math inline">\(10^{23}\)</span> helium atoms in a helium gas. Indeed, the Arrhenius equation pops up everywhere as the time until a system escapes an energetic trap.</p>
<section id="common-applications" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="common-applications">Common applications</h3>
<p>In practice, the Arrhenius equation is used in the form of <span class="math inline">\(\ln r = - a T^{-1} + \Const\)</span>, where <span class="math inline">\(r = 1/T\)</span> is the “reaction rate”, and <span class="math inline">\(a = \frac{\Delta E}{k_B}\)</span> is the slope of the <span class="math inline">\(T^{-1} - \ln f\)</span> plot (“the Arrhenius plot”). Such plots are extensively used in chemistry, but they are not exclusively found in chemistry.</p>
<p>In a glass of water held at constant temperature <span class="math inline">\(300 \ut{K}\)</span>, each water molecule might occasionally reach enough energy to escape into open air. This is evaporation. By this argument, the rate of evaporation follows the Arrhenius law, and indeed it does.</p>
<p>When a pure glass of water is cooled below freezing, it does not actually freeze immediately, because there is still an energetic barrier. Ice is a crystal, but liquid is not a crystal. The barrier between them, where crystal transitions to non-crystal, is penalized. This energetic penalty is what we call “surface tension”. From thermodynamical arguments, at temperature <span class="math inline">\(\Delta T\)</span> belowe freezing point, the Gibbs free energy required to form a sphere of ice with radius <span class="math inline">\(r\)</span> is</p>
<p><span class="math display">\[
G(r) = (4\pi \sigma_{\text{ice-water}})\cdot r^2  - \left(\frac 43 \pi \rho_{\text{ice}} \frac{L\Delta T}{T_{\text{freezing}}}\right) \cdot r^3
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the latent heat of ice, and <span class="math inline">\(\sigma_{\text{ice-water}}\)</span> is the surface tension. This creates an energy barrier of height <span class="math inline">\(\sim \Delta T^{-2}\)</span>, so the metastable phase of the system can survive for <span class="math inline">\(\sim e^{C\frac{1}{T(\Delta T)^2}}\)</span>. <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, chap. 11.3</a>)</span> One must wait for a long while before random chance creates a large enough “seed”, which then grows without bound. This picture we just described is called “bubble nucleation”, as described in <a href="https://en.wikipedia.org/wiki/Classical_nucleation_theory">classical nucleation theory</a>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/nucleation_theory.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption">The free energy barrier to nucleation.</figcaption>
</figure>
</div>
<p>Similarly, slightly <a href="https://en.wikipedia.org/wiki/Supersaturation">supersaturated</a> water vapor can stay metastable for a long time before it rains spontaneously. In real-life clouds, however, there are always enough dust for droplets to form around them, thus taking a shortcut through the energetic barrier.</p>
<p>A similar argument applies for <a href="https://en.wikipedia.org/wiki/Disappearing_polymorphs">disappearing polymorphs</a>. Some chemicals, such as Ritonavir, have two forms (“morphs”) of crystals. Call them A and B. Morph A has higher Gibbs free energy density than morph B, but lower energy barrier of formation. Thus, when a chemist attempts to crystallize it from a solution, by Arrhenius law, morph A would appear much faster than morph B. As soon as one droplet of morph A forms, that would serve as centers around which more of morph A forms. Eventually, however, a small crystal of morph A would spontaneously overcome the energetic barrier, shift all its atomic lattices, and become morph B. That would “infect” all subsequent attempts to crystallize morph A. <span class="citation" data-cites="wardPerilsPolymorphismSize2017">(<a href="#ref-wardPerilsPolymorphismSize2017" role="doc-biblioref">Ward 2017</a>)</span></p>
<p>The matter of the disappearing polymorph has serious legal effects. For example, Ritonavir is a medicine for AIDS. It was marketed in 1996 as Form I crystal, which was thought to be the only crystal form for Ritonavir. In 1998, Form II crystals spontaneously appeared. Any lab in contact with Form II could only produce more of Form II. Eventually the company rediscovered how to create Form I despite infection by Form II, but the delay cost the company 250 million USD. <span class="citation" data-cites="bucarDisappearingPolymorphsRevisited2015">(<a href="#ref-bucarDisappearingPolymorphsRevisited2015" role="doc-biblioref">Bučar, Lancaster, and Bernstein 2015</a>)</span></p>
<blockquote class="blockquote">
<p>In a matter of weeks—maybe five or six weeks, every place the product was became contaminated with Form II crystals.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/ritonavir.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Ritonavir and its two morphs. <span class="citation" data-cites="bucarDisappearingPolymorphsRevisited2015">(<a href="#ref-bucarDisappearingPolymorphsRevisited2015" role="doc-biblioref">Bučar, Lancaster, and Bernstein 2015, fig. 2</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="uncommon-applications" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="uncommon-applications">Uncommon applications</h3>
<p>In the simplest model for biochemical process, we just have one chemical reaction following another, until it is complete. If there is a single biochemical step that is much slower than the other steps, then the waiting time for that step dominates, and the total reaction should depend on the temperature by an Arrhenius law. This might explain the observed Arrhenius-law-like dependence on temperature in biological phenomena like tree cricket chirping, alpha brain wave frequency, etc.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/arrhenius_plots.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">(1) Tree cricket chirping frequency. (2) Firefly flashing frequency. (3) Terrapin heartbeat frequency. (4) Human silent counting rate <span class="citation" data-cites="hoaglandPhysiologicalControlJudgments1933">(<a href="#ref-hoaglandPhysiologicalControlJudgments1933" role="doc-biblioref">Hoagland 1933</a>)</span>. These figures are reproduced in <span class="citation" data-cites="laidlerUnconventionalApplicationsArrhenius1972">(<a href="#ref-laidlerUnconventionalApplicationsArrhenius1972" role="doc-biblioref">Laidler 1972</a>)</span>. (5) evaporation rate of octane <span class="citation" data-cites="brennanEvaporationLiquidsKinetic1974">(<a href="#ref-brennanEvaporationLiquidsKinetic1974" role="doc-biblioref">Brennan, Shapiro, and Watton 1974</a>)</span>. (6) Alpha frequency of brains of normal, <a href="https://en.wikipedia.org/wiki/General_paresis_of_the_insane">syphilitic paretic</a>, and <em>very</em> paretic humans <span class="citation" data-cites="hoaglandPacemakersHumanBrain1936">(<a href="#ref-hoaglandPacemakersHumanBrain1936" role="doc-biblioref">Hoagland 1936</a>)</span>.</figcaption>
</figure>
</div>
<p>While writing this, I suddenly recognized <span class="citation" data-cites="hoaglandPhysiologicalControlJudgments1933">(<a href="#ref-hoaglandPhysiologicalControlJudgments1933" role="doc-biblioref">Hoagland 1933</a>)</span> from when I read Feynman’s book all those years ago!</p>
<blockquote class="blockquote">
<p>When I was in graduate school at Princeton [1939–1942] a kind of dumb psychology paper came out that stirred up a lot of discussion. The author had decided that the thing controlling the “time sense” in the brain is a chemical reaction involving iron… his wife had a chronic fever which went up and down a lot. Somehow he got the idea to test her sense of time. He had her count seconds to herself (without looking at a clock), and checked how long it took her to count up to 60. He had her counting – the poor woman – all during the day: when her fever went up, he found she counted quicker; when her fever went down, she counted slower… he tried to find a chemical reaction whose rates varied with temperature in the same amounts as his wife’s counting did. He found that iron reactions fit the pattern best… it all seemed like a lot of baloney to me – there were so many things that could go wrong in his long chain of reasoning.</p>
<p><span class="citation" data-cites="feynmanWhatYouCare1989">(<a href="#ref-feynmanWhatYouCare1989" role="doc-biblioref">Richard P. Feynman 1989, 55</a>)</span></p>
</blockquote>
<p>And yes, that is the one!</p>
<blockquote class="blockquote">
<p>My wife, having fallen ill with influenza, was used in the first of several experiments. Without, in any way, hinting to her the nature of the experiment, she was asked to count 60 seconds to herself at what she believed to be a rate of 1 per second. Simultaneously the actual duration of the count was observed with a stop-watch.</p>
<p><span class="citation" data-cites="hoaglandPhysiologicalControlJudgments1933">(<a href="#ref-hoaglandPhysiologicalControlJudgments1933" role="doc-biblioref">Hoagland 1933</a>)</span></p>
</blockquote>
</section>
</section>
<section id="sec-cft" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-cft">Crooks fluctuation theorem</h2>
<p>The field of genuinely non-equilibrium thermodynamics is yet unsettled, so this section is more provisional than the previous ones. Here, we are still considering classical mechanics, with time-reversible Hamiltonian dynamics.</p>
<p>The key to modern non-equilibrium thermodynamics is the Crooks fluctuation theorem (CFT), which surprisingly was proven only in 1999. As fundamental physics go, this is really new!</p>
<section id="in-a-closed-system-microcanonical" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="in-a-closed-system-microcanonical">In a closed system (microcanonical)</h3>
<p>Consider a system held under variable constraints <span class="math inline">\(x\)</span>. For example, we have a piston whose head is held at a given length. Alternatively, its head is held at a given force. The point is simply to pick one type of constraint, then stick with it.</p>
<p>Let the system start in a microcanonical ensemble, at energy <span class="math inline">\(E_1\)</span>, then we change the constraints <span class="math inline">\(x(t)\)</span>, quickly or slowly, over a time interval <span class="math inline">\(t\in [0, \tau]\)</span>. The system’s microstate follows some trajectory <span class="math inline">\(y(t)\)</span>, arriving <span class="math inline">\(y(\tau)\)</span>, which falls within some new microcanonical ensemble with energy <span class="math inline">\(E_2\)</span>.</p>
<p>Putting a prime means time-reversal. For example, <span class="math inline">\(x', y'\)</span> are <span class="math inline">\(x, y\)</span>, but time-reversed.</p>
<p>Whereas <span class="math inline">\(E_1\)</span> is known, <span class="math inline">\(E_2\)</span> is randomly sampled, as it is determined by <span class="math inline">\(E_1\)</span> (known), <span class="math inline">\(x(t)\)</span> (known), <span class="math inline">\(y(0)\)</span> (randomly sampled from the microcanonical ensemble).</p>
<p>During the forward process, if the system undergoes microstate trajectory <span class="math inline">\(y(t)\)</span>, then we have to expend work <span class="math inline">\(W[x(t), y(t)] = E_2 - E_1\)</span>.</p>
<p>Let <span class="math inline">\(S_1^*\)</span> be the maximal entropy of the system when held under the constraints of <span class="math inline">\(x(0)\)</span>, and when the system has energy <span class="math inline">\(E_1\)</span>. Similarly, let <span class="math inline">\(S_2^*\)</span> be the maximal entropy of the system when held under the constraints of <span class="math inline">\(x(\tau)\)</span>, and when the system has energy <span class="math inline">\(E_2\)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math inline">\(S_1 = S_1^*\)</span>, since the system starts in thermal equilibrium. However, by Liouville’s theorem, entropy is <em>conserved</em>! So we actually have <span class="math inline">\(S_2 = S_1 \neq S_2^*\)</span>, because the system does not end in thermal equilibrium.</p>
<p>The key is this: if the system starts at a microcanonical ensemble, then it does <em>not</em> in general end up at another microcanonical ensemble, because as we exert external forcing upon the system, the energy shell deforms, no longer the shell of <em>any</em> microcanonical ensemble.</p>
</div>
</div>
<p>For example, if we have a piston of gas made of only a few gas molecules, then the constraint is the volume <span class="math inline">\(V\)</span>, and we want to study the probability of expending work <span class="math inline">\(W\)</span> if we give the piston head a push. The push can be slow or fast – arbitrarily far from equilibrium. CFT applies no matter how we push the piston head.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Crooks_sethna_2021_4_10.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">During the forward time-evolution of an energy shell <span class="math inline">\(E_1\)</span>, the shell is no longer a microcanonical ensemble. The particular trajectory sampled lands upon an energy shell <span class="math inline">\(E_2\)</span>. <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 4.10</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Crooks_sethna_2021_4_11.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The same situation but in the opposite direction. <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 4.11</a>)</span></figcaption>
</figure>
</div>
<p>Because the CFT involves the density of not states, but <em>state trajectories</em>, we need to set up the formalism for path integrals.</p>
<p>Let <span class="math inline">\(\delta E_1, \delta E_2\)</span> be infinitesimals, and let <span class="math inline">\(E_1, E_2\)</span> be real numbers.</p>
<p>Given an infinitesimal small bundle of microtrajectories <span class="math inline">\(y\)</span>, we can measure its path-space volume as <span class="math inline">\(D[y]\)</span>. Suppose they start on the energy shell <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span>, then they would end up <em>somewhere</em>. If we’re lucky, they would end up on the energy shell <span class="math inline">\([E_2 + \delta E_2]\)</span>.</p>
<p>Suppose the system starts in the microcanonical ensemble on the energy shell <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span>, and we perform the constraint-variation <span class="math inline">\(x\)</span>, then there is a certain probability <span class="math inline">\(\delta P\)</span> that we would sample a trajectory from the small bundle. That small probability is</p>
<p><span class="math display">\[\rho(y | x) D[y]\]</span></p>
<p>where <span class="math inline">\(\rho(y | x)\)</span> is a probability density over path-space. In particular, <span class="math inline">\(\rho(y | x) = 0\)</span> identically, unless <span class="math inline">\(y(0)\)</span> is on the energy shell <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span>.</p>
<p>Running the argument backwards, we can define <span class="math inline">\(\rho'(y' | x')\)</span>, another probability density over paths. This one satisfies <span class="math inline">\(\rho'(y'| x') = 0\)</span> unless <span class="math inline">\(y'(0)\)</span> is on the energy shell <span class="math inline">\([E_2, E_2 + \delta E_2]\)</span>.</p>
<div id="thm-cft-micro" class="theorem">
<p><span class="theorem-title"><strong>Theorem 11 (microcanonical CFT)</strong></span> For any trajectory <span class="math inline">\(y\)</span> such that it starts on the <span class="math inline">\([E_1, E_1 + \delta E_1]\)</span> energy shell, and ends on the <span class="math inline">\([E_2, E_2 + \delta E_2]\)</span> energy shell,</p>
<p><span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} = e^{\Delta S}\]</span></p>
<p>where <span class="math inline">\(\Delta S= \ln\Omega_2 - \ln\Omega_1\)</span>, <span class="math inline">\(\Omega_1\)</span> is the phase space volume of the first energy shell, and <span class="math inline">\(\Omega_2\)</span> the second.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(y\)</span> does not start on the first energy shell, or does not end on the second energy shell, then either the nominator or the denominator is zero, and so the equation fails to hold.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-35-contents" aria-controls="callout-35" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-35" class="callout-35-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In the forward process, the probability of going along that trajectory is <span class="math display">\[\rho(x|y) D[x] = \frac{\delta V}{\Omega_1}\]</span></p>
<p>where <span class="math inline">\(\delta V\)</span> is the phase-space volume of the shaded set.</p>
<p>In the backward process, the probability of reversing that trajectory is <span class="math display">\[\rho'(x'|y') D[x']= \frac{\delta V'}{\Omega_2}\]</span></p>
<p><span class="math inline">\(\delta V' = \delta V\)</span> by Liouville’s theorem, and <span class="math inline">\(D[x] = D[x']\)</span> because <span class="math inline">\(x'\)</span> is just <span class="math inline">\(x\)</span> time-reversed.</p>
</div>
</div>
</div>
</section>
<section id="in-an-energy-bath-canonical" class="level3">
<h3 class="anchored" data-anchor-id="in-an-energy-bath-canonical">In an energy bath (canonical)</h3>
<p>Now, suppose we take the same piston of gas, and put it in energy-contact with an energy bath, then at thermal equilibrium, the piston of gas would have the Boltzmann distribution <span class="math inline">\(\propto e^{-\beta E}\)</span>. We can then give the piston head a push, which would cause it to undergo some kind of time-evolution.</p>
<div id="thm-cft" class="theorem">
<p><span class="theorem-title"><strong>Theorem 12 (Crooks fluctuation theorem)</strong></span> <span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] - \Delta F^*)}\]</span></p>
<p>where <span class="math inline">\(S[x, y]\)</span> is the entropy produced during the forward process, after the system has equilibrated, and <span class="math inline">\(\Delta F^* = F^*_2 - F^*_2\)</span> is the increase in <em>equilibrium</em> Helmholtz free energy of the system.</p>
<p>Equivalently, <span class="math inline">\(D_{KL}(\mathcal P_{\text{forward}} \| \mathcal P_{\text{backward}}) = \beta (\braket{W}_{\text{forward}} - \Delta F^*)\)</span>, where <span class="math inline">\(\mathcal P_{\text{forward}}\)</span> is the probability distribution over microtrajectories in the forward process, and similarly for <span class="math inline">\(\mathcal P_{\text{backward}}\)</span>.</p>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="equilibrium Helmholtz">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
equilibrium Helmholtz
</div>
</div>
<div class="callout-body-container callout-body">
<p>In both forward and backward cases, we start with a thermal equilibrium, and end with a thermal <em>dis</em>equilibrium.</p>
<p>For example, suppose we have a small tank of a few gas molecules in thermal equilibrium with a large energy bath.</p>
<p>Now, we <em>quickly</em> push the piston head in according to the function <span class="math inline">\(x(t)\)</span>. The trajectory of the system would go through is <span class="math inline">\(y(t)\)</span>, which is <em>determined</em> by both <span class="math inline">\(x(t)\)</span> and the initial state of both the system and the energy bath.</p>
<p>Now, we <em>wait a long time</em>, until the tank is in thermal equilibrium again. Then we pull the piston head out with time-reversed trajectory. Because the forward trajectory was quick, the backward trajectory was also quick. In both cases, the system starts at equilibrium, then becomes disequilibrated, and then becomes equilibrated again. The definition of <span class="math inline">\(W\)</span> is measured during the equilibrium-disequilibrium <em>trajectory</em>, while <span class="math inline">\(F^*\)</span> is measured by comparing the equilibrium-equilibrium, <em>regardless</em> of trajectory.</p>
<p>We wrote <span class="math inline">\(F^*\)</span> instead of <span class="math inline">\(F\)</span>, to emphasize that we are dealing with <em>equilibrium</em> Helmholtz free energy, defined by <span class="math inline">\(F^* = \min_\rho (\braket{E} - TS[\rho])\)</span>, and <em>not</em> the generic version <span class="math inline">\(\braket{E} - TS[\rho]\)</span>.</p>
<p>This is vitally important, because at time <span class="math inline">\(\tau\)</span>, when the constraints have just reached their new values, the system is <em>not</em> in equilibrium. We would have to hold the constraints constant for a while for the system to return to equilibrium with the energy bath. Despite this, CFT uses <span class="math inline">\(\Delta F^*\)</span>, which is computed at equilibrium.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-37-contents" aria-controls="callout-37" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-37" class="callout-37-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Apply the microcanonical version of CFT to the entire compound system that includes both the bath and the system, then integrate over all possible microstate trajectories of the bath <span class="math inline">\(y_{\text{bath}}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  S[x, y] &amp;= \Delta S_{\text{bath}} + \Delta S_{\text{system}} \\
  &amp;= \beta \Delta E_{\text{bath}} + \Delta S_{\text{system}} \\
  &amp;= \beta(W[x, y] - \braket{\Delta E_{\text{system}}}_2) + \Delta S_{\text{system}} \\
  &amp;= \beta (W[x, y] - \Delta F^*)
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\braket{\cdot}_2\)</span> means the canonical ensemble average under constraint <span class="math inline">\(x(\tau)\)</span>.</p>
<p>Notice that the work expended/entropy produced depends only on the system’s microtrajectory <span class="math inline">\(y(t)\)</span>, and <em>not</em> on the bath’s microtrajectory <span class="math inline">\(y_{\text{bath}}(t)\)</span>. That is,</p>
<p><span class="math display">\[S[x, y, y_{\text{bath}}] = S[x, y]\]</span></p>
<p>This will be used again in the next step when we integrate over <span class="math inline">\(D[y_{\text{bath}}]\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  \rho(y|x) &amp;= \int_{y, y_{\text{bath}}, x \text{ is valid}}D[y_{\text{bath}}]\; \rho(y, y_{\text{bath}} | x)  \\
  &amp;=  \underbrace{\int_{y', y'_{\text{bath}}, x' \text{ is valid}}D[y'_{\text{bath}}]}_{\text{reversible dynamics}}\; \underbrace{e^{S[x, y, y_{\text{bath}}]}\rho'(y', y'_{\text{bath}} | x')}_{\text{microcanonical CFT}}  \\
  &amp;=  \int D[y'_{\text{bath}}] \; e^{{\color{red} S[x, y]}}\rho'(y', y'_{\text{bath}} | x') \\
  &amp;=  e^{S[x,y]} \int D[y'_{\text{bath}}] \; \rho'(y', y'_{\text{bath}} | x') \\
  &amp;=  e^{S[x,y]} \rho'(y'|x')
\end{aligned}\]</span></p>
</div>
</div>
</div>
<div id="cor-todo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 3</strong></span> <span class="math display">\[\frac{\rho(W | x)}{\rho'(-W | x')} =  e^{\beta (W - \Delta F^*)}\]</span></p>
<p>where <span class="math inline">\(\rho(W|x)\)</span> is the probability density of expending work <span class="math inline">\(W\)</span> in the forward process.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-38-contents" aria-controls="callout-38" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-38" class="callout-38-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Integrate over all forward microtrajectories <span class="math inline">\(y\)</span> satisfying <span class="math inline">\(W[x, y] \in [W, W+\delta W]\)</span>. By reversibility, <span class="math inline">\(W[x', y'] = [-W - \delta W, -W]\)</span> for such microtrajectories.</p>
</div>
</div>
</div>
<p>Looking at the proof for CFT for the canonical ensemble, we immediately obtain many other possible CFTs, one per free entropy.</p>
<p>Suppose we have a piston of magnetic gas in energy-and-volume contact with a bath of constant temperature <span class="math inline">\(\beta\)</span> and pressure <span class="math inline">\(P\)</span>. Now suppose the gas is in equilibrium with the bath, and we vary the external magnetic field over a trajectory <span class="math inline">\(x\)</span>. Over the microstate trajectory <span class="math inline">\(x\)</span>, the external world would expend both some energy <span class="math inline">\(W[x, y]\)</span> and some volume <span class="math inline">\(V[x, y]\)</span>. Thus, we obtain the CFT for Gibbs free energy <span class="math inline">\(G\)</span>:</p>
<p><span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] + PV[x, y] - \Delta G^*)}\]</span></p>
<p>Similarly, suppose we have a chemical reaction chamber of fixed volume, and in energy-and-particle contact with a bath with constant temperature <span class="math inline">\(\beta\)</span> and chemical potentials <span class="math inline">\(\mu_i\)</span>, we have the CFT for Landau free energy <span class="math inline">\(\Omega\)</span>:</p>
<p><span class="math display">\[\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S[x, y]} = e^{\beta (W[x, y] -  \sum_i \mu_i N_i[x, y] - \Delta \Omega^*)}\]</span></p>
</section>
<section id="easy-consequences" class="level3">
<h3 class="anchored" data-anchor-id="easy-consequences">Easy consequences</h3>
<p>Let <span class="math inline">\(W\)</span> be the total work we expended by changing the constraints during the interval <span class="math inline">\([0, \tau]\)</span>. Since the work expended depends on the details of the heat bath and the starting state of the system at <span class="math inline">\(t=0\)</span>, this is a random variable.</p>
<div id="thm-todo" class="theorem">
<p><span class="theorem-title"><strong>Theorem 13 (Jarzynski equality)</strong></span> <span class="math display">\[\braket{e^{-\beta W}} = e^{-\beta \Delta F^*}\]</span></p>
<p>where the expectation is taken over many repeats of the same experiment (ensemble average).</p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-39-contents" aria-controls="callout-39" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-39" class="callout-39-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Integrate CFT over all forward trajectories <span class="math inline">\(D[x]\)</span>.</p>
<p><span class="math display">\[\rho(y | x) e^{-\beta W[x, y] } = \rho'(y' | x') e^{-\beta\Delta F}\]</span></p>
<p>now integrate over <span class="math inline">\(\int D[y]\)</span>, using the fact that <span class="math inline">\(D[y] = D[y']\)</span>.</p>
</div>
</div>
</div>
<p>THe Jarzynski equality can be understood as a special case of the following statement: <span class="math inline">\(e^{-\beta S_{[0, t]}}\)</span> is a <a href="https://en.wikipedia.org/wiki/Martingale_(probability_theory)">martingale</a>, where <span class="math inline">\(_{[0, t]}\)</span> is the entropy produced during time-period <span class="math inline">\([0, t]\)</span>. See <span class="citation" data-cites="roldanMartingalesPhysicistsTreatise2024">(<a href="#ref-roldanMartingalesPhysicistsTreatise2024" role="doc-biblioref">Roldán et al. 2024</a>)</span> for a derivation, as well as the martingale-theoretic perspective on thermodynamics.</p>
<div id="cor-todo" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4 (violation of second law is exponentially unlikely)</strong></span> <span class="math display">\[Pr((W - \Delta F^*) \leq - \delta W) \leq e^{-\beta \delta W}\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-40-contents" aria-controls="callout-40" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-40" class="callout-40-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Apply Markov’s inequality.</p>
</div>
</div>
</div>
<p>Since <span class="math inline">\(e^t\)</span> is convex, we have <span class="math inline">\(\Delta F^* \leq \braket{W}\)</span>, meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.</p>
<p>Since <span class="math inline">\(\Delta F^* = -\frac{1}{\beta} \ln\braket{e^{-\beta W}}\)</span>, we find that to second order,</p>
<div id="thm-fdr" class="theorem">
<p><span class="theorem-title"><strong>Theorem 14 (fluctuation-dissipation relation)</strong></span> <span class="math display">\[\underbrace{\braket{W} - \Delta F^*}_{\text{work dissipation}} = \beta \underbrace{\frac 12 \sigma_W^2}_{\text{work fluctuation}}\]</span></p>
</div>
<p>For periodic forcing, the CFT has a simpler form.</p>
<p>Consider a time-reversible dynamical system immersed in an energy bath with inverse temperature <span class="math inline">\(\beta\)</span> , driven by periodically varying constraints. For example, a pendulum in a sticky fluid subjected to a periodic driving torque, or a water-cooled electric circuit driven by a periodic voltage.</p>
<p>Such a system will settle into a “dynamical equilibrium” ensemble, much like a canonical ensemble. If it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.</p>
<div id="thm-gallavotti-cohen" class="theorem">
<p><span class="theorem-title"><strong>Theorem 15 (Gallavotti–Cohen fluctuation theorem)</strong></span> <span class="math display">\[\frac{\rho(Q)}{\rho'(-Q)} = e^{\beta Q}\]</span></p>
<p>where <span class="math inline">\(\rho(Q)\)</span> is the probability density that a forward cycle, randomly sampled from the dynamical equilibrium ensemble, emits energy <span class="math inline">\(Q\)</span> into the energy bath. Similarly, <span class="math inline">\(\rho'\)</span> is for the backward cycle.</p>
</div>
<p>It is often used for time-symmetric driving, such as sine waves. In that case, it is typically written as</p>
<p><span class="math display">\[\frac{\rho(\dot s)}{\rho(-\dot s)} = e^{\beta T \dot s}\]</span></p>
<p>where <span class="math inline">\(T\)</span> is the driving period, and <span class="math inline">\(\dot s\)</span> is the entropy production rate.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Proof">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-41-contents" aria-controls="callout-41" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-41" class="callout-41-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Construct a process that starts at equilibrium, then mount up the periodic driving, runs it for <span class="math inline">\(NT\)</span> time where <span class="math inline">\(N\)</span> is a large integer, then remove the driving. At the <span class="math inline">\(N\to\infty\)</span> limit, the CFT reduces to this equation.</p>
</div>
</div>
</div>
</section>
</section>
<section id="applications-of-cft" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="applications-of-cft">Applications of CFT</h2>
<section id="molecular-machines" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="molecular-machines">Molecular machines</h3>
<p><a href="#sec-rna-hairpin">Remember previously</a> when we studied pulling an RNA hairpin apart then together again, and how, with different pulling speed, the wasted energy per cycle is different? Well, we can use the CFT to quantify this.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/RNA_unfolding_collin_2005.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Several hundred trajectories of pulling and un-pulling an RNA hairpin, at loading rate <span class="math inline">\(7.5 \ut{pN/s}\)</span>. <span class="citation" data-cites="collinVerificationCrooksFluctuation2005">(<a href="#ref-collinVerificationCrooksFluctuation2005" role="doc-biblioref">Collin et al. 2005, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>In <span class="citation" data-cites="collinVerificationCrooksFluctuation2005">(<a href="#ref-collinVerificationCrooksFluctuation2005" role="doc-biblioref">Collin et al. 2005</a>)</span>, they repeatedly pulled on different types of RNA hairpins, at different speeds, obtaining the following distributions. Not only did they verify the CFT, but also used it to measure <span class="math inline">\(\Delta F^*\)</span> as the <span class="math inline">\(W\)</span> where <span class="math inline">\(\rho(W) = \rho'(-W)\)</span>, that is, the crossing point between the two histograms of <span class="math inline">\(\rho(W)\)</span> and <span class="math inline">\(\rho'(-W)\)</span>. This was a breakthrough, extending statistical mechanics to the single-molecule regime, a fast-developing field of study that is still far from equilibrium.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/CFT_mutant_collin_2005.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Histograms of forward work probability <span class="math inline">\(\rho(W)\)</span> and backward work probability <span class="math inline">\(\rho'(-W)\)</span>. The mutant type RNA has <span class="math inline">\(\Delta G = 160 k_BT\)</span>, while the wild-type RNA has <span class="math inline">\(\Delta G = 154 k_BT\)</span>. In the inset, the CFT <span class="math inline">\(\ln\frac{\rho(W)}{\rho'(-W)} = \beta W\)</span> is directly verified. <span class="citation" data-cites="collinVerificationCrooksFluctuation2005">(<a href="#ref-collinVerificationCrooksFluctuation2005" role="doc-biblioref">Collin et al. 2005, fig. 3</a>)</span></figcaption>
</figure>
</div>
<p>Classically, if we have a single system in thermal equilibrium with a single energy-bath, and we perform a cyclic operation on it, then we can’t extract work, lest we violate the second law.</p>
<p>Because the requirement only says that <span class="math inline">\(\braket{e^{-\beta W}} = 1\)</span>, it is entirely possible for us to extract arbitrarily large amounts of work with arbitrarily high probability, as long as there is a corresponding small probability of losing a lot of work:</p>
<p><span class="math display">\[
e^{-\beta W}(1-\delta) + e^{-\beta W}\delta = 1, \quad W = \beta^{-1} \ln \frac{1}{\delta}
\]</span></p>
<p><span class="citation" data-cites="mailletOptimalProbabilisticWork2019">(<a href="#ref-mailletOptimalProbabilisticWork2019" role="doc-biblioref">Maillet et al. 2019</a>)</span> constructed a quantum mechanical device with a single-electron transistor. The electron can expend work. They managed to extract work from the device with over 75% probability.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/maillet_2019_3_c.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Histogram of work extraction. Over 75% of the histogram is in the <span class="math inline">\(W &gt; 0\)</span> part. Despite this, <span class="math inline">\(\braket{e^{-\beta W}} = 0.989 \pm 0.03\)</span>, verifying the Jarzynski equation. <span class="citation" data-cites="mailletOptimalProbabilisticWork2019">(<a href="#ref-mailletOptimalProbabilisticWork2019" role="doc-biblioref">Maillet et al. 2019, fig. 3.c</a>)</span></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Martingales or bust!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Martingales or bust!
</div>
</div>
<div class="callout-body-container callout-body">
<p>As described, the Jarzynski equality is really a consequence of the fact that <span class="math inline">\(e^S\)</span> is a martingale. Martingales, roughly speaking, are payoffs in fair gambles. Imagine playing at a fair casino. You start with cash <span class="math inline">\(X_0\)</span>, and after each game, your cash grows or shrinks, resulting in a sequence <span class="math inline">\(X_1, X_2, \dots\)</span>. The game ends when you decide you’ve had enough, goes bankrupt, or the casino goes bankrupt. A fair casino is one where <span class="math inline">\(\braket{X_{t+1} - X_t | X_t} = 0\)</span>, meaning that no matter what your strategy is, at time <span class="math inline">\(t\)</span>, your expected payoff is still zero.</p>
<p>Consider the double-or-nothing gamble. The casino flips a coin, and if heads, you get double the bet, else you get nothing. The classic martingale strategy is to start at betting 1 dollar, and doubling the bet at each loss. It is clear that this strategy has “guaranteed” return – eventually the coin will land heads and you will recoup all losses plus one dollar.</p>
<p>This “guaranteed” return is an illusion, because you (or the casino) can go bankrupt. Suppose you start with 127 dollars, then by tracing out all possibilities during one martingale run, you will find that the martingale ends with winning 1 dollar with probability <span class="math inline">\(\frac{127}{128}\)</span>, and losing 127 dollars with probability <span class="math inline">\(\frac{1}{128}\)</span>.</p>
<p>In this sense, martingale strategies cannot create free money out of a fair gamble, much like how machines cannot create free energy out of thermal noise. However, martingale strategies can shape the probability distribution of the payoff. For example, in the double-or-nothing game, the martingale strategy shifts almost all probability mass to winning one dollar, at the cost of a small but nonzero probability of losing everything. Similarly, a machine can create a large probability of extracting a little work from a thermal bath, at the price of a small probability of losing a lot of work to the thermal bath.</p>
</div>
</div>
</section>
<section id="worked-example-bouncing-ball" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="worked-example-bouncing-ball">Worked example: bouncing ball</h3>
<p>This example is from <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021</a>, exercise 4.8)</span>, which itself derives from <span class="citation" data-cites="luaPracticalApplicabilityJarzynski2005">(<a href="#ref-luaPracticalApplicabilityJarzynski2005" role="doc-biblioref">Lua and Grosberg 2005</a>)</span>. See also <span class="citation" data-cites="hijarJarzynskiEqualityIllustrated2010">(<a href="#ref-hijarJarzynskiEqualityIllustrated2010" role="doc-biblioref">Híjar and de Zárate 2010</a>)</span> for another solved example, of a chest expander with mass points stuck in the middle of the springs. You might need to read my tutorial on <a href="https://yuxi-liu-wired.github.io/sketches/posts/field-theory-how-to/">field-theoretic calculations</a> before attempting that example.</p>
<p>We have a one-dimensional system, of a single ball bounding between two walls of a piston. The only control we have is that we can move one of the piston heads. At the start, the piston has length <span class="math inline">\(L\)</span>, and the system is in thermal equilibrium at inverse temperature <span class="math inline">\(\beta\)</span>. We plunge the piston head at velocity <span class="math inline">\(v\)</span> for time <span class="math inline">\(\Delta L / v\)</span>, then immediately reverse it, taking another <span class="math inline">\(\Delta L / v\)</span>. We explicitly calculate that <span class="math inline">\(\braket{e^{-\beta W}} = 1\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Jarzynski_bouncing_ball_sethna_2021_4_12.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021, fig. 4.12</a>)</span></figcaption>
</figure>
</div>
<p>The phase space of the ball has 2 dimensions, <span class="math inline">\((p, x)\)</span>. The Boltzmann distribution is</p>
<p><span class="math display">\[\rho(p, x) = \rho(p) \rho(x) = \frac{1}{\sqrt{2\pi m/\beta}}e^{-\frac{\beta}{2m}p^2} \times \frac{1}{L}\]</span></p>
<p>We assume that <span class="math inline">\(L\)</span> is large enough, such that the ball hits the piston head at most once. There are three possibilities:</p>
<ol type="1">
<li>If the piston head hits the ball during the in-stroke, then the ball’s velocity increases by <span class="math inline">\(2v\)</span>, and its kinetic energy increases by <span class="math display">\[W = \Delta KE = 2v(mv - p)\]</span></li>
<li>If the piston head hits the ball during the out-stroke, then the ball’s velocity decreases by <span class="math inline">\(2v\)</span>, and its kinetic energy increases by<br>
<span class="math display">\[W = 2v(mv+p)\]</span></li>
<li>Otherwise, the piston head avoids the ball, and we have <span class="math inline">\(W = 0\)</span>.</li>
</ol>
<p>If at <span class="math inline">\(t=0\)</span>, the ball is in the phase space region labelled “in region”, then it will be hit in the in-stroke. If at <span class="math inline">\(t=\Delta L/v\)</span>, the ball is in the phase space region labelled “out region”, then it will be hit in the out-stroke. Otherwise, it will not be hit.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Jarzynski_bouncing_ball.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Phase space of the bouncing balls.</figcaption>
</figure>
</div>
<p>Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
    \braket{e^{-\beta W}} &amp;= \int_{in} \rho dpdx \; e^{-\beta 2v(mv-p)} + \int_{out} \rho dpdx \; e^{-\beta 2v(mv+p)} + \int_{other} \rho dpdx \; 1 \\
    &amp;= e^{-2\beta mv^2} \left(\int_{in} \rho dpdx \; e^{2\beta vp} + \int_{out} \rho dpdx \; e^{-2\beta vp}\right) +  \int_{other} \rho dpdx \; 1
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\rho(p, x) = \rho(-p, x - \Delta L)\)</span>, the first two integrals can be combined by flipping the “out region”, then moving it by <span class="math inline">\(\Delta L\)</span>, to “out’ region”.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Because <span class="math inline">\(L\)</span> is large, this is <em>mostly</em> correct, as the regions where this is incorrect has <span class="math inline">\(\rho\)</span> so small that it is negligible, as seen in the figure.</p>
</div>
</div>
<p>Now we continue:</p>
<p><span class="math display">\[
\begin{aligned}
\braket{e^{-\beta W}} &amp;\approx e^{-2\beta mv^2} \int_{in, out'} \rho dpdx \; e^{2\beta vp} +  \int_{other} \rho dpdx \; 1 \\
&amp;= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}(p - 2mv)^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right)
\end{aligned}
\]</span></p>
<p>Because the “in-out’ region” is symmetric across the <span class="math inline">\(p = mv\)</span> line, we can reflect the first integral across the <span class="math inline">\(p=mv\)</span> line and obtain <span class="math display">\[
= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}p^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right) = 1
\]</span></p>
</section>
<section id="other-examples" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="other-examples">Other examples</h3>
<p>Suppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.</p>
<p>By Bayes theorem,</p>
<p><span class="math display">\[Pr(\text{forward}|x, y) = \frac{1}{1 + e^{-S[x, y]}}\]</span></p>
<p>where <span class="math inline">\(S[x, y] = \beta(W[x, y] - \Delta F^*)\)</span>.</p>
<p>In words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time. Possibly this is why molecular machines in biology tend to operate with dissipation of around <span class="math inline">\(5 k_B T\)</span>, even when less dissipation is possible, to provide a good enough forward bias. Too little dissipation would mean that there is no way to distinguish the direction of time, yet time is inherent in life.</p>
<p>Maxwell’s demon<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> has made it clear that information is physical, and vice versa. It is reasonable to guess that Maxwell’s demon is a converter between information and usable work. Or stated in another way, the information that Maxwell’s demon has is a form of usable work. In <span class="citation" data-cites="parrondoThermodynamicsInformation2015">(<a href="#ref-parrondoThermodynamicsInformation2015" role="doc-biblioref">Parrondo, Horowitz, and Sagawa 2015</a>)</span>, this analogy is made precise.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;I tried to write an essay purely on equilibrium statistical mechanics, but alas, there is no escape from that demon.</p></div></div><p>Consider a “demon” machine made of cameras, motors, and other contraptions, all controlled by its <a href="https://en.wikipedia.org/wiki/Finite-state_machine">finite-state machine</a> “brain”. There are two boxes of gas of oxygen and nitrogen, that started out in equilibrium at temperature <span class="math inline">\(T\)</span>. The demon uses its camera to measure some particle locations, which then causes some changes in its internal state, driving some motors to open a few gates while closing some other gates. The demon finishes its work after a fixed amount of time <span class="math inline">\(t\)</span>, ending up with two tanks of gas at temperature <span class="math inline">\(T\)</span>, one enriched in oxygen, the other in nitrogen.</p>
<p>The system has changed by Helmholtz free energy <span class="math inline">\(\Delta F^*\)</span>. By the Jarzynski inequality, <span class="math inline">\(\braket{e^{-\Delta S}} = 1\)</span> where <span class="math inline">\(\Delta S\)</span> is the entropy production. Here, because the demon might end up with a state that is different from where it started, we need to account for that demon’s inner state. As Landauer and Bennett argued, the problem with Maxwell’s demon is that it cannot forget without paying an energetic cost. And what is forgotten when a demon forgets? It forgets its statistical correlation with the tank of gas. Thus, we should account for that information entropy term by <span class="math inline">\(I\)</span>, where <span class="math inline">\(I\)</span> is the <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> between the demon’s end-state, and the trajectory of the system. This allows us to rescue the second law of thermodynamics:</p>
<p><span class="math display">\[
\text{entropy production} = \beta(W - \Delta F^*) + I
\]</span></p>
<p>and the Jarzynski inequality becomes <span class="math inline">\(\braket{e^{-\beta(W - \Delta F^*) + I}}\)</span>.</p>


<!-- -->


</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-andersenClusterMethodsEquilibrium1977" class="csl-entry" role="listitem">
Andersen, Hans C. 1977. <span>“Cluster <span>Methods</span> in <span>Equilibrium Statistical Mechanics</span> of <span>Fluids</span>.”</span> In <em>Statistical <span>Mechanics</span>: <span>Part A</span>: <span>Equilibrium Techniques</span></em>, edited by Bruce J. Berne, 1–45. Boston, MA: Springer US. <a href="https://doi.org/10.1007/978-1-4684-2553-6_1">https://doi.org/10.1007/978-1-4684-2553-6_1</a>.
</div>
<div id="ref-aspremPonderingImponderablesOccultism2011" class="csl-entry" role="listitem">
Asprem, Egil. 2011. <span>“Pondering <span>Imponderables</span>: <span>Occultism</span> in the Mirror of Late Classical Physics.”</span> <em>Aries-Journal for the Study of Western Esotericism</em> 11 (2): 129. <a href="https://www.academia.edu/download/11714432/Asprem_(2011)_Pondering_Imponderables.pdf">https://www.academia.edu/download/11714432/Asprem_(2011)_Pondering_Imponderables.pdf</a>.
</div>
<div id="ref-ben-naimFarewellEntropyStatistical2008" class="csl-entry" role="listitem">
Ben-Naim, Arieh. 2008. <em>Farewell <span>To Entropy</span>, <span>A</span>: <span>Statistical Thermodynamics Based On Information</span></em>. Illustrated edition. Hackensack, N.J: Wspc.
</div>
<div id="ref-bergPhysicsChemoreception1977" class="csl-entry" role="listitem">
Berg, H C, and E M Purcell. 1977. <span>“Physics of Chemoreception.”</span> <em>Biophysical Journal</em> 20 (2): 193–219. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1473391/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1473391/</a>.
</div>
<div id="ref-bergRandomWalksBiology1993" class="csl-entry" role="listitem">
Berg, Howard C. 1993. <em>Random Walks in Biology</em>. Expanded ed. Princeton, N.J: Princeton University Press.
</div>
<div id="ref-bergMotileBehaviorBacteria2000" class="csl-entry" role="listitem">
———. 2000. <span>“Motile <span>Behavior</span> of <span>Bacteria</span>.”</span> <em>Physics Today</em> 53 (1): 24–29. <a href="https://doi.org/10.1063/1.882934">https://doi.org/10.1063/1.882934</a>.
</div>
<div id="ref-bergChemotaxisEscherichiaColi1972" class="csl-entry" role="listitem">
Berg, Howard C., and Douglas A. Brown. 1972. <span>“Chemotaxis in <span>Escherichia</span> Coli Analysed by <span class="nocase">Three-dimensional Tracking</span>.”</span> <em>Nature</em> 239 (5374): 500–504. <a href="https://doi.org/10.1038/239500a0">https://doi.org/10.1038/239500a0</a>.
</div>
<div id="ref-boltzmannLecturesGasTheory2011" class="csl-entry" role="listitem">
Boltzmann, Ludwig. 2011. <em>Lectures on <span>Gas Theory</span></em>. Reprint edition. New York: Dover Publications.
</div>
<div id="ref-brennanEvaporationLiquidsKinetic1974" class="csl-entry" role="listitem">
Brennan, J. F., J. S. Shapiro, and E. C. Watton. 1974. <span>“Evaporation of Liquids: <span>A</span> Kinetic Approach.”</span> <em>Journal of Chemical Education</em> 51 (4): 276. <a href="https://doi.org/10.1021/ed051p276">https://doi.org/10.1021/ed051p276</a>.
</div>
<div id="ref-bucarDisappearingPolymorphsRevisited2015" class="csl-entry" role="listitem">
Bučar, Dejan-Krešimir, Robert W. Lancaster, and Joel Bernstein. 2015. <span>“Disappearing <span>Polymorphs Revisited</span>.”</span> <em>Angewandte Chemie International Edition</em> 54 (24): 6972–93. <a href="https://doi.org/10.1002/anie.201410356">https://doi.org/10.1002/anie.201410356</a>.
</div>
<div id="ref-collinVerificationCrooksFluctuation2005" class="csl-entry" role="listitem">
Collin, D., F. Ritort, C. Jarzynski, S. B. Smith, I. Tinoco, and C. Bustamante. 2005. <span>“Verification of the <span>Crooks</span> Fluctuation Theorem and Recovery of <span>RNA</span> Folding Free Energies.”</span> <em>Nature</em> 437 (7056): 231–34. <a href="https://doi.org/10.1038/nature04061">https://doi.org/10.1038/nature04061</a>.
</div>
<div id="ref-discherNewInsightsErythrocyte2000" class="csl-entry" role="listitem">
Discher, Dennis E. 2000. <span>“New Insights into Erythrocyte Membrane Organization and Microelasticity.”</span> <em>Current Opinion in Hematology</em> 7 (2): 117. <a href="https://journals.lww.com/co-hematology/fulltext/2000/03000/New_insights_into_erythrocyte_membrane.8.aspx">https://journals.lww.com/co-hematology/fulltext/2000/03000/New_insights_into_erythrocyte_membrane.8.aspx</a>.
</div>
<div id="ref-duncanConstructingQuantumMechanics2019" class="csl-entry" role="listitem">
Duncan, Anthony, and Michel Janssen. 2019. <em>Constructing Quantum Mechanics. <span>Volume</span> 1: <span>The</span> Scaffold 1900-1923</em>. First edition. Oxford, United Kingdom ; New York, NY: Oxford University Press.
</div>
<div id="ref-feynmanStatisticalMechanicsSet2018" class="csl-entry" role="listitem">
Feynman, Richard P. 2018. <em>Statistical <span>Mechanics</span>: A <span>Set Of Lectures</span></em>. <a href="https://www.taylorfrancis.com/books/9780429961588">https://www.taylorfrancis.com/books/9780429961588</a>.
</div>
<div id="ref-feynmanWhatYouCare1989" class="csl-entry" role="listitem">
Feynman, Richard P. 1989. <em>"<span>What</span> Do You Care What Other People Think?": Further Adventures of a Curious Character</em>. Edited by Ralph Leighton. New York: Bantam Books.
</div>
<div id="ref-feynmanFeynmanLecturesComputation1996" class="csl-entry" role="listitem">
———. 1996. <em>Feynman Lectures on Computation</em>. Edited by Anthony J. G. Hey and Robin W. Allen. Reading, Mass: Addison-Wesley.
</div>
<div id="ref-garciaQuantitativeDissectionSimple2011" class="csl-entry" role="listitem">
Garcia, Hernan G., and Rob Phillips. 2011. <span>“Quantitative Dissection of the Simple Repression Input–Output Function.”</span> <em>Proceedings of the National Academy of Sciences</em> 108 (29): 12173–78. <a href="https://doi.org/10.1073/pnas.1015616108">https://doi.org/10.1073/pnas.1015616108</a>.
</div>
<div id="ref-garcia-garciaFinitesizeCorrectionsBlackbody2008" class="csl-entry" role="listitem">
García-García, Antonio M. 2008. <span>“Finite-Size Corrections to the Blackbody Radiation Laws.”</span> <em>Physical Review A</em> 78 (2): 023806. <a href="https://doi.org/10.1103/PhysRevA.78.023806">https://doi.org/10.1103/PhysRevA.78.023806</a>.
</div>
<div id="ref-grimus100thAnniversarySackur2013" class="csl-entry" role="listitem">
Grimus, Walter. 2013. <span>“100th Anniversary of the <span>Sackur</span>–<span>Tetrode</span> Equation.”</span> <em>Annalen Der Physik</em> 525 (3). <a href="https://doi.org/10.1002/andp.201300720">https://doi.org/10.1002/andp.201300720</a>.
</div>
<div id="ref-hazoglouCommunicationMaximumCaliber2015" class="csl-entry" role="listitem">
Hazoglou, Michael J., Valentin Walther, Purushottam D. Dixit, and Ken A. Dill. 2015. <span>“Communication: <span>Maximum</span> Caliber Is a General Variational Principle for Nonequilibrium Statistical Mechanics.”</span> <em>The Journal of Chemical Physics</em> 143 (5): 051104. <a href="https://doi.org/10.1063/1.4928193">https://doi.org/10.1063/1.4928193</a>.
</div>
<div id="ref-hijarJarzynskiEqualityIllustrated2010" class="csl-entry" role="listitem">
Híjar, Humberto, and José M. Ortiz de Zárate. 2010. <span>“Jarzynski’s Equality Illustrated by Simple Examples.”</span> <em>European Journal of Physics</em> 31 (5): 1097. <a href="https://iopscience.iop.org/article/10.1088/0143-0807/31/5/012/meta">https://iopscience.iop.org/article/10.1088/0143-0807/31/5/012/meta</a>.
</div>
<div id="ref-hoaglandPhysiologicalControlJudgments1933" class="csl-entry" role="listitem">
Hoagland, Hudson. 1933. <span>“The <span>Physiological Control</span> of <span>Judgments</span> of <span>Duration</span>: <span>Evidence</span> for a <span>Chemical Clock</span>.”</span> <em>The Journal of General Psychology</em> 9 (2): 267–87. <a href="https://doi.org/10.1080/00221309.1933.9920937">https://doi.org/10.1080/00221309.1933.9920937</a>.
</div>
<div id="ref-hoaglandPacemakersHumanBrain1936" class="csl-entry" role="listitem">
———. 1936. <span>“Pacemakers of Human Brain Waves in Normals and in General Paretics.”</span> <em>American Journal of Physiology-Legacy Content</em> 116 (3): 604–15. <a href="https://doi.org/10.1152/ajplegacy.1936.116.3.604">https://doi.org/10.1152/ajplegacy.1936.116.3.604</a>.
</div>
<div id="ref-jaynesGibbsParadox1992" class="csl-entry" role="listitem">
Jaynes, E. T. 1992. <span>“The <span>Gibbs Paradox</span>.”</span> In <em>Maximum <span>Entropy</span> and <span>Bayesian Methods</span>: <span>Seattle</span>, 1991</em>, edited by C. Ray Smith, Gary J. Erickson, and Paul O. Neudorfer, 1–21. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-94-017-2219-3_1">https://doi.org/10.1007/978-94-017-2219-3_1</a>.
</div>
<div id="ref-jaynesProbabilityTheoryLogic2003" class="csl-entry" role="listitem">
———. 2003. <em>Probability <span>Theory</span>: <span>The Logic</span> of <span>Science</span></em>. Edited by G. Larry Bretthorst. Annotated edition. Cambridge: Cambridge University Press.
</div>
<div id="ref-krutchBestTwoWorlds1953" class="csl-entry" role="listitem">
Krutch, Joseph Wood. 1953. <em>The <span>Best</span> of <span>Two Worlds</span></em>. William Sloane Associates.
</div>
<div id="ref-laidlerUnconventionalApplicationsArrhenius1972" class="csl-entry" role="listitem">
Laidler, Keith J. 1972. <span>“Unconventional Applications of the <span>Arrhenius</span> Law.”</span> <em>Journal of Chemical Education</em> 49 (5): 343. <a href="https://doi.org/10.1021/ed049p343">https://doi.org/10.1021/ed049p343</a>.
</div>
<div id="ref-lemonsTrailBlackbodyRadiation2022" class="csl-entry" role="listitem">
Lemons, Don S., William R. Shanahan, and Louis Buchholtz. 2022. <em>On the Trail of Blackbody Radiation: <span>Max Planck</span> and the Physics of His Era</em>. Cambridge, Massachusetts: The MIT Press.
</div>
<div id="ref-liphardtReversibleUnfoldingSingle2001" class="csl-entry" role="listitem">
Liphardt, Jan, Bibiana Onoa, Steven B. Smith, Ignacio Tinoco, and Carlos Bustamante. 2001. <span>“Reversible <span>Unfolding</span> of <span>Single RNA Molecules</span> by <span>Mechanical Force</span>.”</span> <em>Science</em> 292 (5517): 733–37. <a href="https://doi.org/10.1126/science.1058498">https://doi.org/10.1126/science.1058498</a>.
</div>
<div id="ref-luEngineeringMaxwellDemon2014" class="csl-entry" role="listitem">
Lu, Zhiyue, Dibyendu Mandal, and Christopher Jarzynski. 2014. <span>“Engineering <span>Maxwell</span>’s Demon.”</span> <em>Physics Today</em> 67 (8): 60–61. <a href="https://doi.org/10.1063/PT.3.2490">https://doi.org/10.1063/PT.3.2490</a>.
</div>
<div id="ref-luaPracticalApplicabilityJarzynski2005" class="csl-entry" role="listitem">
Lua, Rhonald C., and Alexander Y. Grosberg. 2005. <span>“Practical <span>Applicability</span> of the <span>Jarzynski Relation</span> in <span>Statistical Mechanics</span>:  <span>A Pedagogical Example</span>.”</span> <em>The Journal of Physical Chemistry B</em> 109 (14): 6805–11. <a href="https://doi.org/10.1021/jp0455428">https://doi.org/10.1021/jp0455428</a>.
</div>
<div id="ref-maStatisticalMechanics1985" class="csl-entry" role="listitem">
Ma, Shang-keng. 1985. <em>Statistical Mechanics</em>. Philadelphia: World Scientific.
</div>
<div id="ref-mailletOptimalProbabilisticWork2019" class="csl-entry" role="listitem">
Maillet, Olivier, Paolo A. Erdman, Vasco Cavina, Bibek Bhandari, Elsa T. Mannila, Joonas T. Peltonen, Andrea Mari, et al. 2019. <span>“Optimal <span>Probabilistic Work Extraction</span> Beyond the <span>Free Energy Difference</span> with a <span>Single-Electron Device</span>.”</span> <em>Physical Review Letters</em> 122 (15): 150604. <a href="https://doi.org/10.1103/PhysRevLett.122.150604">https://doi.org/10.1103/PhysRevLett.122.150604</a>.
</div>
<div id="ref-nashElementsStatisticalThermodynamics2006" class="csl-entry" role="listitem">
Nash, Leonard K. 2006. <em>Elements of <span>Statistical Thermodynamics</span>: <span>Second Edition</span></em>. Second edition. Mineola, N.Y: Dover Publications.
</div>
<div id="ref-nelsonBiologicalPhysicsEnergy2003" class="csl-entry" role="listitem">
Nelson, Philip. 2003. <em>Biological <span>Physics</span>: <span>Energy</span>, <span>Information</span>, <span>Life</span></em>. First Edition. New York: W. H. Freeman.
</div>
<div id="ref-parrondoThermodynamicsInformation2015" class="csl-entry" role="listitem">
Parrondo, Juan M. R., Jordan M. Horowitz, and Takahiro Sagawa. 2015. <span>“Thermodynamics of Information.”</span> <em>Nature Physics</em> 11 (2): 131–39. <a href="https://doi.org/10.1038/nphys3230">https://doi.org/10.1038/nphys3230</a>.
</div>
<div id="ref-penroseFoundationsStatisticalMechanics2005" class="csl-entry" role="listitem">
Penrose, Oliver. 2005. <em>Foundations of Statistical Mechanics: A Deductive Treatment</em>. Mineola: Dover Publ.
</div>
<div id="ref-pressePrinciplesMaximumEntropy2013" class="csl-entry" role="listitem">
Pressé, Steve, Kingshuk Ghosh, Julian Lee, and Ken A. Dill. 2013. <span>“Principles of Maximum Entropy and Maximum Caliber in Statistical Physics.”</span> <em>Reviews of Modern Physics</em> 85 (3): 1115–41. <a href="https://doi.org/10.1103/RevModPhys.85.1115">https://doi.org/10.1103/RevModPhys.85.1115</a>.
</div>
<div id="ref-reifFundamentalsStatisticalThermal1998" class="csl-entry" role="listitem">
Reif, Frederick. 1998. <em>Fundamentals of Statistical and Thermal Physics</em>. 43. [pr.]. <span>McGraw-Hill</span> Series in Fundamentals of Physics. New York: McGraw-Hill.
</div>
<div id="ref-reiserGeometricEffectsBlackbody2013" class="csl-entry" role="listitem">
Reiser, Ariel, and Levi Schächter. 2013. <span>“Geometric Effects on Blackbody Radiation.”</span> <em>Physical Review A</em> 87 (3): 033801. <a href="https://doi.org/10.1103/PhysRevA.87.033801">https://doi.org/10.1103/PhysRevA.87.033801</a>.
</div>
<div id="ref-roldanMartingalesPhysicistsTreatise2024" class="csl-entry" role="listitem">
Roldán, Édgar, Izaak Neri, Raphael Chetrite, Shamik Gupta, Simone Pigolotti, Frank Jülicher, and Ken Sekimoto. 2024. <span>“Martingales for Physicists: A Treatise on Stochastic Thermodynamics and Beyond.”</span> <em>Advances in Physics</em>, May, 1–258. <a href="https://doi.org/10.1080/00018732.2024.2317494">https://doi.org/10.1080/00018732.2024.2317494</a>.
</div>
<div id="ref-schrodingerStatisticalThermodynamics1989" class="csl-entry" role="listitem">
Schrodinger, Erwin. 1989. <em>Statistical <span>Thermodynamics</span></em>. Revised ed. edition. New York: Dover Publications.
</div>
<div id="ref-sethnaStatisticalMechanicsEntropy2021" class="csl-entry" role="listitem">
Sethna, James P. 2021. <em>Statistical Mechanics: Entropy, Order Parameters, and Complexity</em>. Second Edition. Oxford Master Series in Statistical, Computational, and Theoretical Physics. Oxford, United Kingdom ; New York, NY: Oxford University Press.
</div>
<div id="ref-sommerfeldLecturesTheoreticalPhysics1950" class="csl-entry" role="listitem">
Sommerfeld, Arnold. 1950. <em>Lectures on Theoretical Physics</em>. New York, Academic Press. <a href="http://archive.org/details/lecturesontheore05somm">http://archive.org/details/lecturesontheore05somm</a>.
</div>
<div id="ref-tolmanPrinciplesStatisticalMechanics1980" class="csl-entry" role="listitem">
Tolman, Richard C. 1980. <em>The Principles of Statistical Mechanics</em>. Unabridged and unaltered republ. of the original (1938) ed. Dover Books on Physics. New York, NY: Dover.
</div>
<div id="ref-wardPerilsPolymorphismSize2017" class="csl-entry" role="listitem">
Ward, Michael D. 2017. <span>“Perils of <span>Polymorphism</span>: <span>Size Matters</span>.”</span> <em>Israel Journal of Chemistry</em> 57 (1-2): 82–92. <a href="https://doi.org/10.1002/ijch.201600071">https://doi.org/10.1002/ijch.201600071</a>.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Statistical Mechanics"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-07-04"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2025-03-04"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [math, physics, probability, statistics, biology]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    resources:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        - "figure/**"</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "How to think like a classical statistical-mechanic, with many examples from gas theory, biology, probability, and information theory. Prerequisites: thermodynamics, calculus, probability, and mathematical maturity."</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner/banner.png"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="an">image-alt:</span><span class="co"> "A bricolage representing modern statistical mechanics in all its multifaceted chaotic order. Containing numbers (information), particles in a box, Jupiter's great red eye, RNA hairpins, bacteria with wavy flagella.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">Made in Ideogram V1, with prompt 'An abstract conceptual illustration of a brutalist structure, with sharp angles and a minimalistic design, made of concrete, steel balls, and rebars. The structure appears to be made up of particles in a box, jamming together, creating bits and numbers in the realm of 0-1. The design also features an RNA hairpin, with eddies upon eddies upon eddies, evoking the image of Jupiter's red spot. The random walk of swimming bacteria with wavy flagella is depicted, adding to the complexity of the design. The illustration is presented on a white background, with high contrast and a monochromatic color scheme, reminiscent of a flat style vector SVG art., illustration, conceptual art'."</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "certain"</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 7</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">### What this essay contains</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>Classical mechanics, electrodynamics, and thermodynamics are all conceptually simple, but settled. Things are less clear with statistical mechanics. To say it kindly, it is a lively field of research. To say it unkindly, it is a field whose very conceptual foundation is in doubt. The main problem is not with quantum mechanics, as quantum statistical mechanics works very well, but with how to handle systems far-from-equilibrium. Fortunately, as long as we stick to the (near-)equilibrium parts, then it is mostly settled, and so this is how this essay is going to be written about. We will avoid quantum because that deserves its own entire essay, and avoid far-from-equilibrium because it is unsettled. What we can present is still a beautifully precise mathematical toolbox with surprisingly wide applications.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>Statistical mechanics is technically independent of thermodynamics, but it is strongly related. You can acquire the basic skills in thermodynamics by working through the first half of my previous essay <span class="co">[</span><span class="ot">*Classical Thermodynamics and Economics*</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/equilibrium-thermoeconomics/)</span>.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>About half of the essay is taken up with calculations. The theoretical core of statistical mechanics is small, and most of the skills are in applying it to actual systems. Therefore, a lot of worked-through examples are necessary. I have tried to make them flow well and skimmable.</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>The essay contains: entropy, free entropies, partition function, about 12 useful theorems, fluctuation-dissipation relations, maximal caliber, Crooks fluctuation theorem, Jarzynski equality, rubber bands, kinetic gas theory, van der Waals law, blackbody radiation, combinatorics, chi-squared test, large deviation theory, bacteria hunting, unzipping RNA hairpins, Arrhenius equation, martingales, Maxwell's demon, Laplace's demon.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>It does not contain: quantum statistical mechanics, nonequilibrium statistical mechanics, linear response theory, Onsager reciprocal relations, statistical field theory, phase transitions, stochastic processes, Langevin equation, diffusion theory, Fokker--Planck equation, Feynman--Kac formula, Brownian ratchets.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>The prerequisites are thermodynamics, multivariate calculus, probability, combinatorics, and mathematical maturity. It’s good to be familiar with biology and the basics of random walk as well.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### Quick reference</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$D_{KL}$: <span class="co">[</span><span class="ot">Kullback--Leibler divergence</span><span class="co">](https://en.wikipedia.org/wiki/Kullback-Leibler_divergence)</span>.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$: entropy of probability distribution $\rho$.</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$S^*$: maximal entropy under constraints.</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$f<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$: Helmholtz free entropy of probability distribution $\rho$.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$f^*_{X|y}$: maximal Helmholtz free entropy under the constraint that $Y = y$.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$Z$: the partition function.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\beta$: inverse temperature.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$N$: number of particles, or some other quantity that can get very large.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$n$: number of dimensions, or some other quantity that is fixed.</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$F$: Helmholtz free energy.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\binom{m}{n}$: <span class="co">[</span><span class="ot">binomial coefficient</span><span class="co">](https://en.wikipedia.org/wiki/Binomial_coefficient)</span>.</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\braket{f(x, y) | z}_y$: The expectation of $f$ where we fix $x$, let $y$ vary, and conditional on $z$.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\mathrm{Var}$: variance.</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\int (\cdots) D<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$: path integral where $x$ varies over the space of all possible paths.</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>As usual, we set $k_B = 1$, so that $\beta = 1/T$, except when we need a numerical answer in SI units.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further readings</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>Unfortunately, I never learned statistical mechanics from any textbook. I just understood things gradually on my own after trying to make sense of things. This means I cannot recommend any introductory textbook based on my personal experience.</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Books I did learn from:</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@sethnaStatisticalMechanicsEntropy2021</span><span class="co">]</span> shows how a modern statistical mechanist thinks. It is a rather eclectic book, because modern statistical mechanics is full of weird applications, from music theory to economics.<span class="ot">[^buzzwords]</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@nelsonBiologicalPhysicsEnergy2003</span><span class="co">]</span> teaches basic statistical thermodynamics in the context of biology, and <span class="co">[</span><span class="ot">@bergRandomWalksBiology1993</span><span class="co">]</span> teaches random walks.</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@ben-naimFarewellEntropyStatistical2008; @feynmanFeynmanLecturesComputation1996</span><span class="co">]</span> show how to combine (unify?) statistical mechanics with information theory.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@jaynesProbabilityTheoryLogic2003</span><span class="co">]</span> gives Jaynes' entire philosophy of information, one application of which is his theory of why entropy is maximized in statistical mechanics.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@lemonsTrailBlackbodyRadiation2022</span><span class="co">]</span> closely follows the story of how Planck actually derived the blackbody radiation law. Reading it, you almost have the illusion that you too could have discovered what he discovered.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@penroseFoundationsStatisticalMechanics2005</span><span class="co">]</span> gives an elegant mathematical deduction that brings philosophers and mathematical logicians joy. However, it is not useful for applications.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Books I did not learn from, but feel obliged to recommend:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@maStatisticalMechanics1985; @tolmanPrinciplesStatisticalMechanics1980; @feynmanStatisticalMechanicsSet2018; @schrodingerStatisticalThermodynamics1989</span><span class="co">]</span> are books that apparently every real physicist must read before they die. Like those other "1001 books you must read before you die", I did not read them.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@nashElementsStatisticalThermodynamics2006</span><span class="co">]</span> is a concise introduction for chemistry students.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span><span class="co">[</span><span class="ot">@sommerfeldLecturesTheoreticalPhysics1950, volume 5</span><span class="co">]</span> is by the master, Sommerfeld. If you want to do 19th century style thermodynamics, then it is very good, but otherwise, I don't know what this book is for.</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ot">[^buzzwords]: </span>The book has quite many buzzwords like "fractals", "complexity", "avalanche", and "edge of chaos", buzzy in the 1990s. A joke is that during the 1980s, as the Cold War was winding down, physicists were overproduced and underemployed, and had to find someway to get employed. Thus, they went into economics, social sciences, etc, resulting in the discipline of "econophysics", the nebulous non-discipline of "complexity studies", etc.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philosophical comments</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>It is fair to say that, although it originated in the 19th century like all other classical fields of physics, statistical mechanics is unsettled. </span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>Trajectory-centric statistical mechanics. In this view, we start with the equations of motion for a physical system, then study statistical properties of individual trajectories, or collections of them. For example, if we have a pendulum hanging in air, being hit by air molecules all the time, we would study the total trajectory $(\theta, x_1, y_1, z_1, x_2, y_2, z_2, \dots)$, where $\theta$ is the angle of the pendulum swing, and $(x_i, y_i, z_i)$ is the location of the $i$-th air molecule. Then we may ask that, over a long enough period, how frequent would the pendulum visit a certain angle range of $<span class="co">[</span><span class="ot">\theta_0, \theta_0 + \delta\theta</span><span class="co">]</span>$:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>Pr(\theta \in <span class="co">[</span><span class="ot">\theta_0, \theta_0 + \delta\theta</span><span class="co">]</span>) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{+T} 1<span class="co">[</span><span class="ot">\theta \in [\theta_0, \theta_0 + \delta\theta]</span><span class="co">]</span> dt</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>In the trajectory-centric view, there are the following issues:</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Problem of ergodicity: When does time-average equal ensemble-average? A system is called "ergodic" iff for almost all starting conditions, the time-average of the trajectory is the ensemble-average over all trajectories.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Problem of entropy: How is entropy defined *on a single trajectory*?</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>H-theorem: In what sense, and under what conditions, does entropy increase?</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Problem of equilibrium: What does it mean to say that a trajectory is *in equilibrium*?</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Approach to equilibrium: In what sense, and under what conditions, does the trajectory converge to an equilibrium?</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Reversibility problem (*Umkehreinwand*): If individual trajectories are reversible, why does entropy increase instead of decrease?</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>While these philosophical problems are quite diverting, we will avoid them as much as possible, because we will be working with the ensemble-centric equilibrium statistical mechanics. This is the statistical mechanics that every working physicist uses, and this is what we will present. If you are interested in the philosophical issues, read the <span class="co">[</span><span class="ot">Stanford Encyclopedia entry on the *Philosophy of Statistical Mechanics*</span><span class="co">](https://plato.stanford.edu/entries/statphys-statmech/)</span>.</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="fu">### Principles of statistical mechanics</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A **physical system** is a classical system with a state space, evolving according to some equation of motion.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>An **ensemble** of that system is a probability distribution over its state space.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The idea of (ensemble-centric) statistical mechanics is to study the evolution of an entire probability distribution over all possible states.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The **entropy** of a probability distribution $\rho$ is</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>$$S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> := -\int dx\; \rho(x) \ln \rho(x)$$</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Under any constraint, there exists a unique ensemble, named the **equilibrium ensemble**, which maximizes entropy under constraint.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>Most of the times, the state space is a phase space, and the equation of motion is described by a Hamiltonian function. However, the machinery of statistical mechanics, as given above, is purely mathematical. It can be used to study any problem in probability whatsoever, even those with no physical meaning.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>Believe it or not, the above constitutes the entirety of equilibrium statistical mechanics. So far, it is a purely mathematical theory, with no falsifiability (Popperians shouting in the background). To make it falsifiable, we need to add one more assumption, necessarily fuzzy:<span class="ot">[^einstein-quote-reality]</span></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The equilibrium ensemble is physically meaningful and describes the observable behavior of physical systems.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>In other words, when a physical system is *at equilibrium*, then everything observable can be found by studying it *as if* it has the maximum entropy distribution under constraint.</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="ot">[^einstein-quote-reality]</span>: </span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>    <span class="at">&gt; As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality. </span></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="at">    &gt; </span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="at">    &gt; --- Albert Einstein, *Address to Prussian Academy of Sciences* (1921)</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>Of course, just what that "is physically meaningful" means, is another source of endless philosophical arguments. I would trust that you will know what is physically meaningful, and leave it at that, while those who have a taste for philosophy can grapple with the <span class="co">[</span><span class="ot">Duhem--Quine thesis</span><span class="co">](https://plato.stanford.edu/entries/scientific-underdetermination/)</span>.</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="fu">### Differential entropy depends on coordinates choice {#sec-differential-entropy-ill-defined}</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>There is a well-known secret among information theorists: differential entropy is ill-defined.</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>Consider the uniform distribution on $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. It is the maximal-entropy distribution on $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ -- relative to the Lebesgue measure. However, why should we pick the Lebesgue measure, and what happens if we don't?</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>Suppose we now stretch the $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ interval nonlinearly, by $f(x) = x^2$, then the maximal-entropy distribution relative to *that* would no longer be the uniform distribution on $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. Instead, it would be the uniform distribution after stretching.</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>The problem is this: Differential entropy is not coordinate-free. If we change the coordinates, we change the base measure, and the differential entropy changes as well.</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>To fix this, we need to use the KL-divergence, which is invariant under a change of base measure, as in</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>$$-D_{KL}(\rho <span class="sc">\|</span> \mu) := - \int dx\; \rho(x) \ln\frac{\rho(x)}{\mu(x)}$$</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>In typical situations, we don't need to worry ourselves with KL-divergence, as we just pick the uniform distribution $\mu$. When the state space is infinite in volume, the uniform distribution is not a probability measure, but it will work. Bayesians say that it is an *improper prior*.</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>In this interpretation, the principle of "maximum entropy distribution under constraint" becomes the principle of "minimal KL-divergence under constraint", which *is* Bayesian inference, with exactly the same formulas.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>In almost all cases, we use the uniform prior over phase space. This is how Gibbs did it, and he didn't really justify it other than saying that *it just works*, and suggesting it has something to do with Liouville's theorem. Now with a century of hindsight, we know that it works because of quantum mechanics: We *should use* the uniform prior over phase space, because phase space volume has a natural unit of measurement: $h^N$, where $h$ is Planck's constant, and $2N$ is the dimension of phase space. As Planck's constant is a universal constant, independent of where we are in phase space, we should weight all of the phase space equally, resulting in a uniform prior.</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a><span class="fu">### Jaynes' epistemological interpretation</span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>The question at the foundation of statistical mechanics is: Why maximize entropy? The practical scientist would say, "Because it works.", and that is well and good, but we will give one possible answer to the why, from the most ardent proponent of maximal entropy theory, <span class="co">[</span><span class="ot">E. T. Jaynes</span><span class="co">](https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes)</span>.</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>According to Jaynes, statistical mechanics is epistemological. That is, probability is not out there in the world, but in here in our minds, and statistical mechanics is nothing more than maximal entropy inference applied to physics. Macroscopic properties are evidences, and the ensemble $\rho$ is the posterior after we incorporate the evidences.</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>It might be difficult to swallow Jaynes' interpretation, as it seems obvious that entropy is objective, but epistemology is subjective. How could he explain objective entropy by subjective information? I might make this more palatable by three examples.</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@jaynesGibbsParadox1992</span><span class="co">]</span>, he proposed the following thought experiment: Suppose we have a tank of neon gas, separated in the middle by a slab, as in the <span class="co">[</span><span class="ot">Gibbs paradox</span><span class="co">](https://en.wikipedia.org/wiki/Gibbs_paradox)</span>. If both sides have the same temperature and pressure, then the system has the same entropy even after we remove the slab. But suddenly, Jaynes' demon tells us that the left side contains neon-20, while the right side contains neon-22, and thus, we were not subject to the Gibbs paradox after all! And so we have just wasted some perfectly good free energy for nothing.</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>We ask, "Wasted? How could we have wasted anything unless there is a practical way to extract energy? You say they are two distinct gases, but what difference does it make if you simply call one side 'neon-20' and the other 'neon-22'?"</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>So Jaynes' demon gives us two membranes, one permeable only to neon-20, while the other only permeable to neon-22. This allows us to put both of them in the middle, and slowly let the two gases diffuse into the middle, extracting mechanical work by the pressure on the two membranes.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>The point of the thought-experiment is that the entropy of a system is, in a practical sense, subjective. The tank of gas *might as well* have maximal entropy if we don't have the two membranes. But as soon as we have the two membranes, it expands our space of possible actions, and previously "lost" work, suddenly becomes extractable, and the entropy of the world drops.</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>The following is a more concrete example from optics. Shoot some hard balls through a bumpy region (an analogy of shining a laser light through a bumpy sheet of glass). The balls would be scattered. It would increase the entropy of the system... unless we reflect them via a corner mirror, then the balls would be reflected right back through the bumpy region, and return to the previous zero-entropy state!</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>Did we violate the second law? Not so, if we think of entropy as a measure of our *actionable ignorance*. Without a corner mirror, we cannot use the detailed information of the scattered ball beam, and have to treat it as essentially random. However, all the detailed information is still there, in the bumpy region and in the scattered beam, and it takes a corner mirror to "unlock" the information for us.</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>Thus, with a corner mirror, the scattered beam still has zero entropy, but without it, the scattered beam has positive entropy.</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@sethnaStatisticalMechanicsEntropy2021, Figure 5.27</span><span class="co">]</span>](figure/Jaynes_irreversible_scattering.png)</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The old adage 'knowledge is power' is a very cogent truth, both in human relations and in thermodynamics.</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- E. T. Jaynes</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>Still, there is a nagging feeling that, even if we have no corner reflector, and so the beam of light truly has increased in entropy *relative to us*, as long as the corner reflector is theoretically possible, then the entropy of the beam of light is still unchanged *in itself*. Isn't this blatant [psychologism](https://en.wikipedia.org/wiki/Psychologism)? Haven't we reduced objective entropy *out there* to subjective information *in here*? Surely even if nobody is around to see it, if a tank of gas explodes in a forest, entropy still makes a sound.</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>Responding to this would entangle us into a whole mess of philosophical arguments about objective vs subjective probability, and observer vs observed phenomena. My quick answer is simply that knowledge isn't magical, and even nature does not laugh at the difficulty of inference.<span class="ot">[^nature-laughs]</span> If the entropy of a tank of gas appears high to us, then chances are, it would appear even higher to a car engine, for the car engine has only a few ways to interact with this tank of gas, unlike us, who have all of modern technology. A car engine can only burn up the tank of gas, but we can distill it, extract it, push it through membranes, etc. The tank of gas has higher entropy *relative to the car engine* -- yes, the car engine has an opinion about the world, as much as we do. It has subjective beliefs about everything it can touch and burn, and we can listen to it with the language of math.</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="ot">[^nature-laughs]</span>: </span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>    Supposedly, Laplace said "Nature laughs at the difficulties of integration.", but when I tried to hunt it down, all citations led directly to a 1953 essay, which cites an anonymous "mathematician". I have tried searching for it in French, with no results. I think this is actually a pseudo-Laplace quote, a <span class="co">[</span><span class="ot">paper ghost</span><span class="co">](https://gwern.net/leprechaun)</span>, much like how people kept attributing things to <span class="co">[</span><span class="ot">pseudo-Aristotle</span><span class="co">](https://en.wikipedia.org/wiki/Pseudo-Aristotle)</span>.</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; It would take a physicist a long time to work out the problem and he could achieve only an approximation at that. Yet presumably the coin will stop exactly where it should. Some very rapid calculations have to be made before it can do so, and they are, presumably, always accurate. And then, just as I was blushing at what I supposed he must regard as my folly, the mathematician came to my rescue by informing me that Laplace had been puzzled by exactly the same fact. "Nature laughs at the difficulties of integration." [@krutchBestTwoWorlds1953, page 148]</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="in">    However, I'm inclined to believe that even nature does not laugh at the difficulties of integration. In fact, one of my hobbies is to "explain" natural laws as ways for nature to avoid really difficult integrations. For example, Newtonian gravity is "explained" by the [Barnes--Hut algorithm](https://en.wikipedia.org/wiki/Barnes%E2%80%93Hut_simulation) that allows n-body gravity to be calculated in $O(n \ln n)$ time.</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>What is the payoff of this long detour? I think it is to provide an intuitive feeling of the identity of physics and information. Information is physical, and physics is informational. If you are a physicist, then this allows you to invade into other fields, much like statistical mechanists "invaded" other fields like artificial intelligence and economics. If you are a mathematician or a computer scientist, then this allows you to translate intuition about physical objects into intuition about high-dimensional probability distributions and large combinatorial objects. And if you are Maxwell's demon, then you won't listen to me -- I would gladly listen to *you*, since magically transforming information and physics back and forth is your entire reason of existence!</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Is entropy unique?"}</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>In the above formulation, maximal entropy inference is interpreted as how rational agents can act optimally under limited information. An alternative viewpoint argues that it is not the entropy that is fundamental, but the argmax of *something* that is fundamental. In this view, if we replaced the entropy function $S[\cdot]$ with a... *kentropy* function $K<span class="co">[</span><span class="ot">\cdot</span><span class="co">]</span>$, such that any scientist who uses reasons about experiments on a system using </span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>$$\rho^* = \argmax_{\rho: \rho\text{ satisfies constraints }C}K<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$$</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>would still satisfy certain axioms of rationality, then $K$ should be as good as $S$.</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>In this vein, there is a Shore--Johnson theorem which shows that if a scientist using a certain kentropy function $K$ would end up satisfying these certain axioms of rationality, then</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>$$\argmax_{\rho: \rho\text{ satisfies constraints }C}S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> = \argmax_{\rho: \rho\text{ satisfies constraints }C}K<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$$</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>In other words, as long as we are doing constrained maximization, the choice of the entropy doesn't matter. In particular, the standard entropy function $S$ is *good enough* -- any constraint-maximizing rational thinker thinks *as if* it is doing constraint-maximizing entropy inference, so we *might as well* use $S$ and stop worrying about alternative ones like $K$.</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>IF you are steeped in Bayesian epistemology, this is in the same vein as those Bayes-theological theorems proving that any rational being must use Bayes theorem for updates. <span class="co">[</span><span class="ot">@pressePrinciplesMaximumEntropy2013</span><span class="co">]</span> Other examples include <span class="co">[</span><span class="ot">Blackwell's informativeness theorem</span><span class="co">](https://en.wikipedia.org/wiki/Blackwell%27s_informativeness_theorem)</span>, <span class="co">[</span><span class="ot">Aumann's agreement theorem</span><span class="co">](https://en.wikipedia.org/wiki/Aumann's_agreement_theorem)</span>, <span class="co">[</span><span class="ot">Cox's theorem</span><span class="co">](https://en.wikipedia.org/wiki/Cox's_theorem)</span>, <span class="co">[</span><span class="ot">Dutch book theorems</span><span class="co">](https://en.wikipedia.org/wiki/Dutch_book_theorems)</span>, etc.</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mathematical developments</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fundamental theorems</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>::: {#thm-liouville}</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="fu">## Liouville's theorem</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>For any phase space and any Hamiltonian over it (which can change with time), phase-space volume is conserved under motion.</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>For any probability distribution $\rho_0$, if after time $t$, it evolves to $\rho_t$, and a point $x(0)$ evolves to $x(t)$, then $\rho_0(x(0)) = \rho_t(x(t))$.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>The proof is found in any textbook, and also <span class="co">[</span><span class="ot">Wikipedia</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Liouville's_theorem_(Hamiltonian)). Since it is already simple enough, and I can't really improve upon it, I won't.</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>::: {#cor-conservation-entropy}</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a><span class="fu">## conservation of entropy</span></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>For a Hamiltonian system, with any Hamiltonian (which can change with time), for any probability distribution $\rho$ over its phase space, its entropy is conserved over time.</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>In particular, we have the following corollary:</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>::: {#cor-ensemble-conservation}</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>Given any set of constraints, if the Hamiltonian preserves these constraints over time, then any constrained-maximal entropy distribution remains constrained-maximal under time-evolution.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>In most cases, the constraint is of a particular form: the expectation is known. In that case, we have the following theorem:</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>::: {#thm-constrained-optimization}</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a><span class="fu">## maximal entropy under linear constraints</span></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>For the following constrained optimization problem</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>\max_\rho S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>\int A_1(x) \rho(x) &amp;= \bar A_1 <span class="sc">\\</span></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>\cdots &amp;= \cdots <span class="sc">\\</span></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>\int A_n(x) \rho(x) &amp;= \bar A_n <span class="sc">\\</span></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>Consider the following ansatz</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>where $Z(a_1, \dots, a_n) = \int e^{-\sum_i a_i A_i(x)} dx$, and $a_1, \dots, a_n$ are chosen such that the constraints $\int A_i(x) \rho(x) = \bar A_i$ are satisfied.</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>*If* the ansatz exists, then it is the unique solution.</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>The ansatz solution is what you get by Lagrangian multipliers. For a refresher, see the <span class="co">[</span><span class="ot">*Analytical Mechanics*#Lagrange's devil at Disneyland</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/analytical-mechanics/index.html#lagranges-devil-at-disneyland)</span>. The theorem shows that the solution is unique -- provided that it exists. Does it exist? Yes, in physics. If it doesn't exist, then we are clearly not modelling a physically real phenomenon.</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>In physics, these are "Boltzmann distributions" or "Gibbs distributions". In statistics, these are <span class="co">[</span><span class="ot">exponential families</span><span class="co">](https://en.wikipedia.org/wiki/Exponential_family)</span>. Because they are everywhere, they have many names.</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>Define a distribution $\rho$ as given in the statement of the theorem. That is,</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>\rho(x) = \frac{1}{Z(a_1, \dots, a_n)} e^{-\sum_i a_i A_i(x)}</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>etc. </span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>Now, it remains to prove that for any other $\rho'$ that satisfies the constraints, we have $S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> \geq S<span class="co">[</span><span class="ot">\rho'</span><span class="co">]</span>$.</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>By routine calculation, for any probability distribution $\rho'$,</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>D_{KL}(\rho' <span class="sc">\|</span> \rho) = -S<span class="co">[</span><span class="ot">\rho'</span><span class="co">]</span> + \sum_i a_i \braket{A_i}_{\rho'} + \ln Z(a_1, \dots, a_n)</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>If $\rho'$ satisfies the given constraints, then $D_{KL}(\rho' <span class="sc">\|</span> \rho) = -S<span class="co">[</span><span class="ot">\rho'</span><span class="co">]</span> + \Const$ where the constant does not depend on $\rho'$, as long as it satisfies the constraints. Therefore, $S<span class="co">[</span><span class="ot">\rho'</span><span class="co">]</span>$ is maximized when $D_{KL}(\rho' <span class="sc">\|</span> \rho)$ is minimized, which is exactly $\rho$.</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>The following proposition is often used when we want to maximize entropy in a two-step process:</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>::: {#thm-compound-entropy}</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a><span class="fu">## compound entropy</span></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>If $\rho_{X,Y}$ is a probability distribution over two variables $(X, Y)$, then</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>$$S<span class="co">[</span><span class="ot">\rho_{X,Y}</span><span class="co">]</span> = S<span class="co">[</span><span class="ot">\rho_Y</span><span class="co">]</span> + \braket{S<span class="co">[</span><span class="ot">\rho_{X|y}</span><span class="co">]</span>}_y$$</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>or more succinctly,</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>$$S_{X,Y} = S_Y + \braket{S_{X|y}}_y$$</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Notations"}</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a>$\rho_Y$ is the probability distribution over $Y$, after we integrate/marginalize $X$ away:</span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>\rho_Y(y) := \int \rho_{X,Y}(x,y)dx</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>$\rho_{X|y}$ is the conditional probability distribution over $X$, conditional on $Y=y$:</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>\rho_{X|y}(x) := \frac{\rho_{X,Y}(x,y)}{\int \rho_{X,Y}(x,y) dx}</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>$\braket{\cdot}_y$ is the expectation over $\rho_Y$:</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>\braket{S_{X|y}}_y := \int S_{X|y} \rho_Y(y)dy</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>Consider a compound system in ensemble $\rho(x, y)$. Its entropy is</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>$$S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> = -\int dxdy \; \rho(x, y) \ln \rho(x, y)$$</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>We can take the calculation in two steps:</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>$$S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> = -\int dxdy \; \rho(x|y)\rho(y) (\ln \rho(x|y) + \ln  \rho(y)) = S<span class="co">[</span><span class="ot">\rho_Y</span><span class="co">]</span> + \braket{S<span class="co">[</span><span class="ot">\rho_{X|y}</span><span class="co">]</span>}_y$$</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>Intuitively, what does $S_{X,Y} = S_Y + \braket{S_{X|y}}_y$ mean? It means that the entropy in $(X, Y)$ can be decomposed into two parts: the part due to $Y$, and the part remaining after we know $Y$, but not yet knowing $X$. In the language of information theory, the total information in $(X, Y)$ is equal to the information in $Y$, plus the information of $X$ conditional over $Y$:</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>I(X, Y) = I(Y) + I(X|Y)</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### Microcanonical ensembles</span></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>If the only constraint is the constant-energy constraint $H(x) = E$, then the maximal entropy distribution is the uniform distribution on the shell of constant energy $H = E$. It is uniform, because once we enforce $H(x) = E$, there are no other constraints, and so by @thm-constrained-optimization, the distribution is uniform. </span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>Thus, we obtain the **microcanonical ensemble**:</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>$$\rho_E(x) \propto 1_{H(x) = E}$$</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>It is sometimes necessary to deal with the "thickness" of the energy shell. In that case, $\rho_E(x) \propto \delta(H(x) - E)$, where $\delta$ is the Dirac delta function.</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>By @thm-constrained-optimization, the microcanonical ensemble is the unique maximizer of entropy under the constraint of constant energy. In particular, if the Hamiltonian does not change over time, then any microcanonical ensemble is preserved over time. In words, if we uniformly "dust" the energy shell of $H(x) = E$ with a cloud of system states, and let all of them evolve over time, then though the dust particles move about, the cloud remains exactly the same.</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>More generally, we can impose more (in)equality constraints, and still obtain a microcanonical ensemble. For example, consider a ball flying around in an empty room with no gravity. The Hamiltonian is $H(q, p) = \frac{p^2}{2m}$, and its microcanonical ensemble is $\rho(q, p) \propto \delta(p = \sqrt{2mE})1<span class="co">[</span><span class="ot">p \in \text{the room}</span><span class="co">]</span>$. That is, its velocity is on the energy shell, while its position is uniform over the entire room.</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>If we want to specify the number of particles for each chemical species, then that can be incorporated into the microcanonical ensemble as well. For example, if we want the number of species $i$ be exactly $N_{i0}$, then we multiply $\rho$ by $1<span class="co">[</span><span class="ot">N_i = N_{i0}</span><span class="co">]</span>$.</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a><span class="fu">### Canonical ensembles</span></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>Consider a small cup of liquid in a giant tank of chemical fluids, or a small lump of air in the whole atmosphere. These are all examples of "a small system in contact with a giant system". In general, if we have a small system connected to a large system, then we typically don't care about the large system, and only want to study the small system's ensemble. How do we do that? Rigorously, we would need to first find the microcanonical ensemble for the total compound small--large system, then take an integral over all states of the large system, resulting in an ensemble over just the small system, as in</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>$$\rho_{\text{small}}(x) = \int \rho_{\text{total}}(x, y) dy$$</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>where $x$ ranges over the states of the small system, and $y$ of the large system.</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>However, this is difficult to perform in general, because the large system, having so many particles, has a huge state space. We cannot do it in general. However, there is an easy way out. Whenever we have a big system changing only a little bit, we can assume linearity. Whenever we have a function $f(x)$ where $x$ changes only a little bit around $x_0$, we can assume $f(x) \approx f(x_0) + f'(x_0) (x - x_0)$. This is the trick that will allow us to solve the problem.</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>Assuming that the energy of the compound system is extensive, we obtain the canonical ensemble. Assuming that the energy and volume are both extensive, we obtain the grand canonical ensemble, etc. The following table would be very useful</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> extensive constraint <span class="pp">|</span> ensemble <span class="pp">|</span> free entropy <span class="pp">|</span></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="pp">| ---</span> <span class="pp">| ---</span> <span class="pp">| ---</span> <span class="pp">|</span></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> none <span class="pp">|</span> microcanonical <span class="pp">|</span> entropy <span class="pp">|</span></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy <span class="pp">|</span> canonical <span class="pp">|</span> Helmholtz free entropy <span class="pp">|</span></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy, volume <span class="pp">|</span> ? <span class="pp">|</span> Gibbs free entropy <span class="pp">|</span></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy, particle count <span class="pp">|</span> grand canonical <span class="pp">|</span> Landau free entropy <span class="pp">|</span></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy, volume, particle count <span class="pp">|</span> ? <span class="pp">|</span> ? <span class="pp">|</span></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a>There are some question marks in the above table, because there are no consensus names for those question marks. What is more surprising is that there is no name for the ensemble of constrained energy and volume. I would have expected something like the "Gibbs ensemble", but history isn't nice to us like that. Well, then I will name it first, as the *big canonical ensemble*. And while we're at it, let's fill the last row as well:</span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> extensive constraint <span class="pp">|</span> ensemble <span class="pp">|</span> free entropy <span class="pp">|</span></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a><span class="pp">| ---</span> <span class="pp">| ---</span> <span class="pp">| ---</span> <span class="pp">|</span></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> none <span class="pp">|</span> microcanonical <span class="pp">|</span> entropy <span class="pp">|</span></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy <span class="pp">|</span> canonical <span class="pp">|</span> Helmholtz free entropy <span class="pp">|</span></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy, volume <span class="pp">|</span> big canonical <span class="pp">|</span> Gibbs free entropy <span class="pp">|</span></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy, particle count <span class="pp">|</span> grand canonical <span class="pp">|</span> Landau free entropy <span class="pp">|</span></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> energy, volume, particle count <span class="pp">|</span> gross canonical <span class="pp">|</span> EVN free energy <span class="pp">|</span></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Extensivity"}</span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>In classical thermodynamics, extensivity means that entropy of the compound system can be calculated in a two-step process: calculate the entropy of each subsystem, then add them up. The important fact is that a subsystem still has enough independence to have its own entropy.</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>This is not always obvious. If we have two galaxies of stars, we can think of each as a "cosmic gas" where each particle is a star. Now, if we put them near each other, then the gravity between the two galaxies would mean it is no longer meaningful to speak of "the entropy of galaxy 1", but only "the entropy of galaxy-compound 1-2".</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>In statistical mechanics, extensivity means a certain property of each subsystem is unaffected by the state of the other subsystems, and the total is the sum of them. So for example, if $A$ is an extensive property, then it means</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>A(x_1, \dots, x_n) = A_1(x_1) + \dots + A_n(x_n)</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>Like most textbooks, we assume extensivity by default, although as we noted in <span class="co">[</span><span class="ot">*Classical Thermodynamics and Economics*</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/equilibrium-thermoeconomics/)</span>, both classical thermodynamics and statistical mechanics do not require extensivity. We assume extensivity because it is mathematically convenient, and good enough for most applications.</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>In the following theorem, we assume that the total system is extensive, and is already in the maximal entropy distribution (microcanonical ensemble)</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>::: {#thm-canonical-ensembles}</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>If the two systems are in energy-contact, and energy is conserved, and energy is extensive, and the compound system is in a microcanonical ensemble, then the small system is in the **canonical ensemble**</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>\rho(x) \propto e^{-\beta H(x)}</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>where $\beta$ is the marginal entropy of energy of the large system:   </span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a>$$\beta := \partial_E S<span class="co">[</span><span class="ot">\rho_{bath, E}</span><span class="co">]</span>$$</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>Similarly, if the two systems are in energy-and-particle-contact, then the small system is in the **grand canonical ensemble**</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>\rho(x) \propto e^{-(\beta H(x) + (-\beta \mu) N(x))}</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>where $-\beta\mu$ is the marginal entropy of particle of the large system:   </span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>$$-\beta\mu := (\partial_N S<span class="co">[</span><span class="ot">\rho_{bath, E, N}</span><span class="co">]</span>)_{E}$$</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a>Most generally, if the two systems are in $q_1, \dots, q_m$ contact, and $q_1, \dots, q_m$ are conserved and extensive quantity, then</span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>$$\rho(x) \propto e^{-\sum_i p_i q_i(x)}$$</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a>where $p_i = (\partial_{q_i} S<span class="co">[</span><span class="ot">\rho_{bath, q}</span><span class="co">]</span>)_{q}$ is the marginal entropy of $q_i$ of the large system.</span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>We prove the case for the canonical ensemble. The other cases are similar.</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>Since the total distribution of the whole system is the maximal entropy distribution, we are faced with a constrained maximization problem:</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a>$$\max_\rho S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$$</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>By @thm-compound-entropy, </span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>$$S = S_{\text{system}}(E_{\text{system}}) + \braket{S_{bath|system}(E_{total} - E_{\text{system}})}_{\text{system}}$$</span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>Since the bath is so much larger than the system, we can take just the first term in its Taylor expansion:</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>$$S_{bath|system}(E_{total} - E_{\text{system}}) = S_{\text{bath}}(E_{total}) - \beta E_{\text{system}}$$</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>where $E_{total}$ is the total energy for the compound system, $\beta = \partial_E S_{\text{bath}}|_{E = E_{total}}$ is the marginal entropy per energy, and $E_{\text{system}}$ is the energy of the system.  </span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>This gives us the linearly constrained maximization problem of</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>$$\max_{\rho_{\text{system}}} (S_{\text{system}} - \beta \braket{E_{\text{system}}}_{\rho_{\text{system}}})$$</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>and we apply Lagrange multipliers to finish the proof.</span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Extensivity, again"}</span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a>Extensivitiy in statistical mechanics yields extensivity in thermodynamics. Specifically, writing $S_{\text{bath}}(E)$, instead of $S_{\text{bath}}(E, E_{\text{system}})$, requires the assumption of extensivity. Precisely because the bath and the system do not affect each other, we are allowed to calculate the entropy of the bath without knowing anything about the energy of the system.</span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>$S_{\text{bath}}$ is the logarithm of the surface area of the energy shell $H_{\text{bath}} = E_{\text{bath}}$. By extensivity, $H(x_{\text{bath}}, x_{\text{system}}) = H_{\text{bath}}(x_{\text{bath}}) + H_{\text{system}}(x_{\text{system}})$, so the energy shells of the bath depends on only $E_{\text{bath}}$, not $E_{\text{system}}$.</span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>The proof showed something extra: If the small system is in distribution $\rho$ that does not equal to the equilibrium distribution $\rho_B$, then the total system's entropy is</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a>$$S = S_{max} - D_{KL}(\rho <span class="sc">\|</span> \rho_B)$$</span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>which is related to of Sanov's theorem and large deviation theory, though I don't know how to make this precise.</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Enthalpic ensemble"}</span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>What if we have a system in volume-contact, but not thermal-contact? This might happen when the system is a flexible bag of gas held in an atmosphere, but the bag is thermally insulating. Notice that in this case, the small system still exchanges energy with the large system via $d\braket{E} = -Pd\braket{V}$. We don't have $E = -PdV$, because the small system might get unlucky. During a moment of weakness, all its particles has abandoned their frontier posts, and the bath has taken advantage of this by encroaching on its land. The system loses volume by $\delta V$, without earning a compensating $\delta E = P \delta V$. In short, the thermodynamic equality $E = -PdV$ is inexact in statistical mechanics, and only holds true on the ensemble average.</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a>In this case, because pressure is a constant, we have $d(E + PV) = 0$, and so we have the enthalpic ensemble $\rho \propto e^{-\beta H}$, where $H := E + PV$ is the <span class="co">[</span><span class="ot">enthalpy</span><span class="co">](https://en.wikipedia.org/wiki/Enthalpy)</span><span class="ot">[^enthalpy-confusion]</span>.</span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a>Specifically, if you work through the same argument, you would end up with the following constrained maximization problem:</span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>\max_{\rho_{\text{system}}} (S_{\text{system}} - \beta \braket{E_{\text{system}}}_{\rho_{\text{system}}} - \beta P \braket{V}) <span class="sc">\\</span></span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>\braket{E_{\text{system}}} + P\braket{V_{\text{system}}} = \Const</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a>yielding the enthalpic ensemble (or the <span class="co">[</span><span class="ot">isoenthalpic-isobaric ensemble</span><span class="co">](https://en.wikipedia.org/wiki/Isoenthalpic%E2%80%93isobaric_ensemble)</span>).</span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a><span class="ot">[^enthalpy-confusion]: </span>Sorry, I know this is not the Hamiltonian, but we are running out of letters to use.</span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a><span class="fu">### Free entropies</span></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>Just like in thermodynamics, it is useful to consider free entropies, which are the convex duals of the entropy:</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Helmholtz free entropy: $f<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> := S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))$.</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Gibbs free entropy: $g<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> := S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> - \beta \braket{E} - \beta P \braket{V}$.</span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Landau free entropy: $\omega<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> := S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> - \beta \braket{E} - \beta (-\mu) \braket{N}$. Note that the sign of $(-\mu)$ is not a typo. It is simply that 19th-century chemists have messed up the sign convention, like how Benjamin Franklin messed up the sign convention of electric charge.</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>Etc. Of those, we would mostly use the Helmholtz free energy, so I will write it down again:</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a>f<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> := S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> - \beta \braket{E} = \int dx \; \rho(x) (-\ln \rho(x) - \beta E(x))</span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>::: {#thm-chain-rule}</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a><span class="fu">## chain rule for free entropies</span></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>$f_X = S_Y + \braket{f_{X|y}}_y$, and similarly $g_X = S_Y + \braket{g_{X|y}}_y$, and similarly for all other free entropies.</span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>  f_X &amp;= S_X - \beta \braket{E}_x  <span class="sc">\\</span></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a>  &amp;= S_Y + \braket{S_{X|y}}_y - \beta \braket{\braket{E}_{x \sim X|y}}_y <span class="sc">\\</span></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>  &amp;= S_Y + \braket{f_{X|y}}_y</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a>A common trick in statistical mechanics is to characterize the same equilibrium in many different perspectives. For example, the canonical ensemble has at least 4 characterizations. "Muscle memory" in statistical mechanics would allow you to nimbly applying the most suitable one for any occasion.</span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>::: {#thm-canonical-characterization}</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a><span class="fu">## 4 characterizations of the canonical ensemble</span></span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>(total entropy under fixed energy constraint) The canonical ensemble maximizes total entropy when the system is in energy-contact with an energy bath that satisfies $\partial_E S_{\text{bath}} = \beta$, under the constraint that $E + E_{\text{bath}}$ is fixed.</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>(entropy under mean energy constraint) Let $E_0$ be a real number, and let $\beta$ be the unique solution to $\int dx \; e^{-\beta E(x)} = E_0$. A system maximizes its entropy under constraint $\braket{E} = E_0$ when it assumes the canonical ensemble with $\beta$.</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>(Boltzann's thermodynamic limit argument): Take $N$ copies of a system, and connect them by energy-contacts. Inject the system with total energy $NE_0$, and let the system reach its microcanonical ensemble. Then at the thermodynamic limit of $N\to \infty$, the distribution of a single system is the canonical distribution with $\beta$ that is the unique solution to $\int dx \; e^{-\beta E(x)} = E_0$.</span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>(free entropy) A system maximizes its Helmholtz free entropy when it assumes the canonical ensemble. At the optimal distribution $\rho^*$, the maximal Helmholtz free entropy is $f[\rho^*] = \ln Z$, where $Z = \int dx \; e^{-\beta E(x)}$ is the partition function.  </span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We already proved this.</span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use the Lagrange multiplier.</span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Isolate one system, and treat the rest as an energy-bath.</span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>$f<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span> = \ln Z - D_{KL}(\rho <span class="sc">\|</span> \rho_B)$.</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a><span class="fu">### The partition function</span></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a>When the system is in a canonical ensemble, we can define a convenient variable $Z = \int dx\; e^{-\beta E(x)}$ called the **partition function**. As proven in @thm-canonical-characterization, the partition function is equal to $e^f$, where $f$ is the Helmholtz free entropy of the canonical ensemble.</span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a>::: {#thm-partition-cumulant}</span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a><span class="fu">## the partition function is the cumulant generating function of energy</span></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a>Let a system be in canonical ensemble with inverse temperature $\beta$, and let $K(t) := \ln \braket{e^{tE}}$ be the <span class="co">[</span><span class="ot">cumulant generating function</span><span class="co">](https://en.wikipedia.org/wiki/Cumulant)</span> of its energy, then</span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>$$K(t) = \ln Z(\beta-t) - \ln Z(\beta)$$</span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a>In particular, the $n$-th cumulant of energy is  </span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>$$\kappa_n(E) = K^{(n)}(t) |_{t=0} = (-\partial_\beta)^n (\ln Z)$$</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>A similar proposition applies for the other ensembles and their free entropies.</span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a>The proof is by direct computation.</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a>For example, the first two cumulants are the mean and variance:</span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>$$\braket{E} = (-\partial_\beta) (\ln Z), \quad \mathrm{Var}(E) = \partial_\beta^2 (\ln Z)$$</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a>Typical systems are made of $N$ particles, where $N$ is large, and that these particles are only weakly interacting. In this case, the total Helmholtz free entropy per particle converges at the thermodynamic limit of $N \to \infty$:</span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a>\lim_N \frac 1N \ln Z \to \bar f_\beta</span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>Thus, for large but finite $N$, we have</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a>$$\braket{E} \approx -N \partial_\beta \bar f_\beta, \quad \mathrm{Var}(E) = N\partial_\beta^2 \bar f_\beta$$</span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a>In particular, the relative fluctuation scales like $\frac{\sqrt{\mathrm{Var}(E)}}{\braket{E}} \sim N^{-1/2}$.</span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditional entropies</span></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>Given any two random variable $X, Y$, and an "observable" variable $Y$ that is determined by $X$ by some function $h$, such that $Y = h(X)$. If we know $X$, we would know $Y$, but it is not so conversely, as multiple $X$ may correspond to the same $Y$. Typically, we use $Y$ as a "summary statistic" for the more detailed, but more complicated $X$. For example, we might have multiple particles in a box, such that $X$ is their individual locations, while $Y$ is their center of mass.</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a>::: {#thm-cond-ent}</span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a><span class="fu">## conditional entropy</span></span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a>Given any random variable $X$, and an "observable" variable $Y$ that is determined by $X$, and some constraints $c$ on $X$, if $X$ is the distribution that maximizes entropy under constraints $c$, with entropy $S_X^*$, then the observable $Y$ is distributed as</span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a>$$\rho_Y^*(y) = e^{S_{X|y}^* - S_X^*}, \quad e^{S_X^*} = \int dy\; e^{S_{X|y}^*}$$</span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a>where $S_{X|y}^*$ is the maximal entropy for $X$ conditional on the same constraints, plus the extra constraint that $Y = y$.</span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>By assumption, $X$ is the unique solution to the constrained optimization problem</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a>    \max S_X <span class="sc">\\</span></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a>    \text{constraints on $x$}</span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>By @thm-compound-entropy, the problem is equivalent to:</span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a>    \max S_Y + \braket{S_{X|y}}_{y\sim Y} <span class="sc">\\</span></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a>    \text{constraints on $x$}</span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>Now, we can solve the original problem in a two-step process: For each possible observable $y\sim Y$, we solve an extra-constrained problem:</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a>    \max S_{X|y} <span class="sc">\\</span></span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>    \text{original constraints on $x$} <span class="sc">\\</span></span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a>    \text{$x$ must be chosen such that the observable $Y = y$}</span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>Then, each such problem gives us a maximal conditional<span class="ot">[^measure-disintegration]</span> entropy $S_{X|y}^*$, and we can follow it up by solving for $Y$ with</span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a>$$\max\lrb{S_Y + \braket{S_{X|y}^*}_{y \sim Y}}$$</span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a><span class="ot">[^measure-disintegration]: </span>If you're a pure mathematician, you can formalize this using <span class="co">[</span><span class="ot">measure disintegration</span><span class="co">](https://en.wikipedia.org/wiki/Disintegration_theorem)</span>.</span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>Again, the solution is immediate once we see it is just the KL-divergence:</span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a>$$S_Y + \braket{S_{X|y}^*}_{y \sim Y} = - \int dy \; \rho_Y(y) \ln\frac{\rho_Y(y)}{e^{S_{X|y}^*}} = \ln Z - D_{KL}(\rho_Y \| \rho_Y^*)$$</span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a>$$Z = \int dy\; e^{S_{X|y}^*}, \quad \rho_Y^*(y) = \frac{e^{S_{X|y}^*}}{Z}$$</span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a>At the optimal point, the entropy for $X$ is maximized at $S_X^* = \ln Z - 0$, so $Z = e^{S_X^*}$.</span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="deriving the canonical ensemble yet again"}</span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a>Consider a small system with energy states $E_1, E_2, \dots$ and a large bath system, in energy contact. We can set $X$ to be the combined state of the whole system, and $Y$ to be the state of the small system. Once we observe $y$, we have fully determined the small system, so the small system has zero entropy, and so all the entropy comes from the bath system:</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a>$$S_{X|y}^* = S_{\text{bath}} = S_{\text{bath}}(E_{total}) - \beta E_y$$</span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a>Consequently, the distribution of the small system is $\rho_Y(y) \propto e^{-\beta E_y}$, as we expect.</span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>A similar calculation gives us the grand canonical ensemble, etc.</span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a>::: {#thm-cond-free-ent}</span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a><span class="fu">## conditional free entropy</span></span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a>Given any random variable $X$, and an "observable" variable $Y$ that is determined by $X$, and some constraints $c$ on $X$, if $X$ is the distribution that maximizes Helmholtz free entropy under constraints $c$, with Helmholtz free entropy $f_X^*$, then the observable $Y$ is distributed as</span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a>$$\rho_Y^*(y) = e^{f_{X|y}^* - f_X^*}, \quad e^{f_X^*} = \int dy\; e^{f_{X|y}^*}$$</span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a>where $f_{X|y}^*$ is the maximal Helmholtz free entropy for $X$ conditional on the same constraints, plus the constraint that $Y = y$.  </span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a>Similarly for Gibbs free entropy, and all other free entropies.</span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a>First note that $f_X = S_Y + \braket{f_{X|y}}_y$, then argue in the same way.</span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kinetic gas theory</span></span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a>Kinetic gas theory is *the* paradigm for pre-1930 statistical mechanics. Boltzmann devoted his best years to kinetic gas theory. The connection between kinetic gas theory and statistical mechanics was so strong that it was often confused as one. Modern statistical mechanics has grown to be so much more than this, so we will only settle for deriving the van der Waals equation. This strikes a balance between triviality (the ideal gas equation could be derived in literally two lines) and complication (Boltzmann's monumental *Lectures on Gas Theory* has 500 pages <span class="co">[</span><span class="ot">@boltzmannLecturesGasTheory2011</span><span class="co">]</span>).</span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a>To review, the van der Waals gas equation is</span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a>$$P = \frac{N/\beta}{V- bN} - \frac{cN^2}{V^2}$$</span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a>where $b, c$ are real numbers that depend on the precise properties of the gas molecules. The term $V - bN$ accounts for the fact that each gas molecule excludes some volume, so that, as $N$ grows, it corrects for the ideal gas pressure $P_{ideal}$ by $\sim P_{ideal}\frac{bN}{V}$. The term $\frac{cN^2}{V^2}$ accounts for overall interaction energy between gas molecules. Suppose the interaction is overall attractive, then we would have $c &gt; 0$, and otherwise $c &lt; 0$.</span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ideal gas</span></span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a>Consider a tank of ideal gas consisting of $N$ point-masses, flying around in a free space with volume $V$. The tank of gas has inverse temperature $\beta$, so its phase-space distribution is</span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a>\rho(q_{1:N}, p_{1:N}) = \prod_{i\in 1:N} \rho(q_i, p_i), \quad \rho(q, p) = \ub{\frac{1}{V}}{free space} \times \ub{\frac{e^{-\beta \frac{<span class="sc">\|</span>p_i<span class="sc">\|</span>^2}{2m}}}{(2\pi m/\beta)^{3/2}}}{Boltzmann momentum distribution}</span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a>The total energy of the gas has no positional term, so it is all due to momentum. Because the momenta coordinates $p_{1,x}, p_{1,y}, \dots, p_{N,y}, p_{N,z}$ do not interact, their kinetic energies simply sum, giving </span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a>U = 3N \times \int_{\R}dp\; \frac{p^2}{2m} \frac{e^{-\frac{p^2}{2m/\beta}}}{\sqrt{2\pi m/\beta}} = \frac{3N}{2\beta}</span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a>This is the same as Boltzmann's derivation so far. However, although entropy is exactly defined when there are only finitely or countably many possible states, as $\sum_{j \in \N} -p_j \ln p_j$, this is not so when state space is uncountably large, like $\R^{6N}$. When Boltzmann encountered the issue, he solved it by discretizing the phase space into *arbitrary but small* cubes. The effect is that he could rederive the ideal gas laws, but the entropy has an additive constant that depends on the exact choice of the cube size. This was not a problem for Boltzmann, who was trying to found classical thermodynamics upon statistical mechanics, and in classical thermodynamics, entropy *does* have an indeterminate additive constant.</span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a>Later, Planck in his derivation of the blackbody radiation law, used the same trick. Ironically, Planck did not believe in atoms nor quantized light, but he did make the correct assumption that there is a natural unit of measurement for phase space area, which he called $h$, and which we know as Planck's constant. <span class="co">[</span><span class="ot">@duncanConstructingQuantumMechanics2019, chapter 2</span><span class="co">]</span>.</span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a>Following Planck, we discretize the phase space into little cubes of size $h^{3N}$, and continue:</span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a>    S &amp;= -\sum_{i \in\text{Little cubes}} p_i \ln p_i <span class="sc">\\</span></span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a>    &amp;\approx -\sum_{i \in\text{Little cubes}} (\rho(i) h^{3N}) \ln (\rho(i) h^{3N}) <span class="sc">\\</span></span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a>    &amp;\approx -\int_{\R^{6N}} dp_{1:N}dq_{1:N} \; \rho(p_{1:N}, q_{1:N}) \ln (\rho(p_{1:N}, q_{1:N}) h^{3N}) <span class="sc">\\</span></span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a>    &amp;= -\int_{\R^{6N}} dp_{1:N}dq_{1:N} \; \rho(p_{1:N}, q_{1:N}) \ln \rho(p_{1:N}, q_{1:N}) - 3N \ln h <span class="sc">\\</span></span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a>    &amp;= -\ub{N\int_{\R^{6}} dpdq \; \rho(p, q) \ln \rho(p, q)}{non-interacting particles} - 3N \ln h</span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a>Now, the entropy of a single atom $\int_{\R^{6}} dpdq \; \rho(p, q) \ln \rho(p, q)$ factors again into one position space and three momentum spaces:</span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a>    -\int_{\R^{6}} dpdq \; \rho(p, q) \ln \rho(p, q) &amp;= -\int_{\R^3} dq \rho(q) \ln \rho(q) - \sum_{i = x, y, z} \int_{\R} dp_i \ln \rho(p_i) <span class="sc">\\</span></span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>    &amp;= \ln V + 3 \times \ub{(\text{entropy of }\mathcal N(0, m/\beta))}{check Wikipedia} <span class="sc">\\</span></span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a>    &amp;= \ln V + \frac 32 \ln(2\pi m/\beta) + \frac 32 <span class="sc">\\</span></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a>Does this remind you of our previous discussion about how <span class="co">[</span><span class="ot">differential entropy is ill-defined</span><span class="co">](#sec-differential-entropy-ill-defined)</span>? Finally that discussion is paying off! The choice of a natural unit of measurement in phase space is *equivalent* to fixing a natural base measure on phase space, such that differential entropy becomes well-defined.</span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a>The above is not yet correct, because permuting the atoms does not matter. That is, we have grossly inflated the state space. For example, if $N = 2$, then we have counted the state $(q_1, p_1, q_2, p_2)$, then $(q_2, p_2, q_1, p_1)$, as if they are different, but they must be counted as the same. We must remove this redundancy by "quotienting out" the permutation group over the particles. The effect is dividing the phase space by $\ln N!$:</span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a>    \frac SN &amp;= \ln V + \frac 32 \ln(2\pi m/\beta) + \frac 32 - 3 \ln h - \ub{\frac 1N \ln N!}{Stirling's approximation} <span class="sc">\\</span></span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a>    &amp;= \ln\lrs{\frac{V}{N} \lrb{\frac{2\pi m}{\beta h^2}}^{\frac 32}} + \frac 52</span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a>giving us the **Sackur--Tetrode formula**:</span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a>S(U, V, N) = \ln\lrs{\frac{V}{N} \lrb{\frac{4\pi m U}{3N h^2}}^{\frac 32}} + \frac 52</span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a>All other thermodynamic quantities can then be derived from this. For example, the pressure is </span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a>$$P = \beta^{-1}(\partial_V S)_{U, N} = \frac{1}{\beta V}$$</span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a>more conventionally written as $PV = \beta^{-1} = Nk_BT$, the **ideal gas equation**, where we have re-inserted the Boltzmann constant in respect for tradition.</span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="How Tetrode measured $h$"}</span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a>In early 1900s, Walther Nernst proposed the third law of thermodynamics. The history is rather messy, but suffice to say that the version we are going to care about says, "At the absolute zero of temperature the entropy of every chemically homogeneous solid or liquid body has a zero value.". In support, he studied experimentally the thermodynamic properties of many materials at temperatures approaching absolute zero. He had a hydrogen liquefier and could reach around $20 \ut{K}$.</span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a>Working on the assumption that $S = 0$ in any chemical at $T = 0$, he could measure the entropy of any substance by slowing heating up a substance (or cooling down), measuring its heat capacity at all temperatures, then take an integral:</span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a>k_B S = \int \frac{CdT}{T}</span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a>The low-temperature data for mercury was the most available (mercury was also the substance with which Onnes discovered superconductivity). However, mercury is mostly in a liquid form at low temperatures. Fortunately, the latent heat of vaporization $\Delta L$ can be measured, and then we can get </span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a>S_{\text{vapor}} = S_{\text{liquid}} + \frac{\Delta L}{k_BT}</span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a>Back then, $k_B = \frac{\text{Gas constant}}{\text{Avogadro constant}}$, and the $S_{\text{liquid}}, \Delta L$ of mercury were all measured, so combining these, Tetrode calculated a value of $h$ that is within $30\%$ of modern measurement. <span class="co">[</span><span class="ot">@grimus100thAnniversarySackur2013</span><span class="co">]</span></span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ideal gas (again)</span></span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a>We rederive the thermodynamic properties of ideal monoatomic gas via Helmholtz free entropy.</span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a>$$Z = \int e^{-\beta E} = \ub{\frac{1}{N!}}{identical particles} \ub{V^N}{position} \ub{(2\pi m/\beta )^{\frac 32 N}}{momentum}$$</span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a>In typical textbooks, they use the Helmholtz free energy, which is defined as</span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a>F = -\beta^{-1} \ln Z = -\beta^{-1} N \lrb{\ln \frac{V}{N} + \frac 32 \ln \frac{2\pi m}{\beta} + \frac{\ln N}{2N}}</span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a>By the same formula from classical thermodynamics,</span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a>d\ln Z = -\braket{E}d\beta + \beta\braket{P} dV \implies </span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a>    \braket{E}   &amp;= \frac 32 \frac{N}{\beta} <span class="sc">\\</span></span>
<span id="cb1-811"><a href="#cb1-811" aria-hidden="true" tabindex="-1"></a>    \braket{P}V  &amp;= \frac{N}{\beta}</span>
<span id="cb1-812"><a href="#cb1-812" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-813"><a href="#cb1-813" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-814"><a href="#cb1-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-815"><a href="#cb1-815" aria-hidden="true" tabindex="-1"></a>Notice how the $\ln N!$ part simply does not matter in this case.</span>
<span id="cb1-816"><a href="#cb1-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-817"><a href="#cb1-817" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hard ball gas (dilute gas limit)</span></span>
<span id="cb1-818"><a href="#cb1-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-819"><a href="#cb1-819" aria-hidden="true" tabindex="-1"></a>In order to refine the approach, we need to account for two effects. </span>
<span id="cb1-820"><a href="#cb1-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-821"><a href="#cb1-821" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Each particle takes up finite volume, which forces the total volume of positional space to be smaller than $V^N$.</span>
<span id="cb1-822"><a href="#cb1-822" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Particle pairs have interactions, which changes the Boltzmann distribution.</span>
<span id="cb1-823"><a href="#cb1-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-824"><a href="#cb1-824" aria-hidden="true" tabindex="-1"></a>The first effect can be modelled by assuming each atom is a hard ball of radius $r$. The particles still have no interaction *except* that their positions cannot come closer than $2r$. </span>
<span id="cb1-825"><a href="#cb1-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-826"><a href="#cb1-826" aria-hidden="true" tabindex="-1"></a>Because there is no potential energy, the Boltzmann distribution on momentum space is the same, and so the Helmholtz free entropy $\ln Z$ still splits into the sum of positional entropy and momentous entropy. The momentum part is still $\frac 32 N \ln\frac{2\pi}{\beta m}$, as the hard balls do not interfere with each other's momentum, but the position part is smaller, because the balls mutually exclude each other.</span>
<span id="cb1-827"><a href="#cb1-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-828"><a href="#cb1-828" aria-hidden="true" tabindex="-1"></a>Let $a = 8V_{ball} = \frac{32}{3}\pi r^3$ be a constant for the gas. </span>
<span id="cb1-829"><a href="#cb1-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-830"><a href="#cb1-830" aria-hidden="true" tabindex="-1"></a>To measure the volume of the diminished position space, we can add one hard ball at a time. The first hard ball can take one of $V$ possible positions, as before. The next ball's center cannot be within $2r$ of the center of the first ball, so its position can only take one of $(V - a)$ positions, where $a = 8V_{ball} = \frac{32}{3}\pi r^3$ is a constant that depends on the shape of the hard balls. We continue this argument, obtaining the total volume in position space:</span>
<span id="cb1-831"><a href="#cb1-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-832"><a href="#cb1-832" aria-hidden="true" tabindex="-1"></a>$$V(V- a) \cdots (V - (N-1)a) \approx V^N e^{0 -\frac{a}{V}-2\frac{a}{V} -\dots -(N-1)\frac{a}{V}} \approx V^N\left(1- \frac{N^2 a}{2V} \right)$$</span>
<span id="cb1-833"><a href="#cb1-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-834"><a href="#cb1-834" aria-hidden="true" tabindex="-1"></a>This gives us</span>
<span id="cb1-835"><a href="#cb1-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-836"><a href="#cb1-836" aria-hidden="true" tabindex="-1"></a>$$\braket{E} = \frac{3N}{2\beta}, \quad \braket{P}V \approx \frac N\beta \left(1 + \frac{a N}{2V}\right) \approx \frac{N/\beta}{V - \frac a2 N}$$</span>
<span id="cb1-837"><a href="#cb1-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-838"><a href="#cb1-838" aria-hidden="true" tabindex="-1"></a>The second equation is the van der Waals equation when the term $c = 0$, meaning there is neither attraction nor repulsion between particles.</span>
<span id="cb1-839"><a href="#cb1-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-840"><a href="#cb1-840" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="virial expansion"}</span>
<span id="cb1-841"><a href="#cb1-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-842"><a href="#cb1-842" aria-hidden="true" tabindex="-1"></a>In the above derivation, we are assuming that only pairwise exclusion matters. That is, we ignore the possibility that three or more balls may simultaneously intersecting each other. We can make a more accurate counting argument via the <span class="co">[</span><span class="ot">inclusion-exclusion principle</span><span class="co">](https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle)</span>, which would lead us to a <span class="co">[</span><span class="ot">virial expansion</span><span class="co">](https://en.wikipedia.org/wiki/Virial_expansion)</span> for gas.</span>
<span id="cb1-843"><a href="#cb1-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-844"><a href="#cb1-844" aria-hidden="true" tabindex="-1"></a>Specifically, if the balls $A, B$ are intersecting, which has probability $a/V$, and $B, C$ are also intersecting, also with probability $a/V$, then $A, C$ are quite likely to be also intersecting, with probability much higher than $a/V$. Therefore, if we have excluded the cases where $A, B$ are intersecting by subtracting with $a/V$, and the cases where $B, C$ are intersecting by subtracting another $a/V$, then we should be subtracting with something less than $a/V$. The cluster expansion principle makes this precise. Unfortunately, it requires some difficult combinatorics. The interested reader should study <span class="co">[</span><span class="ot">@andersenClusterMethodsEquilibrium1977</span><span class="co">]</span>.</span>
<span id="cb1-845"><a href="#cb1-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-846"><a href="#cb1-846" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-847"><a href="#cb1-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-848"><a href="#cb1-848" aria-hidden="true" tabindex="-1"></a><span class="fu">### Soft ball gas (high temperature and dilute gas limit)</span></span>
<span id="cb1-849"><a href="#cb1-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-850"><a href="#cb1-850" aria-hidden="true" tabindex="-1"></a>In the above derivation, we got one part of van der Waals equation right -- the part where particles take up space. However, we have not yet accounted for the force between particles. We expect that if the particles attract each other, then $P$ should be smaller, and if the particles repel each other, then $P$ should be larger.</span>
<span id="cb1-851"><a href="#cb1-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-852"><a href="#cb1-852" aria-hidden="true" tabindex="-1"></a>Let's assume the gas is made of balls that has a hard core and a soft aura. That is, they repulse or attract each other at a distance, and when a pair comes too close. We also assume the force law depends only on the distances between particles.</span>
<span id="cb1-853"><a href="#cb1-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-854"><a href="#cb1-854" aria-hidden="true" tabindex="-1"></a>That is, we can write such a system as having a gas potential energy $V(q_1, \dots, q_N) = \sum_{i &lt; j} V(<span class="sc">\|</span>q_i - q_j<span class="sc">\|</span>)$. To enforce the hard core, we should have $V(r) = \infty$ when $r \in <span class="co">[</span><span class="ot">0, r_0</span><span class="co">]</span>$.</span>
<span id="cb1-855"><a href="#cb1-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-856"><a href="#cb1-856" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Example potential energy field. This is the [Lennard-Jones potential](https://en.wikipedia.org/wiki/Lennard-Jones_potential), with a hard (not perfectly hard, but hard enough!) exclusive core, a soft repelling middle, and an attraction when far away. Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Graph_of_Lennard-Jones_potential.png)</span><span class="co">](figure/Lennard-Jones_potential.png)</span></span>
<span id="cb1-857"><a href="#cb1-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-858"><a href="#cb1-858" aria-hidden="true" tabindex="-1"></a>Now, the partition function becomes</span>
<span id="cb1-859"><a href="#cb1-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-860"><a href="#cb1-860" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-861"><a href="#cb1-861" aria-hidden="true" tabindex="-1"></a>Z = \int e^{-\beta\sum_i \frac{p_i^2}{2m} - \beta\sum_{i &lt; j}V(<span class="sc">\|</span>q_i - q_j<span class="sc">\|</span>)} dqdp</span>
<span id="cb1-862"><a href="#cb1-862" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-863"><a href="#cb1-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-864"><a href="#cb1-864" aria-hidden="true" tabindex="-1"></a>The momentum part is still the same $(2\pi/\beta m)^{\frac 32 N}$, but the position part is more difficult now. Still, we hope it will be close to $V^N$.</span>
<span id="cb1-865"><a href="#cb1-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-866"><a href="#cb1-866" aria-hidden="true" tabindex="-1"></a>That is, we need to calculate:</span>
<span id="cb1-867"><a href="#cb1-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-868"><a href="#cb1-868" aria-hidden="true" tabindex="-1"></a>$$Z = \ub{V^N (2\pi/\beta m)^{\frac 32 N} \frac{1}{V^N}}{ideal gas} \int_{V^N} e^{ - \beta\sum_{i &lt; j}V(<span class="sc">\|</span>q_i - q_j<span class="sc">\|</span>)} dq$$</span>
<span id="cb1-869"><a href="#cb1-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-870"><a href="#cb1-870" aria-hidden="true" tabindex="-1"></a>The integral $\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(<span class="sc">\|</span>q_i - q_j<span class="sc">\|</span>)} dq$ can be evaluated piece-by-piece:</span>
<span id="cb1-871"><a href="#cb1-871" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-872"><a href="#cb1-872" aria-hidden="true" tabindex="-1"></a>\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(<span class="sc">\|</span>q_i - q_j<span class="sc">\|</span>)} dq = \int dq_1 \left(\int dq_2 \; e^{-\beta V(<span class="sc">\|</span> q_1 - q_2 <span class="sc">\|</span>)} \left(\int dq_3 \; e^{-\beta (V(<span class="sc">\|</span> q_1 - q_3 <span class="sc">\|</span>) + V(<span class="sc">\|</span> q_2 - q_3 <span class="sc">\|</span>))} \cdots\right)\right)</span>
<span id="cb1-873"><a href="#cb1-873" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-874"><a href="#cb1-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-875"><a href="#cb1-875" aria-hidden="true" tabindex="-1"></a>Because the chamber is so much larger than the molecular force-field, it is basically infinite. So for almost all of $q_1$ (except when it is right at the walls of the chamber), $\int dq_2 \; e^{-\beta V(<span class="sc">\|</span> q_1 - q_2 <span class="sc">\|</span>)} \approx V - \delta$, where $\delta$ is some residual volume:</span>
<span id="cb1-876"><a href="#cb1-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-877"><a href="#cb1-877" aria-hidden="true" tabindex="-1"></a>$$\delta := \int_{V} dq_2 \; (1 - e^{-\beta V(<span class="sc">\|</span>q_2 <span class="sc">\|</span>)})$$</span>
<span id="cb1-878"><a href="#cb1-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-879"><a href="#cb1-879" aria-hidden="true" tabindex="-1"></a>Furthermore, because we are dealing with a dilute gas, the higher-order interactions don't matter (see previous remark about the virial expansion). Therefore, the integral</span>
<span id="cb1-880"><a href="#cb1-880" aria-hidden="true" tabindex="-1"></a>$$\int_{V} dq_3 \; e^{-\beta (V(<span class="sc">\|</span> q_1 - q_3 <span class="sc">\|</span>) + V(<span class="sc">\|</span> q_2 - q_3 <span class="sc">\|</span>))} \approx \int_{V_1 \cup V_2 \cup V_3} dq_3 \; e^{-\beta (V(<span class="sc">\|</span> q_1 - q_3 <span class="sc">\|</span>) + V(<span class="sc">\|</span> q_2 - q_3 <span class="sc">\|</span>))} $$</span>
<span id="cb1-881"><a href="#cb1-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-882"><a href="#cb1-882" aria-hidden="true" tabindex="-1"></a>where $V_1$ is the "turf" of particle $1$, and $V_2$ is the turf of particle $2$, and $V_3$ is the rest of the volume. Because the gas is dilute, we have basically $V_1$ disjoint from $V_2$, giving us  </span>
<span id="cb1-883"><a href="#cb1-883" aria-hidden="true" tabindex="-1"></a>$$\approx \sum_{j = 1, 2, 3}\int_{V_j} dq_3\; e^{-\beta V(<span class="sc">\|</span> q_j - q_3 <span class="sc">\|</span>)} \approx V - 2\delta$$</span>
<span id="cb1-884"><a href="#cb1-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-885"><a href="#cb1-885" aria-hidden="true" tabindex="-1"></a>Together, we have</span>
<span id="cb1-886"><a href="#cb1-886" aria-hidden="true" tabindex="-1"></a>$$\int_{V^N} e^{ - \beta\sum_{i &lt; j}V(<span class="sc">\|</span>q_i - q_j<span class="sc">\|</span>)} dq  \approx V(V-\delta) \cdots(V - (N-1)\delta)$$</span>
<span id="cb1-887"><a href="#cb1-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-888"><a href="#cb1-888" aria-hidden="true" tabindex="-1"></a>Giving us</span>
<span id="cb1-889"><a href="#cb1-889" aria-hidden="true" tabindex="-1"></a>$$\ln Z \approx \ln Z_{\text{ideal}} - \frac{N^2 \delta}{2V}$$</span>
<span id="cb1-890"><a href="#cb1-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-891"><a href="#cb1-891" aria-hidden="true" tabindex="-1"></a>It remains to calculate the residual volume. It has two parts, one due to the hard core and one due to the soft halo:</span>
<span id="cb1-892"><a href="#cb1-892" aria-hidden="true" tabindex="-1"></a>$$\delta = \int_{<span class="sc">\|</span>q_2 <span class="sc">\|</span> \leq r_0} dq_2 \; (1 - e^{-\infty}) + \int_{<span class="sc">\|</span>q_2 <span class="sc">\|</span> &gt; r_0} dq_2 \; (1 - e^{-\beta V(<span class="sc">\|</span>q_2<span class="sc">\|</span>)})$$</span>
<span id="cb1-893"><a href="#cb1-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-894"><a href="#cb1-894" aria-hidden="true" tabindex="-1"></a>The first part is just $a$, as calculated previously. The second part depends on the exact shape of the potential well. However, when temperature is high, $\beta$ would be very small, so the second part is approximately $\int dq_2 (\beta V)$, which is a constant times $\beta$.</span>
<span id="cb1-895"><a href="#cb1-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-896"><a href="#cb1-896" aria-hidden="true" tabindex="-1"></a>Thus, we have</span>
<span id="cb1-897"><a href="#cb1-897" aria-hidden="true" tabindex="-1"></a>$$\ln Z \approx \ln Z_{\text{ideal}} - \frac{N^2}{V}(a + b \beta)$$</span>
<span id="cb1-898"><a href="#cb1-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-899"><a href="#cb1-899" aria-hidden="true" tabindex="-1"></a>for some constants $a, b$. This gives us the van der Waals equation:</span>
<span id="cb1-900"><a href="#cb1-900" aria-hidden="true" tabindex="-1"></a>$$\braket{P} V = \frac{N}{\beta} + \frac{N^2}{\beta V}a + \frac{N^2}{V} b$$</span>
<span id="cb1-901"><a href="#cb1-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-902"><a href="#cb1-902" aria-hidden="true" tabindex="-1"></a><span class="fu">## Other classical examples</span></span>
<span id="cb1-903"><a href="#cb1-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-904"><a href="#cb1-904" aria-hidden="true" tabindex="-1"></a><span class="fu">### Countably many states</span></span>
<span id="cb1-905"><a href="#cb1-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-906"><a href="#cb1-906" aria-hidden="true" tabindex="-1"></a>In a surprising number of applications, we have a single system in an energy bath. The system has finitely many, or countably infinitely many, distinguishable states, each with a definite energy: $E_0 \leq E_1 \leq E_2 \leq \cdots$. In particular, this covers most of the basic examples from quantum mechanics. In such a system, the probability of being in state $i$ is $p_i = \frac 1Z e^{-\beta E_i}$ where</span>
<span id="cb1-907"><a href="#cb1-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-908"><a href="#cb1-908" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-909"><a href="#cb1-909" aria-hidden="true" tabindex="-1"></a>Z = \sum_i e^{-\beta E_i}</span>
<span id="cb1-910"><a href="#cb1-910" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-911"><a href="#cb1-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-912"><a href="#cb1-912" aria-hidden="true" tabindex="-1"></a>Because I don't like sections that are literally two paragraphs long, I will reformulate this as multinomial regression in mathematical statistics.</span>
<span id="cb1-913"><a href="#cb1-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-914"><a href="#cb1-914" aria-hidden="true" tabindex="-1"></a>In the problem of classification, we observe some vector $\vec X$, and we need to classify it into one of finitely many states $<span class="sc">\{</span>1, 2, \dots<span class="sc">\}</span>$. With multinomial regression, we construct one vector $\vec b_i$ for each possible state $i$, and then declare that the probability of being in state $k$ is</span>
<span id="cb1-915"><a href="#cb1-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-916"><a href="#cb1-916" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-917"><a href="#cb1-917" aria-hidden="true" tabindex="-1"></a>Pr(i | X) = \frac{e^{-\vec X \cdot \vec b_i}}{Z(X)}, \quad Z(\vec X) = \sum_j e^{-\vec b_j \cdot \vec X}</span>
<span id="cb1-918"><a href="#cb1-918" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-919"><a href="#cb1-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-920"><a href="#cb1-920" aria-hidden="true" tabindex="-1"></a>To make the parallel clearer:</span>
<span id="cb1-921"><a href="#cb1-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-922"><a href="#cb1-922" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-923"><a href="#cb1-923" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-924"><a href="#cb1-924" aria-hidden="true" tabindex="-1"></a>\text{log probability} &amp; &amp; \text{observable } &amp; &amp;\text{ feature} &amp; &amp; \text{normalization constant} <span class="sc">\\</span></span>
<span id="cb1-925"><a href="#cb1-925" aria-hidden="true" tabindex="-1"></a>    \ln p(i | \beta) &amp;=&amp; -\beta &amp; \; &amp; E_i &amp; &amp; - \ln Z <span class="sc">\\</span></span>
<span id="cb1-926"><a href="#cb1-926" aria-hidden="true" tabindex="-1"></a>    \ln Pr(i | \vec X) &amp;=&amp;    -\vec X     &amp; \cdot &amp; \vec b_i &amp; &amp; - \ln Z</span>
<span id="cb1-927"><a href="#cb1-927" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-928"><a href="#cb1-928" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-929"><a href="#cb1-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-930"><a href="#cb1-930" aria-hidden="true" tabindex="-1"></a>We can make the analogy exact by adding multiple observables. Specifically, if we solve the following constrained optimization problem</span>
<span id="cb1-931"><a href="#cb1-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-932"><a href="#cb1-932" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-933"><a href="#cb1-933" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-934"><a href="#cb1-934" aria-hidden="true" tabindex="-1"></a>\max S <span class="sc">\\</span></span>
<span id="cb1-935"><a href="#cb1-935" aria-hidden="true" tabindex="-1"></a>\braket{\vec b} = \vec b_0</span>
<span id="cb1-936"><a href="#cb1-936" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-937"><a href="#cb1-937" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-938"><a href="#cb1-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-939"><a href="#cb1-939" aria-hidden="true" tabindex="-1"></a>then the solution is a multinomial classifier, with $\vec X$ playing the role of $\beta$.</span>
<span id="cb1-940"><a href="#cb1-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-941"><a href="#cb1-941" aria-hidden="true" tabindex="-1"></a>Interpreting the physics as statistics, we can think of $\beta$ as an "observable". It is as if we are asking the physical system "What state are you in?" but we can only ask a very crude question "What is your energy on average?" Knowing that, we can make a reasonable guess by using the maximal entropy compatible with that answer.</span>
<span id="cb1-942"><a href="#cb1-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-943"><a href="#cb1-943" aria-hidden="true" tabindex="-1"></a>Interpreting the statistics as physics, we can think of the observable $\vec X$ as "entropic forces", trying to push the system towards the distribution of maximal entropy. At the equilibrium of zero entropic force, we have a multinomial classifier. This is the prototypical idea of <span class="co">[</span><span class="ot">energy-based statistical modelling</span><span class="co">](https://en.wikipedia.org/wiki/Energy-based_model)</span>.</span>
<span id="cb1-944"><a href="#cb1-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-945"><a href="#cb1-945" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fluctuation by $N^{-1/2}$</span></span>
<span id="cb1-946"><a href="#cb1-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-947"><a href="#cb1-947" aria-hidden="true" tabindex="-1"></a>Suppose we have several tanks of oxygen gas that can exchange energy. They are in a microcanonical ensemble. Now, if we measure the total energy in the first tank, we would get a value $E_1$. We sample it again after a while, and we would get another value. Averaging them, we would get $\braket{E_1}$, which ought to match the prediction by classical thermodynamics. However, if thermodynamics is the theory of the average, then to go beyond it, statistical mechanics must predict the variance as well.</span>
<span id="cb1-948"><a href="#cb1-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-949"><a href="#cb1-949" aria-hidden="true" tabindex="-1"></a>In @thm-partition-cumulant, We had already seen that the partition function generates the mean, the variance, and all other terms. Here we expand on this.</span>
<span id="cb1-950"><a href="#cb1-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-951"><a href="#cb1-951" aria-hidden="true" tabindex="-1"></a>Take several systems, and let them exchange energy, but nothing else. For concreteness, we can imagine taking several copper tanks of gas, and let them touch each other. The tanks hold their shape, not expanding or contracting. The system has total entropy </span>
<span id="cb1-952"><a href="#cb1-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-953"><a href="#cb1-953" aria-hidden="true" tabindex="-1"></a>$$S = \sum_i S_i(E_i, A_i)$$</span>
<span id="cb1-954"><a href="#cb1-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-955"><a href="#cb1-955" aria-hidden="true" tabindex="-1"></a>where $A_i$ stand for the other state variables we don't care about, because they are held constant. Now, there is a single constraint of constant total energy:</span>
<span id="cb1-956"><a href="#cb1-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-957"><a href="#cb1-957" aria-hidden="true" tabindex="-1"></a>$$E = \sum_i E_i$$</span>
<span id="cb1-958"><a href="#cb1-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-959"><a href="#cb1-959" aria-hidden="true" tabindex="-1"></a>In the thermodynamical limit, the compound system reaches the maximal entropy state $E_1^*, \dots, E_n^*$, which solves the following constrained maximization</span>
<span id="cb1-960"><a href="#cb1-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-961"><a href="#cb1-961" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-962"><a href="#cb1-962" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-963"><a href="#cb1-963" aria-hidden="true" tabindex="-1"></a>    \max \sum_i S_i(E_i, A_i)<span class="sc">\\</span></span>
<span id="cb1-964"><a href="#cb1-964" aria-hidden="true" tabindex="-1"></a>    E = \sum_i E_i</span>
<span id="cb1-965"><a href="#cb1-965" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-966"><a href="#cb1-966" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-967"><a href="#cb1-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-968"><a href="#cb1-968" aria-hidden="true" tabindex="-1"></a>By calculus, at the optimal point, all systems satisfy</span>
<span id="cb1-969"><a href="#cb1-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-970"><a href="#cb1-970" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-971"><a href="#cb1-971" aria-hidden="true" tabindex="-1"></a>(\partial_{E_i} S_i)_{A_i} = \beta</span>
<span id="cb1-972"><a href="#cb1-972" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-973"><a href="#cb1-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-974"><a href="#cb1-974" aria-hidden="true" tabindex="-1"></a>for some number $\beta$. This is the *zeroth law of thermodynamics*.</span>
<span id="cb1-975"><a href="#cb1-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-976"><a href="#cb1-976" aria-hidden="true" tabindex="-1"></a>However, we are in statistical mechanics, so the compound system actually does not stay exactly at the optimal point. Instead, the energy levels fluctuate. Write the **fluctuation vector** as </span>
<span id="cb1-977"><a href="#cb1-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-978"><a href="#cb1-978" aria-hidden="true" tabindex="-1"></a>$$Y = (\Delta E_1, \dots, \Delta E_n)$$</span>
<span id="cb1-979"><a href="#cb1-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-980"><a href="#cb1-980" aria-hidden="true" tabindex="-1"></a>which satisfies the constraint $\sum_i \Delta E_i = 0$.  </span>
<span id="cb1-981"><a href="#cb1-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-982"><a href="#cb1-982" aria-hidden="true" tabindex="-1"></a>Suppose we observe the fluctuation vector to be a certain value $Y = y$, then by @thm-cond-ent, </span>
<span id="cb1-983"><a href="#cb1-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-984"><a href="#cb1-984" aria-hidden="true" tabindex="-1"></a>$$\rho_Y(y) \propto e^{S^*_{X|y}}$$</span>
<span id="cb1-985"><a href="#cb1-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-986"><a href="#cb1-986" aria-hidden="true" tabindex="-1"></a>where $S^*_{X|y}$ is the entropy of the compound system, given $Y = y$. For small fluctuations, we do a Taylor expansion:</span>
<span id="cb1-987"><a href="#cb1-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-988"><a href="#cb1-988" aria-hidden="true" tabindex="-1"></a>$$S^*_{X|y} = \sum_i S_i(E_i^*) + \ub{(\partial_{E_i} S_i)_{A_i}}{$=\beta$} \Delta E_i + \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2 + \cdots$$</span>
<span id="cb1-989"><a href="#cb1-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-990"><a href="#cb1-990" aria-hidden="true" tabindex="-1"></a>Since $\sum_i \Delta E_i = 0$ at the equilibrium point,</span>
<span id="cb1-991"><a href="#cb1-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-992"><a href="#cb1-992" aria-hidden="true" tabindex="-1"></a>$$\rho_Y(\Delta E) \propto e^{\sum_i \frac 12 (\partial_{E_i}^2 S_i)_{A_i} (\Delta E_i)^2}$$</span>
<span id="cb1-993"><a href="#cb1-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-994"><a href="#cb1-994" aria-hidden="true" tabindex="-1"></a>Now, $\partial_E S = \beta$, and $\partial_E^2 S = -\frac{1}{T^2 C}$ in typical thermodynamic notation, where $C$ is $\partial_T E$, the heat capacity (holding all other variables $A$ constant), so we have the following equation:</span>
<span id="cb1-995"><a href="#cb1-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-996"><a href="#cb1-996" aria-hidden="true" tabindex="-1"></a>$$\rho_Y(\Delta E) \propto e^{-\sum_i \frac{1}{2T^2 C_{i}} (\Delta E_i)^2}$$</span>
<span id="cb1-997"><a href="#cb1-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-998"><a href="#cb1-998" aria-hidden="true" tabindex="-1"></a>with fluctuation on the order $\Delta E_i \sim \sqrt{T^2 C_i}$. For most substances studied by 19th century physicists, such as gas, that is $\sim \sqrt{N} k_B T$. If they could measure the energy of gas at $T = 500 \ut{K}$ with precision down to $10^{-3} \ut{J}$, that would still require a tank of gas with $N = 10^{34} = 10^{10} \ut{mol}$. If they wanted to study this in oxygen, they would need 0.1 million tonnes of it.</span>
<span id="cb1-999"><a href="#cb1-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1000"><a href="#cb1-1000" aria-hidden="true" tabindex="-1"></a><span class="fu">### Blackbody radiation</span></span>
<span id="cb1-1001"><a href="#cb1-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1002"><a href="#cb1-1002" aria-hidden="true" tabindex="-1"></a>Planck's derivation of the blackbody radiation is the first great success of quantum statistical mechanics. We give a brief presentation here that tracks Planck's original argument. </span>
<span id="cb1-1003"><a href="#cb1-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1004"><a href="#cb1-1004" aria-hidden="true" tabindex="-1"></a>Consider a hollow cubic box with side lengths $L$. The box has perfectly reflecting walls. At thermal equilibrium, the box is full of standing electromagnetic waves. Each standing EM wave has form $\vec E(x, y, z, t) = \vec E_0 \sin(\omega t)\sin(\frac{n_x \pi x}{L})\sin(\frac{n_y \pi y}{L})\sin(\frac{n_z \pi z}{L})$, for some positive integers $n_x, n_y, n_z$. Each wave has wavevectors $\vec k = (n_x, n_y, n_z) \frac{\pi}{L}$. If we draw a region of volume $\delta K$ in the space of wavevectors, then the region would contain about $\delta K \frac{L^3}{\pi^3}$ valid wavevectors. Thus, we say that the wavevector space is $[0, +\infty)^3$, and has **density of states** $\frac{L^3}{\pi^3}$. We can picture it as $[0, +\infty)^3$ with a rectangular grid of points being the valid wavevectors, such that the numerical density of such grid points is $\frac{L^3}{\pi^3}$.</span>
<span id="cb1-1005"><a href="#cb1-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1006"><a href="#cb1-1006" aria-hidden="true" tabindex="-1"></a><span class="al">![The rectangular grid of valid wavevectors in the space of possible wavevectors.](figure/density_of_states.png)</span>{width=60%}</span>
<span id="cb1-1007"><a href="#cb1-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1008"><a href="#cb1-1008" aria-hidden="true" tabindex="-1"></a>At this point, we depart from Planck's derivation. Instead of considering standing waves in a perfectly reflecting chamber, we consider planar waves in a chamber with periodic boundaries. That is, we imagine that we have opened 6 portals, so that its top wall is "ported" to the bottom, etc. In this case, the planar waves have valid wavevectors $\vec k = (n_x, n_y, n_z) \frac{2\pi}{L}$.</span>
<span id="cb1-1009"><a href="#cb1-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1010"><a href="#cb1-1010" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb1-1011"><a href="#cb1-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1012"><a href="#cb1-1012" aria-hidden="true" tabindex="-1"></a>Wait, the numerical density of grid points is now just $\frac{L^3}{8\pi^3}$, which is $1/8$ of what we found previously?</span>
<span id="cb1-1013"><a href="#cb1-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1014"><a href="#cb1-1014" aria-hidden="true" tabindex="-1"></a>Yes, indeed, but it will work out correctly, because whereas the density of states has dropped to just $1/8$ of previously, the state space has increased $8\times$, from $[0, +\infty)^3$ to $\R^3$.</span>
<span id="cb1-1015"><a href="#cb1-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1016"><a href="#cb1-1016" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1017"><a href="#cb1-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1018"><a href="#cb1-1018" aria-hidden="true" tabindex="-1"></a>Now, we need to allow *two* states at each valid wavevector, to account for polarization.</span>
<span id="cb1-1019"><a href="#cb1-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1020"><a href="#cb1-1020" aria-hidden="true" tabindex="-1"></a>At this point, we have decomposed the state space into a composition of oscillators. Because there is no interaction between these oscillators,<span class="ot">[^photon-photon-scattering]</span> it remains to calculate the partition function of each oscillator.</span>
<span id="cb1-1021"><a href="#cb1-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1022"><a href="#cb1-1022" aria-hidden="true" tabindex="-1"></a><span class="ot">[^photon-photon-scattering]</span>:</span>
<span id="cb1-1023"><a href="#cb1-1023" aria-hidden="true" tabindex="-1"></a>    That is, two photons do not interact, except when the energy levels are so high that you would need a quantum field theorist to know what is going on.</span>
<span id="cb1-1024"><a href="#cb1-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1025"><a href="#cb1-1025" aria-hidden="true" tabindex="-1"></a><span class="in">    Whereas modern spiritualists talk of electromagnetic fields and quantum vibrations, a century ago they talked of subatomic structures and ether vibrations. Light resembles ghosts and spirits in that they are massless, untouchable, moving very fast, bright, and vaguely associated with good feelings. During the 19th century, the best scientific theory for light, that of ether theory, became the foundation of many spiritualist world systems. [@aspremPonderingImponderablesOccultism2011] The connection of electromagnetism with [animal magnetism](https://en.wikipedia.org/wiki/Animal_magnetism) did not help.</span></span>
<span id="cb1-1026"><a href="#cb1-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1027"><a href="#cb1-1027" aria-hidden="true" tabindex="-1"></a>Planck considered an ensemble of $N$ oscillators, all at the same wavevector and polarization. If they have average energy $\braket{E}$, the question is to find the total entropy for the whole system, which, when divided by $N$, should yield the entropy of a single oscillator. Here he used the celebrated *quantum hypothesis*: The energy levels are divided into integer levels of $nh\nu$, where $n = 0, 1, 2, \dots$. By the <span class="co">[</span><span class="ot">stars and bars</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)) argument, there are $\binom{N + M-1}{M}$ ways do distribute these energy-quanta between these oscillators, where $M = \frac{N\braket{E}}{h\nu}$.</span>
<span id="cb1-1028"><a href="#cb1-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1029"><a href="#cb1-1029" aria-hidden="true" tabindex="-1"></a>$$S = \frac 1N \ln \binom{N+M-1}{M} \ub{\approx}{Stirling} (1 + a) \ln (1+a) - a \ln a, \quad a = \frac{\braket{E}}{h\nu}$$</span>
<span id="cb1-1030"><a href="#cb1-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1031"><a href="#cb1-1031" aria-hidden="true" tabindex="-1"></a>Given the entropy function, he then matched $\braket{E}$ to temperature $\beta$ by the equality $\beta = \partial_{\braket{E}} S$, giving</span>
<span id="cb1-1032"><a href="#cb1-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1033"><a href="#cb1-1033" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1034"><a href="#cb1-1034" aria-hidden="true" tabindex="-1"></a>\braket{E} = \lrb{\frac{h\nu}{e^{\beta h\nu}-1}}</span>
<span id="cb1-1035"><a href="#cb1-1035" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1036"><a href="#cb1-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1037"><a href="#cb1-1037" aria-hidden="true" tabindex="-1"></a>Now, in any direction $\hat k$, for any wavelength interval $<span class="co">[</span><span class="ot">\lambda, \lambda + d\lambda</span><span class="co">]</span>$, and any span of solid angle $d\Omega$, compute its corresponding wavevector space volume $k dkd\Omega = \frac{4\pi^2}{\lambda^3} d\lambda d\Omega$, and multiply that by the density of states and $\braket{E}$, yielding the blackbody radiation law.</span>
<span id="cb1-1038"><a href="#cb1-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1039"><a href="#cb1-1039" aria-hidden="true" tabindex="-1"></a><span class="al">![In any direction, wavelength, and span of solid angle, we find the corresponding volume in the space of wavevectors, sum up all the states within that volume, to obtain the blackbody radiation law.](figure/blackbody_radiation_wavevector_space.jpg)</span></span>
<span id="cb1-1040"><a href="#cb1-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1041"><a href="#cb1-1041" aria-hidden="true" tabindex="-1"></a>The details are found in any textbook. I will just point out some interesting facts typically passed over in textbooks.</span>
<span id="cb1-1042"><a href="#cb1-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1043"><a href="#cb1-1043" aria-hidden="true" tabindex="-1"></a>In the above derivation of the blackbody radiation law, the allowed wavevectors $\vec k$ are like a dust cloud in the space of possible wavevectors. The cloud is assumed to be dense and even, so that the number of states inside a chunk of volume $\Delta V$ is roughly $\Delta V \rho$, where $\rho$ is the *average* density of states. This only works if $\Delta V \rho \gg 1$, or in other words, $\frac{\Delta \lambda}{\lambda} \gg \frac{\lambda^3}{L^3}$. Thus, when the chamber is small, or when temperature is low enough that the spectral peak is close to the zero, then the murky cloud of wavevectors resolves into individual little specks, and we have deviation from blackbody radiation law. </span>
<span id="cb1-1044"><a href="#cb1-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1045"><a href="#cb1-1045" aria-hidden="true" tabindex="-1"></a>In this limit, the precise shape of the chamber becomes important, since the precise chamber shape has a strong effect on long-wavelength (low-temperature) resonant modes. A tiny cube and a tiny cylinder have different blackbody spectra. See <span class="co">[</span><span class="ot">@reiserGeometricEffectsBlackbody2013</span><span class="co">]</span> for a literature review.</span>
<span id="cb1-1046"><a href="#cb1-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1047"><a href="#cb1-1047" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Red solid line is Planck's blackbody radiation law. Black dashed line is the analytical prediction for a spherical blackbody.[@garcia-garciaFinitesizeCorrectionsBlackbody2008]</span><span class="co">](figure/microscopic_blackbody_spectrum.png)</span></span>
<span id="cb1-1048"><a href="#cb1-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1049"><a href="#cb1-1049" aria-hidden="true" tabindex="-1"></a>According to <span class="co">[</span><span class="ot">Kirchhoff's law of thermal radiation</span><span class="co">](https://en.wikipedia.org/wiki/Kirchhoff's_law_of_thermal_radiation)</span>, a chunk of matter is exactly as absorptive as it is emissive. A blackbody absorbs all light, and conversely it emits light at the maximal level. A white body absorbs no light, and conversely it does not emit light. This can be understood as a consequence of the second law: If a body emits more light than it absorbs, then it would spontaneously get colder when placed inside a blackbody radiation chamber. </span>
<span id="cb1-1050"><a href="#cb1-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1051"><a href="#cb1-1051" aria-hidden="true" tabindex="-1"></a>However, much more can be said than this. Not only is it exactly as absorptive as it is emissive, it is as absorptive as it is emissive at *any angle*, at any wavelength, and any polarization. So for example, if a piece of leaf is not absorptive when viewed from an angle, at the green light wavelength, of clockwise polarization, then it is not emissive under the same angle, wavelength, polarization.</span>
<span id="cb1-1052"><a href="#cb1-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1053"><a href="#cb1-1053" aria-hidden="true" tabindex="-1"></a>Why is that? The standard argument <span class="co">[</span><span class="ot">@reifFundamentalsStatisticalThermal1998, chapter 9.15</span><span class="co">]</span> uses a time-reversal argument, but I like to think of it as yet more instances of protecting the second law. If you look inside a blackbody radiation chamber, you would see a maximal entropy state. Light rushes in all directions equally, at all polarizations equally, and the energy is distributed optimally across the spectrum to maximize entropy (because $\beta$ is constant across the whole spectrum). If we have a material that takes in blue light and outputs green light, then it would spontaneously decrease entropy. Similarly, if it can absorb vertically polarized light to emit diagonally polarized light, it would also spontaneously decrease entropy, etc.</span>
<span id="cb1-1054"><a href="#cb1-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1055"><a href="#cb1-1055" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Relativistic gas?"}</span>
<span id="cb1-1056"><a href="#cb1-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1057"><a href="#cb1-1057" aria-hidden="true" tabindex="-1"></a>A box full of blackbody radiation is also called a photon gas. The photon gas is sometimes treated as the limit of "ultrarelativistic gas". Start with the relativistic energy function $E = \sqrt{m^2 c^4 + <span class="sc">\|</span>p<span class="sc">\|</span>^2 c^2}$, derive its <span class="co">[</span><span class="ot">Boltzmann distribution</span><span class="co">](https://en.wikipedia.org/wiki/Maxwell-J%C3%BCttner_distribution)</span> $\rho(q, p) \propto e^{-\beta \sqrt{m^2 c^4 + <span class="sc">\|</span>p<span class="sc">\|</span>^2 c^2}}$, then take the $m \to 0$ limit. This gives some correct results, such as the $U = 3PV$.</span>
<span id="cb1-1058"><a href="#cb1-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1059"><a href="#cb1-1059" aria-hidden="true" tabindex="-1"></a>However, accounting for the entropy of photon gas, as well as deriving the blackbody radiation, hinges critically on the photon quantization $E = h\nu, 2h\nu, \dots$. I guess it can be done correctly by relativistic quantum mechanics, but it is of course beyond the world of classical mechanics.</span>
<span id="cb1-1060"><a href="#cb1-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1061"><a href="#cb1-1061" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1062"><a href="#cb1-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1063"><a href="#cb1-1063" aria-hidden="true" tabindex="-1"></a>The point is that the blackbody radiation law is not *about* a blackbody. Instead, it is about photons in vacuum. We could have taken a perfectly reflecting mirror box (or if you are fancy, a <span class="co">[</span><span class="ot">three-dimensional torus</span><span class="co">](https://en.wikipedia.org/wiki/3-torus)</span>) and injected it with a gas of $400 \ut{nm}$ photons with zero total momentum and angular momentum. Since no conservation laws are constraining, the system will equilibrate to its maximal entropy state, which is the blackbody radiation spectrum. We simply need to wait a few eternities for <span class="co">[</span><span class="ot">photon-photon interactions</span><span class="co">](https://en.wikipedia.org/wiki/Two-photon_physics)</span> to do the job. Thus, the precise material of the chamber does not matter, and charcoal is merely a better catalyst than titanium oxide.</span>
<span id="cb1-1064"><a href="#cb1-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1065"><a href="#cb1-1065" aria-hidden="true" tabindex="-1"></a><span class="fu">### Rubber bands {#sec-rubber-band}</span></span>
<span id="cb1-1066"><a href="#cb1-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1067"><a href="#cb1-1067" aria-hidden="true" tabindex="-1"></a>It turns out that rubber bands, and generally things made of long polymers, *contract* when heated up, instead of expanding. This is the <span class="co">[</span><span class="ot">Gough--Joule effect</span><span class="co">](https://en.wikipedia.org/wiki/Gough%E2%80%93Joule_effect)</span>. Roughly speaking, this is because elasticity in long polymer material (like rubber) is very different from elasticity in short molecule solids (like copper and ice). In rubber, elasticity is an entropic force, while in copper, it is an electrostatic force caused by attraction between molecules.</span>
<span id="cb1-1068"><a href="#cb1-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1069"><a href="#cb1-1069" aria-hidden="true" tabindex="-1"></a>To model a rubber band, consider a long chain molecule with $N$ joints. Each joint can go forward or backward, with equal energy. Each link between two joints has length $d$. The total length of the system is $L$. </span>
<span id="cb1-1070"><a href="#cb1-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1071"><a href="#cb1-1071" aria-hidden="true" tabindex="-1"></a><span class="al">![A sample shape from the ensemble of all rubber band shapes.](figure/rubber_band.png)</span></span>
<span id="cb1-1072"><a href="#cb1-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1073"><a href="#cb1-1073" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Direct argument (microcanonical)</span></span>
<span id="cb1-1074"><a href="#cb1-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1075"><a href="#cb1-1075" aria-hidden="true" tabindex="-1"></a>The entropy of the system, conditional on $L$, is </span>
<span id="cb1-1076"><a href="#cb1-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1077"><a href="#cb1-1077" aria-hidden="true" tabindex="-1"></a>$$S = \ln \binom{N}{\frac{N + L/d}{2}}$$</span>
<span id="cb1-1078"><a href="#cb1-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1079"><a href="#cb1-1079" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="elastic constant"}</span>
<span id="cb1-1080"><a href="#cb1-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1081"><a href="#cb1-1081" aria-hidden="true" tabindex="-1"></a>The thermodynamic equation for the rubber band is</span>
<span id="cb1-1082"><a href="#cb1-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1083"><a href="#cb1-1083" aria-hidden="true" tabindex="-1"></a>$$0 = TdS + FdL$$</span>
<span id="cb1-1084"><a href="#cb1-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1085"><a href="#cb1-1085" aria-hidden="true" tabindex="-1"></a>because the internal energy of the rubber band is constant, no matter how the joint turns.  </span>
<span id="cb1-1086"><a href="#cb1-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1087"><a href="#cb1-1087" aria-hidden="true" tabindex="-1"></a>Therefore, the elastic force is</span>
<span id="cb1-1088"><a href="#cb1-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1089"><a href="#cb1-1089" aria-hidden="true" tabindex="-1"></a>$$F = -T \partial_L S \approx -T \frac{S(L+2d) - S(L)}{2d} \approx \frac{T}{2d }\ln\frac{Nd+L}{Nd - L}$$</span>
<span id="cb1-1090"><a href="#cb1-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1091"><a href="#cb1-1091" aria-hidden="true" tabindex="-1"></a>When $Nd \gg L$, that is, we have not stretched it close to the breaking point, the elastic force is</span>
<span id="cb1-1092"><a href="#cb1-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1093"><a href="#cb1-1093" aria-hidden="true" tabindex="-1"></a>$$F \approx \frac{TL}{Nd^2} = k L$$</span>
<span id="cb1-1094"><a href="#cb1-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1095"><a href="#cb1-1095" aria-hidden="true" tabindex="-1"></a>where $k = \frac{T}{Nd^2}$ is the elastic constant, proportional to temperature.  </span>
<span id="cb1-1096"><a href="#cb1-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1097"><a href="#cb1-1097" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1098"><a href="#cb1-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1099"><a href="#cb1-1099" aria-hidden="true" tabindex="-1"></a>Why does the rubber band stiffen when temperature rise? We can interpret it as follows. When we place the rubber band in a chamber of hot air, the air particles would often collide with the links in the rubber band, flipping it. When there are more links going to the right than the left, then the air particles would tend to flip the links to the left, decreasing $L$, and conversely. The net force is zero only when there are an equal number of links going either way, which is when $L = 0$.</span>
<span id="cb1-1100"><a href="#cb1-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1101"><a href="#cb1-1101" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Via free entropy (canonical)</span></span>
<span id="cb1-1102"><a href="#cb1-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1103"><a href="#cb1-1103" aria-hidden="true" tabindex="-1"></a>Because the rubber band has $dE = TdS + FdL$, the corresponding free entropy is $S - \beta \braket{E} + \beta F \braket{L}$. Under the canonical distribution, that free entropy is maximized, meaning that $\rho(x) \propto e^{\beta FL(x)}$ where $x$ is a microstate of the rubber band (i.e. the precise position of each link), and $L(x)$ is the corresponding length (macrostate).  </span>
<span id="cb1-1104"><a href="#cb1-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1105"><a href="#cb1-1105" aria-hidden="true" tabindex="-1"></a>The trick of using the free entropy is that it decomposes the entire rubber band into atomic individuals. Like how opening an energy market converts consumers trying to coordinate their energy use into consumers each individually buying and selling energy, and thus simplifying the calculation problem. Like how <span class="co">[</span><span class="ot">Laplace's devil</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/analytical-mechanics/index.html#lagranges-devil-at-disneyland)</span> allows you to calculate the optimal way to schedule your day. Microcanonical ensembles are true, but canonical ensembles are almost just as true, and much easier to use. The idea is that the canonical ensemble and the microcanonical ensemble are essentially the same because the fluctuation is so tiny.</span>
<span id="cb1-1106"><a href="#cb1-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1107"><a href="#cb1-1107" aria-hidden="true" tabindex="-1"></a>Back to the rubber band. Each individual link in the rubber band now is freed from the collective responsibility of reaching exactly length $L$. It is now an atomized individual, maximizing its own free entropy $S - \beta \braket{E} + \beta F \braket{L}$. Let $p$ be its probability of going up, then its free entropy is</span>
<span id="cb1-1108"><a href="#cb1-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1109"><a href="#cb1-1109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1110"><a href="#cb1-1110" aria-hidden="true" tabindex="-1"></a>\ub{-p\ln p - (1-p) \ln(1-p) }{$S$} - 0 + \beta F (d p - d(1-p))</span>
<span id="cb1-1111"><a href="#cb1-1111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1112"><a href="#cb1-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1113"><a href="#cb1-1113" aria-hidden="true" tabindex="-1"></a>This is maximized by the Boltzmann distribution $p = \frac{e^{\beta F 2d}}{1+e^{\beta F 2d}}$, with first two moments </span>
<span id="cb1-1114"><a href="#cb1-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1115"><a href="#cb1-1115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1116"><a href="#cb1-1116" aria-hidden="true" tabindex="-1"></a>\braket{L} = \frac{e^{\beta F 2d} - 1}{e^{\beta F 2d} + 1} d \approx \beta Fd^2,</span>
<span id="cb1-1117"><a href="#cb1-1117" aria-hidden="true" tabindex="-1"></a>\quad \mathbb{V}<span class="co">[</span><span class="ot">L</span><span class="co">]</span> = p(1-p)d^2 \approx d^2/4</span>
<span id="cb1-1118"><a href="#cb1-1118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1119"><a href="#cb1-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1120"><a href="#cb1-1120" aria-hidden="true" tabindex="-1"></a>The total extension of the rubber band has the first two moments</span>
<span id="cb1-1121"><a href="#cb1-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1122"><a href="#cb1-1122" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1123"><a href="#cb1-1123" aria-hidden="true" tabindex="-1"></a>N\braket{L} \approx \beta F Nd^2,</span>
<span id="cb1-1124"><a href="#cb1-1124" aria-hidden="true" tabindex="-1"></a>\quad N\mathbb{V}<span class="co">[</span><span class="ot">L</span><span class="co">]</span> \approx Nd^2/4 = \frac{1}{4F\beta} N\braket{L}</span>
<span id="cb1-1125"><a href="#cb1-1125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1126"><a href="#cb1-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1127"><a href="#cb1-1127" aria-hidden="true" tabindex="-1"></a>The first equation is the same as the previous one. The second equation tells us the fluctuation in rubber band length when held under constant force and temperature. For typical conditions like $F \sim 1 \ut{N}, T \sim 300 \ut{K}, N\braket{L} \sim 1 \ut{m}$, the fluctuation is on the order of $(0.1\ut{nm})^2$, about one atom's diameter. So we see that the difference between the canonical and the microcanonical ensemble are indeed too tiny to speak of.</span>
<span id="cb1-1128"><a href="#cb1-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1129"><a href="#cb1-1129" aria-hidden="true" tabindex="-1"></a><span class="fu">## Combinatorial examples</span></span>
<span id="cb1-1130"><a href="#cb1-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1131"><a href="#cb1-1131" aria-hidden="true" tabindex="-1"></a><span class="fu">### Burning the library of Babel</span></span>
<span id="cb1-1132"><a href="#cb1-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1133"><a href="#cb1-1133" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The universe (which others call the Library) is composed of an indefinite, perhaps infinite number of hexagonal galleries... always the same: 20 bookshelves, 5 to each side, line four of the hexagon's six sides... each bookshelf holds 32 books identical in format; each book contains 410 pages; each page, 40 lines; each line, approximately 80 black letters... punctuation is limited to the comma and the period. Those two marks, the space, and the twenty-two letters of the alphabet are the 25 sufficient symbols...</span></span>
<span id="cb1-1134"><a href="#cb1-1134" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-1135"><a href="#cb1-1135" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *The Library of Babel*, Jorge Luis Borges</span></span>
<span id="cb1-1136"><a href="#cb1-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1137"><a href="#cb1-1137" aria-hidden="true" tabindex="-1"></a>Like the artist <span class="co">[</span><span class="ot">M. C. Escher</span><span class="co">](https://en.wikipedia.org/wiki/M._C._Escher)</span>, the writer <span class="co">[</span><span class="ot">J. L. Borges</span><span class="co">](https://en.wikipedia.org/wiki/Jorge_Luis_Borges)</span> is a favorite of scientists, for his stories that construct precise worlds like elegant thought experiments. The Library of Babel is a thought experiment on combinatorics and entropy. In the universe, there are only books. Each book contains </span>
<span id="cb1-1138"><a href="#cb1-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1139"><a href="#cb1-1139" aria-hidden="true" tabindex="-1"></a>$$410 \ut{page} \times 40\ut{line/page} \times 80 \ut{symbol/line} = 1.3\times 10^6\ut{symbol}$$</span>
<span id="cb1-1140"><a href="#cb1-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1141"><a href="#cb1-1141" aria-hidden="true" tabindex="-1"></a>Suppose the books are uniformly random sequences made of 25 symbols, then each symbol contains $\ln 25$ amount of entropy, and each book contains $1.3\times 10^6\ln 25 = 4.2 \times 10^6$. Now, consider another library of Babel, but this one consists of books filled with white space, so each book has zero entropy. Then, we can take one empty book and "burn" it into a uniformly random book, recovering $4.2 \times 10^6 k_B T$ free energy per book burned. At ambient temperature $300\ut{K}$, that is just $1.4 \times 10^{-14}\ut{J}$ per book. Book-burning isn't going to keep the librarians warm.</span>
<span id="cb1-1142"><a href="#cb1-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1143"><a href="#cb1-1143" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/babel_burning_ideogram.webp)</span></span>
<span id="cb1-1144"><a href="#cb1-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1145"><a href="#cb1-1145" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; After having razed the garden and profaned the chalices and altars, the Huns entered the monastery library on horseback and trampled the incomprehensible books and vituperated and burned them, perhaps fearful that the letters concealed blasphemies against their god, which was an iron scimitar. Palimpsests and codices were consumed, but in the heart of the fire, amid the ashes, there remained almost intact the twelfth book of the *Civitas Dei*, which relates how in Athens Plato taught that, at the centuries' end, all things will recover their previous state and he in Athens, before the same audience, will teach this same doctrine anew.</span></span>
<span id="cb1-1146"><a href="#cb1-1146" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-1147"><a href="#cb1-1147" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *The Theologians*, Jorge Luis Borges</span></span>
<span id="cb1-1148"><a href="#cb1-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1149"><a href="#cb1-1149" aria-hidden="true" tabindex="-1"></a>While it is fanciful to "burn" a book by randomizing its letters, if we take the "information is physical, physics is informational" equivalence seriously, then there must be a way to actually "burn information". Indeed, there is a way.</span>
<span id="cb1-1150"><a href="#cb1-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1151"><a href="#cb1-1151" aria-hidden="true" tabindex="-1"></a>We can consider a chamber containing a single ball inside. The chamber has a removable wall, separating it into two parts labelled <span class="in">`0`</span> and <span class="in">`1`</span>. If we know which side the ball is in, then we can put a piston into the other side, remove the wall, then extract work by isothermal expansion. Of course, the actual work extractable is random, but if we do this for a large number of chambers, then it is the same as isothermal expansion of an ideal gas to double its volume. By the ideal gas equation, we would extract $k_B T \ln 2$ of mechanical energy per chamber on average, exactly as predicted by the information-burning argument.</span>
<span id="cb1-1152"><a href="#cb1-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1153"><a href="#cb1-1153" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A car driven by burning a tape of zero information, emitting "waste information" in its wake. [@sethnaStatisticalMechanicsEntropy2021, figure 5.10]</span><span class="co">](figure/information-burning_engine.png)</span></span>
<span id="cb1-1154"><a href="#cb1-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1155"><a href="#cb1-1155" aria-hidden="true" tabindex="-1"></a>In the tradition of <span class="co">[</span><span class="ot">steampunk</span><span class="co">](https://en.wikipedia.org/wiki/Steampunk)</span>, <span class="co">[</span><span class="ot">@luEngineeringMaxwellDemon2014</span><span class="co">]</span> constructed a purely mechanical model of information-burning. As shown below, we have a belt of paddles that can rotate freely, except when they hit one of the two red rods. The two red rods divide space into two sides: the <span class="in">`0`</span> side and the <span class="in">`1`</span> side. One of the red rods has an opening on it, allowing the paddle to go through. All the paddles start on the top in the <span class="in">`0`</span> side, and slowly move down. As soon as a paddle has moved into the middle, it would be able to be able to freely move between <span class="in">`0`</span> and <span class="in">`1`</span> sides.</span>
<span id="cb1-1156"><a href="#cb1-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1157"><a href="#cb1-1157" aria-hidden="true" tabindex="-1"></a>So, if we blur our vision and look at the average motion of the paddles, we would see an averaged "entropic force" that drives paddles from <span class="in">`0`</span> side to <span class="in">`1`</span> side. We can harvest this entropic force by adding a ring with a paddle inside, which can be hit by the paddles, driving it into rotation. The ring would be able to wind up a spring, thus converting information into mechanical work.</span>
<span id="cb1-1158"><a href="#cb1-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1159"><a href="#cb1-1159" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The mechanical information-engine. [@luEngineeringMaxwellDemon2014]</span><span class="co">](figure/mechanical_maxwell_demon.jpeg)</span></span>
<span id="cb1-1160"><a href="#cb1-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1161"><a href="#cb1-1161" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb1-1162"><a href="#cb1-1162" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">video</span><span class="ot"> controls width</span><span class="op">=</span><span class="st">100%</span><span class="dt">&gt;</span></span>
<span id="cb1-1163"><a href="#cb1-1163" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">source</span><span class="ot"> src</span><span class="op">=</span><span class="st">"figure/mechanical_maxwell_demon.webm"</span><span class="ot"> type</span><span class="op">=</span><span class="st">"video/webm"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb1-1164"><a href="#cb1-1164" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">video</span><span class="dt">&gt;</span></span>
<span id="cb1-1165"><a href="#cb1-1165" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span>Video source: <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"https://www.youtube.com/watch?v=00TyIShzR6o"</span><span class="dt">&gt;</span>Mechanical Maxwell's Demon in Motion - YouTube<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span>.<span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb1-1166"><a href="#cb1-1166" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb1-1167"><a href="#cb1-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1168"><a href="#cb1-1168" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multinomials and the chi-squared test</span></span>
<span id="cb1-1169"><a href="#cb1-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1170"><a href="#cb1-1170" aria-hidden="true" tabindex="-1"></a>Let $\vec p := (p_1, \dots, p_k)$ be a discrete probability distribution, and let $N_1, \dots, N_n$ be integers that depend on $N$, such that</span>
<span id="cb1-1171"><a href="#cb1-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1172"><a href="#cb1-1172" aria-hidden="true" tabindex="-1"></a>$$\lim_{N \to \infty}\vec N / N = \vec p$$</span>
<span id="cb1-1173"><a href="#cb1-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1174"><a href="#cb1-1174" aria-hidden="true" tabindex="-1"></a>where $\vec N := (N_1, \dots, N_n)$, then we have</span>
<span id="cb1-1175"><a href="#cb1-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1176"><a href="#cb1-1176" aria-hidden="true" tabindex="-1"></a>$$\lim_{N \to \infty}\frac 1N \ln \binom{N}{N_1, \dots, N_n} = -\sum_i p_i \ln p_i = S<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$$</span>
<span id="cb1-1177"><a href="#cb1-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1178"><a href="#cb1-1178" aria-hidden="true" tabindex="-1"></a>where $\rho$ is the discrete probability distribution, and $\binom{N}{N_1, \dots, N_n} = \frac{N!}{N_1! \cdots N_n!}$ is the <span class="co">[</span><span class="ot">multinomial coefficient</span><span class="co">](https://en.wikipedia.org/wiki/Multinomial_theorem)</span>, which counts the number of ways for $N$ labelled balls to go into $n$ labelled boxes, such that the $i$-th box contains $N_i$ balls.</span>
<span id="cb1-1179"><a href="#cb1-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1180"><a href="#cb1-1180" aria-hidden="true" tabindex="-1"></a>This can be interpreted as the thermodynamic limit of a lot of balls.</span>
<span id="cb1-1181"><a href="#cb1-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1182"><a href="#cb1-1182" aria-hidden="true" tabindex="-1"></a>Consider a population of particles, all in energy-contact with an energy bath of $\beta = 1$. Each particle has $n$ states, with state $i$ having energy $-\ln p_i$. Thus, at the Boltzmann distribution, each particle is precisely sampled from the categorical distribution $\vec p$. Clearly, this system has entropy $N\sum_i -p_i \ln p_i$. This is a "canonical ensemble".</span>
<span id="cb1-1183"><a href="#cb1-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1184"><a href="#cb1-1184" aria-hidden="true" tabindex="-1"></a>In contrast, consider the "microcanonical ensemble" (in quotation marks, because the constraint is not on the total energy) of $N$ particles, such that there are exactly $N_i$ particles in state $i$.</span>
<span id="cb1-1185"><a href="#cb1-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1186"><a href="#cb1-1186" aria-hidden="true" tabindex="-1"></a>Thinking thermodynamically, we can single out one ball as the system, and regard the other $N$ balls as a bath. Once we calculate the marginal entropies of the bath, we can infer the Boltzmann distribution for the system, and show that it is the same as the Boltzmann distribution in the canonical ensemble.</span>
<span id="cb1-1187"><a href="#cb1-1187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1188"><a href="#cb1-1188" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="non-rigorous part"}</span>
<span id="cb1-1189"><a href="#cb1-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1190"><a href="#cb1-1190" aria-hidden="true" tabindex="-1"></a>We *hope* that as $N \to \infty$, all correlations (pairwise, triple-wise, etc) between balls decay fast enough, such that the interaction-entropy between the balls drop to zero, leaving</span>
<span id="cb1-1191"><a href="#cb1-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1192"><a href="#cb1-1192" aria-hidden="true" tabindex="-1"></a>$$\text{average entropy per ball} = \text{marginal entropy of a ball}$$</span>
<span id="cb1-1193"><a href="#cb1-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1194"><a href="#cb1-1194" aria-hidden="true" tabindex="-1"></a>Justifying this rigorously is generally very difficult, and in fact, the assumption is false at phase transitions, where correlations *do not* decay fast enough, and so the thermodynamic limit is false. However, it is typically good enough to check that the fluctuations decay as $N^{-1/2}$, and if so, the thermodynamic limit is true.</span>
<span id="cb1-1195"><a href="#cb1-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1196"><a href="#cb1-1196" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1197"><a href="#cb1-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1198"><a href="#cb1-1198" aria-hidden="true" tabindex="-1"></a>Suppose we move the ball from box $i$ to box $j$, then it would force the bath to change all its balls, changing its entropy by</span>
<span id="cb1-1199"><a href="#cb1-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1200"><a href="#cb1-1200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1201"><a href="#cb1-1201" aria-hidden="true" tabindex="-1"></a>\ln\binom{N}{n_1', \dots, n_k'} -\ln \binom{N}{n_1, \dots, n_k}</span>
<span id="cb1-1202"><a href="#cb1-1202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1203"><a href="#cb1-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1204"><a href="#cb1-1204" aria-hidden="true" tabindex="-1"></a>where $n_i' = n_i + 1, n_j' = n_j - 1$, and otherwise unchanged. By definition of multinomials, this is $\ln n_j - \ln(n_i+1) \to (\ln p_j - \ln p_i)$ at large enough $N$.  </span>
<span id="cb1-1205"><a href="#cb1-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1206"><a href="#cb1-1206" aria-hidden="true" tabindex="-1"></a>Let $X$ stand for the total state, including the bath and the singled-out system, and let $Y$ stand for the state of the singled-out system. By the conditional entropy theorem, when the entire system is at the maximal entropy distribution, the distribution of the singled-out system is</span>
<span id="cb1-1207"><a href="#cb1-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1208"><a href="#cb1-1208" aria-hidden="true" tabindex="-1"></a>$$\rho^*_Y(y) \propto e^{S_{X|y}^*}$$</span>
<span id="cb1-1209"><a href="#cb1-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1210"><a href="#cb1-1210" aria-hidden="true" tabindex="-1"></a>From the previous calculation, we have</span>
<span id="cb1-1211"><a href="#cb1-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1212"><a href="#cb1-1212" aria-hidden="true" tabindex="-1"></a>$$S_{X|j}^* - S_{X|j}^* = \ln p_j - \ln p_i$$</span>
<span id="cb1-1213"><a href="#cb1-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1214"><a href="#cb1-1214" aria-hidden="true" tabindex="-1"></a>yielding $S_{X|i}^* = S_0 + \ln p_i$ for some constant $S_0$, and so $\rho^*_Y(y) \propto p_i$.</span>
<span id="cb1-1215"><a href="#cb1-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1216"><a href="#cb1-1216" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="chi-squared test"}</span>
<span id="cb1-1217"><a href="#cb1-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1218"><a href="#cb1-1218" aria-hidden="true" tabindex="-1"></a>Let $X$ stand for the state of the entire population of particles. That is, $X$ describes the precise state of each particle. Let $Y$ stand for the vector of $\vec N$. That is, it counts the number of particles in each state. As typical, at the limit of $N \to \infty$, we should expect $\frac{N_i}{N} \to p_i$ with certainty (the thermodynamic limit), and the variance should scale as $1/N$ (statistical fluctuation). So, if we observe the particle population many times, and plot all the $\frac{N_i}{N}$ on the $n$-simplex, we should see an ellipsoidal cloud centered around $\vec p$ with radius $\sim N^{-1/2}$.</span>
<span id="cb1-1219"><a href="#cb1-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1220"><a href="#cb1-1220" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">video</span><span class="ot"> controls width</span><span class="op">=</span><span class="st">100%</span><span class="dt">&gt;</span></span>
<span id="cb1-1221"><a href="#cb1-1221" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">source</span><span class="ot"> src</span><span class="op">=</span><span class="st">"figure/Sanov_convergence.webm"</span><span class="ot"> type</span><span class="op">=</span><span class="st">"video/webm"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb1-1222"><a href="#cb1-1222" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">video</span><span class="dt">&gt;</span></span>
<span id="cb1-1223"><a href="#cb1-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1224"><a href="#cb1-1224" aria-hidden="true" tabindex="-1"></a>This is an example of how the microcanonical ensemble and the canonical ensemble become indistinguishable at the large particle limit. Indeed, Boltzmann often used this equation to derive the canonical ensemble from microcanonical ensembles.</span>
<span id="cb1-1225"><a href="#cb1-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1226"><a href="#cb1-1226" aria-hidden="true" tabindex="-1"></a>We have argued that the cloud should be ellipsoidal and centered around $(p_1, \dots, p_n)$ with radius $\sim N^{-1/2}$. What exactly is its shape? Well, since each particle's distribution of states is categorical, and the particles are uncorrelated (thanks to the energy market), the mean and covariance of $\vec N= (N_1, \dots, N_n)$ are $\vec p N$ and $<span class="co">[</span><span class="ot">\diag(\vec p) - \vec p \vec p^T</span><span class="co">]</span> N$. The covariance matrix is not linearly independent, because of the constraint $\sum_i N_i/N = 1$.</span>
<span id="cb1-1227"><a href="#cb1-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1228"><a href="#cb1-1228" aria-hidden="true" tabindex="-1"></a>A good trick is worth doing again and again. Like how the microcanonical ensemble is freed up by a free market into a canonical ensemble, here we free up $\vec N$ by *adding noise*, then conditioning on zero noise:</span>
<span id="cb1-1229"><a href="#cb1-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1230"><a href="#cb1-1230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1231"><a href="#cb1-1231" aria-hidden="true" tabindex="-1"></a>\vec N / N + z\vec p/\sqrt N \sim \mathcal N(\vec p, \diag(\vec p) / N) \quad z \sim \mathcal N(0, 1)</span>
<span id="cb1-1232"><a href="#cb1-1232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1233"><a href="#cb1-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1234"><a href="#cb1-1234" aria-hidden="true" tabindex="-1"></a>Therefore, the probability density of $\vec N$ satisfies</span>
<span id="cb1-1235"><a href="#cb1-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1236"><a href="#cb1-1236" aria-hidden="true" tabindex="-1"></a>$$\rho(\vec N / N) \propto \exp\lrb{-\frac 12 \sum_i \frac{(N_i - Np_i)^2}{Np_i}}$$</span>
<span id="cb1-1237"><a href="#cb1-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1238"><a href="#cb1-1238" aria-hidden="true" tabindex="-1"></a>after conditioning on $z = 0$. Therefore, the distribution of $\sum_i \frac{(N_i - Np_i)^2}{Np_i}$ converges to $\chi^2(n-1)$. This is the <span class="co">[</span><span class="ot">chi-squared test</span><span class="co">](https://en.wikipedia.org/wiki/Chi-squared_test)</span>.</span>
<span id="cb1-1239"><a href="#cb1-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1240"><a href="#cb1-1240" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1241"><a href="#cb1-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1242"><a href="#cb1-1242" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sanov's theorem</span></span>
<span id="cb1-1243"><a href="#cb1-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1244"><a href="#cb1-1244" aria-hidden="true" tabindex="-1"></a>Typically in statistical mechanics, we study the fluctuation of a macroscopic variable on the order of $N^{-1/2}$. Large deviation theory studies the fluctuation on the order of $1$. The prototypical example is Sanov's theorem.</span>
<span id="cb1-1245"><a href="#cb1-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1246"><a href="#cb1-1246" aria-hidden="true" tabindex="-1"></a>The problem is as follows: Suppose that we require $\vec N / N \to \vec q$, where $\vec q$ is some other probability vector, what is the behavior of $Pr(\vec N / N \approx \vec q)$? Sanov's theorem states that</span>
<span id="cb1-1247"><a href="#cb1-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1248"><a href="#cb1-1248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1249"><a href="#cb1-1249" aria-hidden="true" tabindex="-1"></a>Pr(\vec N / N \approx \vec q) \sim e^{-ND_{KL}(\vec q <span class="sc">\|</span> \vec p)}</span>
<span id="cb1-1250"><a href="#cb1-1250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1251"><a href="#cb1-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1252"><a href="#cb1-1252" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="derivation" collapse="true"}</span>
<span id="cb1-1253"><a href="#cb1-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1254"><a href="#cb1-1254" aria-hidden="true" tabindex="-1"></a>By @thm-cond-free-ent, </span>
<span id="cb1-1255"><a href="#cb1-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1256"><a href="#cb1-1256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1257"><a href="#cb1-1257" aria-hidden="true" tabindex="-1"></a>\frac 1N \ln Pr(\vec N / N \approx \vec q) = \bar f^*_{|\vec N / N \approx \vec q} - \bar f^*</span>
<span id="cb1-1258"><a href="#cb1-1258" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1259"><a href="#cb1-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1260"><a href="#cb1-1260" aria-hidden="true" tabindex="-1"></a>where $\bar f^*$ is the Helmholtz free entropy per particle for the system without constraint on $\vec N$, and $\bar f^*_{|\vec N / N \approx \vec q}$ is the Helmholtz free entropy per particle, conditional on $\vec N / N \approx \vec q$.</span>
<span id="cb1-1261"><a href="#cb1-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1262"><a href="#cb1-1262" aria-hidden="true" tabindex="-1"></a>The constraint of $\vec N / N \approx \vec q$ is a global constraint, but as usual, when $N$ is large, global constraints over all particles becomes local constraints for each particle individually. In this case, the constraint on individual particle is simply that its state is distributed according to $\vec q$, energy be damned. </span>
<span id="cb1-1263"><a href="#cb1-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1264"><a href="#cb1-1264" aria-hidden="true" tabindex="-1"></a>Thus, we find that</span>
<span id="cb1-1265"><a href="#cb1-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1266"><a href="#cb1-1266" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1267"><a href="#cb1-1267" aria-hidden="true" tabindex="-1"></a>\frac 1N \ln Pr(\vec N / N \approx \vec q) = \lrb{\sum_i -q_i \ln q_i - \sum_i q_i E_i} - \lrb{\sum_i -p_i \ln p_i - \sum_i p_i E_i} = -D_{KL}(\vec q <span class="sc">\|</span> \vec p)</span>
<span id="cb1-1268"><a href="#cb1-1268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1269"><a href="#cb1-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1270"><a href="#cb1-1270" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1271"><a href="#cb1-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1272"><a href="#cb1-1272" aria-hidden="true" tabindex="-1"></a><span class="fu">### Surface area of high-dimensional spheres</span></span>
<span id="cb1-1273"><a href="#cb1-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1274"><a href="#cb1-1274" aria-hidden="true" tabindex="-1"></a>Let $\Omega_N$ be the surface area of a sphere of radius $\sqrt N$ in $\R^N$, then</span>
<span id="cb1-1275"><a href="#cb1-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1276"><a href="#cb1-1276" aria-hidden="true" tabindex="-1"></a>$$\ln\Omega_N = \frac N2 \ln (2\pi e)-\frac 12 \ln (\pi e) + O(N^{-1})$$</span>
<span id="cb1-1277"><a href="#cb1-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1278"><a href="#cb1-1278" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="derivation" collapse="true"}</span>
<span id="cb1-1279"><a href="#cb1-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1280"><a href="#cb1-1280" aria-hidden="true" tabindex="-1"></a>Let $x_1, \dots, x_N$ be sampled IID from $\mathcal N(0, 1)$, and let $r_N^2 = \sum_i x_i^2$. By routine calculation, $\braket{r_N^2} = N$, and $\braket{r_N^4} = 2N + N^2$. Therefore, $r_N^2$ is approximately distributed as $\mathcal N(N, 2N)$, and so $r_N$ is approximately distributed as $\mathcal N(\sqrt N, 1/2)$.</span>
<span id="cb1-1281"><a href="#cb1-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1282"><a href="#cb1-1282" aria-hidden="true" tabindex="-1"></a>Now consider two distributions on $\R^N$. The first is a microcanonical ensemble: $x_{1:N}$ is distributed uniformly on the thin energy shell of $r_N^2 \in <span class="co">[</span><span class="ot">N, N + \delta N</span><span class="co">]</span>$. The second is a canonical ensemble: each $x_i$ is distributed independently according to $\mathcal N(0, 1)$.</span>
<span id="cb1-1283"><a href="#cb1-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1284"><a href="#cb1-1284" aria-hidden="true" tabindex="-1"></a>We can think of them as particles in a potential well of form $V(x) = \frac 12 x^2$. The first ensemble is the microcanonical ensemble where the total energy is fixed, and the second is the canonical ensemble at temperature $\beta = 1$.</span>
<span id="cb1-1285"><a href="#cb1-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1286"><a href="#cb1-1286" aria-hidden="true" tabindex="-1"></a>We can calculate the entropy of the canonical ensemble in two ways. We can calculate it by adding up the entropy of each particle, which are the same since there is no interaction energy between particles. We can also calculate it indirectly, by first sampling a random radius, then a random point from the microcanonical ensemble, then multiplying them together.</span>
<span id="cb1-1287"><a href="#cb1-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1288"><a href="#cb1-1288" aria-hidden="true" tabindex="-1"></a>Because the canonical ensemble is spherically symmetric, the radius and the direction of the vector $x_{1:N}$ are independent. Therefore,</span>
<span id="cb1-1289"><a href="#cb1-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1290"><a href="#cb1-1290" aria-hidden="true" tabindex="-1"></a>$$S_{\text{canonical}} = S_{\text{microcanonical}} + S_{\text{radius}}$$</span>
<span id="cb1-1291"><a href="#cb1-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1292"><a href="#cb1-1292" aria-hidden="true" tabindex="-1"></a>or</span>
<span id="cb1-1293"><a href="#cb1-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1294"><a href="#cb1-1294" aria-hidden="true" tabindex="-1"></a>$$\ln \Omega_N = \ub{S_{\text{canonical}}}{$= N S<span class="co">[</span><span class="ot">\mathcal N(0, 1)</span><span class="co">]</span>$} - \ub{S_{\text{radius}}}{$\approx S<span class="co">[</span><span class="ot">\mathcal N(0, 1/2)</span><span class="co">]</span>$}$$</span>
<span id="cb1-1295"><a href="#cb1-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1296"><a href="#cb1-1296" aria-hidden="true" tabindex="-1"></a>Because the entropy of $\mathcal N(0, \sigma^2)$ is $\frac 12 \ln(2\pi e \sigma^2)$, we plug them in and obtain the result.</span>
<span id="cb1-1297"><a href="#cb1-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1298"><a href="#cb1-1298" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1299"><a href="#cb1-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1300"><a href="#cb1-1300" aria-hidden="true" tabindex="-1"></a>Again, this is an example of a general pattern: the microcanonical ensemble and the canonical ensemble become indistinguishable when the number of particles goes infinite.</span>
<span id="cb1-1301"><a href="#cb1-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1302"><a href="#cb1-1302" aria-hidden="true" tabindex="-1"></a><span class="fu">## Biological examples</span></span>
<span id="cb1-1303"><a href="#cb1-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1304"><a href="#cb1-1304" aria-hidden="true" tabindex="-1"></a><span class="fu">### How elastic is the skin of red blood cell?</span></span>
<span id="cb1-1305"><a href="#cb1-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1306"><a href="#cb1-1306" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@discherNewInsightsErythrocyte2000</span><span class="co">]</span> measured the elasticity of the red blood cell's skin. To a good approximation, it is just a 2D spring, following Hooke's law. He attached a tiny particle to the surface of a red blood cell, and measured its thermal motion.</span>
<span id="cb1-1307"><a href="#cb1-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1308"><a href="#cb1-1308" aria-hidden="true" tabindex="-1"></a>As usual, in the microscopic world, inertia might as well not exist,<span class="ot">[^red-blood-cell-inertia]</span> so the oscillator's energy is entirely elastic. Let it be of form $\frac 12 k(x^2 + y^2)$. By equipartition of energy, we would have</span>
<span id="cb1-1309"><a href="#cb1-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1310"><a href="#cb1-1310" aria-hidden="true" tabindex="-1"></a>$$\frac 12 k\braket{x^2} = \frac 12 \beta^{-1}$$</span>
<span id="cb1-1311"><a href="#cb1-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1312"><a href="#cb1-1312" aria-hidden="true" tabindex="-1"></a><span class="ot">[^red-blood-cell-inertia]: </span>But just to be sure, I checked the original numbers: It is a particle of diameter $40 \ut{nm}$, and moving at about $100 \ut{nm/s}$, so its kinetic energy is $\sim 10^{-33}\ut{J} \sim 10^{-13} k_BT$.</span>
<span id="cb1-1313"><a href="#cb1-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1314"><a href="#cb1-1314" aria-hidden="true" tabindex="-1"></a>The data shows that $\braket{x^2} = (35 \ut{nm})^2$ at temperature $T = 310 \ut{K}$, giving us an effective elastic constant of $k \sim 0.004 \ut{pN/nm}$.</span>
<span id="cb1-1315"><a href="#cb1-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1316"><a href="#cb1-1316" aria-hidden="true" tabindex="-1"></a>A red blood cell has diameter $10^4 \ut{nm}$, so dragging a patch of its skin all across the surface would take only about $40 \ut{pN}$. This is about the force output of 10 kinesins working in concert. Thus we can say that the skin of red blood cell is very slack.</span>
<span id="cb1-1317"><a href="#cb1-1317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1318"><a href="#cb1-1318" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">(A): A tiny particle attached to long molecules in the skin of the red blood cell. (B): Photo of a red blood cell with attached nanoparticle. (C): An example $(x,y)$ trajectory. [@discherNewInsightsErythrocyte2000, figure 6]</span><span class="co">](figure/red_blood_cell_discher_2000.png)</span></span>
<span id="cb1-1319"><a href="#cb1-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1320"><a href="#cb1-1320" aria-hidden="true" tabindex="-1"></a><span class="fu">### The `lac` operon</span></span>
<span id="cb1-1321"><a href="#cb1-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1322"><a href="#cb1-1322" aria-hidden="true" tabindex="-1"></a>This example is from <span class="co">[</span><span class="ot">@garciaQuantitativeDissectionSimple2011</span><span class="co">]</span>.</span>
<span id="cb1-1323"><a href="#cb1-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1324"><a href="#cb1-1324" aria-hidden="true" tabindex="-1"></a>*E. coli* has two main sources of food: glucose and lactose. It prefers glucose, so when it is in an environment rich in glucose, it would start by metabolizing the glucose until it is mostly exhausted, then switch to metabolizing lactose.</span>
<span id="cb1-1325"><a href="#cb1-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1326"><a href="#cb1-1326" aria-hidden="true" tabindex="-1"></a>To simplify, let's consider a single gene, called the <span class="in">`lac`</span>, which codes for a lactose-digesting enzyme (lactases). In front of <span class="in">`lac`</span> there is a site where <span class="in">`repressor`</span> protein can bind to, which stops <span class="in">`lac`</span> transcription. The gene is <span class="in">`on`</span> iff the <span class="in">`repressor`</span> is not bound there.</span>
<span id="cb1-1327"><a href="#cb1-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1328"><a href="#cb1-1328" aria-hidden="true" tabindex="-1"></a>The <span class="in">`repressor`</span> protein can bind to either that specific site, of which there is only one on the entire *E. coli* genome, or any other site on the entire DNA sequence (non-specific binding). Write the binding energy on the specific site be $E_S$, and the binding energy on the non-specific sites be $E_{NS}$.</span>
<span id="cb1-1329"><a href="#cb1-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1330"><a href="#cb1-1330" aria-hidden="true" tabindex="-1"></a>The system is in a delicate balance between energy, which favors specific binding, and entropy, which favors non-specific binding, with $E_S &lt; E_{NS}$ but not $E_S \ll E_{NS}$. That is, specific binding is favored, but not *too* favored. This soft-favorism is what allows <span class="in">`lac`</span> to be controlled. If it is not favored at all, then it would rarely bind. If it is too favored, then it would almost always bind.</span>
<span id="cb1-1331"><a href="#cb1-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1332"><a href="#cb1-1332" aria-hidden="true" tabindex="-1"></a>Suppose there are $R$ <span class="in">`repressors`</span> in the bacterium, then when none is binding specifically, the Gibbs free energy is</span>
<span id="cb1-1333"><a href="#cb1-1333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1334"><a href="#cb1-1334" aria-hidden="true" tabindex="-1"></a>$$G_{\text{on}} = RE_{NS} - \beta^{-1} \ln \binom{N_{NS}}{R}$$</span>
<span id="cb1-1335"><a href="#cb1-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1336"><a href="#cb1-1336" aria-hidden="true" tabindex="-1"></a>Notice how the entropy term $\ln\binom{N_{NS}}{R}$ assumes that the <span class="in">`repressor`</span> proteins are indistinguishable, just like in gas theory. If one of them is binding specifically, the Gibbs free energy is</span>
<span id="cb1-1337"><a href="#cb1-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1338"><a href="#cb1-1338" aria-hidden="true" tabindex="-1"></a>$$G_{\text{off}} = E_S + (R-1)E_{NS} - \beta^{-1} \ln \binom{N_{NS}}{R-1}$$</span>
<span id="cb1-1339"><a href="#cb1-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1340"><a href="#cb1-1340" aria-hidden="true" tabindex="-1"></a>Thus, the probability of <span class="in">`on`</span> vs <span class="in">`off`</span> satisfies</span>
<span id="cb1-1341"><a href="#cb1-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1342"><a href="#cb1-1342" aria-hidden="true" tabindex="-1"></a>$$\frac{p_{\text{on}}}{p_{\text{off}}} = e^{-\beta (G_{\text{on}} - G_{\text{off}})} = \frac{N_{NS} - R + 1}{R} e^{\beta (E_{NS} - E_{S})} \approx \frac{N_{NS}}{R} e^{\beta (E_{NS} - E_{S})}$$</span>
<span id="cb1-1343"><a href="#cb1-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1344"><a href="#cb1-1344" aria-hidden="true" tabindex="-1"></a>giving us</span>
<span id="cb1-1345"><a href="#cb1-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1346"><a href="#cb1-1346" aria-hidden="true" tabindex="-1"></a>$$p_{\text{on}} = \frac{N_{NS}}{N_{NS} + R e^{-\beta (E_{NS} - E_{S})}}$$</span>
<span id="cb1-1347"><a href="#cb1-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1348"><a href="#cb1-1348" aria-hidden="true" tabindex="-1"></a><span class="fu">### Unzipping RNA hairpins {#sec-rna-hairpin}</span></span>
<span id="cb1-1349"><a href="#cb1-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1350"><a href="#cb1-1350" aria-hidden="true" tabindex="-1"></a>The RNA molecules are polymers made of 4 different "letters" that can pair up as A-U and C-G. A common shape for single-stranded RNA is the "hairpin", pictured below.</span>
<span id="cb1-1351"><a href="#cb1-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1352"><a href="#cb1-1352" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">RNA hairpin. Figure from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Stem-loop.svg)</span><span class="co">](figure/rna_hairpin.png)</span></span>
<span id="cb1-1353"><a href="#cb1-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1354"><a href="#cb1-1354" aria-hidden="true" tabindex="-1"></a>Since each base pair has about 100 atoms, an RNA hairpin is a large molecule with $\sim 1000$ atoms. This is large enough for statistical mechanics, but not large enough to smooth out thermal fluctuations, which ought to make it interesting. Now, what happens if we pull on the hairpin? This is the experiment done by <span class="co">[</span><span class="ot">@liphardtReversibleUnfoldingSingle2001</span><span class="co">]</span>. They attached a single RNA hairpin to two beads, and pulled the beads apart very slowly, using force-feedback to keep the force stable within $0.1 \ut{pN}$. The RNA they used is <span class="in">`P5ab`</span>, which has about 20 base pairs, each base of length about 5 Angstrom.</span>
<span id="cb1-1355"><a href="#cb1-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1356"><a href="#cb1-1356" aria-hidden="true" tabindex="-1"></a>If we ignore fluctuation, and treat it by classical thermodynamics, we get $dS = \beta dE - \beta F dx$, where $F$ is the pulling force, and $x$ is the distance between the two tweezers. Looking at this equation, we immediately see that the problem is best analyzed using the following free entropy </span>
<span id="cb1-1357"><a href="#cb1-1357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1358"><a href="#cb1-1358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1359"><a href="#cb1-1359" aria-hidden="true" tabindex="-1"></a>h := \ub{S - \beta \braket{E}}{Helmholtz free entropy} + \beta F \braket{x}</span>
<span id="cb1-1360"><a href="#cb1-1360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1361"><a href="#cb1-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1362"><a href="#cb1-1362" aria-hidden="true" tabindex="-1"></a>which can be interpreted as "one-dimensional Gibbs free entropy".</span>
<span id="cb1-1363"><a href="#cb1-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1364"><a href="#cb1-1364" aria-hidden="true" tabindex="-1"></a>Let $x_0$ be the length of a single RNA unit, $n$ be the number of unzipped RNA base pairs, $2N$ be the total number of RNA bases, and $-E_0$ be the bonding energy of a base pair (assume that the two kinds of base pairs have the same bonding energy). </span>
<span id="cb1-1365"><a href="#cb1-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1366"><a href="#cb1-1366" aria-hidden="true" tabindex="-1"></a>Because the state of the system is fully determined once we know what $n$ is, the entropy $S$ conditional on $n$ is zero, and so, plugging in the @thm-cond-free-ent, the probability of the hairpin in state $n$ is:</span>
<span id="cb1-1367"><a href="#cb1-1367" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1368"><a href="#cb1-1368" aria-hidden="true" tabindex="-1"></a>\rho(n) \propto e^{\beta(2Fx_0 - E_0)n}, \quad 0 \leq 2n \leq 2N</span>
<span id="cb1-1369"><a href="#cb1-1369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1370"><a href="#cb1-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1371"><a href="#cb1-1371" aria-hidden="true" tabindex="-1"></a>a truncated exponential distribution. When $F = E_0/2x_0$, the bonding force and the pulling force are exactly balanced, and the hairpin is equally likely to be in any state. When the pulling force is larger, then $n$ concentrates on the $n = N$ end.</span>
<span id="cb1-1372"><a href="#cb1-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1373"><a href="#cb1-1373" aria-hidden="true" tabindex="-1"></a>For reasons they did not explain, and I do not understand (sorry), in the <span class="co">[</span><span class="ot">@liphardtReversibleUnfoldingSingle2001</span><span class="co">]</span> experiment, the RNA hairpin had *no* intermediate state. It either fully unfolded, or fully folded. In that case, the hairpin became a two-level system, satisfying</span>
<span id="cb1-1374"><a href="#cb1-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1375"><a href="#cb1-1375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1376"><a href="#cb1-1376" aria-hidden="true" tabindex="-1"></a>p_{\text{unfolded}} = \frac{e^{\beta( FL - \Delta G )}}{1 + e^{\beta( FL - \Delta G )}}</span>
<span id="cb1-1377"><a href="#cb1-1377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1378"><a href="#cb1-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1379"><a href="#cb1-1379" aria-hidden="true" tabindex="-1"></a>where $L = 18 \ut{nm}$ is the length increase in unfolding, and $\Delta G$ is the increase in Helmholtz free energy during unfolding. This is exactly what they observed.</span>
<span id="cb1-1380"><a href="#cb1-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1381"><a href="#cb1-1381" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">(Left) As the pulling force increases, the RNA hairpin spends more and more time in the unfolded state. In all cases, the RNA is binary, either fully folded or fully unfolded, never in-between. (Right) The probability of being in an unfolded state increases as a logistic function, as predicted. [@liphardtReversibleUnfoldingSingle2001, figure 1]</span><span class="co">](figure/liphardt_2001_1.jpeg)</span></span>
<span id="cb1-1382"><a href="#cb1-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1383"><a href="#cb1-1383" aria-hidden="true" tabindex="-1"></a>Another interesting finding is that when the pulling force increases slowly, the dissipated energy is small, but when the pulling force increases quickly, the wasted energy is large. In short, we have a form of speed limit second law: fast change requires more entropy production. We will discuss this in much more detail in the section on <span class="co">[</span><span class="ot">Crooks fluctuation theorem</span><span class="co">](#sec-cft)</span>.</span>
<span id="cb1-1384"><a href="#cb1-1384" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-1385"><a href="#cb1-1385" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Trajectories of folding and unfolding cycles. The area between the upper and lower curves is the dissipated energy during one cycle. Fast cycles dissipate more energy. [@liphardtReversibleUnfoldingSingle2001, figure 1]</span><span class="co">](figure/liphardt_2001_2.jpeg)</span>{width=50%}</span>
<span id="cb1-1386"><a href="#cb1-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1387"><a href="#cb1-1387" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hungry hungry bacteria</span></span>
<span id="cb1-1388"><a href="#cb1-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1389"><a href="#cb1-1389" aria-hidden="true" tabindex="-1"></a>This example came from <span class="co">[</span><span class="ot">@bergRandomWalksBiology1993, chapter 6</span><span class="co">]</span>. See <span class="co">[</span><span class="ot">@bergPhysicsChemoreception1977; @bergMotileBehaviorBacteria2000</span><span class="co">]</span> for a detailed look at how bacteria optimally forage for food molecules.</span>
<span id="cb1-1390"><a href="#cb1-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1391"><a href="#cb1-1391" aria-hidden="true" tabindex="-1"></a>Consider a bacteria swimming in water. A typical one, such as *E. coli*, is roughly a rod with length $a = 10^{-6}m$ and swimming at $v = 2\times 10^{-5} m/s$. That is, it swims 20 body-lengths a second.</span>
<span id="cb1-1392"><a href="#cb1-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1393"><a href="#cb1-1393" aria-hidden="true" tabindex="-1"></a>Water has density $\rho = 10^3 kg/m^3$ and viscosity $\eta = 10^{-3} kg/m\cdot s$. As is well-known, the <span class="co">[</span><span class="ot">Reynold number</span><span class="co">](https://en.wikipedia.org/wiki/Reynolds_number)</span> of the system is</span>
<span id="cb1-1394"><a href="#cb1-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1395"><a href="#cb1-1395" aria-hidden="true" tabindex="-1"></a>$$Re = \frac{\rho v a}{\eta} = 2 \times 10^{-5}$$</span>
<span id="cb1-1396"><a href="#cb1-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1397"><a href="#cb1-1397" aria-hidden="true" tabindex="-1"></a>meaning that as soon as the bacteria stops powering itself, its motion ceases after coasting for a length of $\approx 0.1 a \cdot Re = 2\times 10^{-12}m$, less than the length of an atom! Thus, the bacteria lives in an essentially inertia-less world.  </span>
<span id="cb1-1398"><a href="#cb1-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1399"><a href="#cb1-1399" aria-hidden="true" tabindex="-1"></a>Assuming that a bacteria is a sphere, then its swimming dissipates power at</span>
<span id="cb1-1400"><a href="#cb1-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1401"><a href="#cb1-1401" aria-hidden="true" tabindex="-1"></a>$$P = Fv = 6\pi \eta a v^2 \approx 8\times 10^{-18}W \approx 2000 k_BT/s$$</span>
<span id="cb1-1402"><a href="#cb1-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1403"><a href="#cb1-1403" aria-hidden="true" tabindex="-1"></a>where $T = 300 K$ is the standard temperature for biology. Since each glucose at complete metabolism produces $686 kcal/mol \approx 1000 k_BT$, the bacteria just needs to eat 2 molecules of glucose per second to power its swimming.  </span>
<span id="cb1-1404"><a href="#cb1-1404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1405"><a href="#cb1-1405" aria-hidden="true" tabindex="-1"></a>By the FDR, the diffusion constant for the bacteria is</span>
<span id="cb1-1406"><a href="#cb1-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1407"><a href="#cb1-1407" aria-hidden="true" tabindex="-1"></a>$$D = \frac{k_BT}{6\pi \eta a} = 2 \times 10^{-13} m^2/s$$</span>
<span id="cb1-1408"><a href="#cb1-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1409"><a href="#cb1-1409" aria-hidden="true" tabindex="-1"></a>meaning that within 1 second, the bacteria diffuses a distance of $\sim \sqrt{2 D \Delta t} \sim a$, about 1 body length. This shows that the swimming motion is 20 times faster than its thermal motion.  </span>
<span id="cb1-1410"><a href="#cb1-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1411"><a href="#cb1-1411" aria-hidden="true" tabindex="-1"></a>However, if we look at the trajectory of a bacterium under the microscope, we would notice its direction jumping around rapidly. This is due to the thermal motion of its orientation on the rotational group $SO(3)$. Just like how each translational degree of freedom gets $\frac 12 k_BT$ of thermal energy, each rotational degree of freedom gets it as well, although, because the space of rotations is not a vector space, making this precise is tricky.</span>
<span id="cb1-1412"><a href="#cb1-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1413"><a href="#cb1-1413" aria-hidden="true" tabindex="-1"></a>Still, once we make it precise, the FDR is still the same, gives us $D_{\text{rotational}} = \frac{k_B T}{\gamma_{\text{rotational}}}$. Assuming the bacterium is still a sphere of radius $a$, then $\gamma_{\text{rotational}} = 8\pi \eta a^3$, giving $D_{\text{rotational}} = 0.2 \ut{rad^2/s}$.</span>
<span id="cb1-1414"><a href="#cb1-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1415"><a href="#cb1-1415" aria-hidden="true" tabindex="-1"></a>Now, a swimming bacterium does not care about rotation around its velocity axis ("longitude"), but does care about the other two rotational freedoms. The variation in "latitude" angle is the sum:</span>
<span id="cb1-1416"><a href="#cb1-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1417"><a href="#cb1-1417" aria-hidden="true" tabindex="-1"></a>$$\braket{\theta^2} = 2 \times (2D_{\text{rotational}}\Delta t)$$</span>
<span id="cb1-1418"><a href="#cb1-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1419"><a href="#cb1-1419" aria-hidden="true" tabindex="-1"></a>for small time $\Delta t$. This is false for large $\Delta t$, because otherwise we would get $\braket{\theta^2} &gt; \pi^2$, which is absurd because $\theta \in <span class="co">[</span><span class="ot">0, \pi</span><span class="co">]</span>$.  </span>
<span id="cb1-1420"><a href="#cb1-1420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1421"><a href="#cb1-1421" aria-hidden="true" tabindex="-1"></a>This implies that the bacteria veers by about $\braket{\theta^2} \approx (50^\circ)^2$ after a second of swimming. The actual observation gives $\braket{\theta^2} \approx (30^\circ)^2$. In particular, the bacteria cannot keep a direction for more than about 3 seconds, as at that point its velocity vector would have diffused by about 90 degrees. As the bacteria has no "vestibular organ", it essentially "forgets its heading" within a few seconds. Any sense of direction must come from the outside world, such as a chemical gradient.</span>
<span id="cb1-1422"><a href="#cb1-1422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1423"><a href="#cb1-1423" aria-hidden="true" tabindex="-1"></a>The standard strategy for *E. coli* hunting is the "run and tumble" strategy. During the "run" phase, the bacterium keeps swimming forward for 1 -- 10 seconds, curving gently due to rotational diffusion. At a random point it stops and tumbles for about 0.1 seconds, changing its direction randomly. Tumbling essentially samples a random direction for the next run, close but not quite uniformly on the sphere. It remembers the average chemical concentration during the first second and last second of each run. If the chemical concentration seem to increase, it would tumble less often (thus going on longer runs). Otherwise, it would tumble more often. The net effect is an extremely simple homing missile that diffuses up the chemical gradient.</span>
<span id="cb1-1424"><a href="#cb1-1424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1425"><a href="#cb1-1425" aria-hidden="true" tabindex="-1"></a>There is no point for it to swim more than 10 seconds, because it would have totally lost its heading due to diffusion. There is no point in swimming less than 1 second, because it needs to average the chemical concentration over at least a second to overcome the statistical noise. And so, by simple physics, we have explained an *E. coli*'s hunting strategy.</span>
<span id="cb1-1426"><a href="#cb1-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1427"><a href="#cb1-1427" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A spherical bacterium is propelled by spinning flagella. The spherical bacterium swims along the $x$ -axis. Its rotational state diffuses along three possible degrees of freedom. Of those, the one around $x$-axis is irrelevant, and the ones around $y$- and $z$-axes are relevant. [@bergRandomWalksBiology1993, figure 6.6]</span><span class="co">](figure/bacterial_swimming_berg_1993.png)</span>{width=70%}</span>
<span id="cb1-1428"><a href="#cb1-1428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1429"><a href="#cb1-1429" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@bergChemotaxisEscherichiaColi1972</span><span class="co">]</span> tracked the 3D trajectory of *E. coli* in a liquid that is $3\times$ more viscous than water. They found that the bacterium alternates between swimming and tumbling. During the swimming phase, the angular diffusion has a diffusion constant of $0.06 \ut{rad^2/s}$, as theoretically predicted.</span>
<span id="cb1-1430"><a href="#cb1-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1431"><a href="#cb1-1431" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A single 3D trajectory of a single bacterium, projected into the $xy, yz, xz$ planes. Gently winding runs alternate with the tight tumbles. [@bergChemotaxisEscherichiaColi1972, figure 1]</span><span class="co">](figure/bacterial_trajectory_berg_1972.png)</span></span>
<span id="cb1-1432"><a href="#cb1-1432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1433"><a href="#cb1-1433" aria-hidden="true" tabindex="-1"></a><span class="fu">## Statistical field theory</span></span>
<span id="cb1-1434"><a href="#cb1-1434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1435"><a href="#cb1-1435" aria-hidden="true" tabindex="-1"></a><span class="fu">### Maximum caliber</span></span>
<span id="cb1-1436"><a href="#cb1-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1437"><a href="#cb1-1437" aria-hidden="true" tabindex="-1"></a>Equilibrium statistical mechanics is typically called a "static" theory: It deals with the ensemble of states, but not with how the states change over time. Maximal caliber statistical mechanics handles trajectories in the most obvious way: Collect all paths into a "path space", define a measure over path space, then study constrained entropy maximization over path space.</span>
<span id="cb1-1438"><a href="#cb1-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1439"><a href="#cb1-1439" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Edward Jaynes</span><span class="co">](https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes)</span>, who proposed the idea, called path space entropy "caliber", so the name "maximum caliber" stuck, even though it is really just standard equilibrium statistical mechanics in path space. In other words, it is just standard statistical field theory, where the fields are of type $\phi: \R \to \mathcal S$, where $\R$ stand for time, and $\mathcal S$ stand for state space of the system.</span>
<span id="cb1-1440"><a href="#cb1-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1441"><a href="#cb1-1441" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Markov chains</span></span>
<span id="cb1-1442"><a href="#cb1-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1443"><a href="#cb1-1443" aria-hidden="true" tabindex="-1"></a>The simplest example is when time comes in discrete steps. In this case, we can reconstruct Markov chains as a particular kind of maximum caliber distribution.</span>
<span id="cb1-1444"><a href="#cb1-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1445"><a href="#cb1-1445" aria-hidden="true" tabindex="-1"></a>Consider trajectories of type $<span class="sc">\{</span>0, 1, 2, \dots, N<span class="sc">\}</span> \to <span class="sc">\{</span>1, 2, \dots, m<span class="sc">\}</span>$, and let $s_t$ is the state of timestep $t$.</span>
<span id="cb1-1446"><a href="#cb1-1446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1447"><a href="#cb1-1447" aria-hidden="true" tabindex="-1"></a>If we fix the singleton probability $p_i$ of each state, then the maximum entropy distribution is the Markov chain with uniform initial probability and $p_{i\to j} = p_i p_j$.</span>
<span id="cb1-1448"><a href="#cb1-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1449"><a href="#cb1-1449" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="derivation" collapse="true"}</span>
<span id="cb1-1450"><a href="#cb1-1450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1451"><a href="#cb1-1451" aria-hidden="true" tabindex="-1"></a>If we fix the singleton probability of each state, then the problem is a constrained maximization problem</span>
<span id="cb1-1452"><a href="#cb1-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1453"><a href="#cb1-1453" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1454"><a href="#cb1-1454" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-1455"><a href="#cb1-1455" aria-hidden="true" tabindex="-1"></a>    \max S <span class="sc">\\</span></span>
<span id="cb1-1456"><a href="#cb1-1456" aria-hidden="true" tabindex="-1"></a>    \frac 1N \sum_t 1<span class="co">[</span><span class="ot">s_t = k</span><span class="co">]</span> = p_k, \quad \forall k = 1, \dots, m</span>
<span id="cb1-1457"><a href="#cb1-1457" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-1458"><a href="#cb1-1458" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1459"><a href="#cb1-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1460"><a href="#cb1-1460" aria-hidden="true" tabindex="-1"></a>This is the same problem as $N$ balls in a certain constrained microcanonical ensemble. When $N$ is large enough, the discrete "macrostate" $\frac 1N \sum_t 1<span class="co">[</span><span class="ot">s_t = k</span><span class="co">]</span>$ becomes continuous, and we can use the Lagrange multiplier to obtain</span>
<span id="cb1-1461"><a href="#cb1-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1462"><a href="#cb1-1462" aria-hidden="true" tabindex="-1"></a>$$\rho(s_1, \dots, s_N) = \frac 1Z e^{-\sum_{k=1}^m \lambda_k (\frac 1N \sum_{t=1}^N 1<span class="co">[</span><span class="ot">s_t =k</span><span class="co">]</span> - p_k)}$$</span>
<span id="cb1-1463"><a href="#cb1-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1464"><a href="#cb1-1464" aria-hidden="true" tabindex="-1"></a>This factors over time $t$, giving us</span>
<span id="cb1-1465"><a href="#cb1-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1466"><a href="#cb1-1466" aria-hidden="true" tabindex="-1"></a>$$\rho(s_1, \dots, s_N) = \prod_{t=1}^N \rho(s_t)$$</span>
<span id="cb1-1467"><a href="#cb1-1467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1468"><a href="#cb1-1468" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb1-1469"><a href="#cb1-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1470"><a href="#cb1-1470" aria-hidden="true" tabindex="-1"></a>$$\rho(s_t=k) \propto e^{-\sum_{k=1}^m\frac{\lambda_k}{N}(1<span class="co">[</span><span class="ot">s_t =k</span><span class="co">]</span> - p_k)} \propto e^{-\frac{\lambda_k}{N}}$$</span>
<span id="cb1-1471"><a href="#cb1-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1472"><a href="#cb1-1472" aria-hidden="true" tabindex="-1"></a>The multiplier $\lambda_k$ can be found by the typical method of solving $p_k = -\partial_{\lambda_k}\ln Z$, or we can be clever and notice that the constraint implies $\rho(s_t=k) = p_k$.</span>
<span id="cb1-1473"><a href="#cb1-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1474"><a href="#cb1-1474" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1475"><a href="#cb1-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1476"><a href="#cb1-1476" aria-hidden="true" tabindex="-1"></a>Similarly, if we fix the transition probability $p_{i \to j}$ of each state-pair, then the maximum entropy distribution is the Markov chain with uniform initial probability. If we fix the initial probability, then it's still the Markov chain with the same transition probabilities.</span>
<span id="cb1-1477"><a href="#cb1-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1478"><a href="#cb1-1478" aria-hidden="true" tabindex="-1"></a>And more generally, if we fix $n$-th order transition probability $p_{i_1, \dots, i_n \to j}$, then we obtain an $n$-th order Markov chain model.</span>
<span id="cb1-1479"><a href="#cb1-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1480"><a href="#cb1-1480" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="derivation" collapse="true"}</span>
<span id="cb1-1481"><a href="#cb1-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1482"><a href="#cb1-1482" aria-hidden="true" tabindex="-1"></a>Similarly as above, the path-space distribution is</span>
<span id="cb1-1483"><a href="#cb1-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1484"><a href="#cb1-1484" aria-hidden="true" tabindex="-1"></a>$$\rho(s_1, \dots, s_N) \propto \prod_{t=1}^{N-1} e^{-\sum_{k, k' \in 1:m} \frac{\lambda_{k, k'}}{N} 1<span class="co">[</span><span class="ot">s_t = k, s_{t+1} = k'</span><span class="co">]</span>} \propto \prod_{t=1}^{N-1} p_{s_t \to s_{t+1}}$$</span>
<span id="cb1-1485"><a href="#cb1-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1486"><a href="#cb1-1486" aria-hidden="true" tabindex="-1"></a>Because the distribution does not specify $s_1$, it is uniformly distributed on $s_1$. Otherwise, we can constrain $s_1$ with yet another set of Lagrange multipliers and obtain </span>
<span id="cb1-1487"><a href="#cb1-1487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1488"><a href="#cb1-1488" aria-hidden="true" tabindex="-1"></a>$$\rho(s_1, \dots, s_N) \propto \rho(s_1) \times_{t=1}^{N-1} \rho(s_t, s_{t+1})$$</span>
<span id="cb1-1489"><a href="#cb1-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1490"><a href="#cb1-1490" aria-hidden="true" tabindex="-1"></a>Similarly for higher orders.</span>
<span id="cb1-1491"><a href="#cb1-1491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1492"><a href="#cb1-1492" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1493"><a href="#cb1-1493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1494"><a href="#cb1-1494" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Diffusion</span></span>
<span id="cb1-1495"><a href="#cb1-1495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1496"><a href="#cb1-1496" aria-hidden="true" tabindex="-1"></a>In diffusion, we consider paths of type $x: <span class="co">[</span><span class="ot">0, T</span><span class="co">]</span> \to \R^n$ with $x(0) = 0$. The path-space entropy </span>
<span id="cb1-1497"><a href="#cb1-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1498"><a href="#cb1-1498" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1499"><a href="#cb1-1499" aria-hidden="true" tabindex="-1"></a>S = - \int \rho(x) \ln \rho(x) D<span class="co">[</span><span class="ot">x</span><span class="co">]</span></span>
<span id="cb1-1500"><a href="#cb1-1500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1501"><a href="#cb1-1501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1502"><a href="#cb1-1502" aria-hidden="true" tabindex="-1"></a>where $D<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$ means we integrate over path space. Here we see a proper path-integral, which is roughly speaking what happens when we integrate not over $\R^{10}$ or even $\R^{10^{23}}$, but literally $\R^\infty$. </span>
<span id="cb1-1503"><a href="#cb1-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1504"><a href="#cb1-1504" aria-hidden="true" tabindex="-1"></a>As usual, path integrals cannot be done directly, but can be done by first dropping down to $\R^N$, where $N$ is large but still finite, then hope that the result is still sensible at the $N \to \infty$ limit. If this disturbs you, sorry. It disturbs me too, but it is necessary for all but the most trivial calculations in field theory.</span>
<span id="cb1-1505"><a href="#cb1-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1506"><a href="#cb1-1506" aria-hidden="true" tabindex="-1"></a>Discretize continuous-time path $x : <span class="co">[</span><span class="ot">0, T</span><span class="co">]</span> \to \R^n$ into discrete-time path $x: <span class="sc">\{</span>0, 1, 2, \dots, N<span class="sc">\}</span> \to \R^n$. Because we can decompose</span>
<span id="cb1-1507"><a href="#cb1-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1508"><a href="#cb1-1508" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1509"><a href="#cb1-1509" aria-hidden="true" tabindex="-1"></a>\rho(x) = \rho(x_0) \rho(x_1 | x_0) \cdots \rho(x_N | x_{0:N-1})</span>
<span id="cb1-1510"><a href="#cb1-1510" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1511"><a href="#cb1-1511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1512"><a href="#cb1-1512" aria-hidden="true" tabindex="-1"></a>the path-space entropy decomposes sequentially:</span>
<span id="cb1-1513"><a href="#cb1-1513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1514"><a href="#cb1-1514" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1515"><a href="#cb1-1515" aria-hidden="true" tabindex="-1"></a>S = S<span class="co">[</span><span class="ot">x_0</span><span class="co">]</span> + \braket{S<span class="co">[</span><span class="ot">x_1 | x_0</span><span class="co">]</span>} + \braket{S<span class="co">[</span><span class="ot">x_2 | x_{0:1}</span><span class="co">]</span>} + \dots + \braket{S<span class="co">[</span><span class="ot">x_N | x_{0:N-1}</span><span class="co">]</span>}</span>
<span id="cb1-1516"><a href="#cb1-1516" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1517"><a href="#cb1-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1518"><a href="#cb1-1518" aria-hidden="true" tabindex="-1"></a>To prevent the entropy from diverging, we need to impose some constraints. If we constrain the second moment of each step, as </span>
<span id="cb1-1519"><a href="#cb1-1519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1520"><a href="#cb1-1520" aria-hidden="true" tabindex="-1"></a>$$(\braket{x_t|x_{0:t-1}}, \braket{x_t^2|x_{0:t-1}}) = (0, \sigma^2), \quad \forall t \in 0:N$$</span>
<span id="cb1-1521"><a href="#cb1-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1522"><a href="#cb1-1522" aria-hidden="true" tabindex="-1"></a>by reasoning backwards from $t = N$ to $t=0$, we find that the maximal entropy distribution is a white noise:  </span>
<span id="cb1-1523"><a href="#cb1-1523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1524"><a href="#cb1-1524" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1525"><a href="#cb1-1525" aria-hidden="true" tabindex="-1"></a>\rho(x) = \prod_{t\in 0:N} \rho(x_t), \quad \rho(x_t) \propto e^{-\frac{<span class="sc">\|</span>x_t<span class="sc">\|</span>^2}{2\sigma^2}}</span>
<span id="cb1-1526"><a href="#cb1-1526" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1527"><a href="#cb1-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1528"><a href="#cb1-1528" aria-hidden="true" tabindex="-1"></a>If you have studied <span class="co">[</span><span class="ot">dynamic programming</span><span class="co">](https://en.wikipedia.org/wiki/Dynamic_programming)</span> and cybernetics, this should look very similar to the argument by which you derived the <span class="co">[</span><span class="ot">LQR</span><span class="co">](https://en.wikipedia.org/wiki/Linear-quadratic_regulator)</span>.</span>
<span id="cb1-1529"><a href="#cb1-1529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1530"><a href="#cb1-1530" aria-hidden="true" tabindex="-1"></a>To keep the path from exploding into white noise, we instead impose the constraints on the sizes of the steps</span>
<span id="cb1-1531"><a href="#cb1-1531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1532"><a href="#cb1-1532" aria-hidden="true" tabindex="-1"></a>$$(\braket{x_t - x_{t-1}|x_{0:t-1}}, \braket{<span class="sc">\|</span>x_t - x_{t-1}<span class="sc">\|</span>^2|x_{0:t-1}}) = (0, \sigma^2), \quad \forall t \in 1:N$$</span>
<span id="cb1-1533"><a href="#cb1-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1534"><a href="#cb1-1534" aria-hidden="true" tabindex="-1"></a>Now, as in dynamic programming, we reason backwards from $t = N$ to $t=0$, and we find that the maximal entropy distribution is the Brownian motion</span>
<span id="cb1-1535"><a href="#cb1-1535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1536"><a href="#cb1-1536" aria-hidden="true" tabindex="-1"></a>$$\rho(x) \propto e^{-\frac{\sum_{t\in 1:N} <span class="sc">\|</span> x_t-x_{t-1}<span class="sc">\|</span>^2}{2\sigma^2}}$$</span>
<span id="cb1-1537"><a href="#cb1-1537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1538"><a href="#cb1-1538" aria-hidden="true" tabindex="-1"></a>If we constrain the first *and* the second moments of each step, and also allow them to be affected by the previous step, as in</span>
<span id="cb1-1539"><a href="#cb1-1539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1540"><a href="#cb1-1540" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1541"><a href="#cb1-1541" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-1542"><a href="#cb1-1542" aria-hidden="true" tabindex="-1"></a>    \braket{x_t - x_{t-1}|x_{0:t-1}} &amp;= \mu(t, x_{t-1}) <span class="sc">\\</span></span>
<span id="cb1-1543"><a href="#cb1-1543" aria-hidden="true" tabindex="-1"></a>    \braket{(x_t - x_{t-1}) (x_t - x_{t-1})^T|x_{0:t-1}} &amp;= \Sigma(t, x_{t-1})</span>
<span id="cb1-1544"><a href="#cb1-1544" aria-hidden="true" tabindex="-1"></a>\end{cases}, \quad \forall t \in 1:N</span>
<span id="cb1-1545"><a href="#cb1-1545" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1546"><a href="#cb1-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1547"><a href="#cb1-1547" aria-hidden="true" tabindex="-1"></a>then, reasoning backwards as before, we would obtain the <span class="co">[</span><span class="ot">Langevin equation</span><span class="co">](https://en.wikipedia.org/wiki/Langevin_equation)</span>. </span>
<span id="cb1-1548"><a href="#cb1-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1549"><a href="#cb1-1549" aria-hidden="true" tabindex="-1"></a>Other results, such as the <span class="co">[</span><span class="ot">Green--Kubo relations</span><span class="co">](https://en.wikipedia.org/wiki/Green%E2%80%93Kubo_relations)</span>, the <span class="co">[</span><span class="ot">Onsager reciprocal relations</span><span class="co">](https://en.wikipedia.org/wiki/Onsager_reciprocal_relations)</span>, etc, can be similarly derived by imposing the right constraints in path space. <span class="co">[</span><span class="ot">@hazoglouCommunicationMaximumCaliber2015</span><span class="co">]</span></span>
<span id="cb1-1550"><a href="#cb1-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1551"><a href="#cb1-1551" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fluctuation-dissipation relations</span></span>
<span id="cb1-1552"><a href="#cb1-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1553"><a href="#cb1-1553" aria-hidden="true" tabindex="-1"></a>Imagine if you have a weird liquid. You put a piece of pollen into it, and watch it undergo Brownian motion. Now I flip a switch and suddenly all friction disappears from the liquid. What would happen? The pollen is still being hit by random pieces of molecules, so its velocity is still getting pushed this and that way. However, without friction, the velocity at time $t$ is now obtained by integrating all past impulses, with no dissipation whatsoever. Consequently, its velocity undergoes a random walk, and its position undergoes $\int_0^t W_s ds$. This is very different from what we actually observe, which is that its position undergoes a random walk, not a time-integral of it.</span>
<span id="cb1-1554"><a href="#cb1-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1555"><a href="#cb1-1555" aria-hidden="true" tabindex="-1"></a>In order to reproduce the actual observed behavior, each fluctuation of its velocity must be quickly dissipated by friction, in a perfect balance such that $\frac 12 m \braket{v^2} = k_BT$ exactly. This perfect conspiracy is the fluctuation-dissipation relation (FDR), or rather, the *family* of FDRs, because there have been so many of those.</span>
<span id="cb1-1556"><a href="#cb1-1556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1557"><a href="#cb1-1557" aria-hidden="true" tabindex="-1"></a>Each FDR is a mathematical equation of form</span>
<span id="cb1-1558"><a href="#cb1-1558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1559"><a href="#cb1-1559" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1560"><a href="#cb1-1560" aria-hidden="true" tabindex="-1"></a>\text{something about fluctuation} = \text{something about dissipation}</span>
<span id="cb1-1561"><a href="#cb1-1561" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1562"><a href="#cb1-1562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1563"><a href="#cb1-1563" aria-hidden="true" tabindex="-1"></a>The prototypical FDR is the <span class="co">[</span><span class="ot">Einstein relation</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Einstein_relation_(kinetic_theory)), to be derived below:</span>
<span id="cb1-1564"><a href="#cb1-1564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1565"><a href="#cb1-1565" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1566"><a href="#cb1-1566" aria-hidden="true" tabindex="-1"></a>\ub{\beta D}{fluctuation} = \ub{(\gamma\beta)^{-1}}{dissipation}</span>
<span id="cb1-1567"><a href="#cb1-1567" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1568"><a href="#cb1-1568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1569"><a href="#cb1-1569" aria-hidden="true" tabindex="-1"></a>where the left side can be interpreted as the strength of molecular noise, and the right side as the strength of molecular damping.</span>
<span id="cb1-1570"><a href="#cb1-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1571"><a href="#cb1-1571" aria-hidden="true" tabindex="-1"></a><span class="fu">### Equality before the law</span></span>
<span id="cb1-1572"><a href="#cb1-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1573"><a href="#cb1-1573" aria-hidden="true" tabindex="-1"></a>Why should there be a mathematical relation between fluctuation and dissipation? I think the deep reason is "equality before the second law".</span>
<span id="cb1-1574"><a href="#cb1-1574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1575"><a href="#cb1-1575" aria-hidden="true" tabindex="-1"></a>If we pause and think about it, then isn't it strange that, despite the molecular storm happening within a placid cup of water, it does not actually fall out of equilibrium? That, despite every molecule of water rushing this way and that, the cup of water as a whole continues to stay without a ripple? What is this second law, the invisible hand pushing it towards statistical equilibrium and keeping it stable? And that is not a question I'm going to answer here. Perhaps calling it an "invisible hand" is already a bad metaphor. Nevertheless, the second law requires the stability of equilibrium distributions (if equilibria exist).</span>
<span id="cb1-1576"><a href="#cb1-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1577"><a href="#cb1-1577" aria-hidden="true" tabindex="-1"></a>Now, a system in a statistical equilibrium would, once in a while, deviate significantly from the maximally likely state, just like how we might occasionally flip 10 heads in a row. Nevertheless, if the second law is to be upheld, then significant deviations must be pushed back. In other words, fluctuation is dissipated.</span>
<span id="cb1-1578"><a href="#cb1-1578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1579"><a href="#cb1-1579" aria-hidden="true" tabindex="-1"></a>Assuming that in equilibrium, the system is undergoing a random walk, then by the theory of random walks, internally generated fluctuation must be dissipated according to a mathematical law. The law does not need any further input from physics. Indeed, it has no dependence on any physical observations, and belongs to the pure mathematical theory of random walks.</span>
<span id="cb1-1580"><a href="#cb1-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1581"><a href="#cb1-1581" aria-hidden="true" tabindex="-1"></a>For example, a little pollen in a dish of water might occasionally have a large momentum, but it is very rare. Because it is rare, if we do observe it, we can predict that the next time we look at a pollen, it would have a much lower momentum.</span>
<span id="cb1-1582"><a href="#cb1-1582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1583"><a href="#cb1-1583" aria-hidden="true" tabindex="-1"></a>Now, when physicists use the word "dissipation", they mean the restoring effect of a system under *external* dissipation, such as pulling on a pollen by an electrostatic field. However, physical laws are equal, so internal dissipation is external dissipation.</span>
<span id="cb1-1584"><a href="#cb1-1584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1585"><a href="#cb1-1585" aria-hidden="true" tabindex="-1"></a>Thus, we see that each FDR manifests as an "equality before the law":</span>
<span id="cb1-1586"><a href="#cb1-1586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1587"><a href="#cb1-1587" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1588"><a href="#cb1-1588" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-1589"><a href="#cb1-1589" aria-hidden="true" tabindex="-1"></a>&amp;\text{fluctuation} <span class="sc">\\</span></span>
<span id="cb1-1590"><a href="#cb1-1590" aria-hidden="true" tabindex="-1"></a>\ub{=}{random walk theory} &amp;\text{dissipation (of internal fluctuations)} <span class="sc">\\</span></span>
<span id="cb1-1591"><a href="#cb1-1591" aria-hidden="true" tabindex="-1"></a>\ub{=}{equality before the law} &amp;\text{dissipation (of external fluctuations)}</span>
<span id="cb1-1592"><a href="#cb1-1592" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-1593"><a href="#cb1-1593" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1594"><a href="#cb1-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1595"><a href="#cb1-1595" aria-hidden="true" tabindex="-1"></a><span class="fu">### One-dimensional FDR</span></span>
<span id="cb1-1596"><a href="#cb1-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1597"><a href="#cb1-1597" aria-hidden="true" tabindex="-1"></a>Consider a simple model of Brownian motion. We have a particle in a sticky fluid, moving on a line. The particle starts at $x = 0, t = 0$, and at each time-step of $\Delta t$, it moves by $\Delta x$ to the left or the right.</span>
<span id="cb1-1598"><a href="#cb1-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1599"><a href="#cb1-1599" aria-hidden="true" tabindex="-1"></a>The fluid is at temperature $\beta^{-1}$, and we pull on the particle at constant force $F$. We expect that $F = \gamma \braket{v}$, where $v$ is the ensemble-average velocity of the particle, and $\gamma$ is the viscosity constant.</span>
<span id="cb1-1600"><a href="#cb1-1600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1601"><a href="#cb1-1601" aria-hidden="true" tabindex="-1"></a>Now, we let the particle move for a time $t = N\Delta t$, where $N$ is a large number. The particle would have arrived at some point $x$, which is a random variable. The particle's time-averaged velocity is $v = x/t$.</span>
<span id="cb1-1602"><a href="#cb1-1602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1603"><a href="#cb1-1603" aria-hidden="true" tabindex="-1"></a>The number of possible paths that connect $(0, 0)$ with $(t, x)$ is $\binom{N}{\frac N2 - \frac{x}{2\Delta x}}$, therefore, the path-space entropy is</span>
<span id="cb1-1604"><a href="#cb1-1604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1605"><a href="#cb1-1605" aria-hidden="true" tabindex="-1"></a>$$S_{\text{path}} = \ln \binom{N}{\frac N2 - \frac{x}{2\Delta x}} \approx N \lrb{\ln 2 - \lrb{\frac{x}{N\Delta x}}^2}$$</span>
<span id="cb1-1606"><a href="#cb1-1606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1607"><a href="#cb1-1607" aria-hidden="true" tabindex="-1"></a>where the approximation is either by Stirling's approximation, or the binary entropy function.</span>
<span id="cb1-1608"><a href="#cb1-1608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1609"><a href="#cb1-1609" aria-hidden="true" tabindex="-1"></a>Because the external force performs work $Fx$, which is dissipated into the sticky liquid at temperature $\beta$, we also have</span>
<span id="cb1-1610"><a href="#cb1-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1611"><a href="#cb1-1611" aria-hidden="true" tabindex="-1"></a>$$S_{\text{work}} = \beta F x$$</span>
<span id="cb1-1612"><a href="#cb1-1612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1613"><a href="#cb1-1613" aria-hidden="true" tabindex="-1"></a>Because $N$ is large, $\braket{x}$ should be highly concentrated around the point of maximal entropy. That is, we should have</span>
<span id="cb1-1614"><a href="#cb1-1614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1615"><a href="#cb1-1615" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1616"><a href="#cb1-1616" aria-hidden="true" tabindex="-1"></a>\braket{x} \approx \argmax_x (S_{\text{path}} + S_{\text{work}})</span>
<span id="cb1-1617"><a href="#cb1-1617" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1618"><a href="#cb1-1618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1619"><a href="#cb1-1619" aria-hidden="true" tabindex="-1"></a>The equation on the right is quadratic in $\braket{x}$, and achieves maximum at $2\braket{x} = \beta FN(\Delta x)^2$, which simplifies to the Einstein relation </span>
<span id="cb1-1620"><a href="#cb1-1620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1621"><a href="#cb1-1621" aria-hidden="true" tabindex="-1"></a>$$\beta D \gamma = 1$$</span>
<span id="cb1-1622"><a href="#cb1-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1623"><a href="#cb1-1623" aria-hidden="true" tabindex="-1"></a>where $D = \frac{\Delta x^2}{2\Delta t}$ is the diffusion coefficient. </span>
<span id="cb1-1624"><a href="#cb1-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1625"><a href="#cb1-1625" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb1-1626"><a href="#cb1-1626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1627"><a href="#cb1-1627" aria-hidden="true" tabindex="-1"></a>We can calculate not just the mean $\braket{x}$, but also its variance $\braket{x^2}$ if we expand $S_{\text{path}} + S_{\text{work}}$ to second order around its maximum, then apply @thm-cond-ent.</span>
<span id="cb1-1628"><a href="#cb1-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1629"><a href="#cb1-1629" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1630"><a href="#cb1-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1631"><a href="#cb1-1631" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="rubber band"}</span>
<span id="cb1-1632"><a href="#cb1-1632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1633"><a href="#cb1-1633" aria-hidden="true" tabindex="-1"></a>We notice how this is identical to the <span class="co">[</span><span class="ot">rubber band system</span><span class="co">](#sec-rubber-band)</span>. We can think of the shape of a rubber band as a timeless image of the trajectory of in time. Dissipation is identical to the entropic force pulling on the rubber band. Fluctuation is the variance in rubber band length. The dependence of the elastic constant on temperature, $F/L = \frac{T}{Nd^2}$, is then the FDR for the rubber band:</span>
<span id="cb1-1634"><a href="#cb1-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1635"><a href="#cb1-1635" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1636"><a href="#cb1-1636" aria-hidden="true" tabindex="-1"></a>\beta \lrb{\frac{d^2}{1/N}} (F/L) = 1</span>
<span id="cb1-1637"><a href="#cb1-1637" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1638"><a href="#cb1-1638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1639"><a href="#cb1-1639" aria-hidden="true" tabindex="-1"></a>This is the simplest example of the general idea of <span class="co">[</span><span class="ot">Wick rotation</span><span class="co">](https://en.wikipedia.org/wiki/Wick_rotation)</span>, where a problem in $n$-dimensional space and $1$-dimensional time is converted to a problem in $(n+1)$-dimensional space.</span>
<span id="cb1-1640"><a href="#cb1-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1641"><a href="#cb1-1641" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1642"><a href="#cb1-1642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1643"><a href="#cb1-1643" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sound waves</span></span>
<span id="cb1-1644"><a href="#cb1-1644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1645"><a href="#cb1-1645" aria-hidden="true" tabindex="-1"></a>Recall that the Helmholtz free entropy of ideal gas is</span>
<span id="cb1-1646"><a href="#cb1-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1647"><a href="#cb1-1647" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1648"><a href="#cb1-1648" aria-hidden="true" tabindex="-1"></a>f = N \lrb{\ln \frac{V}{N} + \frac 32 \ln \frac{2\pi m}{\beta} + \frac{\ln N}{2N}} \ub{\approx}{large $N$} N \lrb{\ln \frac{V}{N} + \frac 32 \ln \frac{2\pi m}{\beta}}</span>
<span id="cb1-1649"><a href="#cb1-1649" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1650"><a href="#cb1-1650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1651"><a href="#cb1-1651" aria-hidden="true" tabindex="-1"></a>The free entropy density, then, has an elegant formula:</span>
<span id="cb1-1652"><a href="#cb1-1652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1653"><a href="#cb1-1653" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1654"><a href="#cb1-1654" aria-hidden="true" tabindex="-1"></a>f/V = -\rho \ln \rho + \rho \frac 32 \ln \frac{2\pi m}{\beta}</span>
<span id="cb1-1655"><a href="#cb1-1655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1656"><a href="#cb1-1656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1657"><a href="#cb1-1657" aria-hidden="true" tabindex="-1"></a>where $\rho = N/V$ is the particle density. Notice that on both the left side and the right side, the individual particles have completely disappeared, leaving behind only a density field $\rho$, a temperature field $\beta$, and some constants. That is, we have obtained the <span class="co">[</span><span class="ot">continuum limit</span><span class="co">](https://en.wikipedia.org/wiki/Continuum_limit)</span> of statistical mechanics -- a field. Statistical mechanics still works in the limit, where it is called statistical field theory.</span>
<span id="cb1-1658"><a href="#cb1-1658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1659"><a href="#cb1-1659" aria-hidden="true" tabindex="-1"></a>We wish to apply the @thm-cond-free-ent once again. To apply it, we need to construct a field-theoretic observable $Y$, such that the maximal Helmholtz free entropy conditional on $Y = y$ is $f^*_{X|y}$ has a solution. Then, we can write $Pr_Y(y) \propto e^{f^*_{X|y}}$ -- that is, the probability distribution of the $Y$ field being in state $Y=y$ is itself a Boltzmann distribution.</span>
<span id="cb1-1660"><a href="#cb1-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1661"><a href="#cb1-1661" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Density fluctuation of ideal gas. On the right is a certain coarse-grained macrostate $Y=y$, and on the left is a microstate $x$ that is compatible with the coarse-grained macrostate. Modified from [@sethnaStatisticalMechanicsEntropy2021, figure 6.8] and [@sethnaStatisticalMechanicsEntropy2021, figure 6.9]</span><span class="co">](figure/density_fluctuation_gas.png)</span></span>
<span id="cb1-1662"><a href="#cb1-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1663"><a href="#cb1-1663" aria-hidden="true" tabindex="-1"></a>Looking at the picture, it is clear to us that the field-theoretic observable should be the particle density field $\rho$. And what is $f^*_{X|\rho}$? It is the total Helmholtz free entropy of the entire tank of gas, given that its particle density is distributed like $\rho$. As we found previously, it is the integral of the free entropy density:</span>
<span id="cb1-1664"><a href="#cb1-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1665"><a href="#cb1-1665" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1666"><a href="#cb1-1666" aria-hidden="true" tabindex="-1"></a>f^*_{X|\rho} = \int_{\text{Box}} \lrb{-\rho(x) \ln \rho(x) + \rho(x) \frac 32 \ln \frac{2\pi m}{\beta}} dx</span>
<span id="cb1-1667"><a href="#cb1-1667" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1668"><a href="#cb1-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1669"><a href="#cb1-1669" aria-hidden="true" tabindex="-1"></a>It is intuitively clear that the most likely density field $\rho^*$ is the uniform density field $\rho^*(x) = \rho_0$. We can prove this by maximizing $f^*_{X|\rho}$. First, take its functional derivative:</span>
<span id="cb1-1670"><a href="#cb1-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1671"><a href="#cb1-1671" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1672"><a href="#cb1-1672" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-1673"><a href="#cb1-1673" aria-hidden="true" tabindex="-1"></a>\partial_{\rho} f^*_{X|\rho} &amp;= - \ln \rho - 1 + \frac 32 \ln \frac{2\pi m}{\beta} <span class="sc">\\</span></span>
<span id="cb1-1674"><a href="#cb1-1674" aria-hidden="true" tabindex="-1"></a>\partial_{\rho}^2 f^*_{X|\rho} &amp;= -\rho^{-1}</span>
<span id="cb1-1675"><a href="#cb1-1675" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-1676"><a href="#cb1-1676" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1677"><a href="#cb1-1677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1678"><a href="#cb1-1678" aria-hidden="true" tabindex="-1"></a>We cannot naively solve $\partial_{\rho} f^*_{X|\rho} = 0$, because we have the extra constraint that particles cannot be created or destroyed. Let $\rho = \rho_0 + \delta \rho$, with $\int\delta\rho = 0$. Plugging it in, we obtain</span>
<span id="cb1-1679"><a href="#cb1-1679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1680"><a href="#cb1-1680" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1681"><a href="#cb1-1681" aria-hidden="true" tabindex="-1"></a>f^*_{X|\rho} = f^*_{X|\rho_0} - \frac{1}{2\rho_0} \int (\delta \rho)^2dx</span>
<span id="cb1-1682"><a href="#cb1-1682" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1683"><a href="#cb1-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1684"><a href="#cb1-1684" aria-hidden="true" tabindex="-1"></a>which tells us that $Pr(\rho_0 + \delta \rho) \propto e^{- \frac{1}{2\rho_0} \int (\delta \rho)^2dx}$, which is precisely the Boltzmann distribution of a physical system whose energy is </span>
<span id="cb1-1685"><a href="#cb1-1685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1686"><a href="#cb1-1686" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1687"><a href="#cb1-1687" aria-hidden="true" tabindex="-1"></a>\delta E := \frac{1}{2\rho_0 \beta} \int \delta \rho(x)^2dx \ub{=}{$P = \rho_0/\beta$ by ideal gas law} \frac 12 P\int \lrb{\frac{\delta \rho(x)}{\rho_0}}^2 dx</span>
<span id="cb1-1688"><a href="#cb1-1688" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1689"><a href="#cb1-1689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1690"><a href="#cb1-1690" aria-hidden="true" tabindex="-1"></a>Now, if you have studied continuum mechanics, this should look very familiar: it is the elastic energy of air. At this point, we are quite close to a statistical field theory of sound waves, but to actually derive sound waves from this, we need to not only know the free entropy $f^*_{X|\rho_0 + \delta \rho}$, but also know how quickly a perturbation $\delta \rho$ is pulled back to $\delta \rho = 0$.</span>
<span id="cb1-1691"><a href="#cb1-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1692"><a href="#cb1-1692" aria-hidden="true" tabindex="-1"></a>Since $\delta E = \frac 12 P\int \lrb{\frac{\delta \rho(x)}{\rho_0}}^2 dx$ looks like the potential energy of a spring $\frac 12 kx^2$, it is reasonable to guess that we need to add a "kinetic energy" term, giving</span>
<span id="cb1-1693"><a href="#cb1-1693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1694"><a href="#cb1-1694" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1695"><a href="#cb1-1695" aria-hidden="true" tabindex="-1"></a>\delta E = \frac 12 P\int \lrb{\frac{\delta \rho(x)}{\rho_0}}^2 dx + \frac 12 \int \mu \delta \dot\rho(x)^2 dx</span>
<span id="cb1-1696"><a href="#cb1-1696" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1697"><a href="#cb1-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1698"><a href="#cb1-1698" aria-hidden="true" tabindex="-1"></a>At this point, it looks just like a Hamiltonian, and it is more productive to switch to Hamiltonian mechanics. This is a taste of statistical field theory of idea gas and how it reduces to classical field theory of density waves (sound waves). I might write a proper essay about statistical field theory someday.</span>
<span id="cb1-1699"><a href="#cb1-1699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1700"><a href="#cb1-1700" aria-hidden="true" tabindex="-1"></a><span class="fu">## Metastability</span></span>
<span id="cb1-1701"><a href="#cb1-1701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1702"><a href="#cb1-1702" aria-hidden="true" tabindex="-1"></a>Often in chemistry and biology, we have a particle that is stuck in a shallow potential well. If it ever gets over an energetic barrier, then it can drop to a much deeper minimum. If the particle is in thermal equilibrium with a bath, then it will eventually gain enough energy by random chance to get over the barrier. Of course, conversely, a particle in the deeper minimum would occasionally gain enough energy to go back to the shallow well.</span>
<span id="cb1-1703"><a href="#cb1-1703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1704"><a href="#cb1-1704" aria-hidden="true" tabindex="-1"></a>The point is that "getting over a potential barrier" is inherently a non-equilibrium phenomenon, and therefore does not logically belong to an essay on *equilibrium* statistical mechanics. Still, if we are willing to approximate, then we can derive it easily.</span>
<span id="cb1-1705"><a href="#cb1-1705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1706"><a href="#cb1-1706" aria-hidden="true" tabindex="-1"></a>We model a particle in a shallow potential well as a simple harmonic oscillator. If it ever gains more than $\Delta E$ of energy, then it would escape the well. Thus, we can take a look at the particle. If it has yet to escape, then we wait for time $\tau$ (relaxation time) until the particle has reached thermal equilibrium again with the bath, and take another look. The time-until-escape is $N \tau$, where $N$ is the number of times we look at the system.</span>
<span id="cb1-1707"><a href="#cb1-1707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1708"><a href="#cb1-1708" aria-hidden="true" tabindex="-1"></a><span class="al">![Getting over a potential well.](figure/metastable_potential_well.png)</span></span>
<span id="cb1-1709"><a href="#cb1-1709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1710"><a href="#cb1-1710" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="relaxation time"}</span>
<span id="cb1-1711"><a href="#cb1-1711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1712"><a href="#cb1-1712" aria-hidden="true" tabindex="-1"></a>When we look at the oscillator, its state is totally fixed at some $(q, p)$. If we immediately look again, we would not find it in a Boltzmann distribution over phase space, but still at $(q, p)$. How long must we wait before the oscillator state has reasonably "relaxed" from a pointy distribution to the serene bell-shape of the Boltzmann distribution?</span>
<span id="cb1-1713"><a href="#cb1-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1714"><a href="#cb1-1714" aria-hidden="true" tabindex="-1"></a>That is beyond the scope. Suffice to say that we should wait a while, not too short, after which the system is close enough to equilibrium. But we cannot wait for too long either, because in the long run, the particle would have escaped the potential well. So there is a time-scale, neither too long nor too short, which we call the relaxation time $\tau$.</span>
<span id="cb1-1715"><a href="#cb1-1715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1716"><a href="#cb1-1716" aria-hidden="true" tabindex="-1"></a>Formalizing all these things requires stochastic calculus, which I might write about later.</span>
<span id="cb1-1717"><a href="#cb1-1717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1718"><a href="#cb1-1718" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Goldilocks and the three chefs" collapse="true"}</span>
<span id="cb1-1719"><a href="#cb1-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1720"><a href="#cb1-1720" aria-hidden="true" tabindex="-1"></a>Goldilocks, with a rumbling stomach, stumbles upon the house of the three chefs. Each chef is holding a pan in one hand and a bottle of Brownian batter in the other. "Excuse me, might I have some pancakes?"</span>
<span id="cb1-1721"><a href="#cb1-1721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1722"><a href="#cb1-1722" aria-hidden="true" tabindex="-1"></a>"Of course!" exclaims the first chef, whose pan holds a small, quivering mound. "It's just been poured, still brimming with energy!"</span>
<span id="cb1-1723"><a href="#cb1-1723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1724"><a href="#cb1-1724" aria-hidden="true" tabindex="-1"></a>Goldilocks frowns. "It hasn't even reached the edges! This batter needs more time to settle."</span>
<span id="cb1-1725"><a href="#cb1-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1726"><a href="#cb1-1726" aria-hidden="true" tabindex="-1"></a>The second chef, whose pan appears curiously empty, sighs. "And what of mine? I've given it all the time in the world, allowed it to explore every corner of its potential." He gestures at the pan, now bereft of batter, a few crumbs clinging to the edges.</span>
<span id="cb1-1727"><a href="#cb1-1727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1728"><a href="#cb1-1728" aria-hidden="true" tabindex="-1"></a>"But... there's nothing left!" exclaims Goldilocks, aghast. "Given too much time, the batter has vanished completely!"</span>
<span id="cb1-1729"><a href="#cb1-1729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1730"><a href="#cb1-1730" aria-hidden="true" tabindex="-1"></a>The third chef, whose batter has flowed smoothly to coat the pan, smiles warmly. "Perhaps this will be more to your liking. Given much time, but not too much, it's achieved perfect consistency."</span>
<span id="cb1-1731"><a href="#cb1-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1732"><a href="#cb1-1732" aria-hidden="true" tabindex="-1"></a>"Ah, this is perfect!" exclaims Goldilocks, taking a bite of the fluffy pancake. "It's had enough time to spread evenly, but not so long that it's dried out."</span>
<span id="cb1-1733"><a href="#cb1-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1734"><a href="#cb1-1734" aria-hidden="true" tabindex="-1"></a>"Indeed, timing is everything. Too brief, and the batter remains confined to its starting point, unable to fulfill its pan-sized destiny. But too long, it would have escaped the edge of the pan to reach its true destiny -- on the ground. Not too short, not too long, just right... that is meta-stability."</span>
<span id="cb1-1735"><a href="#cb1-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1736"><a href="#cb1-1736" aria-hidden="true" tabindex="-1"></a>--- Guest entry written by <span class="in">`Gemini-1.5-Pro`</span>.</span>
<span id="cb1-1737"><a href="#cb1-1737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1738"><a href="#cb1-1738" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/Goldilocks_pancake.jpg)</span></span>
<span id="cb1-1739"><a href="#cb1-1739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1740"><a href="#cb1-1740" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1741"><a href="#cb1-1741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1742"><a href="#cb1-1742" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1743"><a href="#cb1-1743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1744"><a href="#cb1-1744" aria-hidden="true" tabindex="-1"></a>::: {#thm-arrhenius}</span>
<span id="cb1-1745"><a href="#cb1-1745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1746"><a href="#cb1-1746" aria-hidden="true" tabindex="-1"></a><span class="fu">## Arrhenius equation</span></span>
<span id="cb1-1747"><a href="#cb1-1747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1748"><a href="#cb1-1748" aria-hidden="true" tabindex="-1"></a>The escape time for a simple harmonic oscillator in contact with an energy bath scales as $e^{\beta \Delta E}$, where $\Delta E$ is the energy barrier, and $\beta$ is the temperature of the bath.</span>
<span id="cb1-1749"><a href="#cb1-1749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1750"><a href="#cb1-1750" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1751"><a href="#cb1-1751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1752"><a href="#cb1-1752" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="derivation" collapse="true"}</span>
<span id="cb1-1753"><a href="#cb1-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1754"><a href="#cb1-1754" aria-hidden="true" tabindex="-1"></a>Let the oscillator have $n$ dimensions, then its energy function is $H = \sum_{i=1}^n \frac{p_i^2}{2m_i} + \frac{k_i q_i^2}{2}$, where $q_i, p_i$ are the generalized position and momentum, and $m_i, k_i$ are the effective masses and elastic constants. The Boltzmann distribution is $\rho(q, p) = Z^{-1} e^{-\beta H}$, and the probability that it has enough energy to overcome the barrier is</span>
<span id="cb1-1755"><a href="#cb1-1755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1756"><a href="#cb1-1756" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1757"><a href="#cb1-1757" aria-hidden="true" tabindex="-1"></a>P = \frac{\int_{H \geq \Delta E} \rho(q, p)dqdp}{\int_{H \geq 0} \rho(q, p)dqdp}</span>
<span id="cb1-1758"><a href="#cb1-1758" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1759"><a href="#cb1-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1760"><a href="#cb1-1760" aria-hidden="true" tabindex="-1"></a><span class="al">![Phase space diagram of a particle in a potential well. As long as it remains within the potential well, it behaves like a simple harmonic oscillator constantly impacted by thermal noise, but if it ever falls within the shaded region, it would escape the potential well. If we observe it within the well, we would turn away and wait for relaxation time $\tau$ until its phase space distribution thermalizes, then we make another observation. This continues until we observe its escape.](figure/Arrhenius_oscillator_phase_space.jpg)</span></span>
<span id="cb1-1761"><a href="#cb1-1761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1762"><a href="#cb1-1762" aria-hidden="true" tabindex="-1"></a>Notice that the proportionality constant $Z$ is removed. After a change of variables by $x_i = \frac{p_i}{\sqrt{2m_i}}, y_i = \sqrt{\frac{k_i}{2}} q_i$, we get</span>
<span id="cb1-1763"><a href="#cb1-1763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1764"><a href="#cb1-1764" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1765"><a href="#cb1-1765" aria-hidden="true" tabindex="-1"></a>P = \frac{\int_{\sum_i x_i^2 + y_i^2 \geq \Delta E} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}{\int_{\sum_i x_i^2 + y_i^2 \geq 0} e^{-\beta \sum_i (x_i^2 + y_i^2)} dxdy}</span>
<span id="cb1-1766"><a href="#cb1-1766" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1767"><a href="#cb1-1767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1768"><a href="#cb1-1768" aria-hidden="true" tabindex="-1"></a>Integrating in spherical coordinates, and simplifying, we get $P = e^{-\beta \Delta E}$. Thus, the expected time until escape is</span>
<span id="cb1-1769"><a href="#cb1-1769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1770"><a href="#cb1-1770" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1771"><a href="#cb1-1771" aria-hidden="true" tabindex="-1"></a>T = \braket{N}\tau = \frac{1}{P}\tau = \tau e^{\beta \Delta E}</span>
<span id="cb1-1772"><a href="#cb1-1772" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1773"><a href="#cb1-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1774"><a href="#cb1-1774" aria-hidden="true" tabindex="-1"></a>the Arrhenius equation.</span>
<span id="cb1-1775"><a href="#cb1-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1776"><a href="#cb1-1776" aria-hidden="true" tabindex="-1"></a>The same calculation, for a system held in contact with an energy-and-volume bath, gives us $T = \tau e^{\beta \Delta G}$, where $\Delta G$ is the Gibbs free energy barrier.</span>
<span id="cb1-1777"><a href="#cb1-1777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1778"><a href="#cb1-1778" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1779"><a href="#cb1-1779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1780"><a href="#cb1-1780" aria-hidden="true" tabindex="-1"></a>The argument given above for the Arrhenius equation is quite generic. It only assumes there is a system that is stuck in some kind of potential well, and is held at a constant temperature somehow. There is no requirement for the system to be an actual particle in an actual well. The "particle" can very well be the 100-dimensional configuration of a protein during folding, or even the simultaneous position of $10^{23}$ helium atoms in a helium gas. Indeed, the Arrhenius equation pops up everywhere as the time until a system escapes an energetic trap.</span>
<span id="cb1-1781"><a href="#cb1-1781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1782"><a href="#cb1-1782" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common applications</span></span>
<span id="cb1-1783"><a href="#cb1-1783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1784"><a href="#cb1-1784" aria-hidden="true" tabindex="-1"></a>In practice, the Arrhenius equation is used in the form of $\ln r = - a T^{-1} + \Const$, where $r = 1/T$ is the "reaction rate", and $a = \frac{\Delta E}{k_B}$ is the slope of the $T^{-1} - \ln f$ plot ("the Arrhenius plot"). Such plots are extensively used in chemistry, but they are not exclusively found in chemistry.</span>
<span id="cb1-1785"><a href="#cb1-1785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1786"><a href="#cb1-1786" aria-hidden="true" tabindex="-1"></a>In a glass of water held at constant temperature $300 \ut{K}$, each water molecule might occasionally reach enough energy to escape into open air. This is evaporation. By this argument, the rate of evaporation follows the Arrhenius law, and indeed it does.</span>
<span id="cb1-1787"><a href="#cb1-1787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1788"><a href="#cb1-1788" aria-hidden="true" tabindex="-1"></a>When a pure glass of water is cooled below freezing, it does not actually freeze immediately, because there is still an energetic barrier. Ice is a crystal, but liquid is not a crystal. The barrier between them, where crystal transitions to non-crystal, is penalized. This energetic penalty is what we call "surface tension". From thermodynamical arguments, at temperature $\Delta T$ belowe freezing point, the Gibbs free energy required to form a sphere of ice with radius $r$ is</span>
<span id="cb1-1789"><a href="#cb1-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1790"><a href="#cb1-1790" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1791"><a href="#cb1-1791" aria-hidden="true" tabindex="-1"></a>G(r) = (4\pi \sigma_{\text{ice-water}})\cdot r^2  - \lrb{\frac 43 \pi \rho_{\text{ice}} \frac{L\Delta T}{T_{\text{freezing}}}} \cdot r^3</span>
<span id="cb1-1792"><a href="#cb1-1792" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1793"><a href="#cb1-1793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1794"><a href="#cb1-1794" aria-hidden="true" tabindex="-1"></a>where $L$ is the latent heat of ice, and $\sigma_{\text{ice-water}}$ is the surface tension. This creates an energy barrier of height $\sim \Delta T^{-2}$, so the metastable phase of the system can survive for $\sim e^{C\frac{1}{T(\Delta T)^2}}$. <span class="co">[</span><span class="ot">@sethnaStatisticalMechanicsEntropy2021, chapter 11.3</span><span class="co">]</span> One must wait for a long while before random chance creates a large enough "seed", which then grows without bound. This picture we just described is called "bubble nucleation", as described in <span class="co">[</span><span class="ot">classical nucleation theory</span><span class="co">](https://en.wikipedia.org/wiki/Classical_nucleation_theory)</span>.</span>
<span id="cb1-1795"><a href="#cb1-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1796"><a href="#cb1-1796" aria-hidden="true" tabindex="-1"></a><span class="al">![The free energy barrier to nucleation.](figure/nucleation_theory.png)</span>{width=50%}</span>
<span id="cb1-1797"><a href="#cb1-1797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1798"><a href="#cb1-1798" aria-hidden="true" tabindex="-1"></a>Similarly, slightly <span class="co">[</span><span class="ot">supersaturated</span><span class="co">](https://en.wikipedia.org/wiki/Supersaturation)</span> water vapor can stay metastable for a long time before it rains spontaneously. In real-life clouds, however, there are always enough dust for droplets to form around them, thus taking a shortcut through the energetic barrier.</span>
<span id="cb1-1799"><a href="#cb1-1799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1800"><a href="#cb1-1800" aria-hidden="true" tabindex="-1"></a>A similar argument applies for <span class="co">[</span><span class="ot">disappearing polymorphs</span><span class="co">](https://en.wikipedia.org/wiki/Disappearing_polymorphs)</span>. Some chemicals, such as Ritonavir, have two forms ("morphs") of crystals. Call them A and B. Morph A has higher Gibbs free energy density than morph B, but lower energy barrier of formation. Thus, when a chemist attempts to crystallize it from a solution, by Arrhenius law, morph A would appear much faster than morph B. As soon as one droplet of morph A forms, that would serve as centers around which more of morph A forms. Eventually, however, a small crystal of morph A would spontaneously overcome the energetic barrier, shift all its atomic lattices, and become morph B. That would "infect" all subsequent attempts to crystallize morph A. <span class="co">[</span><span class="ot">@wardPerilsPolymorphismSize2017</span><span class="co">]</span></span>
<span id="cb1-1801"><a href="#cb1-1801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1802"><a href="#cb1-1802" aria-hidden="true" tabindex="-1"></a>The matter of the disappearing polymorph has serious legal effects. For example, Ritonavir is a medicine for AIDS. It was marketed in 1996 as Form I crystal, which was thought to be the only crystal form for Ritonavir. In 1998, Form II crystals spontaneously appeared. Any lab in contact with Form II could only produce more of Form II. Eventually the company rediscovered how to create Form I despite infection by Form II, but the delay cost the company 250 million USD. <span class="co">[</span><span class="ot">@bucarDisappearingPolymorphsRevisited2015</span><span class="co">]</span></span>
<span id="cb1-1803"><a href="#cb1-1803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1804"><a href="#cb1-1804" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In a matter of weeks—maybe five or six weeks, every place the product was became contaminated with Form II crystals.</span></span>
<span id="cb1-1805"><a href="#cb1-1805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1806"><a href="#cb1-1806" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Ritonavir and its two morphs. [@bucarDisappearingPolymorphsRevisited2015, figure 2]</span><span class="co">](figure/ritonavir.jpg)</span></span>
<span id="cb1-1807"><a href="#cb1-1807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1808"><a href="#cb1-1808" aria-hidden="true" tabindex="-1"></a><span class="fu">### Uncommon applications</span></span>
<span id="cb1-1809"><a href="#cb1-1809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1810"><a href="#cb1-1810" aria-hidden="true" tabindex="-1"></a>In the simplest model for biochemical process, we just have one chemical reaction following another, until it is complete. If there is a single biochemical step that is much slower than the other steps, then the waiting time for that step dominates, and the total reaction should depend on the temperature by an Arrhenius law. This might explain the observed Arrhenius-law-like dependence on temperature in biological phenomena like tree cricket chirping, alpha brain wave frequency, etc.</span>
<span id="cb1-1811"><a href="#cb1-1811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1812"><a href="#cb1-1812" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">(1) Tree cricket chirping frequency. (2) Firefly flashing frequency. (3) Terrapin heartbeat frequency. (4) Human silent counting rate [@hoaglandPhysiologicalControlJudgments1933]. These figures are reproduced in [@laidlerUnconventionalApplicationsArrhenius1972]. (5) evaporation rate of octane [@brennanEvaporationLiquidsKinetic1974]. (6) Alpha frequency of brains of normal, [syphilitic paretic](https://en.wikipedia.org/wiki/General_paresis_of_the_insane), and *very* paretic humans [@hoaglandPacemakersHumanBrain1936].</span><span class="co">](figure/arrhenius_plots.png)</span></span>
<span id="cb1-1813"><a href="#cb1-1813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1814"><a href="#cb1-1814" aria-hidden="true" tabindex="-1"></a>While writing this, I suddenly recognized <span class="co">[</span><span class="ot">@hoaglandPhysiologicalControlJudgments1933</span><span class="co">]</span> from when I read Feynman's book all those years ago!</span>
<span id="cb1-1815"><a href="#cb1-1815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1816"><a href="#cb1-1816" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; When I was in graduate school at Princeton </span><span class="sc">\[</span><span class="at">1939--1942</span><span class="sc">\]</span><span class="at"> a kind of dumb psychology paper came out that stirred up a lot of discussion. The author had decided that the thing controlling the "time sense" in the brain is a chemical reaction involving iron... his wife had a chronic fever which went up and down a lot. Somehow he got the idea to test her sense of time. He had her count seconds to herself (without looking at a clock), and checked how long it took her to count up to 60. He had her counting -- the poor woman -- all during the day: when her fever went up, he found she counted quicker; when her fever went down, she counted slower... he tried to find a chemical reaction whose rates varied with temperature in the same amounts as his wife's counting did. He found that iron reactions fit the pattern best... it all seemed like a lot of baloney to me -- there were so many things that could go wrong in his long chain of reasoning.</span></span>
<span id="cb1-1817"><a href="#cb1-1817" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-1818"><a href="#cb1-1818" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@feynmanWhatYouCare1989, page 55</span><span class="co">]</span></span>
<span id="cb1-1819"><a href="#cb1-1819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1820"><a href="#cb1-1820" aria-hidden="true" tabindex="-1"></a>And yes, that is the one!</span>
<span id="cb1-1821"><a href="#cb1-1821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1822"><a href="#cb1-1822" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; My wife, having fallen ill with influenza, was used in the first of several experiments. Without, in any way, hinting to her the nature of the experiment, she was asked to count 60 seconds to herself at what she believed to be a rate of 1 per second. Simultaneously the actual duration of the count was observed with a stop-watch. </span></span>
<span id="cb1-1823"><a href="#cb1-1823" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-1824"><a href="#cb1-1824" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@hoaglandPhysiologicalControlJudgments1933</span><span class="co">]</span></span>
<span id="cb1-1825"><a href="#cb1-1825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1826"><a href="#cb1-1826" aria-hidden="true" tabindex="-1"></a><span class="fu">## Crooks fluctuation theorem {#sec-cft}</span></span>
<span id="cb1-1827"><a href="#cb1-1827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1828"><a href="#cb1-1828" aria-hidden="true" tabindex="-1"></a>The field of genuinely non-equilibrium thermodynamics is yet unsettled, so this section is more provisional than the previous ones. Here, we are still considering classical mechanics, with time-reversible Hamiltonian dynamics.</span>
<span id="cb1-1829"><a href="#cb1-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1830"><a href="#cb1-1830" aria-hidden="true" tabindex="-1"></a>The key to modern non-equilibrium thermodynamics is the Crooks fluctuation theorem (CFT), which surprisingly was proven only in 1999. As fundamental physics go, this is really new!</span>
<span id="cb1-1831"><a href="#cb1-1831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1832"><a href="#cb1-1832" aria-hidden="true" tabindex="-1"></a><span class="fu">### In a closed system (microcanonical)</span></span>
<span id="cb1-1833"><a href="#cb1-1833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1834"><a href="#cb1-1834" aria-hidden="true" tabindex="-1"></a>Consider a system held under variable constraints $x$. For example, we have a piston whose head is held at a given length. Alternatively, its head is held at a given force. The point is simply to pick one type of constraint, then stick with it.</span>
<span id="cb1-1835"><a href="#cb1-1835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1836"><a href="#cb1-1836" aria-hidden="true" tabindex="-1"></a>Let the system start in a microcanonical ensemble, at energy $E_1$, then we change the constraints $x(t)$, quickly or slowly, over a time interval $t\in <span class="co">[</span><span class="ot">0, \tau</span><span class="co">]</span>$. The system's microstate follows some trajectory $y(t)$, arriving $y(\tau)$, which falls within some new microcanonical ensemble with energy $E_2$. </span>
<span id="cb1-1837"><a href="#cb1-1837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1838"><a href="#cb1-1838" aria-hidden="true" tabindex="-1"></a>Putting a prime means time-reversal. For example, $x', y'$ are $x, y$, but time-reversed.</span>
<span id="cb1-1839"><a href="#cb1-1839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1840"><a href="#cb1-1840" aria-hidden="true" tabindex="-1"></a>Whereas $E_1$ is known, $E_2$ is randomly sampled, as it is determined by $E_1$ (known), $x(t)$ (known), $y(0)$ (randomly sampled from the microcanonical ensemble).</span>
<span id="cb1-1841"><a href="#cb1-1841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1842"><a href="#cb1-1842" aria-hidden="true" tabindex="-1"></a>During the forward process, if the system undergoes microstate trajectory $y(t)$, then we have to expend work $W<span class="co">[</span><span class="ot">x(t), y(t)</span><span class="co">]</span> = E_2 - E_1$.</span>
<span id="cb1-1843"><a href="#cb1-1843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1844"><a href="#cb1-1844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1845"><a href="#cb1-1845" aria-hidden="true" tabindex="-1"></a>Let $S_1^*$ be the maximal entropy of the system when held under the constraints of $x(0)$, and when the system has energy $E_1$. Similarly, let $S_2^*$ be the maximal entropy of the system when held under the constraints of $x(\tau)$, and when the system has energy $E_2$.</span>
<span id="cb1-1846"><a href="#cb1-1846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1847"><a href="#cb1-1847" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb1-1848"><a href="#cb1-1848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1849"><a href="#cb1-1849" aria-hidden="true" tabindex="-1"></a>$S_1 = S_1^*$, since the system starts in thermal equilibrium. However, by Liouville's theorem, entropy is *conserved*! So we actually have $S_2 = S_1 \neq S_2^*$, because the system does not end in thermal equilibrium.</span>
<span id="cb1-1850"><a href="#cb1-1850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1851"><a href="#cb1-1851" aria-hidden="true" tabindex="-1"></a>The key is this: if the system starts at a microcanonical ensemble, then it does *not* in general end up at another microcanonical ensemble, because as we exert external forcing upon the system, the energy shell deforms, no longer the shell of *any* microcanonical ensemble.</span>
<span id="cb1-1852"><a href="#cb1-1852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1853"><a href="#cb1-1853" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1854"><a href="#cb1-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1855"><a href="#cb1-1855" aria-hidden="true" tabindex="-1"></a>For example, if we have a piston of gas made of only a few gas molecules, then the constraint is the volume $V$, and we want to study the probability of expending work $W$ if we give the piston head a push. The push can be slow or fast -- arbitrarily far from equilibrium. CFT applies no matter how we push the piston head.</span>
<span id="cb1-1856"><a href="#cb1-1856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1857"><a href="#cb1-1857" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">During the forward time-evolution of an energy shell $E_1$, the shell is no longer a microcanonical ensemble. The particular trajectory sampled lands upon an energy shell $E_2$. [@sethnaStatisticalMechanicsEntropy2021, figure 4.10]</span><span class="co">](figure/Crooks_sethna_2021_4_10.png)</span></span>
<span id="cb1-1858"><a href="#cb1-1858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1859"><a href="#cb1-1859" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The same situation but in the opposite direction. [@sethnaStatisticalMechanicsEntropy2021, figure 4.11]</span><span class="co">](figure/Crooks_sethna_2021_4_11.png)</span></span>
<span id="cb1-1860"><a href="#cb1-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1861"><a href="#cb1-1861" aria-hidden="true" tabindex="-1"></a>Because the CFT involves the density of not states, but *state trajectories*, we need to set up the formalism for path integrals.</span>
<span id="cb1-1862"><a href="#cb1-1862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1863"><a href="#cb1-1863" aria-hidden="true" tabindex="-1"></a>Let $\delta E_1, \delta E_2$ be infinitesimals, and let $E_1, E_2$ be real numbers.</span>
<span id="cb1-1864"><a href="#cb1-1864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1865"><a href="#cb1-1865" aria-hidden="true" tabindex="-1"></a>Given an infinitesimal small bundle of microtrajectories $y$, we can measure its path-space volume as $D<span class="co">[</span><span class="ot">y</span><span class="co">]</span>$. Suppose they start on the energy shell $<span class="co">[</span><span class="ot">E_1, E_1 + \delta E_1</span><span class="co">]</span>$, then they would end up *somewhere*. If we're lucky, they would end up on the energy shell $<span class="co">[</span><span class="ot">E_2 + \delta E_2</span><span class="co">]</span>$.</span>
<span id="cb1-1866"><a href="#cb1-1866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1867"><a href="#cb1-1867" aria-hidden="true" tabindex="-1"></a>Suppose the system starts in the microcanonical ensemble on the energy shell $<span class="co">[</span><span class="ot">E_1, E_1 + \delta E_1</span><span class="co">]</span>$, and we perform the constraint-variation $x$, then there is a certain probability $\delta P$ that we would sample a trajectory from the small bundle. That small probability is</span>
<span id="cb1-1868"><a href="#cb1-1868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1869"><a href="#cb1-1869" aria-hidden="true" tabindex="-1"></a>$$\rho(y | x) D<span class="co">[</span><span class="ot">y</span><span class="co">]</span>$$</span>
<span id="cb1-1870"><a href="#cb1-1870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1871"><a href="#cb1-1871" aria-hidden="true" tabindex="-1"></a>where $\rho(y | x)$ is a probability density over path-space. In particular, $\rho(y | x) = 0$ identically, unless $y(0)$ is on the energy shell $<span class="co">[</span><span class="ot">E_1, E_1 + \delta E_1</span><span class="co">]</span>$.  </span>
<span id="cb1-1872"><a href="#cb1-1872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1873"><a href="#cb1-1873" aria-hidden="true" tabindex="-1"></a>Running the argument backwards, we can define $\rho'(y' | x')$, another probability density over paths. This one satisfies $\rho'(y'| x') = 0$ unless $y'(0)$ is on the energy shell $<span class="co">[</span><span class="ot">E_2, E_2 + \delta E_2</span><span class="co">]</span>$.</span>
<span id="cb1-1874"><a href="#cb1-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1875"><a href="#cb1-1875" aria-hidden="true" tabindex="-1"></a>::: {#thm-cft-micro}</span>
<span id="cb1-1876"><a href="#cb1-1876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1877"><a href="#cb1-1877" aria-hidden="true" tabindex="-1"></a><span class="fu">## microcanonical CFT</span></span>
<span id="cb1-1878"><a href="#cb1-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1879"><a href="#cb1-1879" aria-hidden="true" tabindex="-1"></a>For any trajectory $y$ such that it starts on the $<span class="co">[</span><span class="ot">E_1, E_1 + \delta E_1</span><span class="co">]</span>$ energy shell, and ends on the $<span class="co">[</span><span class="ot">E_2, E_2 + \delta E_2</span><span class="co">]</span>$ energy shell,</span>
<span id="cb1-1880"><a href="#cb1-1880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1881"><a href="#cb1-1881" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(y | x)}{\rho'(y' | x')} = e^{\Delta S}$$</span>
<span id="cb1-1882"><a href="#cb1-1882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1883"><a href="#cb1-1883" aria-hidden="true" tabindex="-1"></a>where $\Delta S= \ln\Omega_2 - \ln\Omega_1$, $\Omega_1$ is the phase space volume of the first energy shell, and $\Omega_2$ the second.</span>
<span id="cb1-1884"><a href="#cb1-1884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1885"><a href="#cb1-1885" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1886"><a href="#cb1-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1887"><a href="#cb1-1887" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-1888"><a href="#cb1-1888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1889"><a href="#cb1-1889" aria-hidden="true" tabindex="-1"></a>If $y$ does not start on the first energy shell, or does not end on the second energy shell, then either the nominator or the denominator is zero, and so the equation fails to hold.</span>
<span id="cb1-1890"><a href="#cb1-1890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1891"><a href="#cb1-1891" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1892"><a href="#cb1-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1893"><a href="#cb1-1893" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="proof" collapse="true"}</span>
<span id="cb1-1894"><a href="#cb1-1894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1895"><a href="#cb1-1895" aria-hidden="true" tabindex="-1"></a>In the forward process, the probability of going along that trajectory is </span>
<span id="cb1-1896"><a href="#cb1-1896" aria-hidden="true" tabindex="-1"></a>$$\rho(x|y) D<span class="co">[</span><span class="ot">x</span><span class="co">]</span> = \frac{\delta V}{\Omega_1}$$</span>
<span id="cb1-1897"><a href="#cb1-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1898"><a href="#cb1-1898" aria-hidden="true" tabindex="-1"></a>where $\delta V$ is the phase-space volume of the shaded set.  </span>
<span id="cb1-1899"><a href="#cb1-1899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1900"><a href="#cb1-1900" aria-hidden="true" tabindex="-1"></a>In the backward process, the probability of reversing that trajectory is </span>
<span id="cb1-1901"><a href="#cb1-1901" aria-hidden="true" tabindex="-1"></a>$$\rho'(x'|y') D<span class="co">[</span><span class="ot">x'</span><span class="co">]</span>= \frac{\delta V'}{\Omega_2}$$</span>
<span id="cb1-1902"><a href="#cb1-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1903"><a href="#cb1-1903" aria-hidden="true" tabindex="-1"></a>$\delta V' = \delta V$ by Liouville's theorem, and $D<span class="co">[</span><span class="ot">x</span><span class="co">]</span> = D<span class="co">[</span><span class="ot">x'</span><span class="co">]</span>$ because $x'$ is just $x$ time-reversed.</span>
<span id="cb1-1904"><a href="#cb1-1904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1905"><a href="#cb1-1905" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1906"><a href="#cb1-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1907"><a href="#cb1-1907" aria-hidden="true" tabindex="-1"></a><span class="fu">### In an energy bath (canonical)</span></span>
<span id="cb1-1908"><a href="#cb1-1908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1909"><a href="#cb1-1909" aria-hidden="true" tabindex="-1"></a>Now, suppose we take the same piston of gas, and put it in energy-contact with an energy bath, then at thermal equilibrium, the piston of gas would have the Boltzmann distribution $\propto e^{-\beta E}$. We can then give the piston head a push, which would cause it to undergo some kind of time-evolution. </span>
<span id="cb1-1910"><a href="#cb1-1910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1911"><a href="#cb1-1911" aria-hidden="true" tabindex="-1"></a>::: {#thm-cft}</span>
<span id="cb1-1912"><a href="#cb1-1912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1913"><a href="#cb1-1913" aria-hidden="true" tabindex="-1"></a><span class="fu">## Crooks fluctuation theorem</span></span>
<span id="cb1-1914"><a href="#cb1-1914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1915"><a href="#cb1-1915" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>} = e^{\beta (W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> - \Delta F^*)}$$</span>
<span id="cb1-1916"><a href="#cb1-1916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1917"><a href="#cb1-1917" aria-hidden="true" tabindex="-1"></a>where $S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>$ is the entropy produced during the forward process, after the system has equilibrated, and $\Delta F^* = F^*_2 - F^*_2$ is the increase in *equilibrium* Helmholtz free energy of the system.</span>
<span id="cb1-1918"><a href="#cb1-1918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1919"><a href="#cb1-1919" aria-hidden="true" tabindex="-1"></a>Equivalently, $D_{KL}(\mathcal P_{\text{forward}} <span class="sc">\|</span> \mathcal P_{\text{backward}}) = \beta (\braket{W}_{\text{forward}} - \Delta F^*)$, where $\mathcal P_{\text{forward}}$ is the probability distribution over microtrajectories in the forward process, and similarly for $\mathcal P_{\text{backward}}$. </span>
<span id="cb1-1920"><a href="#cb1-1920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1921"><a href="#cb1-1921" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1922"><a href="#cb1-1922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1923"><a href="#cb1-1923" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title="equilibrium Helmholtz"}</span>
<span id="cb1-1924"><a href="#cb1-1924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1925"><a href="#cb1-1925" aria-hidden="true" tabindex="-1"></a>In both forward and backward cases, we start with a thermal equilibrium, and end with a thermal *dis*equilibrium.</span>
<span id="cb1-1926"><a href="#cb1-1926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1927"><a href="#cb1-1927" aria-hidden="true" tabindex="-1"></a>For example, suppose we have a small tank of a few gas molecules in thermal equilibrium with a large energy bath.</span>
<span id="cb1-1928"><a href="#cb1-1928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1929"><a href="#cb1-1929" aria-hidden="true" tabindex="-1"></a>Now, we *quickly* push the piston head in according to the function $x(t)$. The trajectory of the system would go through is $y(t)$, which is *determined* by both $x(t)$ and the initial state of both the system and the energy bath.</span>
<span id="cb1-1930"><a href="#cb1-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1931"><a href="#cb1-1931" aria-hidden="true" tabindex="-1"></a>Now, we *wait a long time*, until the tank is in thermal equilibrium again. Then we pull the piston head out with time-reversed trajectory. Because the forward trajectory was quick, the backward trajectory was also quick. In both cases, the system starts at equilibrium, then becomes disequilibrated, and then becomes equilibrated again. The definition of $W$ is measured during the equilibrium-disequilibrium *trajectory*, while $F^*$ is measured by comparing the equilibrium-equilibrium, *regardless* of trajectory.</span>
<span id="cb1-1932"><a href="#cb1-1932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1933"><a href="#cb1-1933" aria-hidden="true" tabindex="-1"></a>We wrote $F^*$ instead of $F$, to emphasize that we are dealing with *equilibrium* Helmholtz free energy, defined by $F^* = \min_\rho (\braket{E} - TS[\rho])$, and *not* the generic version $\braket{E} - TS<span class="co">[</span><span class="ot">\rho</span><span class="co">]</span>$.</span>
<span id="cb1-1934"><a href="#cb1-1934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1935"><a href="#cb1-1935" aria-hidden="true" tabindex="-1"></a>This is vitally important, because at time $\tau$, when the constraints have just reached their new values, the system is *not* in equilibrium. We would have to hold the constraints constant for a while for the system to return to equilibrium with the energy bath. Despite this, CFT uses $\Delta F^*$, which is computed at equilibrium.</span>
<span id="cb1-1936"><a href="#cb1-1936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1937"><a href="#cb1-1937" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1938"><a href="#cb1-1938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1939"><a href="#cb1-1939" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-1940"><a href="#cb1-1940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1941"><a href="#cb1-1941" aria-hidden="true" tabindex="-1"></a>Apply the microcanonical version of CFT to the entire compound system that includes both the bath and the system, then integrate over all possible microstate trajectories of the bath $y_{\text{bath}}$.</span>
<span id="cb1-1942"><a href="#cb1-1942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1943"><a href="#cb1-1943" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-1944"><a href="#cb1-1944" aria-hidden="true" tabindex="-1"></a>  S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> &amp;= \Delta S_{\text{bath}} + \Delta S_{\text{system}} <span class="sc">\\</span></span>
<span id="cb1-1945"><a href="#cb1-1945" aria-hidden="true" tabindex="-1"></a>  &amp;= \beta \Delta E_{\text{bath}} + \Delta S_{\text{system}} <span class="sc">\\</span></span>
<span id="cb1-1946"><a href="#cb1-1946" aria-hidden="true" tabindex="-1"></a>  &amp;= \beta(W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> - \braket{\Delta E_{\text{system}}}_2) + \Delta S_{\text{system}} <span class="sc">\\</span></span>
<span id="cb1-1947"><a href="#cb1-1947" aria-hidden="true" tabindex="-1"></a>  &amp;= \beta (W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> - \Delta F^*)</span>
<span id="cb1-1948"><a href="#cb1-1948" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-1949"><a href="#cb1-1949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1950"><a href="#cb1-1950" aria-hidden="true" tabindex="-1"></a>where $\braket{\cdot}_2$ means the canonical ensemble average under constraint $x(\tau)$.  </span>
<span id="cb1-1951"><a href="#cb1-1951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1952"><a href="#cb1-1952" aria-hidden="true" tabindex="-1"></a>Notice that the work expended/entropy produced depends only on the system's microtrajectory $y(t)$, and *not* on the bath's microtrajectory $y_{\text{bath}}(t)$. That is, </span>
<span id="cb1-1953"><a href="#cb1-1953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1954"><a href="#cb1-1954" aria-hidden="true" tabindex="-1"></a>$$S<span class="co">[</span><span class="ot">x, y, y_{\text{bath}}</span><span class="co">]</span> = S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>$$</span>
<span id="cb1-1955"><a href="#cb1-1955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1956"><a href="#cb1-1956" aria-hidden="true" tabindex="-1"></a>This will be used again in the next step when we integrate over $D<span class="co">[</span><span class="ot">y_{\text{bath}}</span><span class="co">]</span>$.  </span>
<span id="cb1-1957"><a href="#cb1-1957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1958"><a href="#cb1-1958" aria-hidden="true" tabindex="-1"></a>$$\begin{aligned}</span>
<span id="cb1-1959"><a href="#cb1-1959" aria-hidden="true" tabindex="-1"></a>  \rho(y|x) &amp;= \int_{y, y_{\text{bath}}, x \text{ is valid}}D<span class="co">[</span><span class="ot">y_{\text{bath}}</span><span class="co">]</span>\; \rho(y, y_{\text{bath}} | x)  <span class="sc">\\</span></span>
<span id="cb1-1960"><a href="#cb1-1960" aria-hidden="true" tabindex="-1"></a>  &amp;=  \underbrace{\int_{y', y'_{\text{bath}}, x' \text{ is valid}}D[y'_{\text{bath}}]}_{\text{reversible dynamics}}\; \underbrace{e^{S[x, y, y_{\text{bath}}]}\rho'(y', y'_{\text{bath}} | x')}_{\text{microcanonical CFT}}  <span class="sc">\\</span></span>
<span id="cb1-1961"><a href="#cb1-1961" aria-hidden="true" tabindex="-1"></a>  &amp;=  \int D<span class="co">[</span><span class="ot">y'_{\text{bath}}</span><span class="co">]</span> \; e^{\red{S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>}}\rho'(y', y'_{\text{bath}} | x') <span class="sc">\\</span></span>
<span id="cb1-1962"><a href="#cb1-1962" aria-hidden="true" tabindex="-1"></a>  &amp;=  e^{S<span class="co">[</span><span class="ot">x,y</span><span class="co">]</span>} \int D<span class="co">[</span><span class="ot">y'_{\text{bath}}</span><span class="co">]</span> \; \rho'(y', y'_{\text{bath}} | x') <span class="sc">\\</span></span>
<span id="cb1-1963"><a href="#cb1-1963" aria-hidden="true" tabindex="-1"></a>  &amp;=  e^{S<span class="co">[</span><span class="ot">x,y</span><span class="co">]</span>} \rho'(y'|x')</span>
<span id="cb1-1964"><a href="#cb1-1964" aria-hidden="true" tabindex="-1"></a>\end{aligned}$$</span>
<span id="cb1-1965"><a href="#cb1-1965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1966"><a href="#cb1-1966" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1967"><a href="#cb1-1967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1968"><a href="#cb1-1968" aria-hidden="true" tabindex="-1"></a>::: {#cor-todo}</span>
<span id="cb1-1969"><a href="#cb1-1969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1970"><a href="#cb1-1970" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(W | x)}{\rho'(-W | x')} =  e^{\beta (W - \Delta F^*)}$$</span>
<span id="cb1-1971"><a href="#cb1-1971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1972"><a href="#cb1-1972" aria-hidden="true" tabindex="-1"></a>where $\rho(W|x)$ is the probability density of expending work $W$ in the forward process.  </span>
<span id="cb1-1973"><a href="#cb1-1973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1974"><a href="#cb1-1974" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1975"><a href="#cb1-1975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1976"><a href="#cb1-1976" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-1977"><a href="#cb1-1977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1978"><a href="#cb1-1978" aria-hidden="true" tabindex="-1"></a>Integrate over all forward microtrajectories $y$ satisfying $W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> \in <span class="co">[</span><span class="ot">W, W+\delta W</span><span class="co">]</span>$. By reversibility, $W<span class="co">[</span><span class="ot">x', y'</span><span class="co">]</span> = <span class="co">[</span><span class="ot">-W - \delta W, -W</span><span class="co">]</span>$ for such microtrajectories.</span>
<span id="cb1-1979"><a href="#cb1-1979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1980"><a href="#cb1-1980" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-1981"><a href="#cb1-1981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1982"><a href="#cb1-1982" aria-hidden="true" tabindex="-1"></a>Looking at the proof for CFT for the canonical ensemble, we immediately obtain many other possible CFTs, one per free entropy.</span>
<span id="cb1-1983"><a href="#cb1-1983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1984"><a href="#cb1-1984" aria-hidden="true" tabindex="-1"></a>Suppose we have a piston of magnetic gas in energy-and-volume contact with a bath of constant temperature $\beta$ and pressure $P$. Now suppose the gas is in equilibrium with the bath, and we vary the external magnetic field over a trajectory $x$. Over the microstate trajectory $x$, the external world would expend both some energy $W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>$ and some volume $V<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>$. Thus, we obtain the CFT for Gibbs free energy $G$:</span>
<span id="cb1-1985"><a href="#cb1-1985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1986"><a href="#cb1-1986" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>} = e^{\beta (W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> + PV<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> - \Delta G^*)}$$</span>
<span id="cb1-1987"><a href="#cb1-1987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1988"><a href="#cb1-1988" aria-hidden="true" tabindex="-1"></a>Similarly, suppose we have a chemical reaction chamber of fixed volume, and in energy-and-particle contact with a bath with constant temperature $\beta$ and chemical potentials $\mu_i$, we have the CFT for Landau free energy $\Omega$:</span>
<span id="cb1-1989"><a href="#cb1-1989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1990"><a href="#cb1-1990" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(y | x)}{\rho'(y' | x')} =  e^{S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>} = e^{\beta (W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> -  \sum_i \mu_i N_i<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> - \Delta \Omega^*)}$$</span>
<span id="cb1-1991"><a href="#cb1-1991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1992"><a href="#cb1-1992" aria-hidden="true" tabindex="-1"></a><span class="fu">### Easy consequences</span></span>
<span id="cb1-1993"><a href="#cb1-1993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1994"><a href="#cb1-1994" aria-hidden="true" tabindex="-1"></a>Let $W$ be the total work we expended by changing the constraints during the interval $<span class="co">[</span><span class="ot">0, \tau</span><span class="co">]</span>$. Since the work expended depends on the details of the heat bath and the starting state of the system at $t=0$, this is a random variable.</span>
<span id="cb1-1995"><a href="#cb1-1995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1996"><a href="#cb1-1996" aria-hidden="true" tabindex="-1"></a>::: {#thm-todo}</span>
<span id="cb1-1997"><a href="#cb1-1997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1998"><a href="#cb1-1998" aria-hidden="true" tabindex="-1"></a><span class="fu">## Jarzynski equality</span></span>
<span id="cb1-1999"><a href="#cb1-1999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2000"><a href="#cb1-2000" aria-hidden="true" tabindex="-1"></a>$$\braket{e^{-\beta W}} = e^{-\beta \Delta F^*}$$</span>
<span id="cb1-2001"><a href="#cb1-2001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2002"><a href="#cb1-2002" aria-hidden="true" tabindex="-1"></a>where the expectation is taken over many repeats of the same experiment (ensemble average).  </span>
<span id="cb1-2003"><a href="#cb1-2003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2004"><a href="#cb1-2004" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2005"><a href="#cb1-2005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2006"><a href="#cb1-2006" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-2007"><a href="#cb1-2007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2008"><a href="#cb1-2008" aria-hidden="true" tabindex="-1"></a>Integrate CFT over all forward trajectories $D<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$.</span>
<span id="cb1-2009"><a href="#cb1-2009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2010"><a href="#cb1-2010" aria-hidden="true" tabindex="-1"></a>$$\rho(y | x) e^{-\beta W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> } = \rho'(y' | x') e^{-\beta\Delta F}$$</span>
<span id="cb1-2011"><a href="#cb1-2011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2012"><a href="#cb1-2012" aria-hidden="true" tabindex="-1"></a>now integrate over $\int D<span class="co">[</span><span class="ot">y</span><span class="co">]</span>$, using the fact that $D<span class="co">[</span><span class="ot">y</span><span class="co">]</span> = D<span class="co">[</span><span class="ot">y'</span><span class="co">]</span>$.  </span>
<span id="cb1-2013"><a href="#cb1-2013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2014"><a href="#cb1-2014" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb1-2015"><a href="#cb1-2015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2016"><a href="#cb1-2016" aria-hidden="true" tabindex="-1"></a>THe Jarzynski equality can be understood as a special case of the following statement: $e^{-\beta S_{<span class="co">[</span><span class="ot">0, t</span><span class="co">]</span>}}$ is a <span class="co">[</span><span class="ot">martingale</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Martingale_(probability_theory)), where $_{<span class="co">[</span><span class="ot">0, t</span><span class="co">]</span>}$ is the entropy produced during time-period $<span class="co">[</span><span class="ot">0, t</span><span class="co">]</span>$. See <span class="co">[</span><span class="ot">@roldanMartingalesPhysicistsTreatise2024</span><span class="co">]</span> for a derivation, as well as the martingale-theoretic perspective on thermodynamics.</span>
<span id="cb1-2017"><a href="#cb1-2017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2018"><a href="#cb1-2018" aria-hidden="true" tabindex="-1"></a>::: {#cor-todo}</span>
<span id="cb1-2019"><a href="#cb1-2019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2020"><a href="#cb1-2020" aria-hidden="true" tabindex="-1"></a><span class="fu">## violation of second law is exponentially unlikely</span></span>
<span id="cb1-2021"><a href="#cb1-2021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2022"><a href="#cb1-2022" aria-hidden="true" tabindex="-1"></a>$$Pr((W - \Delta F^*) \leq - \delta W) \leq e^{-\beta \delta W}$$</span>
<span id="cb1-2023"><a href="#cb1-2023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2024"><a href="#cb1-2024" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2025"><a href="#cb1-2025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2026"><a href="#cb1-2026" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-2027"><a href="#cb1-2027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2028"><a href="#cb1-2028" aria-hidden="true" tabindex="-1"></a>Apply Markov's inequality.</span>
<span id="cb1-2029"><a href="#cb1-2029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2030"><a href="#cb1-2030" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2031"><a href="#cb1-2031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2032"><a href="#cb1-2032" aria-hidden="true" tabindex="-1"></a>Since $e^t$ is convex, we have $\Delta F^* \leq \braket{W}$, meaning that the average work expended is more than the increase in Helmholtz free energy. This has better be true, else we would be violating the second law of thermodynamics on average.</span>
<span id="cb1-2033"><a href="#cb1-2033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2034"><a href="#cb1-2034" aria-hidden="true" tabindex="-1"></a>Since $\Delta F^* = -\frac{1}{\beta} \ln\braket{e^{-\beta W}}$, we find that to second order,</span>
<span id="cb1-2035"><a href="#cb1-2035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2036"><a href="#cb1-2036" aria-hidden="true" tabindex="-1"></a>::: {#thm-fdr}</span>
<span id="cb1-2037"><a href="#cb1-2037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2038"><a href="#cb1-2038" aria-hidden="true" tabindex="-1"></a><span class="fu">## fluctuation-dissipation relation</span></span>
<span id="cb1-2039"><a href="#cb1-2039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2040"><a href="#cb1-2040" aria-hidden="true" tabindex="-1"></a>$$\underbrace{\braket{W} - \Delta F^*}_{\text{work dissipation}} = \beta \underbrace{\frac 12 \sigma_W^2}_{\text{work fluctuation}}$$</span>
<span id="cb1-2041"><a href="#cb1-2041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2042"><a href="#cb1-2042" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2043"><a href="#cb1-2043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2044"><a href="#cb1-2044" aria-hidden="true" tabindex="-1"></a>For periodic forcing, the CFT has a simpler form.</span>
<span id="cb1-2045"><a href="#cb1-2045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2046"><a href="#cb1-2046" aria-hidden="true" tabindex="-1"></a>Consider a time-reversible dynamical system immersed in an energy bath with inverse temperature $\beta$ , driven by periodically varying constraints. For example, a pendulum in a sticky fluid subjected to a periodic driving torque, or a water-cooled electric circuit driven by a periodic voltage.</span>
<span id="cb1-2047"><a href="#cb1-2047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2048"><a href="#cb1-2048" aria-hidden="true" tabindex="-1"></a>Such a system will settle into a "dynamical equilibrium" ensemble, much like a canonical ensemble. If it is driven by the same process time-reversed, then, it will settle into another dynamical equilibrium ensemble.</span>
<span id="cb1-2049"><a href="#cb1-2049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2050"><a href="#cb1-2050" aria-hidden="true" tabindex="-1"></a>::: {#thm-gallavotti-cohen}</span>
<span id="cb1-2051"><a href="#cb1-2051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2052"><a href="#cb1-2052" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gallavotti--Cohen fluctuation theorem</span></span>
<span id="cb1-2053"><a href="#cb1-2053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2054"><a href="#cb1-2054" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(Q)}{\rho'(-Q)} = e^{\beta Q}$$</span>
<span id="cb1-2055"><a href="#cb1-2055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2056"><a href="#cb1-2056" aria-hidden="true" tabindex="-1"></a>where $\rho(Q)$ is the probability density that a forward cycle, randomly sampled from the dynamical equilibrium ensemble, emits energy $Q$ into the energy bath. Similarly, $\rho'$ is for the backward cycle.</span>
<span id="cb1-2057"><a href="#cb1-2057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2058"><a href="#cb1-2058" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2059"><a href="#cb1-2059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2060"><a href="#cb1-2060" aria-hidden="true" tabindex="-1"></a>It is often used for time-symmetric driving, such as sine waves. In that case, it is typically written as</span>
<span id="cb1-2061"><a href="#cb1-2061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2062"><a href="#cb1-2062" aria-hidden="true" tabindex="-1"></a>$$\frac{\rho(\dot s)}{\rho(-\dot s)} = e^{\beta T \dot s}$$</span>
<span id="cb1-2063"><a href="#cb1-2063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2064"><a href="#cb1-2064" aria-hidden="true" tabindex="-1"></a>where $T$ is the driving period, and $\dot s$ is the entropy production rate.</span>
<span id="cb1-2065"><a href="#cb1-2065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2066"><a href="#cb1-2066" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Proof" collapse="true"}</span>
<span id="cb1-2067"><a href="#cb1-2067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2068"><a href="#cb1-2068" aria-hidden="true" tabindex="-1"></a>Construct a process that starts at equilibrium, then mount up the periodic driving, runs it for $NT$ time where $N$ is a large integer, then remove the driving. At the $N\to\infty$ limit, the CFT reduces to this equation.</span>
<span id="cb1-2069"><a href="#cb1-2069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2070"><a href="#cb1-2070" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2071"><a href="#cb1-2071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2072"><a href="#cb1-2072" aria-hidden="true" tabindex="-1"></a><span class="fu">## Applications of CFT</span></span>
<span id="cb1-2073"><a href="#cb1-2073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2074"><a href="#cb1-2074" aria-hidden="true" tabindex="-1"></a><span class="fu">### Molecular machines</span></span>
<span id="cb1-2075"><a href="#cb1-2075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2076"><a href="#cb1-2076" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Remember previously</span><span class="co">](#sec-rna-hairpin)</span> when we studied pulling an RNA hairpin apart then together again, and how, with different pulling speed, the wasted energy per cycle is different? Well, we can use the CFT to quantify this.</span>
<span id="cb1-2077"><a href="#cb1-2077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2078"><a href="#cb1-2078" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Several hundred trajectories of pulling and un-pulling an RNA hairpin, at loading rate $7.5 \ut{pN/s}$. [@collinVerificationCrooksFluctuation2005, figure 1]</span><span class="co">](figure/RNA_unfolding_collin_2005.jpg)</span></span>
<span id="cb1-2079"><a href="#cb1-2079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2080"><a href="#cb1-2080" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@collinVerificationCrooksFluctuation2005</span><span class="co">]</span>, they repeatedly pulled on different types of RNA hairpins, at different speeds, obtaining the following distributions. Not only did they verify the CFT, but also used it to measure $\Delta F^*$ as the $W$ where $\rho(W) = \rho'(-W)$, that is, the crossing point between the two histograms of $\rho(W)$ and $\rho'(-W)$. This was a breakthrough, extending statistical mechanics to the single-molecule regime, a fast-developing field of study that is still far from equilibrium.</span>
<span id="cb1-2081"><a href="#cb1-2081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2082"><a href="#cb1-2082" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Histograms of forward work probability $\rho(W)$ and backward work probability $\rho'(-W)$. The mutant type RNA has $\Delta G = 160 k_BT$, while the wild-type RNA has $\Delta G = 154 k_BT$. In the inset, the CFT $\ln\frac{\rho(W)}{\rho'(-W)} = \beta W$ is directly verified. [@collinVerificationCrooksFluctuation2005, figure 3]</span><span class="co">](figure/CFT_mutant_collin_2005.jpg)</span></span>
<span id="cb1-2083"><a href="#cb1-2083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2084"><a href="#cb1-2084" aria-hidden="true" tabindex="-1"></a>Classically, if we have a single system in thermal equilibrium with a single energy-bath, and we perform a cyclic operation on it, then we can't extract work, lest we violate the second law.</span>
<span id="cb1-2085"><a href="#cb1-2085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2086"><a href="#cb1-2086" aria-hidden="true" tabindex="-1"></a>Because the requirement only says that $\braket{e^{-\beta W}} = 1$, it is entirely possible for us to extract arbitrarily large amounts of work with arbitrarily high probability, as long as there is a corresponding small probability of losing a lot of work:</span>
<span id="cb1-2087"><a href="#cb1-2087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2088"><a href="#cb1-2088" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2089"><a href="#cb1-2089" aria-hidden="true" tabindex="-1"></a>e^{-\beta W}(1-\delta) + e^{-\beta W}\delta = 1, \quad W = \beta^{-1} \ln \frac{1}{\delta}</span>
<span id="cb1-2090"><a href="#cb1-2090" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2091"><a href="#cb1-2091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2092"><a href="#cb1-2092" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@mailletOptimalProbabilisticWork2019</span><span class="co">]</span> constructed a quantum mechanical device with a single-electron transistor. The electron can expend work. They managed to extract work from the device with over 75% probability.</span>
<span id="cb1-2093"><a href="#cb1-2093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2094"><a href="#cb1-2094" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Histogram of work extraction. Over 75% of the histogram is in the $W &gt; 0$ part. Despite this, $\braket{e^{-\beta W}} = 0.989 \pm 0.03$, verifying the Jarzynski equation. [@mailletOptimalProbabilisticWork2019, figure 3.c]</span><span class="co">](figure/maillet_2019_3_c.png)</span></span>
<span id="cb1-2095"><a href="#cb1-2095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2096"><a href="#cb1-2096" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Martingales or bust!"}</span>
<span id="cb1-2097"><a href="#cb1-2097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2098"><a href="#cb1-2098" aria-hidden="true" tabindex="-1"></a>As described, the Jarzynski equality is really a consequence of the fact that $e^S$ is a martingale. Martingales, roughly speaking, are payoffs in fair gambles. Imagine playing at a fair casino. You start with cash $X_0$, and after each game, your cash grows or shrinks, resulting in a sequence $X_1, X_2, \dots$. The game ends when you decide you've had enough, goes bankrupt, or the casino goes bankrupt. A fair casino is one where $\braket{X_{t+1} - X_t | X_t} = 0$, meaning that no matter what your strategy is, at time $t$, your expected payoff is still zero.</span>
<span id="cb1-2099"><a href="#cb1-2099" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2100"><a href="#cb1-2100" aria-hidden="true" tabindex="-1"></a>Consider the double-or-nothing gamble. The casino flips a coin, and if heads, you get double the bet, else you get nothing. The classic martingale strategy is to start at betting 1 dollar, and doubling the bet at each loss. It is clear that this strategy has "guaranteed" return -- eventually the coin will land heads and you will recoup all losses plus one dollar. </span>
<span id="cb1-2101"><a href="#cb1-2101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2102"><a href="#cb1-2102" aria-hidden="true" tabindex="-1"></a>This "guaranteed" return is an illusion, because you (or the casino) can go bankrupt. Suppose you start with 127 dollars, then by tracing out all possibilities during one martingale run, you will find that the martingale ends with winning 1 dollar with probability $\frac{127}{128}$, and losing 127 dollars with probability $\frac{1}{128}$.</span>
<span id="cb1-2103"><a href="#cb1-2103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2104"><a href="#cb1-2104" aria-hidden="true" tabindex="-1"></a>In this sense, martingale strategies cannot create free money out of a fair gamble, much like how machines cannot create free energy out of thermal noise. However, martingale strategies can shape the probability distribution of the payoff. For example, in the double-or-nothing game, the martingale strategy shifts almost all probability mass to winning one dollar, at the cost of a small but nonzero probability of losing everything. Similarly, a machine can create a large probability of extracting a little work from a thermal bath, at the price of a small probability of losing a lot of work to the thermal bath.</span>
<span id="cb1-2105"><a href="#cb1-2105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2106"><a href="#cb1-2106" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2107"><a href="#cb1-2107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2108"><a href="#cb1-2108" aria-hidden="true" tabindex="-1"></a><span class="fu">### Worked example: bouncing ball</span></span>
<span id="cb1-2109"><a href="#cb1-2109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2110"><a href="#cb1-2110" aria-hidden="true" tabindex="-1"></a>This example is from <span class="co">[</span><span class="ot">@sethnaStatisticalMechanicsEntropy2021, exercise 4.8</span><span class="co">]</span>, which itself derives from <span class="co">[</span><span class="ot">@luaPracticalApplicabilityJarzynski2005</span><span class="co">]</span>. See also <span class="co">[</span><span class="ot">@hijarJarzynskiEqualityIllustrated2010</span><span class="co">]</span> for another solved example, of a chest expander with mass points stuck in the middle of the springs. You might need to read my tutorial on <span class="co">[</span><span class="ot">field-theoretic calculations</span><span class="co">](https://yuxi-liu-wired.github.io/sketches/posts/field-theory-how-to/)</span> before attempting that example.</span>
<span id="cb1-2111"><a href="#cb1-2111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2112"><a href="#cb1-2112" aria-hidden="true" tabindex="-1"></a>We have a one-dimensional system, of a single ball bounding between two walls of a piston. The only control we have is that we can move one of the piston heads. At the start, the piston has length $L$, and the system is in thermal equilibrium at inverse temperature $\beta$. We plunge the piston head at velocity $v$ for time $\Delta L / v$, then immediately reverse it, taking another $\Delta L / v$. We explicitly calculate that $\braket{e^{-\beta W}} = 1$.</span>
<span id="cb1-2113"><a href="#cb1-2113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2114"><a href="#cb1-2114" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@sethnaStatisticalMechanicsEntropy2021, figure 4.12</span><span class="co">]</span>](figure/Jarzynski_bouncing_ball_sethna_2021_4_12.png)</span>
<span id="cb1-2115"><a href="#cb1-2115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2116"><a href="#cb1-2116" aria-hidden="true" tabindex="-1"></a>The phase space of the ball has 2 dimensions, $(p, x)$. The Boltzmann distribution is</span>
<span id="cb1-2117"><a href="#cb1-2117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2118"><a href="#cb1-2118" aria-hidden="true" tabindex="-1"></a>$$\rho(p, x) = \rho(p) \rho(x) = \frac{1}{\sqrt{2\pi m/\beta}}e^{-\frac{\beta}{2m}p^2} \times \frac{1}{L}$$</span>
<span id="cb1-2119"><a href="#cb1-2119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2120"><a href="#cb1-2120" aria-hidden="true" tabindex="-1"></a>We assume that $L$ is large enough, such that the ball hits the piston head at most once. There are three possibilities:</span>
<span id="cb1-2121"><a href="#cb1-2121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2122"><a href="#cb1-2122" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If the piston head hits the ball during the in-stroke, then the ball's velocity increases by $2v$, and its kinetic energy increases by</span>
<span id="cb1-2123"><a href="#cb1-2123" aria-hidden="true" tabindex="-1"></a>$$W = \Delta KE = 2v(mv - p)$$</span>
<span id="cb1-2124"><a href="#cb1-2124" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If the piston head hits the ball during the out-stroke, then the ball's velocity decreases by $2v$, and its kinetic energy increases by  </span>
<span id="cb1-2125"><a href="#cb1-2125" aria-hidden="true" tabindex="-1"></a>$$W = 2v(mv+p)$$</span>
<span id="cb1-2126"><a href="#cb1-2126" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Otherwise, the piston head avoids the ball, and we have $W = 0$.  </span>
<span id="cb1-2127"><a href="#cb1-2127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2128"><a href="#cb1-2128" aria-hidden="true" tabindex="-1"></a>If at $t=0$, the ball is in the phase space region labelled "in region", then it will be hit in the in-stroke. If at $t=\Delta L/v$, the ball is in the phase space region labelled "out region", then it will be hit in the out-stroke. Otherwise, it will not be hit.</span>
<span id="cb1-2129"><a href="#cb1-2129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2130"><a href="#cb1-2130" aria-hidden="true" tabindex="-1"></a><span class="al">![Phase space of the bouncing balls.](figure/Jarzynski_bouncing_ball.jpg)</span></span>
<span id="cb1-2131"><a href="#cb1-2131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2132"><a href="#cb1-2132" aria-hidden="true" tabindex="-1"></a>Therefore,</span>
<span id="cb1-2133"><a href="#cb1-2133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2134"><a href="#cb1-2134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2135"><a href="#cb1-2135" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-2136"><a href="#cb1-2136" aria-hidden="true" tabindex="-1"></a>    \braket{e^{-\beta W}} &amp;= \int_{in} \rho dpdx \; e^{-\beta 2v(mv-p)} + \int_{out} \rho dpdx \; e^{-\beta 2v(mv+p)} + \int_{other} \rho dpdx \; 1 <span class="sc">\\</span></span>
<span id="cb1-2137"><a href="#cb1-2137" aria-hidden="true" tabindex="-1"></a>    &amp;= e^{-2\beta mv^2} \left(\int_{in} \rho dpdx \; e^{2\beta vp} + \int_{out} \rho dpdx \; e^{-2\beta vp}\right) +  \int_{other} \rho dpdx \; 1</span>
<span id="cb1-2138"><a href="#cb1-2138" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-2139"><a href="#cb1-2139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2140"><a href="#cb1-2140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2141"><a href="#cb1-2141" aria-hidden="true" tabindex="-1"></a>Since $\rho(p, x) = \rho(-p, x - \Delta L)$, the first two integrals can be combined by flipping the "out region", then moving it by $\Delta L$, to "out' region".</span>
<span id="cb1-2142"><a href="#cb1-2142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2143"><a href="#cb1-2143" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-2144"><a href="#cb1-2144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2145"><a href="#cb1-2145" aria-hidden="true" tabindex="-1"></a>Because $L$ is large, this is *mostly* correct, as the regions where this is incorrect has $\rho$ so small that it is negligible, as seen in the figure.</span>
<span id="cb1-2146"><a href="#cb1-2146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2147"><a href="#cb1-2147" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-2148"><a href="#cb1-2148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2149"><a href="#cb1-2149" aria-hidden="true" tabindex="-1"></a>Now we continue:</span>
<span id="cb1-2150"><a href="#cb1-2150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2151"><a href="#cb1-2151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2152"><a href="#cb1-2152" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-2153"><a href="#cb1-2153" aria-hidden="true" tabindex="-1"></a>\braket{e^{-\beta W}} &amp;\approx e^{-2\beta mv^2} \int_{in, out'} \rho dpdx \; e^{2\beta vp} +  \int_{other} \rho dpdx \; 1 <span class="sc">\\</span></span>
<span id="cb1-2154"><a href="#cb1-2154" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}(p - 2mv)^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right)</span>
<span id="cb1-2155"><a href="#cb1-2155" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-2156"><a href="#cb1-2156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2157"><a href="#cb1-2157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2158"><a href="#cb1-2158" aria-hidden="true" tabindex="-1"></a>Because the "in-out' region" is symmetric across the $p = mv$ line, we can reflect the first integral across the $p=mv$ line and obtain</span>
<span id="cb1-2159"><a href="#cb1-2159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2160"><a href="#cb1-2160" aria-hidden="true" tabindex="-1"></a>= \frac{1}{L\sqrt{2\pi m/\beta}} \left(\int_{in, out'} e^{-\frac{\beta}{2m}p^2} dpdx + \int_{other} e^{-\frac{\beta}{2m}p^2} dpdx\right) = 1</span>
<span id="cb1-2161"><a href="#cb1-2161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2162"><a href="#cb1-2162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2163"><a href="#cb1-2163" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other examples</span></span>
<span id="cb1-2164"><a href="#cb1-2164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2165"><a href="#cb1-2165" aria-hidden="true" tabindex="-1"></a>Suppose a scientist has recorded a microscopic movie of pulling on an RNA, flipped a coin to decide whether to reverse the movie, then given you the movie. Your task is to guess whether the movie is reversed.</span>
<span id="cb1-2166"><a href="#cb1-2166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2167"><a href="#cb1-2167" aria-hidden="true" tabindex="-1"></a>By Bayes theorem,</span>
<span id="cb1-2168"><a href="#cb1-2168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2169"><a href="#cb1-2169" aria-hidden="true" tabindex="-1"></a>$$Pr(\text{forward}|x, y) = \frac{1}{1 + e^{-S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span>}}$$</span>
<span id="cb1-2170"><a href="#cb1-2170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2171"><a href="#cb1-2171" aria-hidden="true" tabindex="-1"></a>where $S<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> = \beta(W<span class="co">[</span><span class="ot">x, y</span><span class="co">]</span> - \Delta F^*)$.  </span>
<span id="cb1-2172"><a href="#cb1-2172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2173"><a href="#cb1-2173" aria-hidden="true" tabindex="-1"></a>In words, the higher the entropy production, the more likely it is that the movie is playing forward. This is one way to say that entropy production provides an arrow of time. Possibly this is why molecular machines in biology tend to operate with dissipation of around $5 k_B T$, even when less dissipation is possible, to provide a good enough forward bias. Too little dissipation would mean that there is no way to distinguish the direction of time, yet time is inherent in life.</span>
<span id="cb1-2174"><a href="#cb1-2174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2175"><a href="#cb1-2175" aria-hidden="true" tabindex="-1"></a>Maxwell's demon<span class="ot">[^no-escape-maxwell]</span> has made it clear that information is physical, and vice versa. It is reasonable to guess that Maxwell's demon is a converter between information and usable work. Or stated in another way, the information that Maxwell's demon has is a form of usable work. In <span class="co">[</span><span class="ot">@parrondoThermodynamicsInformation2015</span><span class="co">]</span>, this analogy is made precise.</span>
<span id="cb1-2176"><a href="#cb1-2176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2177"><a href="#cb1-2177" aria-hidden="true" tabindex="-1"></a>Consider a "demon" machine made of cameras, motors, and other contraptions, all controlled by its <span class="co">[</span><span class="ot">finite-state machine</span><span class="co">](https://en.wikipedia.org/wiki/Finite-state_machine)</span> "brain". There are two boxes of gas of oxygen and nitrogen, that started out in equilibrium at temperature $T$. The demon uses its camera to measure some particle locations, which then causes some changes in its internal state, driving some motors to open a few gates while closing some other gates. The demon finishes its work after a fixed amount of time $t$, ending up with two tanks of gas at temperature $T$, one enriched in oxygen, the other in nitrogen.</span>
<span id="cb1-2178"><a href="#cb1-2178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2179"><a href="#cb1-2179" aria-hidden="true" tabindex="-1"></a><span class="ot">[^no-escape-maxwell]: </span>I tried to write an essay purely on equilibrium statistical mechanics, but alas, there is no escape from that demon.</span>
<span id="cb1-2180"><a href="#cb1-2180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2181"><a href="#cb1-2181" aria-hidden="true" tabindex="-1"></a>The system has changed by Helmholtz free energy $\Delta F^*$. By the Jarzynski inequality, $\braket{e^{-\Delta S}} = 1$ where $\Delta S$ is the entropy production. Here, because the demon might end up with a state that is different from where it started, we need to account for that demon's inner state. As Landauer and Bennett argued, the problem with Maxwell's demon is that it cannot forget without paying an energetic cost. And what is forgotten when a demon forgets? It forgets its statistical correlation with the tank of gas. Thus, we should account for that information entropy term by $I$, where $I$ is the <span class="co">[</span><span class="ot">mutual information</span><span class="co">](https://en.wikipedia.org/wiki/Mutual_information)</span> between the demon's end-state, and the trajectory of the system. This allows us to rescue the second law of thermodynamics:</span>
<span id="cb1-2182"><a href="#cb1-2182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2183"><a href="#cb1-2183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2184"><a href="#cb1-2184" aria-hidden="true" tabindex="-1"></a>\text{entropy production} = \beta(W - \Delta F^*) + I</span>
<span id="cb1-2185"><a href="#cb1-2185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-2186"><a href="#cb1-2186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2187"><a href="#cb1-2187" aria-hidden="true" tabindex="-1"></a>and the Jarzynski inequality becomes $\braket{e^{-\beta(W - \Delta F^*) + I}}$.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>