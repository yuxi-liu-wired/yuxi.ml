<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-12-26">
<meta name="description" content="Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.">

<title>The Backstory of Backpropagation – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="The Backstory of Backpropagation – Yuxi on the Wired">
<meta property="og:description" content="Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/backstory-of-backpropagation/figure/banner_cropped.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="652">
<meta property="og:image:width" content="1024">
<meta property="og:image:alt" content="Stylized illustration of a computational graph, with gibberish text that resemble vector calculus, thick and thin lines representing the flow of numbers in the graph. In the center of the vaguely octagonal graph is a letter D, D for derivative. High contrast, monochromatic, minimalistic, in the style of vector svg art.">
<meta name="twitter:title" content="The Backstory of Backpropagation – Yuxi on the Wired">
<meta name="twitter:description" content="Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/backstory-of-backpropagation/figure/banner_cropped.png">
<meta name="twitter:image-height" content="652">
<meta name="twitter:image-width" content="1024">
<meta name="twitter:image:alt" content="Stylized illustration of a computational graph, with gibberish text that resemble vector calculus, thick and thin lines representing the flow of numbers in the graph. In the center of the vaguely octagonal graph is a letter D, D for derivative. High contrast, monochromatic, minimalistic, in the style of vector svg art.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">The Backstory of Backpropagation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">math</div>
                <div class="quarto-category">physics</div>
                <div class="quarto-category">history</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 26, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 22, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#the-backpropagation-algorithm" id="toc-the-backpropagation-algorithm" class="nav-link" data-scroll-target="#the-backpropagation-algorithm">The backpropagation algorithm</a>
  <ul class="collapse">
  <li><a href="#discrete-logical-time" id="toc-discrete-logical-time" class="nav-link" data-scroll-target="#discrete-logical-time">Discrete logical time</a></li>
  <li><a href="#continuous-logical-time" id="toc-continuous-logical-time" class="nav-link" data-scroll-target="#continuous-logical-time">Continuous logical time</a></li>
  <li><a href="#hybrid-logical-time" id="toc-hybrid-logical-time" class="nav-link" data-scroll-target="#hybrid-logical-time">Hybrid logical time</a></li>
  <li><a href="#optimal-control-theory" id="toc-optimal-control-theory" class="nav-link" data-scroll-target="#optimal-control-theory">Optimal control theory</a></li>
  </ul></li>
  <li><a href="#leibniz" id="toc-leibniz" class="nav-link" data-scroll-target="#leibniz">Leibniz</a></li>
  <li><a href="#mcculloch-and-pitts" id="toc-mcculloch-and-pitts" class="nav-link" data-scroll-target="#mcculloch-and-pitts">McCulloch and Pitts</a></li>
  <li><a href="#frank-rosenblatt" id="toc-frank-rosenblatt" class="nav-link" data-scroll-target="#frank-rosenblatt">Frank Rosenblatt</a></li>
  <li><a href="#bernard-widrow-and-marcian-hoff" id="toc-bernard-widrow-and-marcian-hoff" class="nav-link" data-scroll-target="#bernard-widrow-and-marcian-hoff">Bernard Widrow and Marcian Hoff</a></li>
  <li><a href="#seppo-linnainmaa" id="toc-seppo-linnainmaa" class="nav-link" data-scroll-target="#seppo-linnainmaa">Seppo Linnainmaa</a></li>
  <li><a href="#david-rumelhart" id="toc-david-rumelhart" class="nav-link" data-scroll-target="#david-rumelhart">David Rumelhart</a></li>
  <li><a href="#terence-sejnowski" id="toc-terence-sejnowski" class="nav-link" data-scroll-target="#terence-sejnowski">Terence Sejnowski</a></li>
  <li><a href="#geoffrey-hinton" id="toc-geoffrey-hinton" class="nav-link" data-scroll-target="#geoffrey-hinton">Geoffrey Hinton</a></li>
  <li><a href="#paul-werbos" id="toc-paul-werbos" class="nav-link" data-scroll-target="#paul-werbos">Paul Werbos</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.</p>
<p>Seppo Linnainmaa’s claim of priority is his 1970 master thesis – in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.</p>
<p>Paul Werbos’ claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.</p>
<p>David Rumelhart reinvented it in 1982. As he freely admits, he didn’t cite previous work because they were so obscure. The 1986 paper he coauthored <span class="citation" data-cites="rumelhartLearningRepresentationsBackpropagating1986">(<a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span> became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.</p>
<p>I was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:</p>
<ul>
<li>Bad luck.</li>
<li>They were misled by the contemporary understanding of real neurons, and the McCulloch–Pitts model. Back then everyone “knew” that real neurons are 0-1 spike generators.</li>
<li>They thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.</li>
<li>They distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the “blessing of scale”: local optima are almost as good as the global optimum.</li>
<li>They attempted to achieve parity with digital computers, which were discrete.</li>
<li>They attempted to distance themselves from cybernetics, which included optimal control theory.</li>
</ul>
</section>
<section id="the-backpropagation-algorithm" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-backpropagation-algorithm">The backpropagation algorithm</h2>
<p>To set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.</p>
<p>The graph can be finite or infinite, but in all cases, its acyclic directedness allows us to assign a “logical time” to each node.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Some nodes are independent variables: they point to other nodes, but no nodes point to <em>them</em>. Other nodes are dependent. If we know the independent variables, we can propagate their values <em>forward</em> in logical time and determine the values of every node. This is the “forward pass”. Backpropagation goes backwards in logical time.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;An existence proof that anything we might care about must have a logical time: Anything that happens, happens in the real world. The real world has a single direction in time. Therefore, any computation that can happen, must have a logical time that is identical with physical time.</p>
<p>Note that logical time does not have to coincide with physical time. For example, to model goal-directed behavior, it might be better to put the physical future into the logical past.</p></div></div><p>We use the convention of putting derivatives on the rows. So for example, for <span class="math inline">\(f: \mathbb{R}^2\to\mathbb{R}^2\)</span>, we have</p>
<p><span class="math display">\[
\nabla_x f = \frac{df}{dx} = \begin{bmatrix}
\frac{df_1}{dx_1} &amp; \frac{df_1}{dx_2} \\
\frac{df_2}{dx_1} &amp; \frac{df_2}{dx_2}
\end{bmatrix}
\]</span></p>
<p>This convention simplifies a lot of equations, and completely avoids transposing any matrix.</p>
<section id="discrete-logical-time" class="level3">
<h3 class="anchored" data-anchor-id="discrete-logical-time">Discrete logical time</h3>
<p>Consider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions. We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as <span class="math inline">\(x_0, x_1, \dots, x_T\)</span>. By our ordering, <span class="math inline">\(x_1\)</span> depends on only <span class="math inline">\(x_0\)</span>, and <span class="math inline">\(x_2\)</span> depends on only <span class="math inline">\(x_1, x_2\)</span>, and so on:</p>
<p><span class="math display">\[
\begin{aligned}
x_0 &amp;= x_0 \\
x_1 &amp;= f_1(x_0) \\
&amp;\cdots \\
x_T &amp;= f_T(x_0, x_1, \dots , x_{T-1})
\end{aligned}
\]</span></p>
<p>Now we perform an infinitesimal perturbation on every one of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as <span class="math inline">\(dx_0, dx_1, \dots, dx_T\)</span>. We can now compute the derivative of <span class="math inline">\(dx_T\)</span> with respect to every other variable by backpropagating the perturbation. Suppose we can see only <span class="math inline">\(dx_T\)</span>, then the change in <span class="math inline">\(x_T\)</span> due to <span class="math inline">\(dx_T\)</span> is the identity. That is,</p>
<p><span class="math display">\[
\frac{dx_T}{dx_T} = I
\]</span></p>
<p>Now suppose we can see only <span class="math inline">\(dx_{T-1}\)</span>, then the change in <span class="math inline">\(x_T\)</span> due to <span class="math inline">\(dx_{T-1}\)</span> can only come from the final step in the forward propagation. Therefore</p>
<p><span class="math display">\[
\frac{dx_T}{dx_{T-1}} = \nabla_{x_{T-1}} f_T(x_0, x_1, \dots , x_{T-1})
\]</span></p>
<p>Similarly, the change in <span class="math inline">\(x_T\)</span> due to <span class="math inline">\(dx_{T-2}\)</span> can come from either directly changing <span class="math inline">\(x_T\)</span> or from changing <span class="math inline">\(x_{T-1}\)</span> and thereby changing <span class="math inline">\(x_T\)</span>. Therefore,</p>
<p><span class="math display">\[
\frac{dx_T}{dx_{T-2}} =
    \nabla_{x_{T-1}} f_{T}(x_0, x_1, \dots , x_{T-2}) +
    \underbrace{\frac{dx_T}{dx_{T-1}}\nabla_{x_{T-2}} f_{T-1}(x_0, x_1, \dots , x_{T-2})}_{\text{the chain rule}}
\]</span></p>
<p>This generalizes to the rest of the steps.</p>
<p>The above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each <span class="math inline">\(x_t\)</span> only directly influence a few more nodes down the line. In standard neural networks, typically <span class="math inline">\(x_{t}\)</span> only directly influences <span class="math inline">\(x_{t+1}, x_{t+2}\)</span>. Thus, sparsity is vital for backpropagation to be relevant.</p>
<p>As a side note, we could in fact compute <em>all</em> derivatives, not just the first, in one single backward pass. Other than the second derivatives <span class="math inline">\(\nabla^2_{x_t}x_T\)</span>, there is rarely any use for the other derivatives, such as <span class="math inline">\(\nabla_{x_t}\nabla_{x_s}x_T, \nabla^3_{x_t}x_T\)</span>, etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only keep the positive diagonal entries of <span class="math inline">\(\nabla^2_{x_t}x_T\)</span> and set all other entries to zero <span class="citation" data-cites="lecunGeneralizationNetworkDesign1989">(<a href="#ref-lecunGeneralizationNetworkDesign1989" role="doc-biblioref">LeCun 1989</a>)</span>.</p>
</section>
<section id="continuous-logical-time" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="continuous-logical-time">Continuous logical time</h3>
<p>Consider the problem of controlling a car along a highway. We discard all details, so that the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time <span class="math inline">\(t\in [0, T]\)</span>. Write the state variable as follows:</p>
<p><span class="math display">\[
x_t = (\text{location at time }t, \text{velocity at time }t)
\]</span></p>
<p>It might be confusing to use <span class="math inline">\(x_t\)</span> for the state at time <span class="math inline">\(t\)</span>, instead of for location, but it makes the notation consistent.</p>
<p>Suppose the only thing we can influence is how much we press the pedal. Write <span class="math inline">\(u_t\)</span> to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume that the car is slowed down by friction that is proportional to velocity. We then have:</p>
<p><span class="math display">\[
\dot x_t = f(x_t, u_t)
\]</span></p>
<p>where <span class="math inline">\(f(x_t, u_t) = (x_{t, 1}, -\mu x_{t, 1} + u_t)\)</span> is the dynamics equation of the system, and <span class="math inline">\(\mu\)</span> is the friction coefficient.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;To allow for time-varying dynamics, simply replace <span class="math inline">\(f(x_t, u_t)\)</span> with <span class="math inline">\(f(t, x_t, u_t)\)</span>. This clutters the notation without involving new ideas.</p></div></div><p>Now, we can draw the computation graph as before. The computation graph has a continuum of nodes, but it has a very simple structure. The graph has two kinds of nodes: the <span class="math inline">\(x_t\)</span> nodes and the <span class="math inline">\(u_t\)</span> nodes. Its independent variables are <span class="math inline">\(x_0\)</span> and all the <span class="math inline">\(u_t\)</span> nodes. Each <span class="math inline">\(x_{t+dt}\)</span> depends on only <span class="math inline">\(x_{t}\)</span> and <span class="math inline">\(u_t\)</span>. This makes the two propagations particularly simple.</p>
<p>The forward propagation is obtained by integration, which we can unwrap into a form resembling the case of the discrete logical time:</p>
<p><span class="math display">\[
\begin{aligned}
x_0 &amp;= x_0 \\
x_{dt} &amp;= x_0 + f(x_0, u_0) dt \\
&amp;\cdots \\
x_{T} &amp;= x_{T-dt} + f(x_{T-dt}, u_{T-dt}) dt \\
\end{aligned}
\]</span></p>
<p>The backpropagation is similarly obtained. By inspecting the computation graph, we can see that each <span class="math inline">\(u_t\)</span> only directly influences <span class="math inline">\(x_{t+dt}\)</span>, giving</p>
<p><span class="math display">\[
\frac{dx_T}{du_t} = \frac{dx_T}{dx_{t+dt}} \nabla_{u_t}f(x_t, u_t) dt.
\]</span></p>
<p>It remains to compute <span class="math inline">\(\frac{dx_T}{dx_t}\)</span>. This can be found by backpropagation too, since each <span class="math inline">\(x_t\)</span> only directly influences <span class="math inline">\(x_{t+dt}\)</span>, we have</p>
<p><span class="math display">\[
\frac{dx_T}{dx_t} = \frac{dx_T}{dx_{t+dt}} \left[I + \nabla_{x_t} f(x_t, u_t) dt\right].
\]</span></p>
<p>If we denote the gradient as <span class="math inline">\(g_t := \frac{dx_T}{dx_t}\)</span>, then we find an equation for <span class="math inline">\(g_t\)</span>:</p>
<p><span class="math display">\[
g_t = \left[I + (g_t + \dot g_t dt) \nabla_{x_t} f(x_t, u_t) dt\right]\implies \dot g_t = -g_t\nabla_{x_t} f(x_t, u_t)
\]</span></p>
<p>This equation bottoms out at the end time, <span class="math inline">\(t=T\)</span>, for which <span class="math inline">\(g_T = \frac{dx_T}{dx_T} = I\)</span>. Thus we have the <a href="https://en.wikipedia.org/wiki/Costate_equation">costate equations</a>:</p>
<p><span class="math display">\[
\begin{cases}
g_T &amp;= I \\
\dot g_t &amp;= - g_t \nabla_{x_t} f(x_t, u_t)
\end{cases}
\]</span></p>
<p>which must, as you can see, be integrated <em>backwards in time</em> – backpropagation again! Indeed, control theory practically compels us to find backpropagation.</p>
</section>
<section id="hybrid-logical-time" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="hybrid-logical-time">Hybrid logical time</h3>
<p>When the computation graph has both nodes with discrete logical times and nodes with continuous logical times, we call such a system as having hybrid logical time.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;The name “hybrid” comes from “<a href="https://en.wikipedia.org/wiki/Hybrid_system">hybrid control theory</a>”, which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.</p></div></div><p>The idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as <span class="math inline">\(J\)</span>, representing the “cost” of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write <span class="math inline">\(J\)</span> down as something like</p>
<p><span class="math display">\[
J = \underbrace{(x_{T, 0} - x_{goal})^2}_{\text{location should be at the goal location at the end time}}
    + \underbrace{(x_{T, 1} - 0)^2}_{\text{speed should be zero at the end time}} + \int_0^T u_t^2 dt
\]</span></p>
<p>More generally, the objective to be optimized is in the form</p>
<p><span class="math display">\[
J = A(x_T) + \int_0^T L(x_t, u_t)dt
\]</span></p>
<p>for some real-valued functions <span class="math inline">\(A, L\)</span>.</p>
<p>Of course, we can care about more than the state at the last time-step. We can care about multiple time-steps <span class="math inline">\(t_0, t_1, \dots, t_n\)</span> by writing down a cost function <span class="math inline">\(J = \sum_{i=0}^n A_i(x_{t_i}) + \int_0^T L(x_T, u_T)dt\)</span>, but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.</p>
<p>Define the costate <span class="math inline">\(\lambda_t := \nabla_{x_t} J\)</span>, then the costate backpropagates as:</p>
<p><span class="math display">\[
\lambda_t = L(x_t, u_t) dt + \lambda_{t+dt}(I + \nabla_{x_t}f(x_t, u_t)dt)
\]</span></p>
<p>and by simplifying, we have the costate equation:</p>
<p><span class="math display">\[
\begin{cases}
\lambda_T &amp;= \nabla_{x_T} A(x_T) \\
\dot \lambda_t &amp;= - \nabla_{x_t} L(x_t, u_t) - \lambda_t \nabla_{x_t} f(x_t, u_t)
\end{cases}
\]</span></p>
<p>which can be solved by integrating backward in time.</p>
<p>Once we have obtained all the costates, we can compute <span class="math inline">\(\nabla_{u_t} J\)</span>. Since <span class="math inline">\(u_t\)</span> can only influence <span class="math inline">\(J\)</span> either directly via <span class="math inline">\(L(x_t, u_t)\)</span> or indirectly via <span class="math inline">\(x_t\)</span>, we have</p>
<p><span class="math display">\[
\nabla_{u_t} J = \left[\nabla_{u_t}f(x_t, u_t) \lambda_t + \nabla_{u_t}L(x_t, u_t)\right]dt
\]</span></p>
<p>Note that <span class="math inline">\(\nabla_{u_t} J\)</span> is an infinitesimal in <span class="math inline">\(dt\)</span>. This is qualitatively different from <span class="math inline">\(\nabla_{x_t}J = g_t\)</span>, which is not an infinitesimal, but a mere matrix. Why is it so? The simplest example suffices to illustrate.</p>
<p>Consider a mass point sliding on a frictionless plane. When we perturb <span class="math inline">\(x_t\)</span>, we push it to the side by <span class="math inline">\(\delta x_{t, 0}\)</span> and also change its velocity by <span class="math inline">\(\delta x_{t, 1}\)</span>, and so at the end time <span class="math inline">\(T\)</span>, we would have changed <span class="math inline">\(x_T\)</span> by <span class="math inline">\((\delta x_{t, 0} + (T-t)\delta x_{t, 1}, \delta x_{t, 1})\)</span>, which is the same order of infinitesimal. Now, we can control the mass point by applying a force <span class="math inline">\(u_t\)</span>, which gives us the dynamics equation</p>
<p><span class="math display">\[
\dot x_t = (x_{t, 1}, u_t)
\]</span></p>
<p>To “perturb” <span class="math inline">\(u_t\)</span> by <span class="math inline">\(\delta u_t\)</span> does not make sense on its own, as a “spike” of <span class="math inline">\(\delta u_t\)</span> that lasts for zero amount of time does not show up in the system trajectory. One can apply a durationless jump in state, but one cannot apply a durationless force. Therefore, it only makes sense to perturb <span class="math inline">\(u_t\)</span> by <span class="math inline">\(\delta u_t\)</span> and persist the perturbation for <span class="math inline">\(dt\)</span> time. This perturbs the state at the end time by <span class="math inline">\(((T-t)\delta u_t dt , \delta u_t dt)\)</span>, which means that <span class="math inline">\(\nabla_{u_t}x_T\)</span> is proportional to <span class="math inline">\(dt\)</span>.</p>
</section>
<section id="optimal-control-theory" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="optimal-control-theory">Optimal control theory</h3>
<p>An optimal trajectory must have <span class="math inline">\(\nabla_{u_t} J = 0\)</span>, since otherwise, we could shave off a little piece of cost by giving <span class="math inline">\(u_t\)</span> a little boost in the opposite direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy</p>
<p><span class="math display">\[
\begin{cases}
- \dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) \\
0                &amp;= \nabla_{u_t} L(x_t, u_t) + \lambda_t \nabla_{u_t} f(x_t, u_t) \\
\end{cases}
\]</span></p>
<p>Now, what is the effect of perturbing <span class="math inline">\(u_t\)</span> by <span class="math inline">\(du_t\)</span>? It would perturb <span class="math inline">\(x_{t+dt}\)</span> by <span class="math inline">\(\nabla_{u_t} f(x_t, u_t) du_t dt\)</span>, a second-order infinitesimal. Consequently, it would perturb <span class="math inline">\(x_T\)</span> by only a second-order infinitesimal, and thus <span class="math inline">\(\lambda\)</span> too. Therefore, we have</p>
<p><span class="math display">\[
\nabla_{u_t}\lambda_t = 0
\]</span></p>
<p>giving us simplified equations for optimality:</p>
<p><span class="math display">\[
\begin{cases}
- \dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) \\
0                &amp;= \nabla_{u_t} (L(x_t, u_t) + \lambda_t f(x_t, u_t)) \\
\end{cases}
\]</span></p>
<p>Unfortunately, we cannot simplify the first equation similarly because <span class="math inline">\(\nabla_{x_t}\lambda_t \neq 0\)</span>. Still, it seems <span class="math inline">\(L(x_t, u_t) + \lambda_t f(x_t, u_t)\)</span> should be an important quantity:</p>
<p><span class="math display">\[
H(x_t, u_t, \lambda_t) := L(x_t, u_t) + \lambda_t f(x_t, u_t)
\]</span></p>
<p>The letters are meaningful. <span class="math inline">\(L\)</span> is the “Lagrangian”, and <span class="math inline">\(H\)</span> is the “Hamiltonian”. Indeed, classical Hamiltonian mechanics is a special case of optimal<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> control theory.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;To be completely precise, it is a necessary but insufficient condition for optimality. Just like how a function can have zero derivatives on the peaks, valleys and shoulders, a trajectory can have zero functional derivative, even if it is the best, the worst, and the … shouldered? In jargon, we say that those conditions are <strong>first order optimality conditions</strong>, since they use only the derivative, not the second-derivative.</p></div></div><p>If we interpret economically the quantities, then <span class="math inline">\(J\)</span> is the cost of the entire trajectory, <span class="math inline">\(\lambda_t\)</span> is the marginal cost of the point <span class="math inline">\(x_t\)</span> in the trajectory, and <span class="math inline">\(L(x_t, u_t)\)</span> is the cost-rate at time <span class="math inline">\(t\)</span>. The second equation of optimality <span class="math inline">\(\nabla_{u_t} H(x_t, u_t, \lambda_t) = 0\)</span> states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.</p>
<p>Therefore, define the “optimized” Hamiltonian and the optimized control relative to it:</p>
<p><span class="math display">\[
\begin{cases}
H^*(x_t, \lambda_t) &amp;:= \min_{u_t}H(x_t, u_t, \lambda_t) = \min_{u_t} \left(L(x_t, u_t) + \lambda_t f(x_t, u_t)\right) \\
u^*(x_t, \lambda_t) &amp;:= \mathop{\mathrm{argmin}}_{u_t}H(x_t, u_t, \lambda_t)
\end{cases}
\]</span></p>
<p>Then, by <a href="https://en.wikipedia.org/wiki/Hotelling%27s_lemma">Hotelling’s lemma</a>, we derive the <a href="https://en.wikipedia.org/wiki/Hamiltonian_mechanics">Hamiltonian equations of motion</a>:</p>
<p><span class="math display">\[
\begin{cases}
- \dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t^*) + \lambda_t \nabla_{x_t} f(x_t, u_t^*) &amp;= \nabla_{x_t} H^*(x_t, \lambda_t) \\
  \dot x_t       &amp;= f(x_t, u_t^*)                                                     &amp;= \nabla_{\lambda_t} H^*(x_t, \lambda_t) \\
\end{cases}
\]</span></p>
<p>This is often called the <a href="https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle">Pontryagin’s maximum principle</a>, as Pontryagin’s school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it was used from the <a href="https://en.wikipedia.org/wiki/Space_Race">space race</a> to the <a href="https://en.wikipedia.org/wiki/Nuclear_arms_race">missile race</a>.</p>
<p>In classical control theory, the equation is sometimes solved in closed form, as in the case of <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">linear quadratic control</a>. When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Similar to how the Hamiltonian equations have the alternative form of the <a href="https://en.wikipedia.org/wiki/Hamilton-Jacobi_equation">Hamilton–Jacobi equation</a>, the Pontryagin equations have an alternative form in the <a href="https://en.wikipedia.org/wiki/Hamilton-Jacobi_equation">Hamilton–Jacobi–Bellman equation</a>. Possibly, the name “dynamic programming” appears later in Paul Werbos’ invention of backpropagation, which he named “dynamic feedback”.</p>
<p>In economics, one can modify this framework slightly to allow discounting cost over time, yielding theories about “optimal investment” such as the <a href="https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model">Ramsey optimal growth theory</a>.</p>
</section>
</section>
<section id="leibniz" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="leibniz">Leibniz</h2>
<p>The chain rule dates back to <em>Calculus Tangentium differentialis</em> [Differential calculus of tangents], a manuscript by Leibniz dated 1676 November <span class="citation" data-cites="childManuscriptsLeibnizHis1917">(<a href="#ref-childManuscriptsLeibnizHis1917" role="doc-biblioref">Child 1917</a>)</span>. It says</p>
<blockquote class="blockquote">
<p>it does not matter, whether or no the letters <span class="math inline">\(x, y, z\)</span> have any known relation, for this can be substituted afterward.</p>
</blockquote>
<p>In mathematical notation, he found that <span class="math inline">\(dy = dx \frac{dy}{dx}\)</span>, or in his notation, <span class="math inline">\(\overline{dy} = \overline{dx} \frac{dy}{dx}\)</span>, where the overbar denotes the thing to be differentiated. You can read it as a bracket: <span class="math inline">\(d(y) = d(x) \frac{dy}{dx}\)</span>.</p>
<p>He then gave the following examples<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Yes, there is a sign error. No, I’m not going to fix it. He just has to live with his mistakes.</p></div></div><p><span class="math display">\[
\begin{aligned}
&amp; \overline{d \sqrt[2]{a+b z+c z^2}} \text {. Let } a+b z+c z^2=x \text {; } \\
\text{Then} \quad  &amp; \overline{d \sqrt[2]{x}}=-\frac{1}{2 \sqrt{x}} \text {, and } \frac{d x}{d z}=b+2 c z \text {; } \\
\text{Therefore} \quad  &amp; \overline{d \sqrt{a+b z+c z^2}}=-\frac{b+2 c z}{2 \overline{d z} \sqrt{a+b z+c z^2}} \\
&amp;
\end{aligned}
\]</span></p>
</section>
<section id="mcculloch-and-pitts" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="mcculloch-and-pitts">McCulloch and Pitts</h2>
<p>In the famous paper <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span>, McCulloch and Pitts proposed that</p>
<blockquote class="blockquote">
<p>Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.</p>
</blockquote>
<p>The McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. In fact, it uses the same abstruse notations of the <a href="https://en.wikipedia.org/wiki/Principia_Mathematica"><em>Principia Mathematica</em></a>, which is cited in the paper.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/principia_mathematica.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The infamous proof of 1+1=2 in Principia Mathematica</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/principia_mathematica_McCulloch_and_Pitts.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The same notation is used by McCulloch and Pitts</figcaption>
</figure>
</div>
<p>The McCulloch and Pitts paper, like the <em>Perceptrons</em> book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967, chap. 3</a>)</span>, which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc.</p>
<p><img src="figure/minsky_1976_finite_state_machine.png" class="img-fluid"></p>
<p><img src="figure/minsky_1976_serial_binary_addition_network.png" class="img-fluid"></p>
</section>
<section id="frank-rosenblatt" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="frank-rosenblatt">Frank Rosenblatt</h2>
<p>Frank Rosenblatt is the originator of the term “backpropagation”, or more precisely, “back-propagating error-correction procedure” <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, chap. 13</a>)</span>, although it was quite far from our current use of backpropagation. He is the originator of many concepts in neural networks, and he was even popularly famous during his own time. During 1957–1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room.</p>
<p>A perceptron is a function of the form <span class="math inline">\(\theta(w^T x + b)\)</span>, where <span class="math inline">\(\theta\)</span> is the 0-1 step function, and <span class="math inline">\(w \in \mathbb{R}^n, b \in \mathbb{R}\)</span> are its learnable parameters. A perceptron network is a network of perceptrons. Rosenblatt experimented with a large number of perceptron network architectures, but most often, he experimented with a certain 2-layered feedforward architecture, which is sometimes called “<em>the</em> perceptron machine”.</p>
<p><em>The</em> perceptron machine is a machine that computes a function of type <span class="math inline">\(\{0, 1\}^n \to \{0, 1\}\)</span>. Its input layer is composed of units named “Stimulus units” or “S units”. The S units do not perform any computation, but merely pass binary outputs to the hidden layer. In the Mark I perceptron, the S units were 20×20 cadmium sulfide photocells, making a 400-pixel image. The hidden layer is composed of perceptrons named “Association units” or “A units”. Their outputs pass on to the output layer, composed of perceptrons named “Response units” or “R units”.</p>
<p>We can describe <em>the</em> perceptron machine in one equation:</p>
<p><span class="math display">\[
f(x) = \theta\left(b^{R} + \sum_i w^{AR}_i \theta\left((w^{SA, i})^T x + b^{A}_i\right)\right)
\]</span></p>
<p>Rosenblatt proved some mathematical theorems, the most important of which is the <a href="https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron">perceptron convergence theorem</a>, which shows that assuming the dataset is linearly separable, then the perceptron learning algorithm converges after making a bounded number of errors<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. He also performed experiments on the Mark I Perceptron machine, IBM computers, and some other computers.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;There is another technical assumption on the dataset, but it is omitted for space. Suffice to say it is satisfied for all finite datasets.</p></div></div><p>His theorems and experiments were exhaustively documented in his book <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962</a>)</span>. Its breadth is quite astonishing. It contains:</p>
<ul>
<li>perceptrons with continuous activation functions (section 10.2);</li>
<li>perceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);</li>
<li>perceptron layers with random delay in transmission time (chapter 11);</li>
<li>layers with connections between units within the same layer, with possibly closed loops (chapter 17–19);</li>
<li>layers with connections from a later layer to a previous layer (chapter 20);</li>
<li>residual connections (Figure 42);</li>
<li>multimodal perceptron networks that learns to associate image and audio inputs (Figure 58);</li>
<li>program-learning perceptrons (chapter 22);</li>
<li>perceptron networks that analyze videos and audios (chapter 23).</li>
</ul>
<p>From our vantage point, we can fairly say that he invented randomization, residual connections, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks…</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rosenblatt_figure_58.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="margin-caption">Figure 58. The first multimodal neural network?</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rosenblatt_figure_42.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption class="margin-caption">Figure 42. The dashed lines denote variable weights, and the solid lines denote fixed weights. This is the residual connection.</figcaption>
</figure>
</div>
<p>What is even more astonishing is that, as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff. The perceptron learning rule converges, but only for a single-layered perceptron network on linearly separable data. Thus, when it came to two-layered perceptron networks, he randomly wired the first layer, then froze it, and only adapted the second layer. This would become the focal point of the “<a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/neural-network-winter">perceptron controversy</a>”.</p>
<p>In the chapter where he talked about backpropagation <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, chap. 13</a>)</span>, he tried to train both layers in the two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The desired outputs for the output units are supplied by the experimenter, but the desired outputs for the hidden units could not be known, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to “backpropagate the error” to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.</p>
<p>One last thing about his backpropagation rule: he also discovered the layer-wise learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.</p>
<blockquote class="blockquote">
<p>It is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur.</p>
</blockquote>
</section>
<section id="bernard-widrow-and-marcian-hoff" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="bernard-widrow-and-marcian-hoff">Bernard Widrow and Marcian Hoff</h2>
<p>Widrow and Hoff worked on neural networks in the early 1960s. They started with training a single perceptron with gradient descent on the squared loss, then proceeded to spend years trying to train a two-layered network <em>without</em> gradient descent. I know – I cannot make this sound any less puzzling.</p>
<p>The Widrow–Hoff machine, which they called the ADALINE (“ADAptive Linear NEuron”), is a function of type <span class="math inline">\(\mathbb{R}^n \to \{0,1\}\)</span> defined by</p>
<p><span class="math display">\[
f(x) = \theta(w^T x + b)
\]</span></p>
<p>and here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was <em>trained</em> by gradient descent with the squared loss function <span class="math inline">\(\frac 12 (w^T x + b - y)^2\)</span>, which is <em>continuous</em>, not discrete:</p>
<p><span class="math display">\[
w \leftarrow w - \alpha (w^T x + b - y) w, \quad b \leftarrow b - \alpha (w^T x + b - y) b
\]</span></p>
<p>The first ADALINE machine was a box that learned to classify binary patterns on a <span class="math inline">\(4 \times 4\)</span> grid. It was pretty amusing, as everything was done manually. The patterns were inputted by flipping 16 switches by hand. The error <span class="math inline">\(w^T x + b - y\)</span> was read from a voltmeter, and the parameters <span class="math inline">\(w, b\)</span> were individually adjusted by turning knobs controlling rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memistors, though the pattern input <span class="math inline">\(x\)</span> and the desired output <span class="math inline">\(y\)</span> were still entered by manual switches.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/ADALINE.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="margin-caption">All components of the ADALINE machine, color-labelled, from the algorithm, to the circuit diagram, to the front-panel of the physical machine.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/widrow_2022_memistor_adaline.jpg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="margin-caption">A memistor ADALINE with glass-sealed memistors. <span class="citation" data-cites="widrowAncientHistory2023">(<a href="#ref-widrowAncientHistory2023" role="doc-biblioref">Widrow 2023, fig. 26.12</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/widrow_2022_learning_curve.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="margin-caption">Test patterns for the ADALINE machine. It also shows one of the first learning curves in machine learning. <span class="citation" data-cites="widrowAncientHistory2023">(<a href="#ref-widrowAncientHistory2023" role="doc-biblioref">Widrow 2023, fig. 26.4</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/MADALINE.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A MADALINE from 1962. The “JOB ASSIGNER” learning rule is too hacky to explain. <span class="citation" data-cites="widrowGeneralizationInformationStorage1962">(<a href="#ref-widrowGeneralizationInformationStorage1962" role="doc-biblioref">Widrow 1962, fig. 11</a>)</span></figcaption>
</figure>
</div>
<p>Widrow recounts an amusing encounter with Rosenblatt:</p>
<blockquote class="blockquote">
<p>I just put the pattern in and the Adaline went “phut,” and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt’s students put the pattern into the perceptron. You could see it in the lights on the perceptron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went “blip ,” and there it was, adapted. They put it in the perceptron, and it’s still grinding away. We put in a couple more patterns. Then we test the Adaline and test the perceptron to see whether the patterns are still in there.</p>
<p>They’re in the Adaline. In the perceptron, they’re all gone. I don’t know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, “You’d be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer.” He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there’s a first layer that’s randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it’s adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.</p>
</blockquote>
<p>After the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE (“MADALINE”, or “many ADALINE”), which of course could not be trained by gradient descent, because the hidden layer ADALINE units used the Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function and tried every other trick <em>except</em> that. They ended up with the <a href="https://en.wikipedia.org/wiki/ADALINE#MADALINE">MADALINE I rule</a>. In short, it was a heuristic rule for synthesizing supervision signals for the hidden layer, much like <a href="#frank-rosenblatt">Rosenblatt’s heuristic rule</a>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;The MADALINE I machine consists of multiple hidden ADALINE units, and a single output layer consisting of a single unit. The output unit merely takes the majority vote from the hidden units. If the desired output is <span class="math inline">\(+1\)</span>, but the actual output is <span class="math inline">\(-1\)</span>, then the machine picks those ADALINE units that are negative, but closest to being positive, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of “minimal disturbance principle”.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;Marvin Minsky would approve.</p>
<blockquote class="blockquote">
<p>“[The perceptron] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of.”<span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
</div></div><p>Frustrated by the difficulty, they left neural network research. Hoff went to Intel to co-invent the microprocessor, and Widrow set about applying the ADALINE to small problems that it could solve well<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.</p>
<blockquote class="blockquote">
<p>Engineers at Apple have told me that every iPhone, since the iPhone 5, uses the LMS algorithm all over the device. They could not tell me where because, if they did, they would have to shoot me. Apple keeps secrets. <span class="citation" data-cites="widrowCyberneticsGeneralTheory2022">(<a href="#ref-widrowCyberneticsGeneralTheory2022" role="doc-biblioref">Widrow 2022</a>, preface, page xix)</span></p>
</blockquote>
<p>Perhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself, it was a blind spot for all researchers:</p>
<blockquote class="blockquote">
<p>The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic first layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it’s very difficult to adapt a hidden layer. … We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn’t that we didn’t try. I mean we would have given our eye teeth to come up with something like backprop.</p>
<p>Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; <strong>you have to have a smooth nonlinearity … no one knew anything about it at that time.</strong> This was long before Paul Werbos. <strong>Backprop to me is almost miraculous</strong>. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>When he heard about the “miraculous” backpropagation in the 1980s, he immediately started writing papers in neural networks again.</p>
<p>If this was an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military:</p>
<blockquote class="blockquote">
<p>… the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. … The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don’t show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track. <span class="citation" data-cites="widrowOralHistoryBernard1997">(<a href="#ref-widrowOralHistoryBernard1997" role="doc-biblioref">Widrow 1997</a>)</span></p>
</blockquote>
<p>The problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it “the best piece of work I ever did in my whole life”. He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise <span class="citation" data-cites="widrowQuantizationNoiseRoundoff2008">(<a href="#ref-widrowQuantizationNoiseRoundoff2008" role="doc-biblioref">Widrow and Kollár 2008</a>)</span>.</p>
<p>So regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the <em>other</em> pioneers went down the same wrong path.</p>
</section>
<section id="seppo-linnainmaa" class="level2">
<h2 class="anchored" data-anchor-id="seppo-linnainmaa">Seppo Linnainmaa</h2>
<p>It’s said that Seppo Linnainmaa’s master’s thesis in 1970 contains the backpropagation algorithm, but it is in Finnish and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. <span class="citation" data-cites="griewankWhoInventedReverse2012">(<a href="#ref-griewankWhoInventedReverse2012" role="doc-biblioref">Griewank 2012</a>)</span> describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.</p>
<p>I checked all his English papers during the 1970s, and it seems only <span class="citation" data-cites="linnainmaaTaylorExpansionAccumulated1976">(<a href="#ref-linnainmaaTaylorExpansionAccumulated1976" role="doc-biblioref">Linnainmaa 1976</a>)</span> has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives.</p>
</section>
<section id="david-rumelhart" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="david-rumelhart">David Rumelhart</h2>
<p>This section is mainly based on a 1995 interview with David Rumelhart <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 12</a>)</span>.</p>
<p>In 1979-06, Rumelhart attended a conference organized by James A. Anderson, who knew many of those who were still doing neural network research even through this AI winter. This got him interested in neural networks, especially those done by Geoffrey Hinton and James L. McClelland. He couldn’t get the idea out of his head, and during a train ride, decided to just do a “five-year plan” to figure out what the fuss is about, and maybe compile a comprehensive literature review. The world would know this book as <em>Parallel Distributed Processing</em> (1986).</p>
<blockquote class="blockquote">
<p>“I think it’s important enough that I should spend at least five years figuring out what’s going on.” … When I came back to San Diego from this trip, I went to see McClelland, and I said to Jay, “Look, Geoffrey’s coming back in December. Let’s spend two quarters going over all the stuff on these neural network things… We’ll have meetings every day. I’m on sabbatical. I can do this.” Jay was on a Young Investigator Award, and Geoffrey was coming as a kind of a postdoctoral continuation… “Six months, that’s plenty of time for doing this”, we thought.</p>
</blockquote>
<p>So they set about reading all the literature they could find about neural networks. Rumelhart saw that the XOR problem was the key. Solve that with a general method, and neural networks would be reborn.</p>
<blockquote class="blockquote">
<p>I’d read <em>Perceptrons</em>. And I thought, ’Well , somehow we have to figure out how to teach, how to train, a network that has more than one layer. Why can’t we do this?” … I knew about, as I say, the work of Rosenblatt and about the way he tried to do it. He had this idea of sending error signals back across layers, but he didn’t have a very principled way of doing it.</p>
</blockquote>
<p>Rumelhart knew that Bernard Widrow’s ADALINE essentially performs a linear regression <span class="math inline">\(Wx + b\)</span>, trained by the ADALINE learning rule, also called the “<a href="https://en.wikipedia.org/wiki/Delta_rule">delta rule</a>”:</p>
<p><span class="math display">\[
\Delta W = -\eta(Wxx^T + (b-y)x^T), \quad \Delta b = -\eta(b + Wx - y)
\]</span></p>
<p>So he thought, how do I extend this to a “generalized delta rule”?</p>
<blockquote class="blockquote">
<p>So what we thought was we’d pretend they were linear, and we would compute the derivatives as if they were linear, and then we could train them. We did this on a one-layer system. That was what we called delta learning… “Well, we’ll just pretend like it’s linear, and figure out how to train it as if it were linear, and then we’ll put in these sigmoids. In that way, we can make a system that would work.”</p>
</blockquote>
<p>So he thought, well, if the activation functions are linear functions, then we just get a deep linear network, so we can still do the delta rule. If the activation functions are nonlinear, but differentiable, then we can approximate these by linear activations… and that should be a working, generalized delta rule!</p>
<p>Thus, by a long and almost accidental route, Rumelhart arrived at the gradient descent algorithm without thinking about gradients. In hindsight, the ADALINE learning rule is really a special case of gradient descent on the loss function <span class="math inline">\(L = \frac 12 \|Wx + b - y\|^2\)</span>, but people back then didn’t think of it that way. They just thought of it as a not particularly efficient way to solve the linear equation <span class="math inline">\(y \approx Wx + b\)</span>. Of course, once he had discovered the generalized delta rule, he recognized it is really just gradient descent on L2 loss. Once you have figured out the result, you often become embarrassed by the pages upon pages of wasteful calculations you used to get there. A very relatable moment!</p>
<p>Backpropagation was rediscovered in 1982. The world would soon know it, and this time, it would not forget.</p>
<p>It was not all smooth-sailing though. Backpropagation was terribly slow, taking over 1000 steps to converge, and Hinton believed it would get stuck in local minima. Indeed, these two objections, that backpropagation is slow, and gets stuck in local minima, are perennial objections, as we would see in the <a href="https://yuxi-liu-wired.github.io/TODO">history of the second neural network winter</a>.</p>
<p>Somewhat disappointed by the two objections, Rumelhart tried other methods, such as <a href="https://en.wikipedia.org/wiki/Competitive_learning">competitive learning</a>. Hinton left the book project to focus on his beloved Boltzmann machines, and would not return until 1985.</p>
<p>In fact, Rumelhart avoided backprop whenever he could, as well! When he was trying to make a neural network to convert verbs (represented as a list of <a href="https://en.wikipedia.org/wiki/Phoneme">phonemes</a>) to their past tense forms, he thought that he had to resort to an MLP with backprop, but then realized that, no, you can completely avoid backprop if you <a href="https://en.wikipedia.org/wiki/Feature_engineering">hand-design some features</a>, then just train a single layer over those features. An inauspicious omen of the second neural network winter, during which computer vision was stuck with the same paradigm of shallow learning (<a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVM</a>, <a href="https://en.wikipedia.org/wiki/Random_forest">random forests</a>, etc) over cleverly hand-designed features (<a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a>, <a href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients">HoG</a>, etc).</p>
<blockquote class="blockquote">
<p>But then McClelland and I figured out another trick for learning past tenses, the so-called “<a href="https://en.wikipedia.org/wiki/Wickelphone">Wickelfeature</a>” representation. We thought, “Oh well, we don’t need multiple layers.” So I put that aside and went off, and we realized that if we made fancier input representations, we wouldn’t need this intermediate layer.</p>
</blockquote>
<p>The first phoneme of the Bitter Lesson has been spoken.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<em>Death and the Compass</em> (Borges, 1942).</p></div></div><p>Years later, they did train an MLP by backprop for this task, and its publication in 1986 ignited the past tense debate <span class="citation" data-cites="pinkerFutureTense2002">(<a href="#ref-pinkerFutureTense2002" role="doc-biblioref">Pinker and Ullman 2002</a>)</span>. But at the moment, they didn’t really want to use backprop. Indeed, in 1983, backprop was looking so uninteresting that at a lecture, Rumelhart elected to talk about competitive learning instead.</p>
<blockquote class="blockquote">
<p>[In 1983, Hinton and Sejnowski] held a meeting in Pittsburgh… I remember electing at the last minute to talk about the competitive learning work rather than the backpropagation work. I had my slides all made out to do both of these talks, and I gave that one.</p>
</blockquote>
<p>Around 1984, Rumelhart started getting a vibe that they couldn’t just keep doing this, that they really needed MLPs with learned hidden layers, so he tried it again. Things really picked up at this point. Hinton had realized that Boltzmann machines were even more painfully slow than backprop, so he went back. They wrote the famous <em>Nature</em> paper in 1985, and this time, the world listened. <span class="citation" data-cites="rumelhartLearningRepresentationsBackpropagating1986">(<a href="#ref-rumelhartLearningRepresentationsBackpropagating1986" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span></p>
<blockquote class="blockquote">
<p>We had pretty much sorted this out and were working on how to do backpropagation in time. Geoff and I decided we really should write this up, so we started writing our paper and did a whole bunch of experiments in order to have something to say in this paper. The paper was really written in the spring of ’85. I think the <em>Nature</em> paper was in the fall of ’85. By then I was fairly committed to learning how it might work. I guess I’ve now spent about another ten years sorting all of this out the best I could.</p>
</blockquote>
<p>In the interview, he was rather unbothered by the priority dispute:</p>
<blockquote class="blockquote">
<p>I had no idea that Paul Werbos had done work on it. … There are other examples of work in the control literature in the ’60s [the <a href="https://en.wikipedia.org/wiki/Adjoint_state_method">adjoint method</a>]. … it’s just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. <a href="https://en.wikipedia.org/wiki/Shun'ichi_Amari">[Shun’ichi] Amari</a>, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn’t do anything with it. I think that was in the late ’60s. I don’t feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn’t know there were any.</p>
</blockquote>
</section>
<section id="terence-sejnowski" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="terence-sejnowski">Terence Sejnowski</h2>
<p>In 1983, Rumelhart showed backpropagation to Sejnowski, who immediately tried it and found that it was much faster than the Boltzmann machine learning rule. What a refreshing change from all those others who stubbornly refused to try it.</p>
<blockquote class="blockquote">
<p>… I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 14</a>)</span></p>
</blockquote>
<p>This was vitally important later, when Sejnowski used backpropagation to train <a href="https://en.wikipedia.org/wiki/NETtalk_(artificial_neural_network)">NETtalk</a>, a <em>huge</em> network with 18,629 parameters.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> The model was a popular hit and appeared on prime-time television. <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 112–15</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well. It really does seem like scaling skepticism is a researcher universal, if not a human universal.</p>
<blockquote class="blockquote">
<p>There were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 113</a>)</span></p>
</blockquote>
</div></div><p>Sejnowski later commented that backpropagation is inelegant:</p>
<blockquote class="blockquote">
<p>This is a highly efficient way to compute error gradients. Although it has neither the elegance nor the deep roots in physics that the Boltzmann machine learning algorithm has, backprop is more efficient, and it has made possible much more rapid progress.</p>
<p><span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 112</a>)</span></p>
</blockquote>
</section>
<section id="geoffrey-hinton" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="geoffrey-hinton">Geoffrey Hinton</h2>
<p>The interview with Geoffrey Hinton is hilarious, mostly about how he <em>spent several years refusing to use backpropagation</em>. This section is mostly made of quotations from <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 16</a>)</span>.</p>
<p>After learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 <span class="citation" data-cites="ackleyLearningAlgorithmBoltzmann1985">(<a href="#ref-ackleyLearningAlgorithmBoltzmann1985" role="doc-biblioref">Ackley, Hinton, and Sejnowski 1985</a>)</span>.</p>
<blockquote class="blockquote">
<p>I remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of ’83 … I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn’t have a learning algorithm.</p>
<p>Then we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we’d solved the problem. That must be how the brain works.</p>
<p>I guess if it hadn’t been for computer simulations, I’d still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.</p>
<p>I still think that’s the nicest piece of theory I’ll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.</p>
</blockquote>
<p>And now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,</p>
<blockquote class="blockquote">
<p>I first of all explained to him why it wouldn’t work, based on an argument in Rosenblatt’s book, which showed that essentially it was an algorithm that couldn’t break symmetry. … the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule …</p>
<p>The next argument I gave him was that it would get stuck in local minima. There was no guarantee you’d find a global optimum. Since you’re bound to get stuck in local minima, it wasn’t really worth investigating.</p>
<p>Then I tried to use it to get a very obscure effect. I couldn’t get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I’d realized that if you’ve got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. <strong>It didn’t give fast relearning, so I made one of these crazy inferences that people make – which was, that backpropagation is not very interesting</strong>.</p>
</blockquote>
<p>After one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines <em>also</em> got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.</p>
<blockquote class="blockquote">
<p>After initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as “going sour.” We couldn’t initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn’t believe that these networks would degrade as you acquired more knowledge.</p>
<p>It took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.</p>
<p><strong>After investing over a year</strong> in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting [backpropagation].</p>
</blockquote>
<p>Then he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):</p>
<blockquote class="blockquote">
<p>They’d all been thoroughly indoctrinated by then into Boltzmann machines. … They all said, “You know, why would you want to program that?” We had all the arguments: It’s assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It’s ridiculous.” So they just refused to work on it, not even to write a program, so I had to do it myself.</p>
<p>I went off and I spent a weekend. I wrote a LISP program to do it.</p>
</blockquote>
<p>Hinton almost had one last chance at giving up on backpropagation.</p>
<blockquote class="blockquote">
<p>I almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.</p>
<p>In a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they’ve solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn’t have the right structure of weights. … I thought, “Oh well, it turns out backpropagation’s not that good after all.” Then I looked at the error, and the error was zero. I was amazed.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Hinton 8-3-8 task.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The 8-3-8 autoencoder. The input is a one-hot encoding of one integer from <span class="math inline">\((0:7)\)</span>, and the output should be a copy of the input. The middle layer has 3 units, which is sufficient for binary encoding. Hinton found the Boltzmann machine learned exactly this, and expected backpropagation to learn the same thing, but was surprised when it did not. In the picture, the input is a one-hot vector of the integer <code>3</code>, and the hidden layer encodes it as a binary <code>011</code>, which is then decoded to a one-hot vector of <code>3</code> again.</figcaption>
</figure>
</div>
<p>And so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.</p>
<p>Despite this, Hinton remained unhappy with backprop and kept working on Boltzmann machines, as well as its relatives like the Helmholtz machine and the Deep Belief Networks. This is him talking in 1995, already 10 years after he was forced to accept the superiority of backprop:</p>
<blockquote class="blockquote">
<p>That was at the stage when we were just completing the PDP books, so we’d already agreed on what was going to be in the books. The final chapters were being edited. We decided we’d just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn’t nearly as satisfying as Boltzmann machines. … it didn’t have the nice probabilistic interpretation.</p>
</blockquote>
</section>
<section id="paul-werbos" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="paul-werbos">Paul Werbos</h2>
<p>After reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of “originator of the backpropagation algorithm”. He independently developed the algorithm around 1971, but was frustrated at every turn when he tried to publish it, not managing until 1982. After that, it was quickly picked up by connectionists. In this sense, he has both priority and paternity.</p>
<p>In 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, chap. 15</a>)</span>. I will let him speak, only interjecting with brief comments.</p>
<p>Before entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn’t work.</p>
<blockquote class="blockquote">
<p>It was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, “Now I’m going to figure out something with the same flavor that does work.”</p>
<p>[Understanding human learning] will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.</p>
<p>Minsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.</p>
</blockquote>
<p>Sometime before 1968, he was inspired to do backpropagation from reading Freud.</p>
<blockquote class="blockquote">
<p>I talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in ’68 <span class="citation" data-cites="werbosElementsIntelligence1968">(<a href="#ref-werbosElementsIntelligence1968" role="doc-biblioref">P. Werbos 1968</a>)</span>. I talked a lot about what was wrong with the existing [two state] McCulloch–Pitts neuron model, and how it was only “1” and “0.” I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When</p>
</blockquote>
<p>For his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn’t want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.</p>
<blockquote class="blockquote">
<p>Initially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn’t produce a really useful product at the end of <span class="math inline">\(x\)</span> number of months.</p>
<p>So I went back to the committee, and I said, “Gee, I can’t do that, <strong>but I have this little method for adapting multilayer perceptrons. It’s really pretty trivial.</strong> It’s just a by-product of this model of intelligence I developed. And I’d like to do it for my paper for this computer course.”</p>
<p>[Larry] Ho’s position was, “I understand you had this idea, and we were kind of open-minded. But look, at this point, you’ve worked in this course for three months, admittedly on something else. I’m sorry, you’re just going to have to take an incomplete in the course.”</p>
<p>And I said, “You mean I can’t do it?”</p>
<p>“No, no, you’ll have to take an incomplete because, basically, the first thing didn’t work. We’re very skeptical this new thing is going to work.”</p>
<p>“But look, the mathematics is straightforward.”</p>
<p>“Yeah, yeah, but you know, <strong>we’re not convinced it’s so straightforward</strong>. You’ve got to prove some theorems first.”</p>
<p>So they wouldn’t let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by <a href="https://en.wikipedia.org/wiki/Arthur_E._Bryson">Bryson</a> and Ho. They don’t realize it was the same <a href="https://en.wikipedia.org/wiki/Yu-Chi_Ho">Larry Ho</a>, who was on my committee and who said this wasn’t going to work.</p>
</blockquote>
<p>I am not sure if this is sarcastic or not. It reminds me of the “summer vision project” <span class="citation" data-cites="papertSummerVisionProject1966">(<a href="#ref-papertSummerVisionProject1966" role="doc-biblioref">Papert 1966</a>)</span> that expected some undergraduate students to construct “a significant part of a visual system” in a single summer.</p>
<blockquote class="blockquote">
<p>By the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I’d better work on that one later and finish my Ph.D.&nbsp;thesis on something small – something I can finish by the end of a few years, like a complete mathematical model of human intelligence.</p>
</blockquote>
<p>The oral was amusing, and touched on the still-hot issue of <a href="https://en.wikipedia.org/wiki/Recent_human_evolution">recent human evolution</a>.</p>
<blockquote class="blockquote">
<p>… I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations’ time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, <a href="https://en.wikipedia.org/wiki/Karl_Deutsch">Karl Deutsch</a>, raised his hand. … His book, <em>The Nerves of Government</em>, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.</p>
<p>He raised his hand and he said, “Wait a minute, you can’t get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That’s crazy.”</p>
<p>Next to him was a mathematical biologist by the name of <a href="https://en.wikipedia.org/wiki/William_H._Bossert">Bossert</a>, who was one of the world’s authorities on population biology. He raised his hand and said, “What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments.”</p>
<p>And Deutsch said, “What do you mean, it’s common knowledge? All of our political theories are based on the assumption this cannot happen.” And Bossert said, “Well, it happens. Here’s the data.”</p>
<p>… I passed the orals having said about two sentences and not having discussed models of intelligence.</p>
</blockquote>
<p>It turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.</p>
<blockquote class="blockquote">
<p>But the backpropagation was not used to adapt a supervised learning system; it was to translate Freud’s ideas into mathematics, to implement a flow of what Freud called “psychic energy” through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in ’71 or ’72. … The thesis committee said, “We were skeptical before, but this is just unacceptable … You have to find a patron. You must find a patron anyway to get a Ph.D.&nbsp;That’s the way Ph.D.s work.</p>
</blockquote>
<p>The committee gave him three acceptable patrons. He first went to <a href="https://en.wikipedia.org/wiki/Stephen_Grossberg">Stephen Grossberg</a>.</p>
<blockquote class="blockquote">
<p>… he said, ’Well, you’re going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I’m sure you can pull through in the end, but you’re going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you’ve done, it’s already been done before. Or else it’s wrong. I’m not sure which of the two, but I know it’s one of the two.”</p>
</blockquote>
<p>Thanks, Grossberg, for using the law of excluded middle to crush Werbos’ dream.</p>
<p>He then went to <a href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a>, who gave us some new clues about why backpropagation took so long to discover: “everybody knows a neuron is a 1-0 spike generator”!</p>
<blockquote class="blockquote">
<p>“I’ve got a way now to adapt multilayer perceptrons, and the key is that they’re not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch–Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it.”</p>
<p>Minsky basically said, “Look, <strong>everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists</strong>. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It’s totally crazy. I can’t get involved in anything like this.”</p>
<p>He was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.</p>
</blockquote>
<p>Although I believe Minsky would have disapproved of the idea of backpropagation even if he had thought that neurons are not strictly 1-0 spike generators. In the epilogue to <span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988</a>)</span>, he claimed that gradient descent does not scale, and <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/#differentiable-activation-is-just-a-hack">using differentiable activation functions is just a hack intended to make backpropagation work</a>, a pointless hack, as backpropagation would not scale.</p>
<blockquote class="blockquote">
<p>However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold. … We have the impression that many people in the connectionist community do not understand that this is merely a particular way to compute a gradient and have assumed instead that back-propagation is a new learning scheme that somehow gets around the basic limitations of hill-climbing.</p>
</blockquote>
<p>Further, in a 1991 interview, Minsky made the same kind of statement:</p>
<blockquote class="blockquote">
<p>I don’t know what they would have done with the money. The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea. There was someone . . . [trying to remember].</p>
<p>Question: Paul Werbos?</p>
<p>Answer: That’s it! [excited]. But, you see, it’s not a good discovery. It’s alright, but it takes typically 100,000 repetitions. It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It’s certainly trivial. The idea is how you do gradient descent. I didn’t consider it practical.</p>
<p>Question: Because of the computational costs?</p>
<p>Answer: Yes, but also, with artificial intelligence, we had the experience that when you make a process like that you usually get stuck at a local minimum. We still don’t have any theory of what range of problems they work well for.” (Minsky, interview) <span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 249</a>)</span></p>
</blockquote>
<p>Out of curiosity, I looked up the “Rosenblith” book <span class="citation" data-cites="rosenblithSensoryCommunicationContributions2012">(<a href="#ref-rosenblithSensoryCommunicationContributions2012" role="doc-biblioref">Rosenblith 2012</a>)</span> that Werbos mentioned, and indeed there were a few tracings that show continuously varying neural activation.</p>
<div id="fig-rosenblith" class="quarto-layout-panel page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-rosenblith-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rosenblith" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-however" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-however-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figure/no_bg_beidler_2.png" class="img-fluid figure-img" data-ref-parent="fig-rosenblith">
</div>
<figcaption class="quarto-float-caption-margin quarto-subfloat-caption quarto-subfloat-fig" id="fig-however-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Page 146 of the book. <em>Mechanisms of gustatory and olfactory receptor stimulation</em>, Figure 2.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-rosenblith" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-whatever" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-whatever-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figure/no_bg_goldsmith_4.png" class="img-fluid figure-img" data-ref-parent="fig-rosenblith">
</div>
<figcaption class="quarto-float-caption-margin quarto-subfloat-caption quarto-subfloat-fig" id="fig-whatever-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Page 366 of the book. <em>The physiological basis of wavelength discrimination in the eye of the honeybee</em>, Figure 4.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-rosenblith-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Rosenblith images.
</figcaption>
</figure>
</div>
<p>Then Minsky dunked on reinforcement learning as well, because he had an unpublished “jitters machine” that failed to optimize its reward. Presumably the name “jitters machine” refers to how it would jitter in place, not able to move towards the goal.</p>
<blockquote class="blockquote">
<p>Minsky also said, “You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I’ll hand you the tech report, which we have deliberately never published.”</p>
<p>It was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn’t work. I later looked at that paper … He had a system that was highly multivariate with a single reinforcement signal. The system can’t learn efficiently with that. At any rate, he was totally turned off.</p>
</blockquote>
<p>The brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was <em>so</em> unpublished that I found only two other references in the entire literature <span class="citation" data-cites="werbosApplicationsAdvancesNonlinear1982 werbosBuildingUnderstandingAdaptive1987">(<a href="#ref-werbosApplicationsAdvancesNonlinear1982" role="doc-biblioref">P. J. Werbos 1982</a>, <a href="#ref-werbosBuildingUnderstandingAdaptive1987" role="doc-biblioref">1987</a>)</span>, both by Werbos. According to him,</p>
<blockquote class="blockquote">
<p>There are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch–Pitts model of the neuron, which was both discontinuous and non-differentiable. It was felt, in the 1950’s, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete “spikes.” More recent work in neurology has shown that higher brain cells output “bursts” or “volleys” of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a “jitters” machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters <span class="math inline">\((N)\)</span>, which may be huge.</p>
<p><span class="citation" data-cites="werbosBuildingUnderstandingAdaptive1987">(<a href="#ref-werbosBuildingUnderstandingAdaptive1987" role="doc-biblioref">P. J. Werbos 1987</a>)</span></p>
</blockquote>
<p>This makes things clear enough. The jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would estimate <span class="math inline">\(\partial_{\theta_i} R(\theta)\)</span> using finite difference, but since it used finite difference, it could estimate the partial derivative for one of the parameters <span class="math inline">\(\theta_i\)</span> – only <em>one</em>! No wonder it never managed to learn.</p>
<p>It is almost comical how much they failed to just use gradient descent. It sometimes feels as if they did everything to <em>avoid</em> just taking the gradient. In the case of Minsky, he made it very clear, in the epilogue of <span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988</a>)</span>, that he did not believe in gradient descent, period. But what explains the gradient-phobia of the others…?</p>
<p>Anyway, back to the interview. Werbos went to <a href="https://en.wikipedia.org/wiki/Jerome_Lettvin">Jerome Lettvin</a>, the neuroscientist famous for <em>What the Frog’s Eye Tells the Frog’s Brain</em>. Turns out he was a proto-<a href="https://en.wikipedia.org/wiki/Eliminativism">eliminativist</a>. While I am an eliminativist too, Werbos was a Freudian, which must have collided badly with eliminativism.</p>
<blockquote class="blockquote">
<p>“Oh yeah, well, you’re saying that there’s motive and purpose in the human brain.” He said, “That ‘s not a good way to look at brains. I’ve been telling people, ’You cannot take an anthropomorphic view of the human brain.’ In fact, people have screwed up the frog because they’re taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific.”</p>
</blockquote>
<p>Without patrons, he faced the committee again.</p>
<blockquote class="blockquote">
<p>I tried to simplify it. I said, “Look, I’ll pull out the backprop part and the multilayer perceptron part.” I wrote a paper that was just that - that was, I felt, childishly obvious. I didn’t even use a sigmoid [non-linearity]. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, “Look, this will work, <strong>but this is too trivial</strong> and simple to be worthy of a Harvard Ph.D.&nbsp;thesis.”</p>
</blockquote>
<p>Oh, now it’s too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.</p>
<blockquote class="blockquote">
<p>… they had discontinued support because they were not interested, so I had no money. … Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D.&nbsp;candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn’t ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.</p>
<p>Finally, they said, “Look, you know, we’re not going to allow this.” There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, “We can’t even allow you to stay as a student unless you do something. You’ve got to come up with a thesis, and it can’t be in this area.”</p>
</blockquote>
<p>Karl Deutsch, who believed in Werbos, sponsored his PhD thesis on a “respectable” problem: fitting an <a href="https://en.wikipedia.org/wiki/Autoregressive_moving-average_model">ARMA model</a> to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. <a href="https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method">Box–Jenkins method</a> ran too slowly, so Werbos programmed in the backpropagation. It worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.</p>
<blockquote class="blockquote">
<p>Deutsch said, “You ’re saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who’ve tried to implement this model on real-world data I’ve collected, and they’ve never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?”</p>
<p>The first application of backpropagation in the world in a generalized sense was a command that was put into the <a href="https://en.wikipedia.org/wiki/TSP_(econometrics_software)">TSP</a> at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT ’s report to the DOD [the Department of Defense] and part of the DOD’s report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.</p>
<p>… Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D.&nbsp;thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. … one of the people there said, ’We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass.”</p>
</blockquote>
<p>But the saga is not over. After the PhD, he tried promoting his work. For example, he tried interesting Laveen Kanal in it, who promptly ignored it:</p>
<blockquote class="blockquote">
<p>a young man came by in the spring of 1975 to introduce himself as a new assistant professor in the Government and Politics department at the University. He wanted to talk about his Ph.D.&nbsp;dissertation which he had recently finished at Harvard. He said quite enthusiastically that it related to pattern recognition, learning machines, and intelligent systems. With the best of intentions I told him to lend me a copy and he lent me a report titled “Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences”. Soon the report got buried in the usual flood of paper that unfortunately still remains very much part of my life. I have often wondered in recent years if the title had been different, e.g., if it had mentioned something about a procedure for training multilayer neural networks, or if my desk had been less cluttered, would I have paid more attention to this report or was I so tuned away from artificial neural networks that it would have made no difference? … I was not alone in neglecting his work.</p>
<p><span class="citation" data-cites="kanalPatternCategoriesAlternate1993">(<a href="#ref-kanalPatternCategoriesAlternate1993" role="doc-biblioref">Kanal 1993</a>)</span></p>
</blockquote>
<p>Then he was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 – and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.</p>
<blockquote class="blockquote">
<p>I found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. … I wound up sending a couple of graduate students to create a really good database of Latin America. I said, “You want variance, high variance. Something hard to predict.” I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don’t know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief’s model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn’t improve, though; it was hopeless.</p>
<p>DARPA wasn’t happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn’t like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, “No, no, no, we can’t publish this. This is too hot.”</p>
<p>It was published in <span class="citation" data-cites="werbosEmpiricalTestNew1978">(<a href="#ref-werbosEmpiricalTestNew1978" role="doc-biblioref">P. J. Werbos and Titus 1978</a>)</span> anyway because they couldn’t block the journals, but it didn’t include the appendices. So that paper in 1978 said, “We’ve got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI.” … <strong>the appendix on how to do it was not there because of page limits</strong> … At that point, DARPA was no longer happy.</p>
</blockquote>
<p>So he went to the Department of Energy, used backpropagation to create another model, and was silenced once again, unable to publish that report until 1988 <span class="citation" data-cites="werbosGeneralizationBackpropagationApplication1988">(<a href="#ref-werbosGeneralizationBackpropagationApplication1988" role="doc-biblioref">P. J. Werbos 1988</a>)</span>.</p>
<blockquote class="blockquote">
<p>They had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn’t look like I was trying to rip them apart but was anyway. That’s exactly what they wanted to hire me for, and I didn’t really know that was the motive. These particular people didn’t like modeling very much.</p>
<p>So at some point, they wanted sensitivity analysis. And I said, “You know, I know a little bit about calculating derivatives.” … I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn’t want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don’t have First Amendment rights. That’s one of the terrible things somebody’s got to fix in this country. The reality of the First Amendment has deteriorated. Nobody’s breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around ’86 and ’87. I sent it to the journal Neural Nets – that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in ’88, which makes me very sad because now I gotta worry about, ’Well, gee, didn’t Pineda do this in ’88?</p>
</blockquote>
<p>As a side note, one might feel that Werbos’ “turning Freud into mathematics” seems rather strange. This feeling is completely justified. I found a recent paper by him <span class="citation" data-cites="werbosIntelligenceBrainTheory2009">(<a href="#ref-werbosIntelligenceBrainTheory2009" role="doc-biblioref">P. J. Werbos 2009</a>)</span> with this crackpot illustration:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/werbos_2009.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The text at the end of the arrow says “quantum and collective intelligence (Jung, Dao, Atman…)?”.</figcaption>
</figure>
</div>
<p>I did find this paper where he gave some details about which part of Freud he meant exactly:</p>
<blockquote class="blockquote">
<p>The model in Figure 1 is actually just a mathematical version of Freud’s model of psychodynamics, where the derivative of <span class="math inline">\(R_i\)</span> represents what Freud called the “cathexis” (or affect) or emotional charge or emotional energy attached to the object which <span class="math inline">\(R_i\)</span> represents. In other words, I came up with backpropagation by not just laughing at Freud’s “nonscientific” model, but by translating it into mathematics and showing that it works.</p>
<p>More concretely, Freud said that “if A causes B, a forward association or axon develops from A to B; and then, if there is an emotional charge on B, that energy flows backwards, to put a charge in A, proportional to the charge on B and to the strength of the forwards association from A to B.” That’s exactly what backpropagation does. Chronologically, I translated Freud into a way to calculate derivatives across a network of simple neurons (which Harvard simply did not believe), and then proved the more general chain rule for ordered derivatives to prove it and make it more powerful (and to graduate).</p>
<p>Freud’s term “psychic energy” for this flow really captures the subjective feeling of this subjective reality, which Freud documents many, many times over in his works. (Though of course, it is not conserved like the energy operators of physics. It is a computational flow, however implemented.) But in my view, any really strong collective intelligence would have to be held together by the same kind of thing, propagated over a more complicated network topology, but still the same basic thing. And indeed, almost every major deep culture on earth has a term for the same kind of “psychic energy” at another level – like “qi” or “prana” or “charisma.” What’s more, several different types of derivatives (like derivatives of <span class="math inline">\(J\)</span> versus derivatives of error) need to be propagated, giving rise to different kinds of mental energy. Sensitivity to these flows, to the fact that they are not conserved, and to the mathematics of the factors which govern their flow, is of great importance, in my view and my experience.</p>
<p><span class="citation" data-cites="werbosNeuralNetworksPath2011">(<a href="#ref-werbosNeuralNetworksPath2011" role="doc-biblioref">Paul Werbos 2011</a>)</span></p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Werbos_1972_thesis_proposal.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The backpropagation diagram in Werbos’ 1972 thesis proposal, which was a mathematical translation of Freud’s concept of “psychic energy”. <span class="citation" data-cites="werbosNeuralNetworksPath2011">(<a href="#ref-werbosNeuralNetworksPath2011" role="doc-biblioref">Paul Werbos 2011</a>)</span></figcaption>
</figure>
</div>


<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ackleyLearningAlgorithmBoltzmann1985" class="csl-entry" role="listitem">
Ackley, David H., Geoffrey E. Hinton, and Terrence J. Sejnowski. 1985. <span>“A Learning Algorithm for <span>Boltzmann</span> Machines.”</span> <em>Cognitive Science</em> 9 (1): 147–69. <a href="https://www.sciencedirect.com/science/article/pii/S0364021385800124">https://www.sciencedirect.com/science/article/pii/S0364021385800124</a>.
</div>
<div id="ref-bernsteinMarvinMinskyVision1981" class="csl-entry" role="listitem">
Bernstein, Jeremy. 1981. <span>“Marvin <span>Minsky</span>’s <span>Vision</span> of the <span>Future</span>.”</span> <em>The New Yorker</em>, December. <a href="https://www.newyorker.com/magazine/1981/12/14/a-i">https://www.newyorker.com/magazine/1981/12/14/a-i</a>.
</div>
<div id="ref-childManuscriptsLeibnizHis1917" class="csl-entry" role="listitem">
Child, J. M. 1917. <span>“The <span>Manuscripts</span> of <span>Leibniz</span> on <span>His Discovery</span> of the <span>Differential Calculus</span>. <span>Part II</span> (<span>Continued</span>).”</span> <em>The Monist</em> 27 (3): 411–54. <a href="https://doi.org/10.5840/monist191727324">https://doi.org/10.5840/monist191727324</a>.
</div>
<div id="ref-griewankWhoInventedReverse2012" class="csl-entry" role="listitem">
Griewank, Andreas. 2012. <span>“Who Invented the Reverse Mode of Differentiation.”</span> <em>Documenta Mathematica, Extra Volume ISMP</em> 389400. <a href="https://content.ems.press/assets/public/full-texts/books/251/chapters/online-pdf/978-3-98547-540-7-chapter-4949.pdf">https://content.ems.press/assets/public/full-texts/books/251/chapters/online-pdf/978-3-98547-540-7-chapter-4949.pdf</a>.
</div>
<div id="ref-kanalPatternCategoriesAlternate1993" class="csl-entry" role="listitem">
Kanal, Laveen N. 1993. <span>“On Pattern, Categories, and Alternate Realities.”</span> <em>Pattern Recognition Letters</em> 14 (3): 241–55. <a href="http://lnk.com/prl14.pdf">http://lnk.com/prl14.pdf</a>.
</div>
<div id="ref-lecunGeneralizationNetworkDesign1989" class="csl-entry" role="listitem">
LeCun, Yann. 1989. <span>“Generalization and Network Design Strategies.”</span> <em>Connectionism in Perspective</em> 19 (143-155): 18. <a href="https://www.academia.edu/download/30766382/lecun.pdf">https://www.academia.edu/download/30766382/lecun.pdf</a>.
</div>
<div id="ref-linnainmaaTaylorExpansionAccumulated1976" class="csl-entry" role="listitem">
Linnainmaa, Seppo. 1976. <span>“Taylor Expansion of the Accumulated Rounding Error.”</span> <em>BIT</em> 16 (2): 146–60. <a href="https://doi.org/10.1007/BF01931367">https://doi.org/10.1007/BF01931367</a>.
</div>
<div id="ref-mccullochLogicalCalculusIdeas1943" class="csl-entry" role="listitem">
McCulloch, Warren S., and Walter Pitts. 1943. <span>“A Logical Calculus of the Ideas Immanent in Nervous Activity.”</span> <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115–33. <a href="https://doi.org/10/djsbj6">https://doi.org/10/djsbj6</a>.
</div>
<div id="ref-minskyComputationFiniteInfinite1967" class="csl-entry" role="listitem">
Minsky, Marvin. 1967. <em>Computation: Finite and Infinite Machines</em>. Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-minskyPerceptronsIntroductionComputational1988" class="csl-entry" role="listitem">
Minsky, Marvin, and Seymour Papert. 1988. <em>Perceptrons: An Introduction to Computational Geometry</em>. Expanded ed. Cambridge, Mass: MIT Press.
</div>
<div id="ref-olazaranHistoricalSociologyNeural1991" class="csl-entry" role="listitem">
Olazaran, Mikel. 1991. <span>“A Historical Sociology of Neural Network Research.”</span> PhD thesis, The University of Edinburgh. <a href="https://era.ed.ac.uk/handle/1842/20075">https://era.ed.ac.uk/handle/1842/20075</a>.
</div>
<div id="ref-papertSummerVisionProject1966" class="csl-entry" role="listitem">
Papert, Seymour A. 1966. <span>“The Summer Vision Project.”</span> <a href="https://dspace.mit.edu/handle/1721.1/6125">https://dspace.mit.edu/handle/1721.1/6125</a>.
</div>
<div id="ref-pinkerFutureTense2002" class="csl-entry" role="listitem">
Pinker, Steven, and Michael T. Ullman. 2002. <span>“The Past and Future of the Past Tense.”</span> <em>Trends in Cognitive Sciences</em> 6 (11): 456–63. <a href="https://doi.org/10.1016/S1364-6613(02)01990-3">https://doi.org/10.1016/S1364-6613(02)01990-3</a>.
</div>
<div id="ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1962. <em>Principles of Neurodynamics: <span>Perceptrons</span> and the Theory of Brain Mechanisms</em>. Vol. 55. Spartan books Washington, DC. <a href="https://apps.dtic.mil/sti/citations/AD0256582">https://apps.dtic.mil/sti/citations/AD0256582</a>.
</div>
<div id="ref-rosenblithSensoryCommunicationContributions2012" class="csl-entry" role="listitem">
Rosenblith, Walter A., ed. 2012. <em>Sensory Communication: Contributions to the <span>Symposium</span> on <span>Principles</span> of <span>Sensory Communication</span>, <span>July</span> 19-<span>August</span> 1, 1959, <span>Endicott House</span>, <span>M</span>.<span>I</span>.<span>T</span></em>. Cambridge, Massachusetts: The M.I.T. Press, Massachusetts Institute of Technology.
</div>
<div id="ref-rosenfeldTalkingNetsOral2000" class="csl-entry" role="listitem">
Rosenfeld, Edward, and James A. Anderson, eds. 2000. <em>Talking <span>Nets</span>: <span>An Oral History</span> of <span>Neural Networks</span></em>. Reprint edition. The MIT Press.
</div>
<div id="ref-rumelhartLearningRepresentationsBackpropagating1986" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-sejnowskiDeepLearningRevolution2018" class="csl-entry" role="listitem">
Sejnowski, Terrence J. 2018. <em>The <span>Deep Learning Revolution</span></em>. Illustrated edition. Cambridge, Massachusetts London, England: The MIT Press.
</div>
<div id="ref-werbosElementsIntelligence1968" class="csl-entry" role="listitem">
Werbos, P. 1968. <span>“The Elements of Intelligence.”</span> <em>Cybernetica (Namur)</em> 3: 131–78.
</div>
<div id="ref-werbosNeuralNetworksPath2011" class="csl-entry" role="listitem">
Werbos, Paul. 2011. <span>“Neural Networks as a Path to Self-Awareness.”</span> In <em>The 2011 <span>International Joint Conference</span> on <span>Neural Networks</span></em>, 3264–71. IEEE. <a href="https://ieeexplore.ieee.org/abstract/document/6033654/">https://ieeexplore.ieee.org/abstract/document/6033654/</a>.
</div>
<div id="ref-werbosApplicationsAdvancesNonlinear1982" class="csl-entry" role="listitem">
Werbos, Paul J. 1982. <span>“Applications of Advances in Nonlinear Sensitivity Analysis.”</span> In <em>System <span>Modeling</span> and <span>Optimization</span></em>, edited by R. F. Drenick and F. Kozin, 38:762–70. Berlin/Heidelberg: Springer-Verlag. <a href="https://doi.org/10.1007/BFb0006203">https://doi.org/10.1007/BFb0006203</a>.
</div>
<div id="ref-werbosBuildingUnderstandingAdaptive1987" class="csl-entry" role="listitem">
———. 1987. <span>“Building and Understanding Adaptive Systems: <span>A</span> Statistical/Numerical Approach to Factory Automation and Brain Research.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em> 17 (1): 7–20. <a href="https://doi.org/10.1109/TSMC.1987.289329">https://doi.org/10.1109/TSMC.1987.289329</a>.
</div>
<div id="ref-werbosGeneralizationBackpropagationApplication1988" class="csl-entry" role="listitem">
———. 1988. <span>“Generalization of Backpropagation with Application to a Recurrent Gas Market Model.”</span> <em>Neural Networks</em> 1 (4): 339–56. <a href="https://doi.org/10.1016/0893-6080(88)90007-X">https://doi.org/10.1016/0893-6080(88)90007-X</a>.
</div>
<div id="ref-werbosIntelligenceBrainTheory2009" class="csl-entry" role="listitem">
———. 2009. <span>“Intelligence in the Brain: <span>A</span> Theory of How It Works and How to Build It.”</span> <em>Neural Networks</em>, Goal-<span>Directed Neural Systems</span>, 22 (3): 200–212. <a href="https://doi.org/10.1016/j.neunet.2009.03.012">https://doi.org/10.1016/j.neunet.2009.03.012</a>.
</div>
<div id="ref-werbosEmpiricalTestNew1978" class="csl-entry" role="listitem">
Werbos, Paul J., and Jim Titus. 1978. <span>“An Empirical Test of New Forecasting Methods Derived from a Theory of Intelligence: <span>The</span> Prediction of Conflict in <span>Latin America</span>.”</span> <em>IEEE Transactions on Systems, Man, and Cybernetics</em> 8 (9): 657–66. <a href="https://doi.org/10.1109/TSMC.1978.4310051">https://doi.org/10.1109/TSMC.1978.4310051</a>.
</div>
<div id="ref-widrowGeneralizationInformationStorage1962" class="csl-entry" role="listitem">
Widrow, Bernard. 1962. <span>“Generalization and Information Storage in Networks of Adaline Neurons.”</span> <em>Self-Organizing Systems</em>, 435–61. <a href="https://cir.nii.ac.jp/crid/1573387449478081920">https://cir.nii.ac.jp/crid/1573387449478081920</a>.
</div>
<div id="ref-widrowOralHistoryBernard1997" class="csl-entry" role="listitem">
———. 1997. <span>“Oral <span>History</span>: <span>Bernard Widrow</span>.”</span> <a href="https://ethw.org/Oral-History:Bernard_Widrow">https://ethw.org/Oral-History:Bernard_Widrow</a>.
</div>
<div id="ref-widrowCyberneticsGeneralTheory2022" class="csl-entry" role="listitem">
———. 2022. <em>Cybernetics 2.0: <span>A General Theory</span> of <span>Adaptivity</span> and <span>Homeostasis</span> in the <span>Brain</span> and in the <span>Body</span></em>. Vol. 14. Springer Nature. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=X_eVEAAAQBAJ&amp;oi=fnd&amp;pg=PR9&amp;dq=Cybernetics+2.0%0A&amp;ots=MB6pN7dGcd&amp;sig=hTBUvOlnKBRkfYUNL7Ztn1Zm4Mg">https://books.google.com/books?hl=en&amp;lr=&amp;id=X_eVEAAAQBAJ&amp;oi=fnd&amp;pg=PR9&amp;dq=Cybernetics+2.0%0A&amp;ots=MB6pN7dGcd&amp;sig=hTBUvOlnKBRkfYUNL7Ztn1Zm4Mg</a>.
</div>
<div id="ref-widrowAncientHistory2023" class="csl-entry" role="listitem">
———. 2023. <span>“Ancient <span>History</span>.”</span> In <em>Cybernetics 2.0: <span>A General Theory</span> of <span>Adaptivity</span> and <span>Homeostasis</span> in the <span>Brain</span> and in the <span>Body</span></em>, 277–307. Springer <span>Series</span> on <span>Bio-</span> and <span>Neurosystems</span>. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-98140-2_26">https://doi.org/10.1007/978-3-030-98140-2_26</a>.
</div>
<div id="ref-widrowQuantizationNoiseRoundoff2008" class="csl-entry" role="listitem">
Widrow, Bernard, and István Kollár. 2008. <em>Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control and Communications</em>. 1. publ. Cambridge: Cambridge Univ. Press.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The Backstory of Backpropagation"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-12-26"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2024-11-22"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [AI, math, physics, history]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Why backprop was resisted for 20 years: assumption of discretely spiking neurons, goal of synthesizing Boolean logic, fear of local optima, and bad luck. Werbos has the best claim for invention."</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner_cropped.png"</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="an">image-alt:</span><span class="co"> "Stylized illustration of a computational graph, with gibberish text that resemble vector calculus, thick and thin lines representing the flow of numbers in the graph. In the center of the vaguely octagonal graph is a letter D, D for derivative. High contrast, monochromatic, minimalistic, in the style of vector svg art."</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "likely"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 4</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Abstract</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>The backpropagation algorithm could have been invented by a slight generalization of the adjoint method, or the costate method, which was widely known among control theorists in the 1950s. Hybrid control theory leads almost inexorably to the backpropagation algorithm.</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>Seppo Linnainmaa's claim of priority is his 1970 master thesis -- in Finnish, and never published. An English paper in 1976 was published that may have been relevant to automatic differentiation research, but not to neural network research. He has priority but no paternity.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>Paul Werbos' claim of priority is that he developed the backpropagation algorithm in 1971, but was frustrated at every turn whenever he tried publishing it, not managing until around 1982. After that, it was taken up by many neural network researchers. He has priority and paternity.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>David Rumelhart reinvented it in 1982. As he freely admits, he didn't cite previous work because they were so obscure. The 1986 paper he coauthored <span class="co">[</span><span class="ot">@rumelhartLearningRepresentationsBackpropagating1986</span><span class="co">]</span> became popular enough that nobody would need to reinvent it again. He has no priority, but paternity. As usual in science, it is not the first inventor who gets the credit, but the last re-inventor.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>I was puzzled by how none of the first wave of neural network researchers developed backpropagation. I still am. My guesses are, from most to least likely:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Bad luck.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>They were misled by the contemporary understanding of real neurons, and the McCulloch--Pitts model. Back then everyone "knew" that real neurons are 0-1 spike generators.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>They thought of machine learning as just another way to construct boolean functions, rather than smooth functions, so they kept using the 0-1 activation function.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>They distrusted gradient descent, because it would get stuck at local optima. If only they had had cheap compute and data to experiment with, they would have discovered the "blessing of scale": local optima are almost as good as the global optimum.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>They attempted to achieve parity with digital computers, which were discrete.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>They attempted to distance themselves from cybernetics, which included optimal control theory.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">## The backpropagation algorithm</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>To set the stage, we should at least quickly derive the backpropagation algorithm. In one sentence, backpropagation is an algorithm that calculates the gradient of every parameter in an acyclic directed graph with respect to a single final parameter.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>The graph can be finite or infinite, but in all cases, its acyclic directedness allows us to assign a "logical time" to each node.<span class="ot">[^existence-of-logical-time]</span> Some nodes are independent variables: they point to other nodes, but no nodes point to *them*. Other nodes are dependent. If we know the independent variables, we can propagate their values *forward* in logical time and determine the values of every node. This is the "forward pass". Backpropagation goes backwards in logical time.</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ot">[^existence-of-logical-time]</span>:</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    An existence proof that anything we might care about must have a logical time: Anything that happens, happens in the real world. The real world has a single direction in time. Therefore, any computation that can happen, must have a logical time that is identical with physical time.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="in">    Note that logical time does not have to coincide with physical time. For example, to model goal-directed behavior, it might be better to put the physical future into the logical past.</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>We use the convention of putting derivatives on the rows. So for example, for $f: \R^2\to\R^2$, we have</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>\nabla_x f = \frac{df}{dx} = \begin{bmatrix}</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a> \frac{df_1}{dx_1} &amp; \frac{df_1}{dx_2} <span class="sc">\\</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a> \frac{df_2}{dx_1} &amp; \frac{df_2}{dx_2}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a> \end{bmatrix}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>This convention simplifies a lot of equations, and completely avoids transposing any matrix.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="fu">### Discrete logical time</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>Consider an acyclic directed graph with a finite number of nodes. Each graph node is a finite-dimensional real vector, though they may have different dimensions. We order the acyclic graph on the number line, so that each node depends only on the earlier nodes. Now denote the graph nodes as $x_0, x_1, \dots, x_T$. By our ordering, $x_1$ depends on only $x_0$, and $x_2$ depends on only $x_1, x_2$, and so on:</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>x_0 &amp;= x_0 <span class="sc">\\</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>x_1 &amp;= f_1(x_0) <span class="sc">\\</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>&amp;\cdots <span class="sc">\\</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>x_T &amp;= f_T(x_0, x_1, \dots , x_{T-1})</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>Now we perform an infinitesimal perturbation on every one of its independent variables. A forward propagation then causes an infinitesimal perturbation on every variable, independent or dependent. Denote these as $dx_0, dx_1, \dots, dx_T$. We can now compute the derivative of $dx_T$ with respect to every other variable by backpropagating the perturbation. Suppose we can see only $dx_T$, then the change in $x_T$ due to $dx_T$ is the identity. That is,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>\frac{dx_T}{dx_T} = I</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>Now suppose we can see only $dx_{T-1}$, then the change in $x_T$ due to $dx_{T-1}$ can only come from the final step in the forward propagation. Therefore</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>\frac{dx_T}{dx_{T-1}} = \nabla_{x_{T-1}} f_T(x_0, x_1, \dots , x_{T-1})</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Similarly, the change in $x_T$ due to $dx_{T-2}$ can come from either directly changing $x_T$ or from changing $x_{T-1}$ and thereby changing $x_T$. Therefore,</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>\frac{dx_T}{dx_{T-2}} =</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>    \nabla_{x_{T-1}} f_{T}(x_0, x_1, \dots , x_{T-2}) +</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>    \ub{\frac{dx_T}{dx_{T-1}}\nabla_{x_{T-2}} f_{T-1}(x_0, x_1, \dots , x_{T-2})}{the chain rule}</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>This generalizes to the rest of the steps.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>The above computation is fully general, but this makes it inefficient. In practical applications of backpropagation, the computation graph is usually very sparse, meaning that each $x_t$ only directly influence a few more nodes down the line. In standard neural networks, typically $x_{t}$ only directly influences $x_{t+1}, x_{t+2}$. Thus, sparsity is vital for backpropagation to be relevant.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>As a side note, we could in fact compute *all* derivatives, not just the first, in one single backward pass. Other than the second derivatives $\nabla^2_{x_t}x_T$, there is rarely any use for the other derivatives, such as $\nabla_{x_t}\nabla_{x_s}x_T, \nabla^3_{x_t}x_T$, etc. The second derivative was occasionally used for neural networks circa 1990, with the further simplification that they only  keep the positive diagonal entries of $\nabla^2_{x_t}x_T$ and set all other entries to zero <span class="co">[</span><span class="ot">@lecunGeneralizationNetworkDesign1989</span><span class="co">]</span>.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="fu">### Continuous logical time</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>Consider the problem of controlling a car along a highway. We discard all details, so that the car is just a single dot on a line, and the state of the car is determined by its speed and velocity. To avoid waiting forever, we require time $t\in <span class="co">[</span><span class="ot">0, T</span><span class="co">]</span>$. Write the state variable as follows:</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>x_t = (\text{location at time }t, \text{velocity at time }t)</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>It might be confusing to use $x_t$ for the state at time $t$, instead of for location, but it makes the notation consistent.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>Suppose the only thing we can influence is how much we press the pedal. Write $u_t$ to be the resulting acceleration we inflict on the car by pressing on the pedal. Further, assume that the car is slowed down by friction that is proportional to velocity. We then have:</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>\dot x_t = f(x_t, u_t)</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>where $f(x_t, u_t) = (x_{t, 1}, -\mu x_{t, 1} + u_t)$ is the dynamics equation of the system, and $\mu$ is the friction coefficient.<span class="ot">[^time-varying-dynamics]</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="ot">[^time-varying-dynamics]: </span>To allow for time-varying dynamics, simply replace $f(x_t, u_t)$ with $f(t, x_t, u_t)$. This clutters the notation without involving new ideas.</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>Now, we can draw the computation graph as before. The computation graph has a continuum of nodes, but it has a very simple structure. The graph has two kinds of nodes: the $x_t$ nodes and the $u_t$ nodes. Its independent variables are $x_0$ and all the $u_t$ nodes. Each $x_{t+dt}$ depends on only $x_{t}$ and $u_t$. This makes the two propagations particularly simple.</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>The forward propagation is obtained by integration, which we can unwrap into a form resembling the case of the discrete logical time:</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>x_0 &amp;= x_0 <span class="sc">\\</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>x_{dt} &amp;= x_0 + f(x_0, u_0) dt <span class="sc">\\</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>&amp;\cdots <span class="sc">\\</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>x_{T} &amp;= x_{T-dt} + f(x_{T-dt}, u_{T-dt}) dt <span class="sc">\\</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>The backpropagation is similarly obtained. By inspecting the computation graph, we can see that each $u_t$ only directly influences $x_{t+dt}$, giving</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>\frac{dx_T}{du_t} = \frac{dx_T}{dx_{t+dt}} \nabla_{u_t}f(x_t, u_t) dt.</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>It remains to compute $\frac{dx_T}{dx_t}$. This can be found by backpropagation too, since each $x_t$ only directly influences $x_{t+dt}$, we have</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>\frac{dx_T}{dx_t} = \frac{dx_T}{dx_{t+dt}} \lrs{I + \nabla_{x_t} f(x_t, u_t) dt}.</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>If we denote the gradient as $g_t := \frac{dx_T}{dx_t}$, then we find an equation for $g_t$:</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>g_t = \lrs{I + (g_t + \dot g_t dt) \nabla_{x_t} f(x_t, u_t) dt}\implies \dot g_t = -g_t\nabla_{x_t} f(x_t, u_t)</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>This equation bottoms out at the end time, $t=T$, for which $g_T = \frac{dx_T}{dx_T} = I$. Thus we have the <span class="co">[</span><span class="ot">costate equations</span><span class="co">](https://en.wikipedia.org/wiki/Costate_equation)</span>:</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>g_T &amp;= I <span class="sc">\\</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>\dot g_t &amp;= - g_t \nabla_{x_t} f(x_t, u_t)</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>which must, as you can see, be integrated *backwards in time* -- backpropagation again! Indeed, control theory practically compels us to find backpropagation.</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hybrid logical time</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>When the computation graph has both nodes with discrete logical times and nodes with continuous logical times, we call such a system as having hybrid logical time.<span class="ot">[^hybrid-control-theory]</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="ot">[^hybrid-control-theory]: </span>The name "hybrid" comes from "<span class="co">[</span><span class="ot">hybrid control theory</span><span class="co">](https://en.wikipedia.org/wiki/Hybrid_system)</span>", which are systems with both discrete changes, or jumps, and continuous changes, or flow. They appear whenever a digital device is used to control a continuous system, such as a computer piloting an airplane.</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>The idea can be illustrated by the fundamental problem in control theory: how to optimize control? You cannot optimize without optimizing a real number, so we write down the number as $J$, representing the "cost" of the trajectory. For example, in driving a car, we might want to optimize comfort, time-until-arrival, and fuel cost. We can then write $J$ down as something like</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>J = \ub{(x_{T, 0} - x_{goal})^2}{location should be at the goal location at the end time}</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="ss">    + </span>\ub{(x_{T, 1} - 0)^2}{speed should be zero at the end time} + \int_0^T u_t^2 dt</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>More generally, the objective to be optimized is in the form</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>J = A(x_T) + \int_0^T L(x_t, u_t)dt</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>for some real-valued functions $A, L$.</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>Of course, we can care about more than the state at the last time-step. We can care about multiple time-steps $t_0, t_1, \dots, t_n$ by writing down a cost function $J = \sum_{i=0}^n A_i(x_{t_i}) + \int_0^T L(x_T, u_T)dt$, but this merely creates complications without introducing new ideas. Even more ornate computation graphs, weaving together multiple strands of continuous and discrete logical times, can be backpropagated in the same way without involving new ideas.</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>Define the costate $\lambda_t := \nabla_{x_t} J$, then the costate backpropagates as:</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>\lambda_t = L(x_t, u_t) dt + \lambda_{t+dt}(I + \nabla_{x_t}f(x_t, u_t)dt)</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>and by simplifying, we have the costate equation:</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>\lambda_T &amp;= \nabla_{x_T} A(x_T) <span class="sc">\\</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>\dot \lambda_t &amp;= - \nabla_{x_t} L(x_t, u_t) - \lambda_t \nabla_{x_t} f(x_t, u_t)</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>which can be solved by integrating backward in time.</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>Once we have obtained all the costates, we can compute $\nabla_{u_t} J$. Since $u_t$ can only influence $J$ either directly via $L(x_t, u_t)$ or indirectly via $x_t$, we have</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>\nabla_{u_t} J = \lrs{\nabla_{u_t}f(x_t, u_t) \lambda_t + \nabla_{u_t}L(x_t, u_t)}dt</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>Note that $\nabla_{u_t} J$ is an infinitesimal in $dt$. This is qualitatively different from $\nabla_{x_t}J = g_t$, which is not an infinitesimal, but a mere matrix. Why is it so? The simplest example suffices to illustrate.</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>Consider a mass point sliding on a frictionless plane. When we perturb $x_t$, we push it to the side by $\delta x_{t, 0}$ and also change its velocity by $\delta x_{t, 1}$, and so at the end time $T$, we would have changed $x_T$ by $(\delta x_{t, 0} + (T-t)\delta x_{t, 1}, \delta x_{t, 1})$, which is the same order of infinitesimal. Now, we can control the mass point by applying a force $u_t$, which gives us the dynamics equation</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>\dot x_t = (x_{t, 1}, u_t)</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>To "perturb" $u_t$ by $\delta u_t$ does not make sense on its own, as a "spike" of $\delta u_t$ that lasts for zero amount of time does not show up in the system trajectory. One can apply a durationless jump in state, but one cannot apply a durationless force. Therefore, it only makes sense to perturb $u_t$ by $\delta u_t$ and persist the perturbation for $dt$ time. This perturbs the state at the end time by $((T-t)\delta u_t dt , \delta u_t dt)$, which means that $\nabla_{u_t}x_T$ is proportional to $dt$.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimal control theory</span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>An optimal trajectory must have $\nabla_{u_t} J = 0$, since otherwise, we could shave off a little piece of cost by giving $u_t$ a little boost in the opposite direction (infinitesimal gradient descent!). This gives us two equations that any optimal trajectory must satisfy</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) <span class="sc">\\</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>0                &amp;= \nabla_{u_t} L(x_t, u_t) + \lambda_t \nabla_{u_t} f(x_t, u_t) <span class="sc">\\</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>Now, what is the effect of perturbing $u_t$ by $du_t$? It would perturb $x_{t+dt}$ by $\nabla_{u_t} f(x_t, u_t) du_t dt$, a second-order infinitesimal. Consequently, it would perturb $x_T$ by only a second-order infinitesimal, and thus $\lambda$ too. Therefore, we have</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>\nabla_{u_t}\lambda_t = 0</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>giving us simplified equations for optimality:</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t) + \lambda_t \nabla_{x_t} f(x_t, u_t) <span class="sc">\\</span></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>0                &amp;= \nabla_{u_t} (L(x_t, u_t) + \lambda_t f(x_t, u_t)) <span class="sc">\\</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>Unfortunately, we cannot simplify the first equation similarly because $\nabla_{x_t}\lambda_t \neq 0$. Still, it seems $L(x_t, u_t) + \lambda_t f(x_t, u_t)$ should be an important quantity:</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>H(x_t, u_t, \lambda_t) := L(x_t, u_t) + \lambda_t f(x_t, u_t)</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>The letters are meaningful. $L$ is the "Lagrangian", and $H$ is the "Hamiltonian". Indeed, classical Hamiltonian mechanics is a special case of optimal<span class="ot">[^first-order-optimal]</span> control theory.</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a><span class="ot">[^first-order-optimal]: </span>To be completely precise, it is a necessary but insufficient condition for optimality. Just like how a function can have zero derivatives on the peaks, valleys and shoulders, a trajectory can have zero functional derivative, even if it is the best, the worst, and the ... shouldered? In jargon, we say that those conditions are **first order optimality conditions**, since they use only the derivative, not the second-derivative.</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>If we interpret economically the quantities, then $J$ is the cost of the entire trajectory, $\lambda_t$ is the marginal cost of the point $x_t$ in the trajectory, and $L(x_t, u_t)$ is the cost-rate at time $t$. The second equation of optimality $\nabla_{u_t} H(x_t, u_t, \lambda_t) = 0$ states that when the trajectory is optimal, the marginal cost of control is zero: there is no saving in perturbing the control in any direction.</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>Therefore, define the "optimized" Hamiltonian and the optimized control relative to it:</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>H^*(x_t, \lambda_t) &amp;:= \min_{u_t}H(x_t, u_t, \lambda_t) = \min_{u_t} \lrb{L(x_t, u_t) + \lambda_t f(x_t, u_t)} <span class="sc">\\</span></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>u^*(x_t, \lambda_t) &amp;:= \argmin_{u_t}H(x_t, u_t, \lambda_t)</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>Then, by <span class="co">[</span><span class="ot">Hotelling's lemma</span><span class="co">](https://en.wikipedia.org/wiki/Hotelling%27s_lemma)</span>, we derive the <span class="co">[</span><span class="ot">Hamiltonian equations of motion</span><span class="co">](https://en.wikipedia.org/wiki/Hamiltonian_mechanics)</span>:</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\dot \lambda_t &amp;= \nabla_{x_t} L(x_t, u_t^*) + \lambda_t \nabla_{x_t} f(x_t, u_t^*) &amp;= \nabla_{x_t} H^*(x_t, \lambda_t) <span class="sc">\\</span></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>  \dot x_t       &amp;= f(x_t, u_t^*)                                                     &amp;= \nabla_{\lambda_t} H^*(x_t, \lambda_t) <span class="sc">\\</span></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>This is often called the <span class="co">[</span><span class="ot">Pontryagin's maximum principle</span><span class="co">](https://en.wikipedia.org/wiki/Pontryagin%27s_maximum_principle)</span>, as Pontryagin's school of optimal control theory is based on this principle and its extensions. Control theory was a national security issue during the Cold War, as it was used from the <span class="co">[</span><span class="ot">space race</span><span class="co">](https://en.wikipedia.org/wiki/Space_Race)</span> to the <span class="co">[</span><span class="ot">missile race</span><span class="co">](https://en.wikipedia.org/wiki/Nuclear_arms_race)</span>.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>In classical control theory, the equation is sometimes solved in closed form, as in the case of <span class="co">[</span><span class="ot">linear quadratic control</span><span class="co">](https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator)</span>. When it cannot be solved in closed form, it can still be numerically solved using a continuous form of the Bellman equation. Similar to how the Hamiltonian equations have the alternative form of the <span class="co">[</span><span class="ot">Hamilton--Jacobi equation</span><span class="co">](https://en.wikipedia.org/wiki/Hamilton-Jacobi_equation)</span>, the Pontryagin equations have an alternative form in the <span class="co">[</span><span class="ot">Hamilton--Jacobi--Bellman equation</span><span class="co">](https://en.wikipedia.org/wiki/Hamilton-Jacobi_equation)</span>. Possibly, the name "dynamic programming" appears later in Paul Werbos' invention of backpropagation, which he named "dynamic feedback".</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>In economics, one can modify this framework slightly to allow discounting cost over time, yielding theories about "optimal investment" such as the <span class="co">[</span><span class="ot">Ramsey optimal growth theory</span><span class="co">](https://en.wikipedia.org/wiki/Ramsey%E2%80%93Cass%E2%80%93Koopmans_model)</span>.</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a><span class="fu">## Leibniz</span></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>The chain rule dates back to *Calculus Tangentium differentialis* <span class="sc">\[</span>Differential calculus of tangents<span class="sc">\]</span>, a manuscript by Leibniz dated 1676 November <span class="co">[</span><span class="ot">@childManuscriptsLeibnizHis1917</span><span class="co">]</span>. It says</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; it does not matter, whether or no the letters $x, y, z$ have any known relation, for this can be substituted afterward.</span></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>In mathematical notation, he found that $dy = dx \frac{dy}{dx}$, or in his notation, $\overline{dy} = \overline{dx} \frac{dy}{dx}$, where the overbar denotes the thing to be differentiated. You can read it as a bracket: $d(y) = d(x) \frac{dy}{dx}$.</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>He then gave the following examples<span class="ot">[^live-with-his-mistakes]</span>:</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a> &amp; \overline{d \sqrt<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{a+b z+c z^2}} \text {. Let } a+b z+c z^2=x \text {; } <span class="sc">\\</span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>\text{Then} \quad  &amp; \overline{d \sqrt<span class="co">[</span><span class="ot">2</span><span class="co">]</span>{x}}=-\frac{1}{2 \sqrt{x}} \text {, and } \frac{d x}{d z}=b+2 c z \text {; } <span class="sc">\\</span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>\text{Therefore} \quad  &amp; \overline{d \sqrt{a+b z+c z^2}}=-\frac{b+2 c z}{2 \overline{d z} \sqrt{a+b z+c z^2}} <span class="sc">\\</span></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>&amp;</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="ot">[^live-with-his-mistakes]: </span>Yes, there is a sign error. No, I'm not going to fix it. He just has to live with his mistakes.</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a><span class="fu">## McCulloch and Pitts</span></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>In the famous paper <span class="co">[</span><span class="ot">@mccullochLogicalCalculusIdeas1943</span><span class="co">]</span>, McCulloch and Pitts proposed that</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes.</span></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>The McCulloch and Pitts paper is difficult to read, and is rooted in early 20th century mathematical logic. In fact, it uses the same abstruse notations of the <span class="co">[</span><span class="ot">*Principia Mathematica*</span><span class="co">](https://en.wikipedia.org/wiki/Principia_Mathematica)</span>, which is cited in the paper.</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a><span class="al">![The infamous proof of 1+1=2 in Principia Mathematica](figure/principia_mathematica.png)</span></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a><span class="al">![The same notation is used by McCulloch and Pitts](figure/principia_mathematica_McCulloch_and_Pitts.png)</span></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>The McCulloch and Pitts paper, like the *Perceptrons* book, is something often cited but rarely read. The best introduction to the McCulloch and Pitts neural network is still <span class="co">[</span><span class="ot">@minskyComputationFiniteInfinite1967, Chapter 3</span><span class="co">]</span>, which explicitly designs, neuron by neuron, threshold by threshold, how to construct neural networks that perform various tasks, such as simulating arbitrary Turing machines, process signals, perform additions, etc.</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/minsky_1976_finite_state_machine.png)</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/minsky_1976_serial_binary_addition_network.png)</span></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a><span class="fu">## Frank Rosenblatt</span></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>Frank Rosenblatt is the originator of the term "backpropagation", or more precisely, "back-propagating error-correction procedure"  <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962, Chapter 13</span><span class="co">]</span>, although it was quite far from our current use of backpropagation. He is the originator of many concepts in neural networks, and he was even popularly famous during his own time. During 1957--1962, he did a lot of the theory and experiments with perceptron networks. Some experiments ran on digital computer simulations, and others on the Mark I Perceptron, a bespoke machine that takes up a whole room.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>A perceptron is a function of the form $\theta(w^T x + b)$, where $\theta$ is the 0-1 step function, and $w \in \R^n, b \in \R$ are its learnable parameters. A perceptron network is a network of perceptrons. Rosenblatt experimented with a large number of perceptron network architectures, but most often, he experimented with a certain 2-layered feedforward architecture, which is sometimes called "*the* perceptron machine".</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>*The* perceptron machine is a machine that computes a function of type $<span class="sc">\{</span>0, 1<span class="sc">\}</span>^n \to <span class="sc">\{</span>0, 1<span class="sc">\}</span>$. Its input layer is composed of units named "Stimulus units" or "S units". The S units do not perform any computation, but merely pass binary outputs to the hidden layer. In the Mark I perceptron, the S units were 20×20 cadmium sulfide photocells, making a 400-pixel image. The hidden layer is composed of perceptrons named "Association units" or "A units". Their outputs pass on to the output layer, composed of perceptrons named "Response units" or "R units".</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>We can describe *the* perceptron machine in one equation:</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>f(x) = \theta\lrb{b^{R} + \sum_i w^{AR}_i \theta\lrb{(w^{SA, i})^T x + b^{A}_i}}</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a>Rosenblatt proved some mathematical theorems, the most important of which is the <span class="co">[</span><span class="ot">perceptron convergence theorem</span><span class="co">](https://en.wikipedia.org/wiki/Perceptron#Learning_algorithm_for_a_single-layer_perceptron)</span>, which shows that assuming the dataset is linearly separable, then the perceptron learning algorithm converges after making a bounded number of errors<span class="ot">[^perceptron-convergence-theorem-technicalities]</span>. He also performed experiments on the Mark I Perceptron machine, IBM computers, and some other computers.</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a><span class="ot">[^perceptron-convergence-theorem-technicalities]: </span>There is another technical assumption on the dataset, but it is omitted for space. Suffice to say it is satisfied for all finite datasets.</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>His theorems and experiments were exhaustively documented in his book <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962</span><span class="co">]</span>. Its breadth is quite astonishing. It contains:</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>perceptrons with continuous activation functions (section 10.2);</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>perceptron convergence theorem for perceptrons with continuous activation functions that are also monotonic and sign-preserving (page 249);</span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>perceptron layers with random delay in transmission time (chapter 11);</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>layers with connections between units within the same layer, with possibly closed loops (chapter 17--19);</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>layers with connections from a later layer to a previous layer (chapter 20);</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>residual connections (Figure 42);</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>multimodal perceptron networks that learns to associate image and audio inputs (Figure 58);</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>program-learning perceptrons (chapter 22);</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>perceptron networks that analyze videos and audios (chapter 23).</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>From our vantage point, we can fairly say that he invented randomization, residual connections, weight initiation schemes, neural Turing machines, recurrent neural networks, vision models, time delay neural networks, multimodal neural networks...</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a><span class="al">![Figure 58. The first multimodal neural network?](figure/rosenblatt_figure_58.png)</span>{width=60%}</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a><span class="al">![Figure 42. The dashed lines denote variable weights, and the solid lines denote fixed weights. This is the residual connection.](figure/rosenblatt_figure_42.png)</span>{width=60%}</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>What is even more astonishing is that, as far as I can see, he did not make use of gradient descent learning at all, not even on a single layer like Widrow and Hoff. The perceptron learning rule converges, but only for a single-layered perceptron network on linearly separable data. Thus, when it came to two-layered perceptron networks, he randomly wired the first layer, then froze it, and only adapted the second layer. This would become the focal point of the "<span class="co">[</span><span class="ot">perceptron controversy</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/neural-network-winter)</span>".</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>In the chapter where he talked about backpropagation <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962, Chapter 13</span><span class="co">]</span>, he tried to train both layers in the two-layered perceptron network by extending the perceptron learning algorithm. The algorithm works on each individual perceptron unit, but only if the desired output is known. The desired outputs for the output units are supplied by the experimenter, but the desired outputs for the hidden units could not be known, so Rosenblatt tried some hacks. Essentially, he took the error signals at the output units, and used heuristic rules to "backpropagate the error" to synthesize error signals at the hidden units. The precise details are hacky, and no longer of any relevance.</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>One last thing about his backpropagation rule: he also discovered the layer-wise learning rate trick. Because his backpropagation algorithm was so hacky, he found that he had to stabilize it by making the first layer learn at a much lower rate than the second.</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It is found that if the probabilities of changing the S-A connections are large, and the threshold is sufficiently small, the system becomes unstable, and the rate of learning is hindered rather than helped by the variable S-A network. Under such conditions, the S-A connections are apt to change into some new configuration while the system is still trying to adjust its values to a solution which might be perfectly possible with the old configuration. Better performance is obtained if the rate of change in the S-A network is sufficiently small to permit an attempt at solving the problem before drastic changes occur.</span></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bernard Widrow and Marcian Hoff</span></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>Widrow and Hoff worked on neural networks in the early 1960s. They started with training a single perceptron with gradient descent on the squared loss, then proceeded to spend years trying to train a two-layered network *without* gradient descent. I know -- I cannot make this sound any less puzzling.</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>The Widrow--Hoff machine, which they called the ADALINE ("ADAptive Linear NEuron"), is a function of type $\R^n \to <span class="sc">\{</span>0,1<span class="sc">\}</span>$ defined by</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>f(x) = \theta(w^T x + b)</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>and here it is again, that dreaded Heaviside step-function that was the bane of every neural network before backpropagation. What was odd about this one is that ADALINE was *trained* by gradient descent with the squared loss function $\frac 12 (w^T x + b - y)^2$, which is *continuous*, not discrete:</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>w \leftarrow w - \alpha (w^T x + b - y) w, \quad b \leftarrow b - \alpha (w^T x + b - y) b</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>The first ADALINE machine was a box that learned to classify binary patterns on a $4 \times 4$ grid. It was pretty amusing, as everything was done manually. The patterns were inputted by flipping 16 switches by hand. The error $w^T x + b - y$ was read from a voltmeter, and the parameters $w, b$ were individually adjusted by turning knobs controlling rheostats inside the box. They then automated the knob-turning process using electrochemical devices called memistors, though the pattern input $x$ and the desired output $y$ were still entered by manual switches.</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="al">![All components of the ADALINE machine, color-labelled, from the algorithm, to the circuit diagram, to the front-panel of the physical machine.](figure/ADALINE.png)</span>{width=80%}</span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A memistor ADALINE with glass-sealed memistors. [@widrowAncientHistory2023, Figure 26.12]</span><span class="co">](figure/widrow_2022_memistor_adaline.jpg)</span>{width=80%}</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Test patterns for the ADALINE machine. It also shows one of the first learning curves in machine learning. [@widrowAncientHistory2023, Figure 26.4]</span><span class="co">](figure/widrow_2022_learning_curve.png)</span>{width=80%}</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A MADALINE from 1962. The "JOB ASSIGNER" learning rule is too hacky to explain. [@widrowGeneralizationInformationStorage1962, figure 11]</span><span class="co">](figure/MADALINE.png)</span></span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>Widrow recounts an amusing encounter with Rosenblatt:</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I just put the pattern in and the Adaline went "phut," and the needle was reading to the right or to the left. So I just held the adapt button down so some of the cells are plating while others are deplating, depending on the direction of the error signal. Rosenblatt's students put the pattern into the perceptron. You could see it in the lights on the perceptron. You could hear the potentiometers grinding away. We put another pattern into the Adaline, and it went "blip ," and there it was, adapted. They put it in the perceptron, and it's still grinding away. We put in a couple more patterns. Then we test the Adaline and test the perceptron to see whether the patterns are still in there.</span></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; They're in the Adaline. In the perceptron, they're all gone. I don't know whether the machine was temperamental or what, but it was difficult to train. I argued with Rosenblatt about that first random layer. I said, "You'd be so much better off if you just took the signal from the pixels and ran them straight into the weights of the second layer." He insisted that a perceptron had to be built this way because the human retina is built that way. That is, there's a first layer that's randomly connected to the retina. He said the reason why you can get something to interpret and make sense of random connections is because it's adaptive. You can unravel all this random scrambling. What I was trying to do was to not model nature. I was trying to do some engineering.</span></span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>After the ADALINE showed good behavior, they went on and on trying to train a two-layered ADALINE ("MADALINE", or "many ADALINE"), which of course could not be trained by gradient descent, because the hidden layer ADALINE units used the Heaviside step function! It is almost inconceivable nowadays, but they simply refused to use a continuous activation function and tried every other trick *except* that. They ended up with the <span class="co">[</span><span class="ot">MADALINE I rule</span><span class="co">](https://en.wikipedia.org/wiki/ADALINE#MADALINE)</span>. In short, it was a heuristic rule for synthesizing supervision signals for the hidden layer, much like <span class="co">[</span><span class="ot">Rosenblatt's heuristic rule</span><span class="co">](#frank-rosenblatt)</span>.<span class="ot">[^madaline-rule]</span></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="ot">[^madaline-rule]: </span>The MADALINE I machine consists of multiple hidden ADALINE units, and a single output layer consisting of a single unit. The output unit merely takes the majority vote from the hidden units. If the desired output is $+1$, but the actual output is $-1$, then the machine picks those ADALINE units that are negative, but closest to being positive, and make them update their weights, according to the ADALINE learning rule. It was thought of as a form of "minimal disturbance principle".</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a>Frustrated by the difficulty, they left neural network research. Hoff went to Intel to co-invent the microprocessor, and Widrow set about applying the ADALINE to small problems that it could solve well<span class="ot">[^marvin-minsky-approves]</span>, such as adaptive antennas, adaptive noise filtering in telephone lines, adaptive filtering in medical scanning, etc. Even with just a single ADALINE, he brought breakthroughs to those fields.</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Engineers at Apple have told me that every iPhone, since the iPhone 5, uses the LMS algorithm all over the device. They could not tell me where because, if they did, they would have to shoot me. Apple keeps secrets. </span><span class="co">[</span><span class="ot">@widrowCyberneticsGeneralTheory2022, preface, page xix</span><span class="co">]</span></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a><span class="ot">[^marvin-minsky-approves]</span>:</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>    Marvin Minsky would approve.</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; "\[The perceptron\] is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can't get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of."[@bernsteinMarvinMinskyVision1981]</span></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>Perhaps Widrow and Hoff were limited to considering only learning rules they could implement electrochemically? But it does not seem so, in the words of Widrow himself, it was a blind spot for all researchers:</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The Madaline had an adaptive first layer and a fixed-logic second layer. What Rosenblatt had was a fixed-logic first layer and an adaptive-logic second layer or output layer. Now, it is easy to adapt on output layer. But it's very difficult to adapt a hidden layer. ... We could adapt an adaptive first layer with a fixed second layer as long as we knew what the second layer was. But we never succeeded in developing an algorithm for adapting both layers, so that the second layer is not fixed and both layers are adaptive. It wasn't that we didn't try. I mean we would have given our eye teeth to come up with something like backprop.</span></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Backprop would not work with the kind of neurons that we were using because the neurons we were using all had quantizers that were sharp. In order to make backprop work, you have to have sigmoids; **you have to have a smooth nonlinearity ... no one knew anything about it at that time.** This was long before Paul Werbos. **Backprop to me is almost miraculous**. </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>When he heard about the "miraculous" backpropagation in the 1980s, he immediately started writing papers in neural networks again.</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>If this was an individual case, then I might have dismissed it as Widrow being misled by his background. Before entering a PhD program Widrow took an internship at the military:</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... the sky was divided up into a grid, with one mile by one mile regions, and all the computer sensed was that there was a target within that one mile by one mile square. ... The radar pulses were transmitted periodically, and then periodically you got samples of the flight path. Say you take a piece of graph paper and draw and nice, smooth flight path for an airplane, and then at uniform points in time put a mark on the actual path based on the radar sample. If you don't show anybody that nice, smooth line but only the sequence of grid squares that the airplane was in, one could still reconstruct the smooth track. </span><span class="co">[</span><span class="ot">@widrowOralHistoryBernard1997</span><span class="co">]</span></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>The problem of reconstructing a smooth flight path from discretized radar signals got him interested in quantization, and he did his PhD thesis (1954) in the statistical theory of quantization noise. In the interview 43 years later, he called it "the best piece of work I ever did in my whole life". He is still the world expert on quantization noise, and wrote an entire textbook on quantization noise <span class="co">[</span><span class="ot">@widrowQuantizationNoiseRoundoff2008</span><span class="co">]</span>.</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>So regarding this background, one might be tempted to say that Widrow was misled by his infatuation with quantization, and attempted to use quantized neural networks. However, this does not explain why all the *other* pioneers went down the same wrong path.</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## Seppo Linnainmaa</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>It's said that Seppo Linnainmaa's master's thesis in 1970 contains the backpropagation algorithm, but it is in Finnish and not available online either. A bibliography search shows that his thesis were basically never cited before the 2010s. It is one of those papers that people cite for ritual completion only. I think we can safely say that his thesis has priority but no paternity. <span class="co">[</span><span class="ot">@griewankWhoInventedReverse2012</span><span class="co">]</span> describes in detail what is in the thesis, according to which he developed it to analyze the cumulative effect of round-off error in long chains of computer computations.</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>I checked all his English papers during the 1970s, and it seems only <span class="co">[</span><span class="ot">@linnainmaaTaylorExpansionAccumulated1976</span><span class="co">]</span> has been cited non-negligibly before 2000, and may have had some impact on automatic differentiation, but I cannot tell by how much. It contains two algorithms, one for computing the first derivative, one for computing the first two. Both were special cases of the general backpropagation algorithm for computing arbitrary orders of derivatives.</span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a><span class="fu">## David Rumelhart</span></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>This section is mainly based on a 1995 interview with David Rumelhart <span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, Chapter 12</span><span class="co">]</span>.</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>In 1979-06, Rumelhart attended a conference organized by James A. Anderson, who knew many of those who were still doing neural network research even through this AI winter. This got him interested in neural networks, especially those done by Geoffrey Hinton and James L. McClelland. He couldn't get the idea out of his head, and during a train ride, decided to just do a "five-year plan" to figure out what the fuss is about, and maybe compile a comprehensive literature review. The world would know this book as *Parallel Distributed Processing* (1986).</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "I think it's important enough that I should spend at least five years figuring out what's going on." ... When I came back to San Diego from this trip, I went to see McClelland, and I said to Jay, "Look, Geoffrey's coming back in December. Let's spend two quarters going over all the stuff on these neural network things... We'll have meetings every day. I'm on sabbatical. I can do this." Jay was on a Young Investigator Award, and Geoffrey was coming as a kind of a postdoctoral continuation... "Six months, that's plenty of time for doing this", we thought.</span></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>So they set about reading all the literature they could find about neural networks. Rumelhart saw that the XOR problem was the key. Solve that with a general method, and neural networks would be reborn.</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I'd read *Perceptrons*. And I thought, 'Well , somehow we have to figure out how to teach, how to train, a network that has more than one layer. Why can't we do this?" ... I knew about, as I say, the work of Rosenblatt and about the way he tried to do it. He had this idea of sending error signals back across layers, but he didn't have a very principled way of doing it.</span></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a>Rumelhart knew that Bernard Widrow's ADALINE essentially performs a linear regression $Wx + b$, trained by the ADALINE learning rule, also called the "<span class="co">[</span><span class="ot">delta rule</span><span class="co">](https://en.wikipedia.org/wiki/Delta_rule)</span>":</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>\Delta W = -\eta(Wxx^T + (b-y)x^T), \quad \Delta b = -\eta(b + Wx - y)</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a>So he thought, how do I extend this to a "generalized delta rule"?</span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; So what we thought was we'd pretend they were linear, and we would compute the derivatives as if they were linear, and then we could train them. We did this on a one-layer system. That was what we called delta learning... "Well, we'll just pretend like it's linear, and figure out how to train it as if it were linear, and then we'll put in these sigmoids. In that way, we can make a system that would work."</span></span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a>So he thought, well, if the activation functions are linear functions, then we just get a deep linear network, so we can still do the delta rule. If the activation functions are nonlinear, but differentiable, then we can approximate these by linear activations... and that should be a working, generalized delta rule!</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>Thus, by a long and almost accidental route, Rumelhart arrived at the gradient descent algorithm without thinking about gradients. In hindsight, the ADALINE learning rule is really a special case of gradient descent on the loss function $L = \frac 12 <span class="sc">\|</span>Wx + b - y<span class="sc">\|</span>^2$, but people back then didn't think of it that way. They just thought of it as a not particularly efficient way to solve the linear equation $y \approx Wx + b$. Of course, once he had discovered the generalized delta rule, he recognized it is really just gradient descent on L2 loss. Once you have figured out the result, you often become embarrassed by the pages upon pages of wasteful calculations you used to get there. A very relatable moment!</span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>Backpropagation was rediscovered in 1982. The world would soon know it, and this time, it would not forget.</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>It was not all smooth-sailing though. Backpropagation was terribly slow, taking over 1000 steps to converge, and Hinton believed it would get stuck in local minima. Indeed, these two objections, that backpropagation is slow, and gets stuck in local minima, are perennial objections, as we would see in the <span class="co">[</span><span class="ot">history of the second neural network winter</span><span class="co">](https://yuxi-liu-wired.github.io/TODO)</span>.</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>Somewhat disappointed by the two objections, Rumelhart tried other methods, such as <span class="co">[</span><span class="ot">competitive learning</span><span class="co">](https://en.wikipedia.org/wiki/Competitive_learning)</span>. Hinton left the book project to focus on his beloved Boltzmann machines, and would not return until 1985.</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a>In fact, Rumelhart avoided backprop whenever he could, as well! When he was trying to make a neural network to convert verbs (represented as a list of <span class="co">[</span><span class="ot">phonemes</span><span class="co">](https://en.wikipedia.org/wiki/Phoneme)</span>) to their past tense forms, he thought that he had to resort to an MLP with backprop, but then realized that, no, you can completely avoid backprop if you <span class="co">[</span><span class="ot">hand-design some features</span><span class="co">](https://en.wikipedia.org/wiki/Feature_engineering)</span>, then just train a single layer over those features. An inauspicious omen of the second neural network winter, during which computer vision was stuck with the same paradigm of shallow learning (<span class="co">[</span><span class="ot">SVM</span><span class="co">](https://en.wikipedia.org/wiki/Support_vector_machine)</span>, <span class="co">[</span><span class="ot">random forests</span><span class="co">](https://en.wikipedia.org/wiki/Random_forest)</span>, etc) over cleverly hand-designed features (<span class="co">[</span><span class="ot">SIFT</span><span class="co">](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform)</span>, <span class="co">[</span><span class="ot">HoG</span><span class="co">](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients)</span>, etc).</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; But then McClelland and I figured out another trick for learning past tenses, the so-called "</span><span class="co">[</span><span class="ot">Wickelfeature</span><span class="co">](https://en.wikipedia.org/wiki/Wickelphone)</span><span class="at">" representation. We thought, "Oh well, we don't need multiple layers." So I put that aside and went off, and we realized that if we made fancier input representations, we wouldn't need this intermediate layer.</span></span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a>The first phoneme of the Bitter Lesson has been spoken.<span class="ot">[^first-phoneme-borges]</span></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a><span class="ot">[^first-phoneme-borges]: </span>*Death and the Compass* (Borges, 1942).</span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>Years later, they did train an MLP by backprop for this task, and its publication in 1986 ignited the past tense debate <span class="co">[</span><span class="ot">@pinkerFutureTense2002</span><span class="co">]</span>. But at the moment, they didn't really want to use backprop. Indeed, in 1983, backprop was looking so uninteresting that at a lecture, Rumelhart elected to talk about competitive learning instead.</span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="sc">\[</span><span class="at">In 1983, Hinton and Sejnowski</span><span class="sc">\]</span><span class="at"> held a meeting in Pittsburgh... I remember electing at the last minute to talk about the competitive learning work rather than the backpropagation work. I had my slides all made out to do both of these talks, and I gave that one.</span></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>Around 1984, Rumelhart started getting a vibe that they couldn't just keep doing this, that they really needed MLPs with learned hidden layers, so he tried it again. Things really picked up at this point. Hinton had realized that Boltzmann machines were even more painfully slow than backprop, so he went back. They wrote the famous *Nature* paper in 1985, and this time, the world listened. <span class="co">[</span><span class="ot">@rumelhartLearningRepresentationsBackpropagating1986</span><span class="co">]</span></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We had pretty much sorted this out and were working on how to do backpropagation in time. Geoff and I decided we really should write this up, so we started writing our paper and did a whole bunch of experiments in order to have something to say in this paper. The paper was really written in the spring of '85. I think the *Nature* paper was in the fall of '85. By then I was fairly committed to learning how it might work. I guess I've now spent about another ten years sorting all of this out the best I could.</span></span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>In the interview, he was rather unbothered by the priority dispute:</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I had no idea that Paul Werbos had done work on it. ... There are other examples of work in the control literature in the '60s </span><span class="sc">\[</span><span class="at">the </span><span class="co">[</span><span class="ot">adjoint method</span><span class="co">](https://en.wikipedia.org/wiki/Adjoint_state_method)</span><span class="sc">\]</span><span class="at">. ... it's just no one had really done anything with them. My view is that we not only discovered them, but we realized what we could do with them and did a lot of different things. I consider them independent discoveries, at least three and maybe more. </span><span class="co">[</span><span class="ot">\[Shun'ichi\] Amari</span><span class="co">](https://en.wikipedia.org/wiki/Shun'ichi_Amari)</span><span class="at">, I think, suggests that he had another discovery, and you know, I believe that. You can look at his paper. He had, but he didn't do anything with it. I think that was in the late '60s. I don't feel any problem. You know, maybe we should have done better scholarship and searched out all of the precursors to it, but we didn't know there were any.</span></span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a><span class="fu">## Terence Sejnowski</span></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a>In 1983, Rumelhart showed backpropagation to Sejnowski, who immediately tried it and found that it was much faster than the Boltzmann machine learning rule. What a refreshing change from all those others who stubbornly refused to try it.</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... I was still working on the Boltzmann machine and was beginning to do simulations using backprop. I discovered very rapidly that backprop was about an order of magnitude faster than anything you could do with the Boltzmann machine. And if you let it run longer, it was more accurate, so you could get better solutions. </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, Chapter 14</span><span class="co">]</span></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a>This was vitally important later, when Sejnowski used backpropagation to train <span class="co">[</span><span class="ot">NETtalk</span><span class="co">]</span>(https://en.wikipedia.org/wiki/NETtalk_(artificial_neural_network)), a *huge* network with 18,629 parameters.<span class="ot">[^nettalk-scaling]</span> The model was a popular hit and appeared on prime-time television. <span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, pages 112--115</span><span class="co">]</span></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a>Sejnowski later commented that backpropagation is inelegant:</span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This is a highly efficient way to compute error gradients. Although it has neither the elegance nor the deep roots in physics that the Boltzmann machine learning algorithm has, backprop is more efficient, and it has made possible much more rapid progress.</span></span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, page 112</span><span class="co">]</span></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a><span class="ot">[^nettalk-scaling]</span>:</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>    People told him that it had too many parameters and would hopelessly overfit, but the network just needed enough data and it generalized uncannily well. It really does seem like scaling skepticism is a researcher universal, if not a human universal.</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; There were 18,629 weights in the network, a large number by the standards of 1986, and impossibly large by the standards of mathematical statistics of the time. With that many parameters, we were told that we would overfit the training set, and the network would not be able to generalize. [@sejnowskiDeepLearningRevolution2018, page 113]</span></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geoffrey Hinton</span></span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a>The interview with Geoffrey Hinton is hilarious, mostly about how he *spent several years refusing to use backpropagation*. This section is mostly made of quotations from <span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, Chapter 16</span><span class="co">]</span>.</span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a>After learning about Hopfield networks and simulated annealing both in 1982, he tried combining them, and discovered the Boltzmann learning algorithm in 1983, which he published in 1985 <span class="co">[</span><span class="ot">@ackleyLearningAlgorithmBoltzmann1985</span><span class="co">]</span>.</span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I remember I had to give a talk, a sort of research seminar, at Carnegie Mellon in either February or March of '83 ... I wanted to talk about this simulated annealing in Hopfield nets, but I figured I had to have a learning algorithm. I was terrified that I didn't have a learning algorithm.</span></span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Then we got very excited because now there was this very simple local-learning rule. On paper it looked just great. I mean, you could take this great big network, and you could train up all the weights to do just the right thing, just with a simple local learning rule. It felt like we'd solved the problem. That must be how the brain works.</span></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I guess if it hadn't been for computer simulations, I'd still believe that, but the problem was the noise. It was just a very, very slow learning rule. It got swamped by the noise because in the learning rule, you take the difference between two noisy variables, two sampled correlations, both of which have sampling noise. The noise in the difference is terrible.</span></span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I still think that's the nicest piece of theory I'll ever do. It worked out like a question in an exam where you put it all together and a beautiful answer pops out.</span></span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>And now, the hilarity we have been waiting for. When Rumelhart told him in 1982 about backpropagation,</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I first of all explained to him why it wouldn't work, based on an argument in Rosenblatt's book, which showed that essentially it was an algorithm that couldn't break symmetry. ... the standard way of doing learning is to have random initial weights, or you could just put a little bit of noise in the learning rule ...</span></span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The next argument I gave him was that it would get stuck in local minima. There was no guarantee you'd find a global optimum. Since you're bound to get stuck in local minima, it wasn't really worth investigating.</span></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Then I tried to use it to get a very obscure effect. I couldn't get this very obscure effect with it, so I lost interest in backpropagation. I managed to get this very obscure effect later with a Boltzmann machine. I'd realized that if you've got a net to learn something, and then you add noise to the weights by making it learn something else, it should be much faster at relearning what it had previously learned. You should get very fast relearning of stuff that it previously knew, as compared to the initial learning. We programmed a backpropagation net, and we tried to get this fast relearning. **It didn't give fast relearning, so I made one of these crazy inferences that people make -- which was, that backpropagation is not very interesting**.</span></span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>After one year of failing to get Boltzmann machines to train, he discovered that Boltzmann machines *also* got stuck in local minima. Desperate times call for desperate measures, and he finally resorted to gradient descent.</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; After initially getting them to work, I dedicated over a year to refining their performance. I experimented with various techniques, including weight decay, which helped in preventing large energy barriers. We attempted several other approaches, but none of them yielded satisfactory results. I distinctly recall a point where we observed that training a Boltzmann machine could model certain functions effectively, but prolonged training caused its performance to deteriorate, a phenomenon we referred to as "going sour." We couldn't initially comprehend why this was happening. I generated extensive reports, filling stacks of paper about three inches thick, as I couldn't believe that these networks would degrade as you acquired more knowledge.</span></span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It took me several weeks to realize the root of the problem. The learning algorithm operated under the assumption that it could reach thermal equilibrium. However, as the weights grew larger, the annealing process failed to achieve thermal equilibrium, trapping the system in local minima. Consequently, the learning algorithm started behaving incorrectly. This realization led us to introduce weight decay as a solution to prevent this issue.</span></span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **After investing over a year** in attempting to optimize these methods, I ultimately concluded that they were unlikely to deliver the desired results. In a moment of desperation, I considered revisiting </span><span class="sc">\[</span><span class="at">backpropagation</span><span class="sc">\]</span><span class="at">.</span></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>Then he asked the research group for volunteers. None of the ten students took up the offer (and thus giving up a place as the coauthor of the backpropagation paper??):</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; They'd all been thoroughly indoctrinated by then into Boltzmann machines. ... They all said, "You know, why would you want to program that?" We had all the arguments: It's assuming that neurons can send real numbers to each other; of course, they can only send bits to each other; you have to have stochastic binary neurons; these real-valued neurons are totally unrealistic. It's ridiculous." So they just refused to work on it, not even to write a program, so I had to do it myself.</span></span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I went off and I spent a weekend. I wrote a LISP program to do it.</span></span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a>Hinton almost had one last chance at giving up on backpropagation.</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I almost blew it because the first thing I tried it on was an 8-3-8 encoder; that is, you have eight input units, three hidden units, and eight output units. You want to turn on one of the input units and get the same output unit to be on.</span></span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In a Boltzmann machine, since the units are binary, the states of the three hidden units have to be the eight different binary codes, so you know what the hidden units will look like when they've solved the problem. So I ran backpropagation on this problem and looked at the weights of the hidden units. They didn't have the right structure of weights. ... I thought, "Oh well, it turns out backpropagation's not that good after all." Then I looked at the error, and the error was zero. I was amazed.</span></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a><span class="al">![The 8-3-8 autoencoder. The input is a one-hot encoding of one integer from $(0:7)$, and the output should be a copy of the input. The middle layer has 3 units, which is sufficient for binary encoding. Hinton found the Boltzmann machine learned exactly this, and expected backpropagation to learn the same thing, but was surprised when it did not. In the picture, the input is a one-hot vector of the integer `3`, and the hidden layer encodes it as a binary `011`, which is then decoded to a one-hot vector of `3` again.](figure/Hinton 8-3-8 task.svg)</span></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a>And so one year of excuses later, Hinton finally surrendered to backpropagation. They published the algorithm along with several examples, which became widely cited.</span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>Despite this, Hinton remained unhappy with backprop and kept working on Boltzmann machines, as well as its relatives like the Helmholtz machine and the Deep Belief Networks. This is him talking in 1995, already 10 years after he was forced to accept the superiority of backprop:</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; That was at the stage when we were just completing the PDP books, so we'd already agreed on what was going to be in the books. The final chapters were being edited. We decided we'd just slip in an extra chapter on backpropagation, so it was a late addition to the book. But I was always a bit disappointed. I mean, intellectually, backpropagation wasn't nearly as satisfying as Boltzmann machines. ... it didn't have the nice probabilistic interpretation.</span></span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a><span class="fu">## Paul Werbos</span></span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a>After reviewing the story of multiple people, my personal opinion is that Paul Werbos most deserves the title of "originator of the backpropagation algorithm". He independently developed the algorithm around 1971, but was frustrated at every turn when he tried to publish it, not managing until 1982. After that, it was quickly picked up by connectionists. In this sense, he has both priority and paternity.</span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a>In 1993, Paul Werbos gave an interview, where he described the backstory of his invention of backpropagation <span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, Chapter 15</span><span class="co">]</span>. I will let him speak, only interjecting with brief comments.</span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a>Before entering Harvard, he tried implementing Hebbian learning, but noticed it wouldn't work.</span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It was obvious to me from a statistical point of view that Hebbian learning was going to be measuring correlation coefficients, and for multivariate problems it would not work. I never gave the talk. I said, "Now I'm going to figure out something with the same flavor that does work."</span></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="sc">\[</span><span class="at">Understanding human learning</span><span class="sc">\]</span><span class="at"> will help us understand the human mind; it will help human beings understand themselves. Therefore, they will make better political decisions, and better political decisions are vital for the future of humanity.</span></span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky was one of my major influences. Well, Minsky and Hebb and Asimov and Freud.</span></span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a>Sometime before 1968, he was inspired to do backpropagation from reading Freud.</span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I talk in there about the concept of translating Freud into mathematics. This is what took me to backpropagation, so the basic ideas that took me to backpropagation were in this journal article in '68 </span><span class="co">[</span><span class="ot">@werbosElementsIntelligence1968</span><span class="co">]</span><span class="at">. I talked a lot about what was wrong with the existing </span><span class="sc">\[</span><span class="at">two state</span><span class="sc">\]</span><span class="at"> McCulloch--Pitts neuron model, and how it was only "1" and "0." I wanted to make that neuron probabilistic. I was going to apply Freudian notions to an upper-level, associative drive reinforcement system. When</span></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a>For his masters (1969), he majored in mathematical physics, and minored in decision and control. During his PhD at Harvard, he had to take some compulsory courses, which he didn't want to. To salvage the situation, he took computer courses which would provide him with some computer hours, a scarce resource at the time. I would guess the following event happened early 1971.</span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Initially, I was going to do something on quantum physics. I learned something about partial differential equations, but not enough. I couldn't produce a really useful product at the end of $x$ number of months.</span></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; So I went back to the committee, and I said, "Gee, I can't do that, **but I have this little method for adapting multilayer perceptrons. It's really pretty trivial.** It's just a by-product of this model of intelligence I developed. And I'd like to do it for my paper for this computer course."</span></span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="sc">\[</span><span class="at">Larry</span><span class="sc">\]</span><span class="at"> Ho's position was, "I understand you had this idea, and we were kind of open-minded. But look, at this point, you've worked in this course for three months, admittedly on something else. I'm sorry, you're just going to have to take an incomplete in the course."</span></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; And I said, "You mean I can't do it?"</span></span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "No, no, you'll have to take an incomplete because, basically, the first thing didn't work. We're very skeptical this new thing is going to work."</span></span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "But look, the mathematics is straightforward."</span></span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "Yeah, yeah, but you know, **we're not convinced it's so straightforward**. You've got to prove some theorems first."</span></span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; So they wouldn't let me do it. One of the reasons that is amusing to me is that there are now some people who are saying backprop was invented by </span><span class="co">[</span><span class="ot">Bryson</span><span class="co">](https://en.wikipedia.org/wiki/Arthur_E._Bryson)</span><span class="at"> and Ho. They don't realize it was the same </span><span class="co">[</span><span class="ot">Larry Ho</span><span class="co">](https://en.wikipedia.org/wiki/Yu-Chi_Ho)</span><span class="at">, who was on my committee and who said this wasn't going to work.</span></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a>I am not sure if this is sarcastic or not. It reminds me of the "summer vision project" <span class="co">[</span><span class="ot">@papertSummerVisionProject1966</span><span class="co">]</span> that expected some undergraduate students to construct "a significant part of a visual system" in a single summer.</span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By the time my orals came around, it was clear to me that the nature of reality is a hard problem, that I'd better work on that one later and finish my Ph.D. thesis on something small -- something I can finish by the end of a few years, like a complete mathematical model of human intelligence.</span></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a>The oral was amusing, and touched on the still-hot issue of <span class="co">[</span><span class="ot">recent human evolution</span><span class="co">](https://en.wikipedia.org/wiki/Recent_human_evolution)</span>.</span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... I made a statement that there might be parameters affecting utility functions in the brain, parameters that vary from person to person. You could actually get a significant amount of adaptation in ten generations' time. I was speculating that maybe the rise and fall of human civilizations, as per Toynbee and Spengler, might correlate with these kinds of things. The political scientist on the committee, </span><span class="co">[</span><span class="ot">Karl Deutsch</span><span class="co">](https://en.wikipedia.org/wiki/Karl_Deutsch)</span><span class="at">, raised his hand. ... His book, *The Nerves of Government*, which compares governments to neural networks, is one of the classic, accepted, authoritative books in political science.</span></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; He raised his hand and he said, "Wait a minute, you can't get significant genetic change in ten generations. That cannot be a factor in the rise and fall of civilizations. That's crazy."</span></span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Next to him was a mathematical biologist by the name of </span><span class="co">[</span><span class="ot">Bossert</span><span class="co">](https://en.wikipedia.org/wiki/William_H._Bossert)</span><span class="at">, who was one of the world's authorities on population biology. He raised his hand and said, "What do you mean? In our experiments, we get it in seven generations. This guy is understating it. Let me show you the experiments."</span></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; And Deutsch said, "What do you mean, it's common knowledge? All of our political theories are based on the assumption this cannot happen." And Bossert said, "Well, it happens. Here's the data."</span></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... I passed the orals having said about two sentences and not having discussed models of intelligence.</span></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>It turns out Werbos interpreted backpropagation as the mathematical interpretation of Freudian psychic energy flow. The thesis committee was not amused.</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; But the backpropagation was not used to adapt a supervised learning system; it was to translate Freud's ideas into mathematics, to implement a flow of what Freud called "psychic energy" through the system. I translated that into derivative equations, and I had an adaptive critic backpropagated to a critic, the whole thing, in '71 or '72. ... The thesis committee said, "We were skeptical before, but this is just unacceptable ... You have to find a patron. You must find a patron anyway to get a Ph.D. That's the way Ph.D.s work.</span></span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a>The committee gave him three acceptable patrons. He first went to <span class="co">[</span><span class="ot">Stephen Grossberg</span><span class="co">](https://en.wikipedia.org/wiki/Stephen_Grossberg)</span>.</span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... he said, 'Well, you're going to have to sit down. Academia is a tough business, and you have to develop a tough stomach to survive in it. I'm sure you can pull through in the end, but you're going to have to do some adaptation. So I want you to sit down and hold your stomach, maybe have an antacid. The bottom line is, this stuff you've done, it's already been done before. Or else it's wrong. I'm not sure which of the two, but I know it's one of the two."</span></span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a>Thanks, Grossberg, for using the law of excluded middle to crush Werbos' dream.</span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a>He then went to <span class="co">[</span><span class="ot">Marvin Minsky</span><span class="co">](https://en.wikipedia.org/wiki/Marvin_Minsky)</span>, who gave us some new clues about why backpropagation took so long to discover: "everybody knows a neuron is a 1-0 spike generator"!</span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "I've got a way now to adapt multilayer perceptrons, and the key is that they're not Heaviside functions; they are differentiable. And I know that action potentials, nerve spikes, are 1 or 0, as in McCulloch--Pitts neurons, but here in this book that I had for my first course in neurophysiology are some actual tracings. If you look at these tracings in Rosenblith, they show volleys of spikes, and volleys are the unit of analysis. This is an argument for treating this activity as differentiable, at least as piecewise linear. If you look at that, I can show you how to differentiate through it."</span></span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky basically said, "Look, **everybody knows a neuron is a 1-0 spike generator. That is the official model from the biologists**. Now, you and I are not biologists. If you and I come out and say the biologists are wrong, and this thing is not producing 1s and 0s, nobody is going to believe us. It's totally crazy. I can't get involved in anything like this."</span></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; He was probably right, I guess, but he was clearly very worried about his reputation and his credibility in his community.</span></span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a>Although I believe Minsky would have disapproved of the idea of backpropagation even if he had thought that neurons are not strictly 1-0 spike generators. In the epilogue to <span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988</span><span class="co">]</span>, he claimed that gradient descent does not scale, and <span class="co">[</span><span class="ot">using differentiable activation functions is just a hack intended to make backpropagation work</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/#differentiable-activation-is-just-a-hack)</span>, a pointless hack, as backpropagation would not scale.</span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold. ... We have the impression that many people in the connectionist community do not understand that this is merely a particular way to compute a gradient and have assumed instead that back-propagation is a new learning scheme that somehow gets around the basic limitations of hill-climbing.</span></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a>Further, in a 1991 interview, Minsky made the same kind of statement:</span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I don't know what they would have done with the money. The story of DARPA and all that is just a myth. The problem is that there were no good ideas. The modern idea of backpropagation could be an old idea. There was someone . . . </span><span class="sc">\[</span><span class="at">trying to remember</span><span class="sc">\]</span><span class="at">.</span></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Question: Paul Werbos?</span></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Answer: That's it! </span><span class="co">[</span><span class="ot">excited</span><span class="co">]</span><span class="at">. But, you see, it's not a good discovery. It's alright, but it takes typically 100,000 repetitions. It converges slowly, and it cannot learn anything difficult. Certainly, in 1970 the computers were perhaps too slow and expensive to do it. I know that Werbos thought of that idea. It's certainly trivial. The idea is how you do gradient descent. I didn't consider it practical.</span></span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Question: Because of the computational costs?</span></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Answer: Yes, but also, with artificial intelligence, we had the experience that when you make a process like that you usually get stuck at a local minimum. We still don't have any theory of what range of problems they work well for." (Minsky, interview) </span><span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991, page 249</span><span class="co">]</span></span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>Out of curiosity, I looked up the "Rosenblith" book <span class="co">[</span><span class="ot">@rosenblithSensoryCommunicationContributions2012</span><span class="co">]</span> that Werbos mentioned, and indeed there were a few tracings that show continuously varying neural activation.</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a>::: {#fig-rosenblith layout-ncol=1}</span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a><span class="al">![Page 146 of the book. *Mechanisms of gustatory and olfactory receptor stimulation*, Figure 2.](figure/no_bg_beidler_2.png)</span>{#fig-however}</span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a><span class="al">![Page 366 of the book. *The physiological basis of wavelength discrimination in the eye of the honeybee*, Figure 4.](figure/no_bg_goldsmith_4.png)</span>{#fig-whatever}</span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a>Rosenblith images.</span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>Then Minsky dunked on reinforcement learning as well, because he had an unpublished "jitters machine" that failed to optimize its reward. Presumably the name "jitters machine" refers to how it would jitter in place, not able to move towards the goal.</span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky also said, "You know, I used to believe in all this kind of stuff with reinforcement learning because I knew reinforcement learning would work. I knew how to implement it. I had a nice guy named Oliver Selfridge who came in and acted as my patron and gave me permission to do it. We coauthored a paper, but it was really my idea, and he was just acting as patron on the Jitters machine. I'll hand you the tech report, which we have deliberately never published."</span></span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It was his bad experience with the Jitters machine that turned him off on reinforcement learning and all the neural net ideas. It just didn't work. I later looked at that paper ... He had a system that was highly multivariate with a single reinforcement signal. The system can't learn efficiently with that. At any rate, he was totally turned off.</span></span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a>The brief description of the unpublished jitters machine was not making sense to me, so I looked around the literature. It was *so* unpublished that I found only two other references in the entire literature <span class="co">[</span><span class="ot">@werbosApplicationsAdvancesNonlinear1982; @werbosBuildingUnderstandingAdaptive1987</span><span class="co">]</span>, both by Werbos. According to him,</span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There are also some technical reasons to expect better results with the new approach. Almost all of the old perceptron work was based on the McCulloch--Pitts model of the neuron, which was both discontinuous and non-differentiable. It was felt, in the 1950's, that the output of a brain cell had to be 1 or 0, because the output consisted of sharp discrete "spikes." More recent work in neurology has shown that higher brain cells output "bursts" or "volleys" of spikes, and that the strength of a volley may vary continuously in intensity. This suggests the CLU model to be discussed in the Appendix. In any event, to exploit basic numerical methods, one must use differentiable processing units. Minsky once experimented with a "jitters" machine, which estimated one derivative per time cycle by making brute-force changes in selected variables; however, the methods to be discussed in the Appendix yield derivatives of all variables and parameters in one major time cycle and thereby multiply efficiency in proportion to the number of parameters $(N)$, which may be huge.</span></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@werbosBuildingUnderstandingAdaptive1987</span><span class="co">]</span></span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a>This makes things clear enough. The jitters machine was an RL agent with multiple continuously adjustable parameters running a very primitive form of policy gradient. During every episode, it would estimate $\partial_{\theta_i} R(\theta)$ using finite difference, but since it used finite difference, it could estimate the partial derivative for one of the parameters $\theta_i$ -- only *one*! No wonder it never managed to learn.</span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a>It is almost comical how much they failed to just use gradient descent. It sometimes feels as if they did everything to *avoid* just taking the gradient. In the case of Minsky, he made it very clear, in the epilogue of <span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988</span><span class="co">]</span>, that he did not believe in gradient descent, period. But what explains the gradient-phobia of the others...?</span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a>Anyway, back to the interview. Werbos went to <span class="co">[</span><span class="ot">Jerome Lettvin</span><span class="co">](https://en.wikipedia.org/wiki/Jerome_Lettvin)</span>, the neuroscientist famous for *What the Frog's Eye Tells the Frog's Brain*. Turns out he was a proto-<span class="co">[</span><span class="ot">eliminativist</span><span class="co">](https://en.wikipedia.org/wiki/Eliminativism)</span>. While I am an eliminativist too, Werbos was a Freudian, which must have collided badly with eliminativism.</span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "Oh yeah, well, you're saying that there's motive and purpose in the human brain." He said, "That 's not a good way to look at brains. I've been telling people, 'You cannot take an anthropomorphic view of the human brain.' In fact, people have screwed up the frog because they're taking bachtriomorphic views of the frog. If you really want to understand the frog, you must learn to be objective and scientific."</span></span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a>Without patrons, he faced the committee again.</span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I tried to simplify it. I said, "Look, I'll pull out the backprop part and the multilayer perceptron part." I wrote a paper that was just that - that was, I felt, childishly obvious. I didn't even use a sigmoid </span><span class="sc">\[</span><span class="at">non-linearity</span><span class="sc">\]</span><span class="at">. I used piecewise linear. I could really rationalize that to the point where it looked obvious. I handed that to my thesis committee. I had really worked hard to write it up. They said, "Look, this will work, **but this is too trivial** and simple to be worthy of a Harvard Ph.D. thesis."</span></span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>Oh, now it's too trivial?? I swear the backstory of backpropagation gets more and more ridiculous by the day.</span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... they had discontinued support because they were not interested, so I had no money. ... Not even money to buy food. A generous guy, who was sort of a Seventh Day Adventist and a Harvard Ph.D. candidate in ethnobotany, had a slum apartment that rented for about $40 a month in Roxbury in the ghetto. He let me share a room in his suite, in his suite with the plaster falling off, and didn't ask for rent in advance. I had no money at that time for food. There was a period of three months when I was living there in the slums. To conserve the little bit of money I had, I remember eating soybean soup and chicken neck soup. I remember getting the shakes from inadequate nutrition.</span></span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Finally, they said, "Look, you know, we're not going to allow this." There was this meeting where we sat around the table. The chairman of the applied math department at that time was a numerical analyst D. G. M. Anderson. He said, "We can't even allow you to stay as a student unless you do something. You've got to come up with a thesis, and it can't be in this area."</span></span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a>Karl Deutsch, who believed in Werbos, sponsored his PhD thesis on a "respectable" problem: fitting an <span class="co">[</span><span class="ot">ARMA model</span><span class="co">](https://en.wikipedia.org/wiki/Autoregressive_moving-average_model)</span> to a time-series data of political regimes, and use it to forecast nationalism and political assimilation. <span class="co">[</span><span class="ot">Box–Jenkins method</span><span class="co">](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method)</span> ran too slowly, so Werbos programmed in the backpropagation. It worked, and he finally obtained his PhD in 1974. This is the original reference point for modern neural network backpropagation algorithm.</span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Deutsch said, "You 're saying we need an application to believe this stuff? I have an application that we could believe. I have apolitical forecasting problem. I have this model, this theory of nationalism and social communications? What causes war and peace between nations? I have used up ten graduate students who've tried to implement this model on real-world data I've collected, and they've never been able to make it work. Now, do you think your model of intelligence could solve this problem and help us predict war and peace?"</span></span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The first application of backpropagation in the world in a generalized sense was a command that was put into the </span><span class="co">[</span><span class="ot">TSP</span><span class="co">]</span><span class="at">(https://en.wikipedia.org/wiki/TSP_(econometrics_software)) at MIT , available to the whole MIT community as part of their standard software. It was published as part of MIT 's report to the DOD </span><span class="co">[</span><span class="ot">the Department of Defense</span><span class="co">]</span><span class="at"> and part of the DOD's report to the world . It was part of the computer manual, so the first publication of backpropagation was in a computer manual from MIT for a working command for people to use in statistical analysis. I was a second author of that manual. It was Brode, Werbos, and Dunn.</span></span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... Once I started doing software for them, they discovered I was pretty good at it. Once I had put backpropagation into a TSP command, they offered me a full-time job at the Harvard Cambridge Project, so I did the Ph.D. thesis while having a full-time job. While I was doing these things at MIT, I remember going to one party. ... one of the people there said, 'We have heard through the grapevine that somebody has developed this thing called continuous feedback that allows you to calculate all the derivatives in a single pass."</span></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a>But the saga is not over. After the PhD, he tried promoting his work. For example, he tried interesting Laveen Kanal in it, who promptly ignored it:</span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a young man came by in the spring of 1975 to introduce himself as a new assistant professor in the Government and Politics department at the University. He wanted to talk about his Ph.D. dissertation which he had recently finished at Harvard. He said quite enthusiastically that it related to pattern recognition, learning machines, and intelligent systems. With the best of intentions I told him to lend me a copy and he lent me a report titled "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences". Soon the report got buried in the usual flood of paper that unfortunately still remains very much part of my life. I have often wondered in recent years if the title had been different, e.g., if it had mentioned something about a procedure for training multilayer neural networks, or if my desk had been less cluttered, would I have paid more attention to this report or was I so tuned away from artificial neural networks that it would have made no difference? ... I was not alone in neglecting his work.</span></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@kanalPatternCategoriesAlternate1993</span><span class="co">]</span></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a>Then he was hired by the DoD, and promptly got sucked into several more years of misadventure, which meant that he could not even talk about backpropagation in a public journal until 1978 -- and the algorithm got removed anyway due to page limits. This annoyed the DoD and he had to move to the Department of Energy.</span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I found out that DARPA had spent a lot of effort building a worldwide conflict forecasting model for the Joint Chiefs of Staff that was used in the long-range strategy division of the Joint Chiefs. ... I wound up sending a couple of graduate students to create a really good database of Latin America. I said, "You want variance, high variance. Something hard to predict." I thought conflict in Latin America would be the most beautiful case. I figured there were enough cultural homogeneities that it would be a single stochastic process, but with lots and lots of variance in that process. So we got truly annual data going back, I don't know, twenty or thirty years for all the countries in Latin America and then reestimated the Joint Chief's model on it. It had an r2 of about .0016 at forecasting conflict. By jiggling and jiggling we could raise it to about .05. It could predict GNP and economic things decently. Conflict prediction we couldn't improve, though; it was hopeless.</span></span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; DARPA wasn't happy when I published that result. They wanted me to do something real and practical and useful for the United States. This was useful, but it was an exposé. They didn't like that. Therefore, the report, which included backpropagation in detail and other advanced methods, was not even entered into DOCSUB. Every time I tried to enter it into DOCSUB, somebody jiggled influence to say, "No, no, no, we can't publish this. This is too hot."</span></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It was published in </span><span class="co">[</span><span class="ot">@werbosEmpiricalTestNew1978</span><span class="co">]</span><span class="at"> anyway because they couldn't block the journals, but it didn't include the appendices. So that paper in 1978 said, "We've got this great thing called dynamic feedback, which lets you estimate these things. It calculates derivatives in a single swoop, and you can use it for lots of things, like AI." ... **the appendix on how to do it was not there because of page limits** ... At that point, DARPA was no longer happy.</span></span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a>So he went to the Department of Energy, used backpropagation to create another model, and was silenced once again, unable to publish that report until 1988 <span class="co">[</span><span class="ot">@werbosGeneralizationBackpropagationApplication1988</span><span class="co">]</span>.</span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; They had a million-dollar contract at Oak Ridge National Laboratory to study that model. They wanted me for several things. One, they wanted me to be a translator between engineering and economics. Two, they wanted a critique. They wanted exposés. They wanted me to rip apart the models of the Department of Energy in a very scientific, objective way that didn't look like I was trying to rip them apart but was anyway. That's exactly what they wanted to hire me for, and I didn't really know that was the motive. These particular people didn't like modeling very much.</span></span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; So at some point, they wanted sensitivity analysis. And I said, "You know, I know a little bit about calculating derivatives." ... I applied it to a natural gas model at the Department of Energy. In the course of applying it, I did indeed discover dirt. They didn't want me to publish it because it was too politically sensitive. It was a real-world application, but the problem was that it was too real. At DOE, you know, you don't have First Amendment rights. That's one of the terrible things somebody's got to fix in this country. The reality of the First Amendment has deteriorated. Nobody's breaking the law, but the spirit of the First Amendment has decayed too far for science. At any rate, they finally gave me permission to publish it around '86 and '87. I sent it to the journal Neural Nets -- that is, how you do simultaneous recurrent nets and time-lag recurrent nets together. Then the editorial process messed around with it, made the paper perhaps worse, and it finally got published in '88, which makes me very sad because now I gotta worry about, 'Well, gee, didn't Pineda do this in '88?</span></span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a>As a side note, one might feel that Werbos' "turning Freud into mathematics" seems rather strange. This feeling is completely justified. I found a recent paper by him <span class="co">[</span><span class="ot">@werbosIntelligenceBrainTheory2009</span><span class="co">]</span> with this crackpot illustration:</span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a><span class="al">![The text at the end of the arrow says "quantum and collective intelligence (Jung, Dao, Atman...)?".](figure/werbos_2009.jpg)</span></span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a>I did find this paper where he gave some details about which part of Freud he meant exactly:</span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The model in Figure 1 is actually just a mathematical version of Freud's model of psychodynamics, where the derivative of $R_i$ represents what Freud called the "cathexis" (or affect) or emotional charge or emotional energy attached to the object which $R_i$ represents. In other words, I came up with backpropagation by not just laughing at Freud's "nonscientific" model, but by translating it into mathematics and showing that it works.</span></span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;  More concretely, Freud said that "if A causes B, a forward association or axon develops from A to B; and then, if there is an emotional charge on B, that energy flows backwards, to put a charge in A, proportional to the charge on B and to the strength of the forwards association from A to B." That's exactly what backpropagation does. Chronologically, I translated Freud into a way to calculate derivatives across a network of simple neurons (which Harvard simply did not believe), and then proved the more general chain rule for ordered derivatives to prove it and make it more powerful (and to graduate). </span></span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Freud's term "psychic energy" for this flow really captures the subjective feeling of this subjective reality, which Freud documents many, many times over in his works. (Though of course, it is not conserved like the energy operators of physics. It is a computational flow, however implemented.) But in my view, any really strong collective intelligence would have to be held together by the same kind of thing, propagated over a more complicated network topology, but still the same basic thing. And indeed, almost every major deep culture on earth has a term for the same kind of "psychic energy" at another level – like "qi" or "prana" or "charisma." What's more, several different types of derivatives (like derivatives of $J$ versus derivatives of error) need to be propagated, giving rise to different kinds of mental energy. Sensitivity to these flows, to the fact that they are not conserved, and to the mathematics of the factors which govern their flow, is of great importance, in my view and my experience.</span></span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@werbosNeuralNetworksPath2011</span><span class="co">]</span></span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The backpropagation diagram in Werbos' 1972 thesis proposal, which was a mathematical translation of Freud's concept of "psychic energy". [@werbosNeuralNetworksPath2011]</span><span class="co">](figure/Werbos_1972_thesis_proposal.png)</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>