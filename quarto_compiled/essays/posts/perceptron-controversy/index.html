<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2024-01-01">
<meta name="description" content="Connectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.">

<title>The Perceptron Controversy – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="The Perceptron Controversy – Yuxi on the Wired">
<meta property="og:description" content="Connectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/perceptron-controversy/figure/banner_2_cropped.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="891">
<meta property="og:image:width" content="1437">
<meta property="og:image:alt" content="Brutalist architecture representing a single perceptron. A hexagonal building in the center representing the unit itself, with multiple arc legs representing the input weights, and antennas projecting off the top representing the output. High contrast, monochromatic, minimalistic, in the style of vector svg art.">
<meta name="twitter:title" content="The Perceptron Controversy – Yuxi on the Wired">
<meta name="twitter:description" content="Connectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/perceptron-controversy/figure/banner_2_cropped.png">
<meta name="twitter:image-height" content="891">
<meta name="twitter:image-width" content="1437">
<meta name="twitter:image:alt" content="Brutalist architecture representing a single perceptron. A hexagonal building in the center representing the unit itself, with multiple arc legs representing the input weights, and antennas projecting off the top representing the output. High contrast, monochromatic, minimalistic, in the style of vector svg art.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">The Perceptron Controversy</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Connectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky–Papert anti-scaling hypothesis explained, psychoanalyzed, and buried.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">history</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 1, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">September 22, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#the-enigma-of-marvin-minsky" id="toc-the-enigma-of-marvin-minsky" class="nav-link" data-scroll-target="#the-enigma-of-marvin-minsky">The enigma of Marvin Minsky</a>
  <ul class="collapse">
  <li><a href="#the-intellectual-history-of-minsky" id="toc-the-intellectual-history-of-minsky" class="nav-link" data-scroll-target="#the-intellectual-history-of-minsky">The intellectual history of Minsky</a></li>
  <li><a href="#sec-seymour-papert" id="toc-sec-seymour-papert" class="nav-link" data-scroll-target="#sec-seymour-papert">Seymour Papert</a></li>
  </ul></li>
  <li><a href="#the-perceptron-controversy" id="toc-the-perceptron-controversy" class="nav-link" data-scroll-target="#the-perceptron-controversy">The perceptron controversy</a>
  <ul class="collapse">
  <li><a href="#connectionism-19451970" id="toc-connectionism-19451970" class="nav-link" data-scroll-target="#connectionism-19451970">Connectionism, 1945–1970</a></li>
  <li><a href="#the-perceptron-controversy-1960s" id="toc-the-perceptron-controversy-1960s" class="nav-link" data-scroll-target="#the-perceptron-controversy-1960s">The perceptron controversy, 1960s</a></li>
  <li><a href="#connectionist-retrospectives-1990s" id="toc-connectionist-retrospectives-1990s" class="nav-link" data-scroll-target="#connectionist-retrospectives-1990s">Connectionist retrospectives, 1990s</a></li>
  </ul></li>
  <li><a href="#the-message-of-perceptrons" id="toc-the-message-of-perceptrons" class="nav-link" data-scroll-target="#the-message-of-perceptrons">The message of <em>Perceptrons</em></a>
  <ul class="collapse">
  <li><a href="#minsky-and-papert-struck-back" id="toc-minsky-and-papert-struck-back" class="nav-link" data-scroll-target="#minsky-and-papert-struck-back">Minsky and Papert struck back</a></li>
  <li><a href="#sec-papert-struck-back" id="toc-sec-papert-struck-back" class="nav-link" data-scroll-target="#sec-papert-struck-back">Papert struck back</a></li>
  </ul></li>
  <li><a href="#rebuttal-to-minsky-and-papert" id="toc-rebuttal-to-minsky-and-papert" class="nav-link" data-scroll-target="#rebuttal-to-minsky-and-papert">Rebuttal to Minsky and Papert</a>
  <ul class="collapse">
  <li><a href="#interpreting-the-xor-problem" id="toc-interpreting-the-xor-problem" class="nav-link" data-scroll-target="#interpreting-the-xor-problem">Interpreting the XOR problem</a></li>
  <li><a href="#where-did-they-go-wrong" id="toc-where-did-they-go-wrong" class="nav-link" data-scroll-target="#where-did-they-go-wrong">Where did they go wrong?</a></li>
  <li><a href="#what-is-left-of-perceptrons" id="toc-what-is-left-of-perceptrons" class="nav-link" data-scroll-target="#what-is-left-of-perceptrons">What is left of <em>Perceptrons</em>?</a></li>
  </ul></li>
  <li><a href="#appendix-the-three-camps-of-ai" id="toc-appendix-the-three-camps-of-ai" class="nav-link" data-scroll-target="#appendix-the-three-camps-of-ai">Appendix: The three camps of AI</a>
  <ul class="collapse">
  <li><a href="#cybernetic-ai" id="toc-cybernetic-ai" class="nav-link" data-scroll-target="#cybernetic-ai">Cybernetic AI</a></li>
  <li><a href="#symbolic-ai" id="toc-symbolic-ai" class="nav-link" data-scroll-target="#symbolic-ai">Symbolic AI</a></li>
  </ul></li>
  <li><a href="#appendix-the-chomskyans" id="toc-appendix-the-chomskyans" class="nav-link" data-scroll-target="#appendix-the-chomskyans">Appendix: the Chomskyans</a>
  <ul class="collapse">
  <li><a href="#noam-chomsky" id="toc-noam-chomsky" class="nav-link" data-scroll-target="#noam-chomsky">Noam Chomsky</a></li>
  <li><a href="#the-chomskyans" id="toc-the-chomskyans" class="nav-link" data-scroll-target="#the-chomskyans">The Chomskyans</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>During the 1950s and 1960s, the connectionist and symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This “first neural network winter” lasted until the rise of the connectionist school in the 1980s.</p>
<p>It is often stated that neural networks were killed off by the 1969 publication of <em>Perceptrons</em> by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:</p>
<ul>
<li>Minsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.</li>
<li>The mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.</li>
<li>By 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.</li>
</ul>
<p>The book achieved its mythical status as the “neural network killer” by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.</p>
<p>In the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from <em>Perceptrons</em>. Minsky and Papert objected that the <em>lessons</em> of <em>Perceptrons</em> still applied, but their objections and lessons had by then become irrelevant.</p>
<p>Minsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation–MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures, like backpropagation–MLP, as threats to his vision of a society with different but equally valid ways of knowing.</p>
<p>As of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven.</p>
</section>
<section id="the-enigma-of-marvin-minsky" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-enigma-of-marvin-minsky">The enigma of Marvin Minsky</h2>
<p>In a 1993 interview, <a href="https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen">Robert Hecht-Nielsen</a> recounted an encounter between <a href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a> and the neural network community in the late 1980s<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;This was corroborated by a contemporary news report on the International Conference on Neural Networks of 1988:</p>
<blockquote class="blockquote">
<p>Minsky who has been criticized by many for the conclusions he and Papert make in ‘Perceptrons,’ opened his defense with the line ‘Everybody seems to think I’m the devil.’ Then he made the statement, ‘I was wrong about Dreyfus too, but I haven’t admitted it yet,’ which brought another round of applause. (quoted in <span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 285</a>)</span>)</p>
</blockquote>
</div></div><blockquote class="blockquote">
<p>Minsky had gone to the same New York “science” high school as Frank Rosenblatt, a Cornell psychology Ph.D.&nbsp;whose “perceptron” neural network pattern recognition machine was receiving significant media attention. The wall-to-wall media coverage of Rosenblatt and his machine irked Minsky. One reason was that although Rosenblatt’s training was in “soft science,” his perceptron work was quite mathematical and quite sound—turf that Minsky, with his “hard science” Princeton mathematics Ph.D., didn’t feel Rosenblatt belonged on. Perhaps an even greater problem was the fact that the heart of the perceptron machine was a clever motor-driven potentiometer adaptive element that had been pioneered in the world’s first neurocomputer, the “SNARC”, which had been designed and built by Minsky several years earlier! In some ways, Minsky’s early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community. This view of his career history is not unknown to him. When he was invited to give the keynote address at a large neural network conference in the late 1980s to an absolutely rapt audience, he began with the words: “I am not the Devil!” <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, 303–5</a>)</span></p>
</blockquote>
<p>However, it appears that he had changed his mind later. As recounted by <a href="https://en.wikipedia.org/wiki/Terry_Sejnowski">Terry Sejnowski</a>:</p>
<blockquote class="blockquote">
<p>I was invited to attend the 2006 Dartmouth Artificial Intelligence Conference, “AI@50,” a look back at the seminal 1956 Summer Research Project on artificial intelligence held at Dartmouth and a look forward to the future of artificial intelligence. … These success stories had a common trajectory. In the past, computers were slow and only able to explore toy models with just a few parameters. But these toy models generalized poorly to real-world data. When abundant data were available and computers were much faster, it became possible to create more complex statistical models and to extract more features and relationships between the features.</p>
<p>In his summary talk at the end of the conference, Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: “You’re not working on the problem of general intelligence. You’re just working on applications.” …</p>
<p>There was a banquet on the last day of AI@50. At the end of the dinner, the five returning members of the 1956 Dartmouth Summer Research Project on Artificial Intelligence made brief remarks about the conference and the future of AI. In the question and answer period, I stood up and, turning to Minsky, said: “There is a belief in the neural network community that you are the devil who was responsible for the neural network winter in the 1970s. Are you the devil?” Minsky launched into a tirade about how we didn’t understand the mathematical limitations of our networks. I interrupted him—“Dr.&nbsp;Minsky, I asked you a yes or no question. Are you, or are you not, the devil?” He hesitated for a moment, then shouted out, “Yes, I am the devil!” <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 256–58</a>)</span></p>
</blockquote>
<p>What are we to make of the enigma of Minsky? Was he the devil or not?</p>
<section id="the-intellectual-history-of-minsky" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-intellectual-history-of-minsky">The intellectual history of Minsky</h3>
<p>During his undergraduate years, Minsky was deeply impressed by <a href="https://en.wikipedia.org/wiki/Andrew_M._Gleason">Andrew Gleason</a>,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and decided to work on pure mathematics, resulting in his 1951 undergraduate thesis <em>A Generalization of Kakutani’s Fixed-Point Theorem</em>, which extended an obscure fixed-point theorem of <a href="https://en.wikipedia.org/wiki/Shizuo_Kakutani">Kakutani</a> – not <a href="https://en.wikipedia.org/wiki/Kakutani_fixed-point_theorem">the famous version</a>, as Kakutani proved more than one fixed-point theorem.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><blockquote class="blockquote"><sup>2</sup>&nbsp;
<p>I asked Gleason how he was going to solve it. Gleason said he had a plan that consisted of three steps, each of which he thought would take him three years to work out. Our conversation must have taken place in 1947, when I was a sophomore. Well, the solution took him only about five more years … Gleason made me realize for the first time that mathematics was a landscape with discernible canyons and mountain passes, and things like that. In high school, I had seen mathematics simply as a bunch of skills that were fun to master – but I had never thought of it as a journey and a universe to explore. No one else I knew at that time had that vision, either. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
</div></div><div id="thm-kakutani" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Kakutani’s fixed point theorem on the sphere)</strong></span> If <span class="math inline">\(f\)</span> is an <span class="math inline">\(\mathbb{R}^2\)</span>-valued continuous function on the unit sphere in <span class="math inline">\(\mathbb{R}^3\)</span>, then for any side length <span class="math inline">\(r \in (0, \sqrt{3})\)</span>, there exist <span class="math inline">\(x_1, x_2, x_3\)</span> on the sphere forming an equilateral triangle with side length <span class="math inline">\(r\)</span>, such that <span class="math inline">\(f(x_1) = f(x_2) = f(x_3)\)</span>.</p>
<p>Equivalently, if <span class="math inline">\(x_1, x_2, x_3\)</span> form an equilateral triangle on the unit sphere, then there exists a rotation <span class="math inline">\(T\)</span> such that <span class="math inline">\(f(T(x_1)) = f(T(x_2)) = f(T(x_3))\)</span>.</p>
</div>
<p>Using knot theory, Minsky proved an extension where <span class="math inline">\(x_1, x_2, x_3\)</span> are three points of a square or a regular pentagon <span class="citation" data-cites="minskyMyUndergraduateThesis2011">(<a href="#ref-minskyMyUndergraduateThesis2011" role="doc-biblioref">Minsky 2011</a>)</span>. The manuscript “disappeared” <span class="citation" data-cites="minskySelectedPublicationsMarvin">(<a href="#ref-minskySelectedPublicationsMarvin" role="doc-biblioref">Minsky n.d.</a>)</span>.</p>
<blockquote class="blockquote">
<p>I wrote it up and gave it to Gleason. He read it and said, ‘You are a mathematician.’ Later, I showed the proof to Freeman Dyson, at the Institute for Advanced Study, and he amazed me with a proof <span class="citation" data-cites="dysonContinuousFunctionsDefined1951">(<a href="#ref-dysonContinuousFunctionsDefined1951" role="doc-biblioref">Dyson 1951</a>)</span> that there must be at least one square that has the same temperature at all four vertices. He had found somewhere in my proof a final remnant of unused logic. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>He then became interested in neural networks and reinforcement learning, and constructed a very simple electromechanical machine called SNARC.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The SNARC machine is a recurrent neural network that performs reinforcement learning by the Hebbian learning rule. It simulates a mouse running around a maze, while the operator watches an indicator light showing the mouse. The operator can press a button as a reward signal, which would cause an electric motor to turn a chain. The chain is clutched to rheostats that connect the neurons, with the stretch of the clutch being proportional to the charge in a capacitor. During the operation of the neural network, the capacitor charges up if there is neural co-activation on the connection, and decays naturally, thus serving as a short-term memory. When the reward button is pressed, the clutches turn by an amount proportional to the co-activation of neural connections, thereby completing the Hebbian learning.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;It was published as <span class="citation" data-cites="minskyNeuralanalogueCalculatorBased1952">(<a href="#ref-minskyNeuralanalogueCalculatorBased1952" role="doc-biblioref">Minsky 1952</a>)</span>, but the document is not available online, and I could only piece together a possible reconstruction from the fragments of information.</p></div></div><p>Minsky was impressed by how well it worked. The machine was designed to simulate one mouse, but by some kind of error it simulated multiple mice, and yet it still worked.</p>
<blockquote class="blockquote">
<p>The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. … In those days, even a radio set with twenty tubes tended to fail a lot. I don’t think we ever debugged our machine completely, but that didn’t matter. By having this crazy random design, it was almost sure to work, no matter how you built it. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>This was the last we saw of Minsky’s work with random neural networks. He had crossed the Rubicon, away from the land of brute reason and into the land of genuine insight.</p>
<blockquote class="blockquote">
<p>I had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. … Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don’t you try to find a way to build a nervous system that will just spontaneously create it?’ Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn’t afford to try to build a machine like that. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>For his PhD thesis, Minsky worked on the mathematical theory of McCulloch–Pitts neural networks. In style, it was a fine piece of classical mathematics <span class="citation" data-cites="minskyTheoryNeuralanalogReinforcement1954">(<a href="#ref-minskyTheoryNeuralanalogReinforcement1954" role="doc-biblioref">Minsky 1954</a>)</span>. Minsky would go on to write <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967, chap. 3</a>)</span>, still the best introduction to McCulloch–Pitts neural networks.</p>
<blockquote class="blockquote">
<p>Minsky’s doctoral dissertation in mathematics from Princeton in 1954 was a theoretical and experimental study of computing with neural networks. He had even built small networks from electronic parts to see how they behaved. The story I heard when I was a graduate student at Princeton in physics was that there wasn’t anyone in the Mathematics Department who was qualified to assess his dissertation, so they sent it to the mathematicians at the Institute for Advanced Study in Princeton who, it was said, talked to God. The reply that came back was, “If this isn’t mathematics today, someday it will be,” which was good enough to earn Minsky his PhD. <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 259</a>)</span></p>
</blockquote>
<p>Reading the story, I recalled <a href="https://web.archive.org/web/20231223082954/http://www.catb.org/jargon/html/koans.html">“Sussman attains enlightenment”</a>, a <a href="https://simple.wikipedia.org/wiki/Hacker_koan">hacker koan</a> about Minsky and his student <a href="https://en.wikipedia.org/wiki/Gerald_Jay_Sussman">Sussman</a> <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;This is based on a true story.</p>
<blockquote class="blockquote">
<p>… Sussman told Minsky that he was using a certain randomizing technique in his program because he didn’t want the machine to have any preconceived notions. Minsky said, “Well, it has them, it’s just that you don’t know what they are.” It was the most profound thing Gerry Sussman had ever heard. And Minsky continued, telling him that the world is built a certain way, and the most important thing we can do with the world is avoid randomness, and figure out ways by which things can be planned. <span class="citation" data-cites="levyHackersHeroesComputer2010">(<a href="#ref-levyHackersHeroesComputer2010" role="doc-biblioref">Levy 2010, 110–11</a>)</span></p>
</blockquote>
</div></div><blockquote class="blockquote">
<p>In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?”, asked Minsky. “I am training a randomly wired neural net to play Tic-Tac-Toe” Sussman replied. “Why is the net wired randomly?”, asked Minsky. “I do not want it to have any preconceptions of how to play”, Sussman said. Minsky then shut his eyes. “Why do you close your eyes?”, Sussman asked his teacher. “So that the room will be empty.” At that moment, Sussman was enlightened.</p>
</blockquote>
<p>As for Sussman, I knew him for two things: writing the <a href="https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs"><em>SICP</em></a> book, and being the coordinator of the infamous <em>summer vision project</em> that was to construct “a significant part of a visual system” in a single summer, using only undergraduate student researchers. A brief read of his “<a href="https://web.archive.org/web/20231007210930/http://aurellem.org/thoughts/html/sussman-reading-list.html">reading list</a>” shows where his loyalties lie: firmly in <a href="https://en.wikipedia.org/wiki/Neats_and_scruffies">the school of neats</a>.</p>
<p><span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 28</a>)</span> recounts the background of the summer vision project:</p>
<blockquote class="blockquote">
<p>In the 1960s, the MIT AI Lab received a large grant from a military research agency to build a robot that could play Ping-Pong. I once heard a story that the principal investigator forgot to ask for money in the grant proposal to build a vision system for the robot, so he assigned the problem to a graduate student as a summer project. I once asked Marvin Minsky whether the story was true. He snapped back that I had it wrong: “We assigned the problem to undergraduate students.”</p>
</blockquote>
<p>After rejecting neural networks, Minsky became a leading researcher in AI. His style of AI is typically described as “symbolic AI”, although a more accurate description would be <em>The Society of Mind</em> (SoM). Minsky developed the SoM in the 1960s and 1970s with his long-time collaborator, <a href="#sec-seymour-papert">Seymour Papert</a>, inspired by their difficulty with building robots, and published the definitive account in <span class="citation" data-cites="minskySocietyMind1988">(<a href="#ref-minskySocietyMind1988" role="doc-biblioref">Minsky 1988</a>)</span>. The SoM thesis states that “any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all”.</p>
<p>Stated in this way, it <em>seems</em> patently compatible with neural networks, but only on the surface. Minsky concretely described how he expected a Society of Mind to work, based on his attempts at making Builder, a robot that can play with blocks:</p>
<blockquote class="blockquote">
<p>Both my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children’s building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children’s minds when they learn to “play” with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we’d learned about psychology, that led us to many ideas about societies of mind.</p>
<p>To do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn’t work so well, we added more programs that used the fingers’ feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the Eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do—and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside—or else to correct the mistakes that occurred. … Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent—and yet all this proceeds so automatically that we regard it as “ordinary common sense.” <span class="citation" data-cites="minskySocietyMind1988">(<a href="#ref-minskySocietyMind1988" role="doc-biblioref">Minsky 1988, sec. 2.5</a>)</span></p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/minsky_robot_arm_blocks.jpg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Marvin Minsky and Builder the robot. It’s hard to tell who is having more fun here.</figcaption>
</figure>
</div>
<p>From the concrete description, as well as the many attractive illustrations in the book, it is clear that Minsky intended the “Society of Mind” to be a uniform computing substrate (silicon or carbon) upon which millions of little symbolic programs are running, each capable of running some specific task, each describable by a distinct and small piece of symbolic program. They cannot be mere perceptrons in a uniform block of neural network, or mere logic gates in a uniform block of CPU.</p>
<p>In his 1988 book, Minsky described dozens of these heterogeneous components he thought might make up a Society of Mind. However, the precise details are not relevant,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> as he freely admitted that they are conjectured. He was only adamant about the overarching scheme: heterogeneous little separated components, not a homogeneous big connected piece.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;The conjectured components included “K-lines”, “nomes”, “nemes”, “frames”, “frame-arrays”, etc. Although Minsky meant for this SoM project to last a very long time, building up to general intelligence brick by brick, my literature search shows that there had been no new development since the 2000s, so the overview <span class="citation" data-cites="singhExaminingSocietyMind2003">(<a href="#ref-singhExaminingSocietyMind2003" role="doc-biblioref">Singh 2003</a>)</span> still represents the SOTA of SoM.</p></div></div><p>Perhaps a modern reincarnation of such an idea would be the dream of Internet agents operating in a digital economy, populated by agents performing simple tasks like spam filtering, listening for the price of petroleum, etc. Some agents would interface with reality, while others would interface with agents. Some agents are organized at a higher level into DAOs, created by a small committee of simple “manager agents” serving as the interface and coordinators for other agents. DAOs can interface with other DAOs through little speaker-agents, which consist of a simple text filter for the torrent of information and then outsource to text-weaving agents to compose the actual messages they send out.</p>
</section>
<section id="sec-seymour-papert" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-seymour-papert">Seymour Papert</h3>
<p><a href="https://en.wikipedia.org/wiki/Seymour_Papert">Seymour Papert</a>, the long-time collaborator of Minsky, was the second author of <em>Perceptrons</em>. To unlock the enigma of Minsky, we must look into Papert’s past as well.</p>
<p>In 1958, after earning a doctorate in mathematics, he met <a href="https://en.wikipedia.org/wiki/Jean_Piaget">Jean Piaget</a> and became his pupil for four years. This experience had a formative effect on Papert. Piaget’s work was an important influence on the <a href="https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)">constructivism philosophy in education</a>, and Papert would go on to bring constructivism from books to classrooms. He was particularly hopeful that computers can realize the constructivist dream of unlocking the kaleidoscopic creativity that a child can construct.</p>
<p>The main theme of Jean Piaget’s work was developmental psychology – how children’s understanding of the world changes as they grow up. What goes on in their mind as they progressively understand that things fall down, what dogs are, and that solid steel sinks but hollow steel might float? Piaget discovered that children did not simply start with a blank sheet of paper and gradually fill in sketchy details of the true model. Instead they constructed little models of small facets of reality that would be modified or completely replaced as they encounter new phenomena that their old models cannot explain. In this way, Piaget claimed that children are “little scientists”.</p>
<p>A small example illustrates the idea. When children see that a leaf floats on water, but a stone sinks, they add a rule “Soft things float, while hard things sink.”. Then, they see that a hard plastic boat floats too, so they add a rule “Except hard and light things also float.”. Then, they see that a large boat also floats, so they rewrite the entire model to “Flat-bottomed things float, while small-bottomed things sink.”. And so on.</p>
<p>There are conservative and radical ways of using Piaget’s research for pedagogy. The conservative approach involves studying how children construct their scientific theories and identifying a sequence of evidence to present to these young scientists so they can quickly reach scientific orthodoxy. For example, we might show children videos of curling and air hockey, then let them play with an air hockey table, following this with guided exercises, so they race through <a href="https://en.wikipedia.org/wiki/Animism">animism</a>, <a href="https://en.wikipedia.org/wiki/Aristotelian_physics">Aristotelian physics</a>, <a href="https://en.wikipedia.org/wiki/Theory_of_impetus">impetus theory</a>, etc, and end up with Newton’s laws of motion.</p>
<p>The radical way is to decenter the orthodoxy and let a thousand heterodoxies bloom. Why go for the orthodoxy, when the <a href="https://en.wikipedia.org/wiki/Duhem%E2%80%93Quine_thesis">Duhem–Quine thesis</a> tells us that evidence is never enough to constrain us to only one orthodoxy? And given that objectively no theory deserves the high name of “orthodoxy”, how did the scientific “orthodoxy” become dominant? A critical analysis of the history shows that its dominance over Aboriginal and woman ways of knowing is merely a historical accident due to an alliance with <a href="https://en.wikipedia.org/wiki/World-systems_theory">the hegemonic reason of the metropole over the periphery</a>.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;This is not a joke, since <a href="https://en.wikipedia.org/wiki/Decoloniality">decolonial studies</a> literally begin with this assumption.</p></div></div><p>Papert went with the radical way.</p>
<p>After four years of study under Piaget, he arrived in MIT in 1963, and began working with Minsky on various topics, including the <a href="https://en.wikipedia.org/wiki/Turtle_(robot)">Logo Turtle robot</a>, and the <em>Perceptrons</em> book. The computer revolution was starting, and Papert saw computers as a way to bring radical constructivism to children.</p>
<p>In the real world, phenomena are limited by nature, and aspiring little heterodoxy-builders are limited by their ability to construct theories and check their consequences. In the computer world, every child could program and construct “microworlds” from their own theories. Thus, computers would bring constructivism to the classroom. Furthermore, the constructed world inside computers could then flow out to the physical world via robots. This is why Papert worked on both <em>Logo</em> the programming language and <em>Logo</em> the turtle robots. In his words, he intended to fight “instructionism” with “constructionism” by bringing the power of the computer to every child, so that they would grow up to be “<a href="https://en.wikipedia.org/wiki/The_Savage_Mind">bricoleurs</a>”, working with whatever little tool they have available doing whatever is necessary to accomplish little things. This is a vital piece in his overarching project of epistemological pluralism, to liberate heterodoxical ways of knowing <span class="citation" data-cites="papertChildrenMachineRethinking1994">(<a href="#ref-papertChildrenMachineRethinking1994" role="doc-biblioref">S. A. Papert 1994, chap. 7</a>)</span>:</p>
<blockquote class="blockquote">
<p>Traditional education codifies what it thinks citizens need to know and sets out to feed children this “fish.” Constructionism is built on the assumption that children will do best by finding (“fishing”) for themselves the specific knowledge they need … it is as well to have good fishing lines, which is why we need computers, and to know the location of rich waters, which is why we need to develop a large range of mathetically rich activities or “microworlds.”</p>
<p>… School math, like the ideology, though not necessarily the practice, of modern science, is based on the ideal of generality – the single, universally correct method that will work for all problems and for all people. Bricolage is a metaphor for the ways of the old-fashioned traveling tinker, the jack-of-all-trades who knocks on the door offering to fix whatever is broken. Faced with a job, the tinker rummages in his bag of assorted tools to find one that will fit the problem at hand and, if one tool does nor work for the job, simply tries another without ever being upset in the slightest by the lack of generality. The basic tenets of bricolage as a methodology for intellectual activity are: Use what you’ve got, improvise, make do. And for the true bricoleur, the tools in the bag will have been selected over a long time by a process determined by more than pragmatic utility. These mental tools will be as well worn and comfortable as the physical tools of the traveling tinker; they will give a sense of the familiar, of being at ease with oneself …</p>
<p>Kitchen math provides a clear demonstration of bricolage in its seamless connection with a surrounding ongoing activity that provides the tinker’s bag of tricks and tools. The opposite of bricolage would be to leave the “cooking microworld” for a “math world,” to work the fractions problem using a calculator or, more likely in this case, mental arithmetic. But the practitioner of kitchen math, as a good bricoleur, does not stop cooking and turn to math; on the contrary, the mathematical manipulations of ingredients would be indistinguishable to an outside observer from the culinary manipulations.</p>
<p>… The traditional epistemology is based on the proposition, so closely linked to the medium of text-written and especially printed. Bricolage and concrete thinking always existed but were marginalized in scholarly contexts by the privileged position of text. As we move into the computer age and new and more dynamic media emerge, this will change.</p>
</blockquote>
<p>According to Papert, his project is <strong>epistemological pluralism</strong>, or promoting different ways of knowing:</p>
<blockquote class="blockquote">
<p>The diversity of approaches to programming suggests that equal access to even the most basic elements of computation requires accepting the validity of multiple ways of knowing and thinking, an epistemological pluralism. Here we use the word epistemology in a sense closer to Piaget’s than to the philosopher’s. In the traditional usage, the goal of epistemology is to inquire into the nature of knowledge and the conditions of its validity; and only one form of knowledge, the propositional, is taken to be valid. The step taken by Piaget in his definition of <em>epistemologie genetique</em> was to eschew inquiry into the “true” nature of knowledge in favor of a comparative study of the diverse nature of different kinds of knowledge, in his case the kinds encountered in children of different ages. We differ from Piaget on an important point, however. Where he saw diverse forms of knowledge in terms of stages to a finite end point of formal reason, we see different approaches to knowledge as styles, <strong>each equally valid on its own terms</strong>.</p>
<p>… The development of a new computer culture would require more than environments where there is permission to work with highly personal approaches. It would require a new social construction of the computer, with a new set of intellectual and emotional values more like those applied to harpsichords than hammers. Since, increasingly, computers are the tools people use to write, to design, to play with ideas and shapes and images, they should be addressed with a language that reflects the full range of human experiences and abilities. Changes in this direction would necessitate the reconstruction of our cultural assumptions about formal logic as the “law of thought.” This point brings us full circle to where we began, with the assertion that <strong>epistemological pluralism is a necessary condition for a more inclusive computer culture</strong>.</p>
<p><span class="citation" data-cites="turkleEpistemologicalPluralismStyles1990">(<a href="#ref-turkleEpistemologicalPluralismStyles1990" role="doc-biblioref">Turkle and Papert 1990</a>)</span></p>
</blockquote>
<p>The project of epistemological pluralism erupted into public consciousness during the “<a href="https://en.wikipedia.org/wiki/Science_wars">Science Wars</a>” of 1990s. After that, it had stayed rather quiet.</p>
</section>
</section>
<section id="the-perceptron-controversy" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-perceptron-controversy">The perceptron controversy</h2>
<section id="connectionism-19451970" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="connectionism-19451970">Connectionism, 1945–1970</h3>
<p>In the early days, there were several centers of connectionist research, clustered around <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a>, <a href="https://en.wikipedia.org/wiki/Bernard_Widrow">Bernard Widrow</a>, and the <a href="https://en.wikipedia.org/wiki/SRI_International">Stanford Research Institute</a> (SRI). Out of those centers of research, Minsky and Papert targeted mostly Rosenblatt’s research.</p>
<p>Frank Rosenblatt’s research had three modes: mathematical theory, experiments on bespoke machines, such as the Mark I Perceptron and the Tobermory, and experiments on serial digital computers, usually IBM machines. He was strongly inclined to building two-layered perceptron machines where the first layer was <em>fixed</em> 0-1 weights, and only the second layer contained real-valued weights learned by the perceptron learning rule. This is precisely the abstract model of the perceptron machine used by Minsky and Papert.</p>
<p>After 4 years of research, he published a summary of his work in <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962</a>)</span>. In the book, he noted that there were many problems that the perceptron machines could not learn well. As summarized in <span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 116–21</a>)</span>,</p>
<blockquote class="blockquote">
<p>… two stimuli (presented one after another) had to occupy nearly the same area of the retina in order to be classified as similar. … The lack of an adequate preprocessing system meant that a set of association units had to be dedicated to the recognition of each possible object, and this created an excessively large layer of association units in the perceptron. … Other problems were excessive learning time, excessive dependence on external evaluation (supervision), and lack of ability to separate essential parts in a complex environment. Rosenblatt (1962, pp.&nbsp;309-310) included the ‘figure-ground’ or ‘connectedness’ problem in this last point.</p>
<blockquote class="blockquote">
<p>A number of perceptrons analyzed in the preceding chapters have been analyzed in a purely formal way, yielding equations which are not readily translated into numbers. This is particularly true in the case of the four-layer and cross-coupled systems, where the generality of the equations is reflected in the obscurity of their implications. … The previous questions [from the first to the twelfth] are all in the nature of ‘mopping-up’ operations in areas where some degree of performance is known to be possible . . . [However,] the problems of figure-ground separation (or recognition of unity) and topological relation recognition represent new territory, against which few inroads have been made.” (Rosenblatt, 1962a, pp.&nbsp;580-581)</p>
</blockquote>
</blockquote>
<p>Almost every one of these problems was specifically targeted by the <em>Perceptrons</em> book. For example, the difficulty of testing for “connectedness” was a centerpiece of the entire book, the difficulty of recognizing symmetry was studied by “stratification” and shown to have exponentially growing coefficients (Chapter 7), the requirement for “had to occupy nearly the same area of the retina” was targeted by studies on the limitations of “diameter-limited perceptrons” (Chapter 8), the “figure-ground problem” was targeted by showing “recognition-in-context” has infinite order (Section 6.6), the “generality of the equations is reflected in the obscurity of their implications” was targeted by comments such as “if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head” (Section 13.2), etc.</p>
<p>Bernard Widrow worked mostly in collaboration with <a href="https://en.wikipedia.org/wiki/Marcian_Hoff">Marcian Hoff</a>. Their work is detailed in my essay <a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/#bernard-widrow-and-marcian-hoff"><em>The Backstory of Backpropagation</em></a>. In short, they first developed a least-mean-square gradient descent method to train a single perceptron, then proceeded to two-layered perceptrons and predictably failed to develop backpropagation, as the activation function is not differentiable. Thereafter, Widrow gave up neural networks until learning of backpropagation in the 1980s.</p>
<blockquote class="blockquote">
<p>Widrow and his students developed uses for the Adaline and Madaline. Early applications included, among others, speech and pattern recognition, weather forecasting, and adaptive controls. Work then switched to adaptive filtering and adaptive signal processing after attempts to develop learning rules for networks with multiple adaptive layers were unsuccessful. … After 20 years of research in adaptive signal processing, the work in Widrow’s laboratory has once again returned to neural networks.</p>
<p><span class="citation" data-cites="widrow30YearsAdaptive1990">(<a href="#ref-widrow30YearsAdaptive1990" role="doc-biblioref">Widrow and Lehr 1990</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>At the time that Hoff left, about 1965 or 1966, we had already had lots of troubles with neural nets. My enthusiasm had dropped. But we were beginning to have successful adaptive filters, in other words, finding good applications. … So we stopped, basically stopped on neural nets, and began on adaptive antennas very strongly.</p>
<p>Interview with Widrow, quoted in <span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 129–30</a>)</span></p>
</blockquote>
<p>SRI had a strong AI program, with luminaries such as <a href="https://en.wikipedia.org/wiki/Nils_John_Nilsson">Nils Nilsson</a>, <a href="https://en.wikipedia.org/wiki/Charles_Rosen_(scientist)">Charles Rosen</a>, <a href="https://en.wikipedia.org/wiki/Richard_O._Duda">Duda</a>, and <a href="https://en.wikipedia.org/wiki/Peter_E._Hart">Hart</a>. At first they worked on a series of systems, MINOS I to III.</p>
<blockquote class="blockquote">
<p>When I first interviewed for a position at SRI in 1961, I was warned by one researcher there against joining research on neural networks. Such research, he claimed, was “premature,” and my involvement in it could damage my reputation.</p>
<p><span class="citation" data-cites="nilssonQuestArtificialIntelligence2009">(<a href="#ref-nilssonQuestArtificialIntelligence2009" role="doc-biblioref">Nilsson 2009, chap. 24.2</a>)</span></p>
</blockquote>
<p>MINOS II was representative of the whole series. Made in 1962, it had 3 layers: input, hidden, output, but only one was trainable. The input-to-hidden layer consists of 100 photomasks. That is, given an input image, that image is filtered through a mask, and the light is focussed by a convex lens to a single photosensitive pixel. If the light level exceeds a threshold, it is a 1. Else, it is a 0. Repeat this 100 times, one for each mask, and we have converted an image to 100 binary bits.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/photomask_featurizer_Minsky_Papert_1988_fig_13_1.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Applying a list of photomasks on an input image, resulting in a list of binary bits that featurizes the image, which a perceptron will be able to classify. <span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, fig. 13.1</a>)</span></figcaption>
</figure>
</div>
<p>In the hidden-to-output layer, they ran the standard perceptron learning rule. Presumably, they used <em>hand-designed photomasks</em> because they also had no better training method than the perceptron learning rule. Since they used 0-1 activation functions like everyone else, they were frustrated by the same problem of <em>not doing backpropagation</em>, so they switched to symbolic AI techniques around 1965.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/MINOS_II.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The weight matrix of the MINOS II pattern recognition system, consisting of a grid of magnetic cores. The magnetization of a core is proportional to its matrix weight. Matrix weights that are quite literally weighty. The weight matrices are swappable. <span class="citation" data-cites="rosenResearchDevelopmentProgram1965">(<a href="#ref-rosenResearchDevelopmentProgram1965" role="doc-biblioref">Rosen, Nilsson, and Adams 1965</a>)</span></figcaption>
</figure>
</div>
<p>In 1973, Duda and Hart published the famous “Duda and Hart” book on pattern classification <span class="citation" data-cites="dudaPatternClassificationScene1973">(<a href="#ref-dudaPatternClassificationScene1973" role="doc-biblioref">Duda and Hart 1973</a>)</span>. The book contained two halves. The first half was statistical: Bayes, nearest neighbors, perceptron, clustering, etc. The second half was on scene analysis, a symbolic-AI method for computer vision. Indeed, <a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#chapter-13">Minsky and Papert promoted it as superior to perceptron networks</a>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <a href="https://en.wikipedia.org/wiki/Shakey_the_robot">Shakey the robot</a>, built between 1966 and 1972, was a <em>tour de force</em> of scene analysis, and it could move around a mock-up of an office building, pushing around cubes along the way. Its program was written in LISP, the staple programming language for symbolic AI.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;It is instructive to compare the first edition with the second, published in 2001 <span class="citation" data-cites="dudaPatternClassification2001">(<a href="#ref-dudaPatternClassification2001" role="doc-biblioref">Duda, Hart, and Stork 2001</a>)</span>. It had become almost completely statistical. There were new chapters on neural networks, Boltzmann machines, decision trees, and so on. In contrast, scene analysis was completely removed.</p>
<p>It says something about the obsolescence of scene analysis even in 2001, as Duda and Hart deleted half of their most famous book just to avoid talking about it. In fact, the only mention of “scene analysis” is a condemnation:</p>
<blockquote class="blockquote">
<p>Some of the earliest work on three-dimensional object recognition relied on complex grammars which described the relationships of corners and edges, in block structures such arches and towers. It was found that such systems were very brittle; they failed whenever there were errors in feature extraction, due to occlusion and even minor misspecifications of the model. For the most part, then, grammatical methods have been abandoned for object recognition and scene analysis. <span class="citation" data-cites="dudaPatternClassification2001">(<a href="#ref-dudaPatternClassification2001" role="doc-biblioref">Duda, Hart, and Stork 2001, sec. 8.8</a>)</span></p>
</blockquote>
</div></div><blockquote class="blockquote">
<p>I got very interested for a while in the problem of training more than one layer of weights, and was not able to make very much progress on that problem. … When we stopped the neural net studies at SRI, research money was running out, and we began looking for new ideas. (Nilsson, interview)</p>
<p>About 1965 or 1966 we decided that we were more interested in the other artificial intelligence techniques. … Our group never solved the problem of training more than one layer of weights in an automatic fashion. We never solved that problem. That was most critical. Everybody was aware of that problem. (Rosen, interview)</p>
<p><span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 131–33</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Bill Ridgway (one of Bernard Widrow’s Stanford students) adjusted weights in the first layer of what he called a MADALINE. We had a similar scheme for adjusting weights in the first layer of the MINOS II and MINOS III neural network machines at SRI. Others used various statistical techniques to set weight values. But what stymied us all was how to change weights in more than one layer of multilayer networks. (I recall Charles Rosen, the leader of our group, sitting in his office with yellow quadrille tablets hand-simulating his ever-inventive schemes for making weight changes; none seemed to work out.)</p>
<p><span class="citation" data-cites="nilssonQuestArtificialIntelligence2009">(<a href="#ref-nilssonQuestArtificialIntelligence2009" role="doc-biblioref">Nilsson 2009, chap. 29.4</a>)</span></p>
</blockquote>
</section>
<section id="the-perceptron-controversy-1960s" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-perceptron-controversy-1960s">The perceptron controversy, 1960s</h3>
<blockquote class="blockquote">
<p>In the middle nineteen-sixties, Papert and Minsky set out to kill the Perceptron, or, at least, to establish its limitations – a task that Minsky felt was a sort of social service they could perform for the artificial-intelligence community. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>Although the book was published only in 1969, close to the end of the perceptron controversy, the influence of Minsky and Papert had been felt years earlier as they attended conferences and disseminated their ideas through talks and preprints, sometimes quarreling on stage. Both sides had their motivations and the conflict was real.</p>
<blockquote class="blockquote">
<p>In order to show the extent of the perceptron controversy, it is interesting to repeat some of the rhetorical expressions that were used in it: ‘many remember as great spectator sport the quarrels Minsky and Rosenblatt had;’ ‘Rosenblatt irritated a lot of people;’ ‘Rosenblatt was given to steady and extravagant statements about the performance of his machine;’ ‘Rosenblatt was a press agent’s dream, a real medicine man;’ ‘to hear Rosenblatt tell it, his machine was capable of fantastic things;’ ‘they disparaged everything Rosenblatt did, and most of what ONR did in supporting him;’ ‘a pack of happy bloodhounds;’ ‘Minsky knocked the hell out of our perceptron business;’ ‘Minsky and his crew thought that Rosenblatt’s work was a waste of time, and Minsky certainly thought that our work at SRI was a waste of time;’ ‘Minsky and Papert set out to kill the perceptron, it was a sort of social service they could perform for the Al community;’ ‘there was some hostility;’ ‘we became involved with a somewhat therapeutic compulsion;’ ‘a misconception that would threaten to haunt artificial intelligence;’ ‘the mystique surrounding such machines.’ These rhetorical expressions show the extent (the heat) of the perceptron controversy beyond doubt. <span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991, 112</a>)</span></p>
</blockquote>
<p>Charles Rosen of SRI recalls:</p>
<blockquote class="blockquote">
<p>Minsky and his crew thought that Frank Rosenblatt’s work was a waste of time, and they certainly thought that our work at SRI was a waste of time. Minsky really didn’t believe in perceptrons, he didn’t think it was the way to go. I know he knocked the hell out of our perceptron business. <span class="citation" data-cites="olazaranSociologicalHistoryNeural1993">(<a href="#ref-olazaranSociologicalHistoryNeural1993" role="doc-biblioref">Olazaran 1993, 622</a>)</span></p>
</blockquote>
<p>When <em>Perceptrons</em> was finally published in 1969, the connectionist camp was already deserted. The SRI group had switched to symbolic AI projects; Widrow’s group had switched to adapting single perceptrons to adaptive filtering; Frank Rosenblatt was still labouring, isolated, with dwindling funds, until his early death in 1971.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;The 1972 reprinting of <em>Perceptrons</em> included a handwritten note, “In memory of Frank Rosenblatt”. This was not an ironic dedication, as Minsky and Rosenblatt were personally friendly, although their research paradigms had been fighting for dominance.</p></div></div><p>During the last days of Rosenblatt, he worked on a massive expansion of the Mark I Perceptron, the Tobermory (1961–1967). Named after a talking cat, it was built for speech recognition. It had 4 layers with 45-1600-1000-12 neurons, and 12,000 adjustable weights implemented with tape-wound magnetic cores. As usual for Rosenblatt, these adjustable weights are all in the last layer (<span class="math inline">\(12000 = 1000 \times 12\)</span>). By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines. <span class="citation" data-cites="nagyNeuralNetworksthenNow1991">(<a href="#ref-nagyNeuralNetworksthenNow1991" role="doc-biblioref">George Nagy 1991</a>)</span> Indeed, often during the history of neural networks, someone would think “this calls for a purpose-made computer” and a few years later, Moore’s law obsoleted their effort. A kind of hardware bitter lesson.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Tobermory.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Tobermory schematic. <span class="citation" data-cites="nagySystemCircuitDesigns1963">(<a href="#ref-nagySystemCircuitDesigns1963" role="doc-biblioref">GEORGE Nagy 1963</a>)</span></figcaption>
</figure>
</div>
<p><span class="citation" data-cites="olazaranHistoricalSociologyNeural1991">(<a href="#ref-olazaranHistoricalSociologyNeural1991" role="doc-biblioref">Olazaran 1991</a>)</span> gathered evidence that the publication of <em>Perceptrons</em> was not the cause but a “marker event” for the end of the perceptron controversy and the ascendancy of the symbolic AI school. The book was not the neural network killer, but its epitaph.</p>
</section>
<section id="connectionist-retrospectives-1990s" class="level3">
<h3 class="anchored" data-anchor-id="connectionist-retrospectives-1990s">Connectionist retrospectives, 1990s</h3>
<p>Following the resurgence of connectionism in the 1980s, Anderson and Rosenfeld conducted interviews with prominent connectionists throughout the 1990s, compiled in <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span>. The perceptron controversy is mentioned several times. Reading the interviews gives one a distinct feeling of <a href="https://en.wikipedia.org/wiki/In_a_Grove"><em>Rashomon</em></a>. The same events are recounted from multiple perspectives. I will excerpt some of the most important ones for the essay.</p>
<p>Jack D. Cowan gave an “eyewitness account” of Minsky and Papert’s role in the controversy, before the publication of the book in 1969.</p>
<blockquote class="blockquote">
<p>ER: I’m curious about one thing. You said that Minsky and Papert first presented their notions about exclusive-OR in the <em>Perceptron</em> work [in a 1965 conference].</p>
<p>JC: Well, they first presented their notions about the limitations of perceptrons and what they could and couldn’t do.</p>
<p>ER: They hadn’t gotten to exclusive-OR yet?</p>
<p>JC: They had, but that wasn’t a central issue for them. The essential issue was, suppose you had diameter-limited receptive fields in a perceptron, what could it compute?</p>
<p>ER: How was that received at that first conference?</p>
<p>JC: Both of them were quite persuasive speakers, and it was well received. What came across was the fact that you had to put some structure into the perceptron to get it to do anything, but there weren’t a lot of things it could do. The reason was that it didn’t have hidden units. It was clear that without hidden units, nothing important could be done, and they claimed that the problem of programming the hidden units was not solvable. They discouraged a lot of research and that was wrong. … Everywhere there were people working on perceptrons, but they weren’t working hard on them. Then along came Minsky and Papert’s preprints that they sent out long before they published their book. There were preprints circulating in which they demolished Rosenblatt’s claims for the early perceptrons. In those days, things really did damp down. There’s no question that after ’62 there was a quiet period in the field.</p>
<p>ER: Robert Hecht-Nielsen has told me stories that long before Minsky and Papert ever committed anything to a paper that they delivered at a conference or published anywhere, they were going down to ARPA and saying, “You know, this is the wrong way to go. It shouldn’t be a biological model; it should be a logical model.”</p>
<p>JC: I think that’s probably right. In those days they were really quite hostile to neural networks. I can remember having a discussion with Seymour … in the ’60s. We were talking about visual illusions. He felt that they were all higher-level effects that had nothing to do with neural networks as such. They needed a different, a top-down approach to understand. By then he had become a real, a true opponent of neural networks. I think Marvin had the same feelings as well. To some extent, David Marr had those feelings too. After he got to the AI lab, I think he got converted to that way of thinking. Then Tommy Poggio essentially persuaded him otherwise.</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Teuvo_Kohonen">Teuvo Kohonen</a> seemed also angry at the <a href="https://en.wikipedia.org/wiki/Cognitive_revolution">Chomskyan school</a>, for reasons I sketched out in the <a href="#appendix-the-chomskyans">appendix on the Chomskyans</a>.</p>
<blockquote class="blockquote">
<p>I was one of the people suffering from Minsky and Papert’s book [Perceptrons] because it went roughly this way: you start telling somebody about your work, and this visitor or whoever you talk to says, “Don’t you know that this area is dead?” It is something like what we experienced in the pattern recognition society when everything started to be structural and grammatical and semantic and so on. If somebody said, “I’m doing research on the statistical pattern recognition,” then came this remark, “Hey, don’t you know that is a dead idea already?”</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Michael_A._Arbib">Michael A. Arbib</a> thought the book did not cause the neural network winter, but rather caused by the change in funding.</p>
<blockquote class="blockquote">
<p>Minsky and Papert basically said that if you limit your networks to one layer in depth, then, unless you have very complicated individual neurons, you can’t do very much. This is not too surprising. … Many people see the book as what killed neural nets, but I really don’t think that’s true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. I think it was more that a younger generation of computer scientists who didn’t know the earlier work may have used the book as justification for sticking with “straight AI” and ignoring neural nets.</p>
</blockquote>
<p>Bernard Widrow concurred.</p>
<blockquote class="blockquote">
<p>I looked at that book, and I saw that they’d done some serious work here, and there was some good mathematics in this book, but I said, “My God, what a hatchet job.” I was so relieved that they called this thing the perceptron rather than the Adaline because actually what they were mostly talking about was the Adaline, not the perceptron. I felt that they had sufficiently narrowly defined what the perceptron was, that they were able to prove that it could do practically nothing. Long, long, long before that book, I was already successfully adapting Madaline [Madaline = many Adalines], which is a whole bunch of neural elements. All this worry and agony over the limitations of linear separability, which is the main theme of the book, was long overcome.</p>
<p>We had already stopped working on neural nets. As far as I knew, there wasn’t anybody working on neural nets when that book came out. I couldn’t understand what the point of it was, why the hell they did it. But I know how long it takes to write a book. I figured that they must have gotten inspired to write that book really early on to squelch the field, to do what they could to stick pins in the balloon. But by the time the book came out, the field was already gone. There was just about nobody doing it.</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/James_A._Anderson_(cognitive_scientist)">James A. Anderson</a> pointed out that during the “winter”, neural networks survived outside of AI.</p>
<blockquote class="blockquote">
<p>This was during the period sometimes called the neural network dark ages, after the Minsky and Papert book on perceptrons had dried up most of the funding for neural networks in engineering and computer science. Neural networks continued to be developed by psychologists, however, because they turned out to be effective models in psychology … What happened during the dark ages was that the ideas had moved away from the highly visible areas of big science and technology into areas of science that did not appear in the newspapers.</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/David_Rumelhart">David Rumelhart</a> had nice things to say about Minsky, with no trace of bitterness. It is understandable as he only started working in neural networks years after the controversy died down.</p>
<blockquote class="blockquote">
<p>I always had one course that was like a free course in which I would choose a book of the year and teach out of that. In 1969, I think it was, or maybe ’70, I chose Perceptrons by Minsky and Papert as the book of the year. We then carefully went through it and read it in a group. … This was my most in-depth experience with things related to neural networks, or what were later called neural networks. I was quite interested in Minsky in those days because he also had another book which was called, I think, Semantic Information Processing. That book was a collection, including an article by Ross Quillian. It was a collection of dissertations from his graduate students. In a way, it was Minsky who led me to read about the perceptron more than anybody else.</p>
</blockquote>
<p>Regarding Robert Hecht-Nielsen, we have already seen his belief that Minsky was “Darth Vader” and possibly “the Devil”. Unsurprisingly, he was the most embittered, and placed the blame for the 1970s neural network winter squarely on the publication of <em>Perceptrons</em>.</p>
<blockquote class="blockquote">
<p>By the mid-1970s, Minsky and his colleagues (notably Seymour Papert) began to take actions designed to root out neural networks and ensure large and, in their view, richly deserved funding for AI research by getting the money currently being “wasted” on neural networks, and more to boot, redirected. They did two things. First, Minsky and Papert began work on a manuscript designed to discredit neural network research. Second, they attended neural network and “bionics” conferences and presented their ever-growing body of mathematical results being compiled in their manuscript to what they later referred to as “the doleful responses” of members of their audiences.</p>
<p>At the heart of this effort was Minsky and Papert’s growing manuscript, which they privately circulated for comments. The technical approach they took in the manuscript was based on a mathematical theorem discovered and proven some years earlier—ironically, by a strong supporter of Rosenblatt—that the perceptron was incapable of ever implementing the “exclusive-OR” [X-OR] logic function. What Minsky and Papert and their colleagues did was elaborate and bulk up this idea to book length by devising many variants of this theorem. Some, such as a theorem showing that single-layer perceptrons, of many varied types, cannot compute topological connectedness, are quite clever. To this technical fabric, they wove in what amounted to a personal attack on Rosenblatt. This was the early form of their crusade manifesto.</p>
<p>Later, on the strong and wise advice of colleagues, they expunged the vitriol. They didn’t quite get it all, as a careful reading will show. They did a complete flip-flop, dedicating the book to Rosenblatt! As their colleagues sensed it would, this apparently “objective” evaluation of perceptrons had a much more powerful impact than the original manuscript with its unseemly personal attack would have. Of course, in reality, the whole thing was intended, from the outset, as a book-length damnation of Rosenblatt’s work and many of its variants in particular, and, by implication, all other neural network research in general.</p>
<p>Minsky and Papert’s book, Perceptrons, worked. The field of neural networks was discredited and destroyed. The book and the associated conference presentations created a new conventional wisdom at DARPA and almost all other research sponsorship organizations that some MIT professors have proven mathematically that neural networks cannot ever do anything interesting. The chilling effect of this episode on neural network research lasted almost twenty years.</p>
</blockquote>
</section>
</section>
<section id="the-message-of-perceptrons" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-message-of-perceptrons">The message of <em>Perceptrons</em></h2>
<p>Minsky described how he and Papert felt impelled to write the book by a “therapeutic compulsion”, since they were “appalled” by the influence perceptrons:</p>
<blockquote class="blockquote">
<p>Both of the present authors (first independently and later together) became involved with a somewhat therapeutic compulsion: to dispel what we feared to be the first shadows of a “holistic” or “Gestalt” misconception that would threaten to haunt the fields of engineering and artificial intelligence as it had earlier haunted biology and psychology. For this, and for a variety of more practical and theoretical goals, we set out to find something about the range and limitations of perceptrons.</p>
<p><span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, 20</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Our first formal presentation of the principal results in this book was at an American Mathematical Society symposium on Mathematical Aspects of Computer Science in April 1966. At this time we could prove that <span class="math inline">\(\psi_{CONNECTED}\)</span> was not of finite order and conjectured that the same must be true of the apparently “global” predicates of symmetry and twins described in §7.3.</p>
<p>For the rest of 1966 the matter rested there. We were pleased and encouraged by the enthusiastic reception by many colleagues at the A.M.S. meeting and no less so by the doleful reception of a similar presentation at a Bionics meeting. However, we were now involved in establishing at M.I.T. an artificial intelligence laboratory largely devoted to real “seeing machines”, and gave no attention to perceptrons until we were jolted by attending an I.E.E.E. Workshop on Pattern Recognition in Puerto Rico early in 1967.</p>
<p>Appalled at the persistent influence of perceptrons (and similar ways of thinking) on practical pattern recognition, we determined to set out our work as a book.</p>
<p><span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, 242</a>)</span></p>
</blockquote>
<p>This 1966 encounter had been corroborated in other places.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> According to Kanal, the workshop was in fact held in 1966 October 24–26, and that only 5 out of 30 papers were about neural networks for pattern recognition. I suppose for Minsky and Papert, 5 out of 30 is 5 too many. <span class="citation" data-cites="kanalPatternCategoriesAlternate1993">(<a href="#ref-kanalPatternCategoriesAlternate1993" role="doc-biblioref">Kanal 1993</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="fn9"><blockquote class="blockquote"><sup>9</sup>&nbsp;
<p>In 1966, a few dozen researchers traveled to Puerto Rico, gathering at the Hilton hotel in San Juan. They met to discuss the latest advances in what was then called “pattern recognition”–technology that could identify patterns in images and other data. Whereas Rosenblatt viewed the Perceptron as a model of the brain, others saw it as a means of pattern recognition… Rosenblatt didn’t even travel to Puerto Rico. Inside the Hilton, the tension emerged when a young scientist named John Munson addressed the conference. Munson worked at SRI… There, alongside a larger team of researchers, he was trying to build a neural network that could read handwritten characters, not just printed letters, and with his presentation at the conference, he aimed to show the progress of this research. But when Munson finished the lecture and took questions from the floor, Minsky made himself heard. “How can an intelligent young man like you,” he asked, “waste your time with something like this?” … “This is an idea with no future.” <span class="citation" data-cites="metzGeniusMakersMavericks2021">(<a href="#ref-metzGeniusMakersMavericks2021" role="doc-biblioref">Metz 2021</a>)</span></p>
</blockquote>
</div></div><p>The book has become a true classic: everybody wants to have read and nobody wants to read. Taking the opposite approach, I <em>have</em> read the book, despite not wanting to read it.</p>
<p>Its content can be neatly divided into a greater and a lesser half. The greater half is a mathematical monograph on which functions can be implemented by a single perceptron with fixed featurizers, and the lesser half is a commentary on the wider implications of the monograph. The impact of the work is precisely reversed: most of the impact comes from the commentary derived from the results, and effectively no impact comes from the mathematical results themselves.</p>
<p>Despite this imbalance, the mathematical work is substantial, and the perceptron controversy turns critically on the pliable interpretation sprouting from the solid structure. Therefore, I have detailed the mathematical content in a separate essay, <a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/"><em>Reading Perceptrons</em></a>, to which I refer occasionally to gloss their interpretation.</p>
<section id="minsky-and-papert-struck-back" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="minsky-and-papert-struck-back">Minsky and Papert struck back</h3>
<p>In the 1980s, neural networks rose again to prominence under the name of “connectionism”, prompting an eventual response from Minsky and Papert. The <em>Perceptrons</em> book was reissued in 1988, with new chapters dedicated to rejecting connectionism. They took the 1986 two-volume work of <em>Parallel Distributed Processing</em> (<em>PDP</em>), especially <span class="citation" data-cites="rumelhartLearningInternalRepresentations1985">(<a href="#ref-rumelhartLearningInternalRepresentations1985" role="doc-biblioref">Rumelhart, Hinton, and Williams 1985</a>)</span> <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>, as the representative of connectionism, and made specific objections to them.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;This paper was reprinted in <span class="citation" data-cites="rumelhartParallelDistributedProcessing1986">(<a href="#ref-rumelhartParallelDistributedProcessing1986" role="doc-biblioref">Rumelhart and McClelland 1986, vol. 1</a>, chapter 8)</span>, in which Minsky and Papert read it. This paper is often cited for the backpropagation algorithm, which I have discussed in <a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/"><em>The Backstory of Backpropagation</em></a>.</p></div></div><p>In the prologue, they staked their claim thus: Connectionism is a mistake engendered by a new generation of researchers ignorant of history; though the theorems of the <em>Perceptrons</em> book apply to only a single perceptron, the <em>lessons</em> extend to all neural networks. To back up the claim, they made specific technical, historical, and philosophical objections, all with the central goal of showing that <strong>homogeneous neural networks cannot scale</strong>.</p>
<blockquote class="blockquote">
<p>… when we found that little of significance had changed since 1969, when the book was first published, we concluded that it would be more useful to keep the original text (with its corrections of 1972) and add an epilogue, so that the book could still be read in its original form. One reason why progress has been so slow in this field is that researchers unfamiliar with its history have continued to make many of the same mistakes that others have made before them.</p>
<p>… there has been little clear-cut change in the conceptual basis of the field. The issues that give rise to excitement today seem much the same as those that were responsible for previous rounds of excitement. … many contemporary experimenters assume that, because the perceptron networks discussed in this book are not exactly the same as those in use today, these theorems no longer apply. Yet, as we will show in our epilogue, most of the <em>lessons</em> of the theorems still apply.</p>
</blockquote>
<p>In an earlier interview, Minsky reiterated his belief that the proper place of perceptrons is solving <em>tiny</em> problems with <em>tiny</em> perceptron networks.</p>
<blockquote class="blockquote">
<p>… for certain purposes the Perceptron was actually very good. I realized that to make one all you needed in principle was a couple of molecules and a membrane. So after being irritated with Rosenblatt for overclaiming, and diverting all those people along a false path, I started to realize that for what you get out of it – the kind of recognition it can do – it is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can’t get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of. <span class="citation" data-cites="bernsteinMarvinMinskyVision1981">(<a href="#ref-bernsteinMarvinMinskyVision1981" role="doc-biblioref">Bernstein 1981</a>)</span></p>
</blockquote>
<p>They also urged all AI researchers to adopt the Society of Mind hypothesis, or else face the charge of being unreflective or of drawing lines where none exists. It seems to me that Minsky wrote most of the prologue and epilogue, because in <a href="#sec-papert-struck-back">Papert’s solo paper</a>, he went considerably further with sociological interpretation.</p>
<blockquote class="blockquote">
<p>This broad division makes no sense to us, because these attributes are largely independent of one another; for example, the very same system could combine symbolic, analogical, serial, continuous, and localized aspects. Nor do many of those pairs imply clear opposites; at best they merely indicate some possible extremes among some wider range of possibilities. And although many good theories begin by making distinctions, we feel that in subjects as broad as these there is less to be gained from sharpening bound­aries than from seeking useful intermediates.</p>
<p>… Are there inherent incompatibilities between those connectionist and symbolist views? The answer to that depends on the extent to which one regards each separate connectionist scheme as a self-standing system. If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No.&nbsp;But if we consider each such network as a possible model for a part of a brain, then those two overviews are complementary. This is why we see no reason to choose sides.</p>
<p>… Most researchers tried to bypass [the technical objections], either by ignoring them or by using brute force or by trying to discover powerful and generally applicable methods. Few researchers tried to use them as guides to thoughtful research. We do not believe that any completely general solution to them can exist …</p>
</blockquote>
<p>We now proceed to the epilogue and its arguments.</p>
<section id="s-connectionism-is-not-that-different" class="level4">
<h4 class="anchored" data-anchor-id="s-connectionism-is-not-that-different">1980s connectionism is not that different</h4>
<p>They speculated on the reason for the revival of neural networks. Was it because of the development of backpropagation, multilayer networks, and faster computers? <em>Emphatically not</em>. In fact, 1980s connectionists were not different from the 1960s connectionists. It is only the ignorance of history that made them think otherwise. In both periods, connectionism was focused on making small-scale experiments and then extrapolating to the largest scale, without mathematical theorems to justify the extrapolation. In both periods, connectionism failed (or would fail) to scale beyond toy problems.</p>
<blockquote class="blockquote">
<p>most of the theorems in this book are explicitly about machines with a single layer of adjustable connection weights. But this does not imply (as many modern connectionists assume) that our conclusions don’t apply to multilayered machines. To be sure, those proofs no longer apply unchanged, because their antecedent conditions have changed. But the phenomena they describe will often still persist. One must examine them, case by case.</p>
</blockquote>
<blockquote class="blockquote">
<p>… the situation in the early 1960s: Many people were impressed by the fact that initially unstructured networks composed of very simple devices could be made to perform many interesting tasks – by processes that could be seen as remarkably like some forms of learning. A different fact seemed to have impressed only a few people: While those networks did well on certain tasks and failed on certain other tasks, there was no theory to explain what made the difference – particularly when they seemed to work well on small (“toy”) problems but broke down with larger problems of the same kind. Our goal was to develop analytic tools to give us better ideas about what made the difference.</p>
</blockquote>
</section>
<section id="there-is-no-silver-bullet-in-machine-learning" class="level4">
<h4 class="anchored" data-anchor-id="there-is-no-silver-bullet-in-machine-learning">There is no silver bullet in machine learning</h4>
<p>There are no general algorithms and there are no general problems. There are only particular algorithm-problem pairs. An algorithm-problem pair can be a good fit, or a bad fit. The parity problem is a bad fit with a neural network trained by backpropagation, but it is a good fit with a Turing machine.</p>
<p>There is no general and effective algorithm. Either the algorithm is so general that it is as useless as “just try every algorithm” akin to Ross Ashby’s homeostat, or it is useful but not general. This general lesson is similar to Gödel’s speedup theorem, Blum’s speedup theorem, the no free lunch theorem, etc.</p>
<blockquote class="blockquote">
<p>Clearly, the procedure can make but a finite number of errors before it hits upon a solution. It would be hard to justify the term “learning” for a machine that so relentlessly ignores its experience. The content of the perceptron convergence theorem must be that it yields a better learning procedure than this simple homeostat. Yet the problem of relative speeds of learning of perceptrons and other devices has been almost entirely neglected. <span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, sec. 11.7</a>)</span></p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)">Arthur Samuel</a>’s checker learning algorithm encountered two fundamental problems: credit assignment and inventing novel features. Those two problems are not just for the checker AI, but for <em>all</em> AI. There are <em>no</em> universal and effective solutions to credit assignment, and there are <em>no</em> universally effective solutions to inventing novel features. There could be universal but impractical solutions, such as backpropagation on homogeneous neural networks, Solomonoff induction, trying every Turing machine, etc. There could be practical but not universal solutions, which is precisely what populates the Society of Mind in human brains.</p>
<blockquote class="blockquote">
<p>Rosenblatt’s credit-assignment method turned out to be as effective as any such method could be. When the answer is obtained, in effect, by adding up the contributions of many processes that have no significant interactions among themselves, then the best one can do is reward them in proportion to how much each of them contributed.</p>
</blockquote>
<blockquote class="blockquote">
<p>Several kinds of evidence impel us toward this view. One is the great variety of different and specific functions embodied in the brain’s biology. Another is the similarly great variety of phenomena in the psychology of intelligence. And from a much more abstract viewpoint, we cannot help but be impressed with the practical limitations of each “general” scheme that has been proposed – and with the theoretical opacity of questions about how they behave when we try to scale their applications past the toy problems for which they were first conceived.</p>
</blockquote>
</section>
<section id="there-is-no-efficient-way-to-train-homogeneous-high-order-networks" class="level4">
<h4 class="anchored" data-anchor-id="there-is-no-efficient-way-to-train-homogeneous-high-order-networks">There is no efficient way to train homogeneous, high-order networks</h4>
<p>They ask the reader to think back to the <a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#exm-parity-order">lesson of the parity predicate from Chapter 10</a>: Even though it is learnable by a two-layered perceptron network, it would involve weights exponential in the input pixel count, and therefore take a very long time to learn. They expect this to generalize, so that <em>any</em> problem that require <em>some</em> perceptron in the network to have receptive field of size <span class="math inline">\(\Omega(|R|^\alpha)\)</span>, necessarily require that perceptron to have coefficients growing like <span class="math inline">\(2^{\Omega(|R|^\alpha)}\)</span>, and therefore taking <span class="math inline">\(2^{\Omega(|R|^\alpha)}\)</span> steps to train.</p>
<blockquote class="blockquote">
<p>We could extend them either by scaling up small connectionist models or by combining small-scale networks into some larger organization. In the first case, we would expect to encounter theoretical obstacles to maintaining GD’s effectiveness on larger, deeper nets. And despite the reputed efficacy of other alleged remedies for the deficiencies of hill-climbing, such as “annealing,” we stay with our research conjecture that no such procedures will work very well on large-scale nets, except in the case of problems that turn out to be of low order in some appropriate sense.</p>
<p>The second alternative is to employ a variety of smaller networks rather than try to scale up a single one. And if we choose (as we do) to move in that direction, then our focus of concern as theoretical psychologists must turn toward the organizing of small nets into effective large systems.</p>
</blockquote>
</section>
<section id="there-is-no-effective-use-for-homogeneous-high-order-networks" class="level4">
<h4 class="anchored" data-anchor-id="there-is-no-effective-use-for-homogeneous-high-order-networks">There is no effective use for homogeneous, high-order networks</h4>
<p>Fully connected networks, or indeed any neural network without a strong constraint on “order” or “receptive field”, would hopelessly confuse itself with its own echoes as soon as it scales up, unless it has sufficient “insulation”, meaning almost-zero connection weights, such that it effectively splits into a large number of small subnets. That is, a large fully connected network is useless anyway unless it already decomposes into many tiny networks arranged in a Society of Mind.</p>
<blockquote class="blockquote">
<p>Certain parallel computations are by their nature synergistic and cooperative: each part makes the others easier. But the <a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#thm-and-or">And/Or of theorem 4.0</a> shows that under other circumstances, attempting to make the same network perform two simple tasks at the same time leads to a task that has a far greater order of difficulty. In those sorts of circumstances, there will be a clear advantage to having mechanisms, not to connect things together, but to keep such tasks apart. How can this be done in a connectionist net?</p>
</blockquote>
<blockquote class="blockquote">
<p>… a brain is not a single, uniformly structured network. Instead, each brain contains hundreds of different types of machines, interconnected in specific ways which predestine that brain to become a large, diverse society of partially specialized agencies.</p>
</blockquote>
</section>
<section id="gradient-descent-cannot-escape-local-minima" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="gradient-descent-cannot-escape-local-minima">Gradient descent cannot escape local minima</h4>
<p>Gradient descent, backpropagation, and all other hill-climbing algorithms are all vulnerable to getting trapped in local optima, and therefore they cannot work – except in problem-architecture pairs where the loss landscape of this <em>particular</em> problem, for this <em>particular</em> architecture, using this <em>particular</em> loss function, on this <em>particular</em> dataset, is a single bump whose width is shorter than this <em>particular</em> learning rate.</p>
<p>Gradient descent is just a form of hill-climbing, when the hill is differentiable. The perceptron learning algorithm can be interpreted as a hill-climbing algorithm too, as it makes localized decision to make one step in this direction or that, one error-signal at a time (Section 11.7). Therefore, the generic ineffectiveness of perceptron learning suggests that gradient descent is also generically ineffective and <strong>cannot scale</strong>. It does not even have a convergence theorem, so in that sense it’s <em>worse</em> than perceptron learning algorithm.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;This claim is astonishing, now that we see how powerful backpropagation works, and how the perceptron learning rule had crippled neural network research for 30 years. We can understand their sentiments by remembering that they, like most of the academic community in computer science, favored the certainty of mathematical theorems over mere empirical success. <a href="https://en.wikipedia.org/wiki/Leo_Breiman">Leo Breiman</a> observed that academic statistics had been hamstrung by the same grasp over mathematical certainty, and thus over 95% of its publications were useless. <span class="citation" data-cites="breimanReflectionsRefereeingPapers1995">(<a href="#ref-breimanReflectionsRefereeingPapers1995" role="doc-biblioref">Breiman 1995</a>)</span></p></div></div><blockquote class="blockquote">
<p>We were very pleased to discover (see section 11.6) that PC [Perceptron Convergence theorem] could be represented as hill-climbing; however, that very fact led us to wonder whether such procedures could dependably be generalized, even to the limited class of multilayer machines that we named Gamba perceptrons. The situation seems not to have changed much – we have seen no contemporary connectionist publication that casts much new theoretical light on the situation. Then why has GD [Gradient Descent] become so popular in recent years? … we fear that its reputation also stems from unfamiliarity with the manner in which hill-climbing methods deteriorate when confronted with larger-scale problems. … Indeed, GD can fail to find a solution when one exists, so in that narrow sense it could be considered <em>less</em> powerful than PC.</p>
</blockquote>
</section>
<section id="stochastic-gradient-descent-cannot-see-through-the-noise" class="level4">
<h4 class="anchored" data-anchor-id="stochastic-gradient-descent-cannot-see-through-the-noise">Stochastic gradient descent cannot see through the noise</h4>
<blockquote class="blockquote">
<p>So far as we could tell, every experiment described in <span class="citation" data-cites="rumelhartLearningInternalRepresentations1985">(<a href="#ref-rumelhartLearningInternalRepresentations1985" role="doc-biblioref">Rumelhart, Hinton, and Williams 1985</a>)</span> involved making a complete cycle through all possible input situations before making any change in weights. Whenever this is feasible, it completely eliminates sampling noise—and then even the most minute correlations can become reliably detectable, be­ cause the variance is zero. But no person or animal ever faces situations that are so simple and arranged in so orderly a manner as to provide such cycles of teaching examples. Moving from small to large problems will often demand this transition from exhaustive to statistical sampling, and we suspect that in many realistic situations the resulting sampling noise would mask the signal completely. We suspect that many who read the connectionist literature are not aware of this phenomenon, which dims some of the prospects of successfully applying certain learning procedures to large-scale problems.</p>
</blockquote>
</section>
<section id="differentiable-activation-is-just-a-hack" class="level4">
<h4 class="anchored" data-anchor-id="differentiable-activation-is-just-a-hack">Differentiable activation is just a hack</h4>
<p>Using differentiable activations for neural networks is an artificial trick of questionable future. It makes the learned boolean functions imprecise, and only appears to redeem itself by allowing backpropagation. However, backpropagation is a dead-end because it will not scale. It is better to look for a method that can directly train multilayer perceptron networks with discrete activation functions.</p>
<blockquote class="blockquote">
<p>The trick is to replace the threshold function for each unit with a monotonic and differentiable function … However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold.</p>
</blockquote>
<blockquote class="blockquote">
<p>We conjecture that learning XOR for larger numbers of variables will become increasingly intractable as we increase the numbers of input variables, because by its nature the underlying parity function is absolutely uncorrelated with any function of fewer variables. Therefore, there can exist no useful correlations among the outputs of the lower-order units involved in computing it, and that leads us to suspect that there is little to gain from following whatever paths are indicated by the artificial introduction of smoothing functions that cause partial derivatives to exist.</p>
</blockquote>
</section>
<section id="connectionists-have-no-theory-so-they-should-not-extrapolate-from-experiments" class="level4">
<h4 class="anchored" data-anchor-id="connectionists-have-no-theory-so-they-should-not-extrapolate-from-experiments">Connectionists have no theory, so they should not extrapolate from experiments</h4>
<blockquote class="blockquote">
<p>In the past few years, many experiments have demonstrated that various new types of learning machines, composed of multiple layers of perceptron-like elements, can be made to solve many kinds of small-scale prob­lems. Some of those experimenters believe that these performances can be economically extended to larger problems without encountering the limitations we have shown to apply to single­ layer perceptrons. Shortly, we shall take a closer look at some of those results and see that much of what we learned about simple perceptrons will still remain quite pertinent.</p>
</blockquote>
<p>Without a mathematical theory, experimental data cannot be extrapolated. If neural networks happen to work well on a problem, it merely shows that the problem is a good fit for this <em>particular</em> architecture trained in this <em>particular</em> way at this <em>particular</em> scale, not anything more general than that.</p>
<blockquote class="blockquote">
<p>As the field of connectionism becomes more mature, the quest for a general solution to all learning problems will evolve into an understanding of which types of learning processes are likely to work on which classes of problems. And this means that, past a certain point, we won’t be able to get by with vacuous generalities about hill-climbing. We will really need to know a great deal more about the nature of those surfaces for each specific realm of problems that we want to solve.</p>
</blockquote>
<blockquote class="blockquote">
<p>… the learning procedure required 1,208 cycles through each of the 64 possible examples – a total of 77,312 trials (enough to make us wonder if the time for this procedure to determine suitable coefficients increases exponentially with the size of the retina). <em>PDP</em> does not address this question. What happens when the retina has 100 elements? If such a network required on the order of <span class="math inline">\(2^{200}\)</span> trials to learn. most observers would lose interest.</p>
</blockquote>
</section>
<section id="connectionist-experiments-can-be-extrapolated-to-show-that-they-do-not-scale" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="connectionist-experiments-can-be-extrapolated-to-show-that-they-do-not-scale">Connectionist experiments can be extrapolated to show that they do not scale</h4>
<p>Though lacking a theory of their own on the operation of multilayer perceptrons, Minsky and Papert proceeded to interpret the connectionist experiment data as showing that neural networks would fail to scale.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;Without a mathematical theory of what neural networks can do, extrapolating from their behavior at small scales to the large scale is impossible and only reflect the bias behind those who make the extrapolation.</p></div></div><p>Connectionists demonstrated that two-layered perceptrons, where both layers were trainable, bypassed the limits described in <em>Perceptrons</em>. For example, <span class="citation" data-cites="rumelhartLearningInternalRepresentations1985">(<a href="#ref-rumelhartLearningInternalRepresentations1985" role="doc-biblioref">Rumelhart, Hinton, and Williams 1985</a>)</span> showed that several problems unsolvable by a single perceptron – XOR, parity, symmetry, etc – were solved by a two-layered neural network.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptrons_page_253.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Page 253, Figure 2. Redrawn from <span class="citation" data-cites="rumelhartLearningInternalRepresentations1985">(<a href="#ref-rumelhartLearningInternalRepresentations1985" role="doc-biblioref">Rumelhart, Hinton, and Williams 1985</a>)</span></figcaption>
</figure>
</div>
<p>While the connectionist authors saw the result as a hopeful sign, Minsky and Papert interpreted it as showing that the experiments wouldn’t scale, because the coefficients appeared to grow exponentially – in just the way they proved in <a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#exm-line-symmetry">Chapter 7</a>.</p>
<blockquote class="blockquote">
<p>In <em>PDP</em> it is recognized that the lower-level coefficients appear to be growing exponentially, yet no alarm is expressed about this. In fact, anyone who reads section 7.3 should recognize such a network as employing precisely the type of computational structure that we called stratification.</p>
<p>although certain problems can easily by solved by perceptrons on small scales, the computational costs become prohibitive when the problem is scaled up. The authors of PDP seem not to recognize that the coefficients of this symmetry machine confirm that thesis, and celebrate this performance on a toy problem as a success rather than asking whether it could become a profoundly “bad” form of behavior when scaled up to problems of larger size.</p>
</blockquote>
</section>
</section>
<section id="sec-papert-struck-back" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-papert-struck-back">Papert struck back</h3>
<p>While it appears that Minsky was the main author for the new prologue and epilogue, Papert solo-authored <span class="citation" data-cites="papertOneAIMany1988">(<a href="#ref-papertOneAIMany1988" role="doc-biblioref">S. Papert 1988</a>)</span>, an essay that gave the controversy a uniquely Papert-styled spin. It is an extensive reframing of the perceptron controversy into a social and philosophical issue, with the prediction of ultimate victory for epistemological pluralism:</p>
<blockquote class="blockquote">
<p>The field of artificial intelligence is currently divided into what seem to be several competing paradigms … for mechanisms with a universal application. I do not foresee the future in terms of an ultimate victory for any of the present contenders. What I do foresee is a change of frame, away from the search for universal mechanisms. I believe that we have much more to learn from studying the differences, rather than the sameness, of kinds of knowing.</p>
</blockquote>
<p>He diagnosed the source of the philosophical error as a “category error”.</p>
<blockquote class="blockquote">
<p>There is the same mistake on both sides: the category error of supposing that the existence of a common mechanism provides both an explanation and a unification of all systems, however complex, in which this mechanism might play a central role.</p>
<p>Artificial intelligence, like any other scientific enterprise, had built a scientific culture… more than half of our book is devoted to “pro-perceptron” findings about some very surprising and hitherto unknown things that perceptrons can do. But in a culture set up for global judgment of mechanisms, being understood can be a fate as bad as death. A real understanding of what a mechanism can do carries too much implication about what it cannot do… The same trait of universalism leads the new generation of connectionists to assess their own microlevel experiments, such as Exor, as a projective screen for looking at the largest macroissues in the philosophy of mind. The category error analogous to seeking explanations of the tiger’s stripes in the structure of DNA is not an isolated error. It is solidly rooted in AI’s culture.</p>
</blockquote>
<p>He then discussed the compute-first interpretation, a “<a href="https://web.archive.org/web/20190314204621/http://www.incompleteideas.net/IncIdeas/BitterLesson.html">bitter lesson</a>” for the 1980s, before rejecting it.</p>
<blockquote class="blockquote">
<p>In the olden days of Minsky and Papert, neural networking models were hopelessly limited by the puniness of the computers available at the time and by the lack of ideas about how to make any but the simplest networks learn. Now things have changed. Powerful, massively parallel computers can implement very large nets, and new learning algorithms can make them learn. …</p>
<p>I don’t believe it. The influential recent demonstrations of new networks all run on small computers and could have been done in 1970 with ease. Exor is a “toy problem” run for study and demonstration, but the examples discussed in the literature are still very small. Indeed, Minsky and I, in a more technical discussion of this history (added as a new prologue and epilogue to a reissue of Perceptrons), suggest that the entire structure of recent connectionist theories might be built on quicksand: it is all based on toy-sized problems with no theoretical analysis to show that performance will be maintained when the models are scaled up to realistic size. The connectionist authors fail to read our work as a warning that networks, like “brute force” programs based on search procedures, scale very badly.</p>
</blockquote>
<p>Consider Exor, a certain neural network he picked out of the pages of <em>PDP</em>, which learned to perform the infamous XOR task, but only after 2232 examples. Was it slow, or fast? A proper judgment requires a mathematical understanding of the algorithm-problem fit. By extension, to properly judge whether neural networks were good for any specific problem, one must first mathematically understand the fit. He insinuated that the connectionists who were confident that their neural networks were more than a sterile extension of the perceptron did not do their math, unlike he and Minsky.</p>
<blockquote class="blockquote">
<p>instead of asking whether nets are good, we asked what they are good for. The focus of enquiry shifted from generalities about kinds of machines to specifics about kinds of tasks. From this point of view, Exor raises such questions as: Which tasks would be learned faster and which would be learned even more slowly by this machine? Can we make a theory of tasks that will explain why 2,232 repetitions were needed in this particular act of learning?</p>
<p>… Minsky and I both knew perceptrons extremely well. We had worked on them for many years before our joint project of under standing their limits was conceived… I was left with a deep respect for the extraordinary difficulty of being sure of what a computational system can or cannot do. I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds.</p>
</blockquote>
<details>
<summary>
Interjection
</summary>
<p>I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds:</p>
<blockquote class="blockquote">
<p>There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. <span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, 232</a>)</span></p>
</blockquote>
</details>
<p>What, then, explains the rise of connectionism? Since Papert reframed the fall of perceptron socially, it only stands to reason he would reframe the rise of connectionism as the rise of a social myth caused by other social myths, <em>not</em> by the increase in computing power or new algorithms like backpropagation, convolutional networks, and such. For one, the computing powers used by the breakthrough connectionist models like NETtalk were already within reach even in the 1960s.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> For another, he and Minsky were firm in their conviction that any uniform architecture must scale very badly and that no amount of computing or algorithmic advancement could be anything more than a sterile extension.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;NETtalk, a neural network with 3 layers and 18,629 weights, is entirely within reach for the 1960s. Its dataset was built in weeks <em>by hand</em>, and its training took a single night on a <a href="https://en.wikipedia.org/wiki/Ridge_Computers">Ridge computer</a> that is close to a <a href="https://en.wikipedia.org/wiki/VAX-11#VAX-11/780">VAX 11/780</a>. Now, VAX 11/780 has <span class="math inline">\(\sim 1 \;\rm{MFLOP/sec}\)</span>, so NETtalk took <span class="math inline">\(\sim 10^{11}\;\rm{FLOP}\)</span> to train. <a href="https://yuxi-liu-wired.github.io/docs/posts/1998-hans-moravec/#appendix-dataset">During the 1960s, typical workstations</a> have a computing power of <span class="math inline">\(\sim 0.11 \;\rm{MIPS}\)</span>, so NETtalk could be trained in a month.</p>
<blockquote class="blockquote">
<p>We then used the 20,000-word Brown Corpus and assigned phonemes, as well as stress marks, to each of letters. The alignment of the letters and sounds took weeks, but, once the learning started, the network absorbed the whole corpus in a single night. <span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 115</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>I had picked up a Ridge computer, made by a company that is now defunct, but it had the power of a VAX 11/780 which at that time was the standard candle of computer power. … We had a real computer, and we had a real algorithm, and we looked for a do-able project in language. … I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “<a href="#appendix-the-chomskyans">Chomsky</a> worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.”</p>
<p>… In retrospect it was an ideal choice for a problem. It was difficult with conventional techniques, and it was not clear that the network could handle it. … We knew back then there were many local minima in the network, and we knew we were getting trapped. The surprise was that this did not prevent the network from finding good solutions. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, 324–25</a>)</span></p>
</blockquote>
</div></div><blockquote class="blockquote">
<p>Massively parallel supercomputers do play an important role in the connectionist revival. But I see it as a cultural rather than a technical role, another example of a sustaining myth. Connectionism does not use the new computers as physical machines; it derives strength from the “computer in the mind,” from its public’s largely nontechnical awareness of supercomputers. I see connectionism’s relationship to biology in similar terms. Although its models use biological metaphors, they do not depend on technical findings in biology any more than they do on modern supercomputers. … I also see a more subtle, but not less relevant, cultural resonance. This is a generalized turn away from the hard-edged rationalism of the time connectionism last went into eclipse and a resurgent attraction to more holistic ways of thinking.</p>
</blockquote>
</section>
</section>
<section id="rebuttal-to-minsky-and-papert" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="rebuttal-to-minsky-and-papert">Rebuttal to Minsky and Papert</h2>
<section id="interpreting-the-xor-problem" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="interpreting-the-xor-problem">Interpreting the XOR problem</h3>
<p>When I first heard about the first neural network winter and the <em>Perceptrons</em> book, I was deeply confused by the story. The story went that “<em>Perceptrons</em> proved that the XOR problem is unsolvable by a single perceptron, a result that caused researchers to abandon neural networks”. How could it possibly <em>cause</em> researchers to abandon the field, unless it was news to them? But anyone could see that a single perceptron could only separate linearly separable points, and therefore the XOR problem is unsolvable by a single perceptron. When I first heard the story, I immediately saw why XOR was unsolvable by one perceptron, then took a few minutes to design a two-layered perceptron network that solved the XOR problem. I then noted that the NAND problem is solvable by a single perceptron, after which I immediately knew that perceptron networks are universal <a href="https://en.wikipedia.org/wiki/Functional_completeness">since the NAND gate is</a>.</p>
<p>If a high school student could bypass the XOR problem in a few minutes, how could it possibly have been news to the researchers in 1969?</p>
<p>When I started researching neural networks properly, the standard story about the XOR problem became more nonsensical the more I learned. The 1943 paper by McCulloch and Pitts <span class="citation" data-cites="mccullochLogicalCalculusIdeas1943">(<a href="#ref-mccullochLogicalCalculusIdeas1943" role="doc-biblioref">McCulloch and Pitts 1943</a>)</span> <em>already</em> said that their neural networks were equivalent in power to Turing machines. Marvin Minsky’s 1954 PhD thesis <span class="citation" data-cites="minskyTheoryNeuralanalogReinforcement1954">(<a href="#ref-minskyTheoryNeuralanalogReinforcement1954" role="doc-biblioref">Minsky 1954</a>)</span> develops an entire computer theory out of McCulloch–Pitts neural networks.</p>
<p>On the electrical engineering side, perceptron networks were studied under the name of “linear threshold logic” by electrical engineers since the 1950s, who clearly would not have bothered if they could not even make an XOR gate out of it. In fact, in a standard reference from 1965, there are chapters on “Single-Threshold-Element Synthesis by Iteration” – learning a single perceptron by the perceptron learning algorithm – and “Network Synthesis” – which does not imply machine learning, but rather hand-designing perceptron networks. <span class="citation" data-cites="dertouzosThresholdLogicSynthesis1965">(<a href="#ref-dertouzosThresholdLogicSynthesis1965" role="doc-biblioref">Dertouzos 1965</a>)</span></p>
<p>What is going on?</p>
<p>I believe the story got completely garbled during the teaching process. I am all for changing history for the sake of understanding – history is made for the winners, not the winners made for history – but the standard story about the XOR problem is nonsensical, as I have shown. So how did the story come about?</p>
<p>I believe this is because <em>Perceptrons</em> contained a host of problems that their restricted form of perceptron machines could not do. The simplest one is the XOR problem. Teachers who just wanted to spend two minutes on the first neural network winter and move on, grabbed this XOR problem and pretended that it was the actual cause of it.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;A mantis was crawling on the wheel of a slowly moving train. It gloated, “I am the prime mover of the train!”. When the caterpillar asked it to prove so, it jumped down and waved its arms in front of the train, which promptly crushed it.</p>
<p>This is my retelling of the Taoist story of <a href="https://en.wiktionary.org/wiki/%E8%9E%B3%E8%87%82%E7%95%B6%E8%BB%8A#Chinese">螳臂當車</a>.</p></div></div><p>There is one thing left to explain: what is the significance of the XOR problem to the neural network researchers back in the days? It was clearly significant for something, as when the connectionists rose in the 1980s, one of the first things they did was to check that they could solve the XOR problem. Rumelhart read the <em>Perceptrons</em> book very carefully in 1970; it inspired him to go into neural network research, entirely missing its intended message. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, 273</a>)</span> After he developed backpropagation around 1982, he immediately tried to train an MLP on the XOR problem.</p>
<blockquote class="blockquote">
<p>When I first did the X-OR problem, it took a thousand iterations to solve it. If we thought that was the way it was going to go and that we were going to scale up to a hundred thousand input patterns, my God, we wouldn’t live long enough to see the results. But that’s not the way it’s gone. That problem turned out to be an anomaly. The scaling is about linear. We haven’t hit any exponential curves yet. <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000</a>)</span></p>
</blockquote>
<p>What is the significance of the XOR problem? In the context of the neural network research in the 1960s, the significance becomes clear. Nobody knew how to simultaneously adapt two or more layers well.</p>
<p>Before 1962, Rosenblatt had studied both theoretically and experimentally “four-layer perceptron with adaptive preterminal network”, which means a perceptron network with three layers: the first layer random and fixed, and the second and third layers learned <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, chap. 16</a>)</span>. However, it had not a single derivative in it. The second layer was learned by the Hebbian learning rule, and the third layer was by the perceptron learning rule.</p>
<p>Meanwhile, during the early 1960s, Widrow and Hoff trained a single perceptron with gradient descent, then proceeded to try every trick <em>except</em> gradient descent to train a two-layered perceptron network. They gave up and parted ways. Hoff went on to co-invent the microprocessor at Intel, while Widrow applied a single perceptron to adaptive filter design, revolutionizing electrical engineering in the process. These and more of the ridiculous backstory can be read in <a href="https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/"><em>The Backstory of Backpropagation</em></a>.</p>
<p>In short, due to a variety of unfortunate developments, people spent about twenty years (1950–1970) failing to find an effective algorithm for training the pre-final layers of neural networks. They could train the final layer either by the perceptron learning rule of Rosenblatt or by the Widrow–Hoff rule of gradient descent on the squared error, but that was the extent of the learning they could get the neural networks to do.</p>
<p>Consider a two-layered neural network. The second layer is easy to learn. What should happen to the first layer? Rosenblatt’s solution was mainly just randomization because he mistakenly believed that the retina was randomly wired to the visual cortex, and he believed in emulating nature. Maybe Rosenblatt was working with the standard knowledge of neuroscience in his time, so he could not have known that neural connections were anything but random – the first of the Hubel and Wiesel papers was published only in 1959. However, it seems that Rosenblatt simply had a strong attachment to randomization, as <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962</a>)</span> cites <span class="citation" data-cites="hubelReceptiveFieldsSingle1959">(<a href="#ref-hubelReceptiveFieldsSingle1959" role="doc-biblioref">Hubel and Wiesel 1959</a>)</span> several times, yet he still randomized the first layer for most experiments in the book. Rosenblatt had also experimented with Hebbian learning <span class="citation" data-cites="rosenblattPrinciplesNeurodynamicsPerceptrons1962">(<a href="#ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" role="doc-biblioref">Rosenblatt 1962, vol. 55, sec. 16.1</a>)</span>, but since he did not use this method extensively, I infer that it did not work well.</p>
<p>Widrow’s solution was the MADALINE I rule – a complicated hack and a dead end. Without an effective method to train the first layer, those who worked on two-layered neural networks had only two choices: either randomize the first layer or design it by hand. Both choices played right into the hands of Minsky and Papert.</p>
<p>Seen from the viewpoint of the second layer, the first layer is the featurizer for the raw input. It is intuitively clear that, unless the raw input is featurized and the features are adapted to the problem, the second layer will not be able to solve the problem.</p>
<p>The XOR problem requires two layers. Furthermore, if the first layer is not wired correctly, the second layer will not be able to solve it either.</p>
<p>Put yourself in the place of a 1960s connectionist. How do you solve the XOR problem by a perceptron network? Well, not a single perceptron, as it’s impossible. Not with three layers, because two layers are sufficient, and you already have enough problems with two layers. So, two layers.</p>
<p>How to train it? You know only how to fix the first layer and train the second. How do you fix the first layer? Do you randomize it? Unless you use many hidden perceptrons, this will fail with high probability. Do you design it by hand? But then, Minsky and Papert would interject, “You see, you cannot substitute thinking by tabula-rasa learning! You need some intelligent design to get it to work! The network needs the right representations in the hidden layer, and you cannot expect it to learn the representation from a vacuous generality like the fully connected multilayer perceptron, unless you did not get the lesson from our book. You must design it by hand.”.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Fukushima_1969.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">As an example of the kind of MLP that Minsky approves: a hand-designed deep network <span class="citation" data-cites="fukushimaVisualFeatureExtraction1969">(<a href="#ref-fukushimaVisualFeatureExtraction1969" role="doc-biblioref">Fukushima 1969</a>)</span>. Here is someone who actually tried to understand the problem instead of just tossing it to the computer and expect things to magically work.</figcaption>
</figure>
</div>
<p>Not to give up, you try one of the hacks like the MADALINE I learning rule, or the Hebbian learning rule, but they are extremely fiddly and unable to learn most of the time unless you tune them just right, and it seems to require a different tuning for problems even slightly more complex than the XOR problem. Minsky and Papert interject again, “You see, there is no universal learning algorithm! You need a bespoke learning algorithm for each problem!”.</p>
<p>And so we stood at the impasse of the 1960s. If only we had tried an activation function, any activation function, other than the dreaded 0-1 activation function…</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/XOR_problem_flowchart.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A summary of the XOR argument in a flowchart.</figcaption>
</figure>
</div>
</section>
<section id="where-did-they-go-wrong" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="where-did-they-go-wrong">Where did they go wrong?</h3>
<p>Brains are neural networks in hardware – in this regard, there is no controversy since the 1900s. Intelligence is what happens in the brain. This is the occasion for small controversies from the “embodiment cognition” or “externalism” school, like those of James Gibson and Rodney Brooks, but none that has led to anything substantial yet. Therefore, most people agree that intelligence is something that neural networks do, including those people who are otherwise dismissive of neural networks like Minsky and Papert.</p>
<p>The abstract of a key anti-connectionist paper <span class="citation" data-cites="fodorConnectionismCognitiveArchitecture1988">(<a href="#ref-fodorConnectionismCognitiveArchitecture1988" role="doc-biblioref">Fodor and Pylyshyn 1988</a>)</span> makes the point that the brain is symbolic at the “cognitive level”, and only beneath that level it is connectionist. Interpreted with sufficient charity, this hypothesis is unfalsifiable. None disputes that the brain is connectionist, and the operation of any hardware is symbolic if you use enough symbols to approximate the real numbers. However, at this level of charity, the hypothesis is also useless, therefore we must interpret less charitably.</p>
<p>What did they really mean? They concretely rejected “Parallel Distributed Processing”, and claimed that trained neural networks work if and only if they implement approximations to symbolic programs, where each symbolic variable is represented locally by a small group of neurons (thus not “distributed”), and the variables are processed serially layer by layer through the network (thus not “parallel”). Further, the symbolic programs they approximate are not any kind of symbolic programs (otherwise we fall back to the trivial claim), but symbolic programs that people tend to write, things that on the small scale resemble subroutines and command line scripts, and on the large scale resemble operating systems and <a href="https://en.wikipedia.org/wiki/Cyc">the Cyc project</a>.</p>
<p>At this level, it is quantifiable and thus scientifically testable. However, scientific hypotheses become political disputes when large amounts of money or social justice is on the line. We can consider an alternative history with an alternative Minsky and Papert. In this history, they put this in the epilogue:</p>
<blockquote class="blockquote">
<p>Our mathematical results indicate that we need multilayer perceptrons as well as efficient methods for training them. Furthermore, simple estimates show that brain-level intelligence likely require computing power up to 10 orders of magnitude larger than currently available, suggesting the need for special hardware boards.</p>
<p>We also need to explore alternative architectures capable of correlating global information without using all-to-all connections. Perhaps they should have a two-level structure, with a meta-network generating weights for the network, or perhaps more generic mechanisms for multiplicative interactions. Certain inherently serial operations, such as the connectivity predicate, suggest that there must be ‘serial mode interfaces’ allowing neural networks to call external subroutines. It is a live scientific question whether the number of external subroutines can be kept small. Perhaps a hundred or so would suffice, or perhaps it would turn out that even large neural networks are incapable of most commonsense tasks, in which case the <em>Society of Mind</em> hypothesis would be more viable. However, we consider this an empirical question that can only be answered by attempting to scale up neural networks and seeing what they might do, as <em>a priori</em> estimates of computational difficulty is close to impossible.</p>
</blockquote>
<p>What distinguishes the two possible Minsky–Paperts? Not the facts present, but their prescientific commitments. Minsky’s commitment to elegant mathematics and simple programming structures led him to insist on things for which he could prove theorems – and to denounce empirical methods, especially if large sums of money might be “misdirected” to large-scale neural network machines. Papert, committed to epistemological pluralism, had no choice but to insist on computers that resembled his ideal society – and to denounce any uniform computational structure as flattening, enframing, and reproducing the hegemonic ideology of universalism.</p>
<p>For Papert and Minsky specifically, their claim to be “pro-perceptron” is a sophistry intended to shift the narrative on the perceptron controversy, as they only approved perceptrons with a single layer of learnable parameters. In other words, they were only <em>pro-useless-perceptron</em>. They were trying to kill the project of general large-scale perceptrons, which both Frank Rosenblatt and the new connectionists in the 1980s were working towards.</p>
<blockquote class="blockquote">
<p>There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting “learning theorem” for the multilayered machine will be found. <span class="citation" data-cites="minskyPerceptronsIntroductionComputational1988">(<a href="#ref-minskyPerceptronsIntroductionComputational1988" role="doc-biblioref">Minsky and Papert 1988, 232</a>)</span></p>
</blockquote>
<p>The irony is that decades later, despite the general neglect of neural networks, they quickly overtook symbolic or statistical AI<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> as soon as compute and data price fell so low that they had to appear. And so in 2012, Alex Krizhevsky cobbled together 2 GPUs and train a neural network that outperformed every symbolic or statistical AI.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> There are large homogeneous neural networks that work, and there are hints that some of them have small groups of neurons representing symbolic concepts, some of which are engaged in serial computation across the layers. However, to find these hints of symbolic programs, we had to take a large detour through the brute reason of uniform neural network architecture, uniform GPU architecture, and uniform training objectives.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;More precisely, classical-statistical AI, with fixed parameters, handcrafted features, and solvable models. A classical-statistical model is constructed as some form of <span class="math inline">\(p_\theta(y|x)\)</span>, where <span class="math inline">\(\theta\)</span> are the parameters, <span class="math inline">\(x\)</span> are the inputs, and <span class="math inline">\(y\)</span> are the outputs.</p>
<p>The difference from neural networks is that for classical-statistical models, <span class="math inline">\(p_\theta\)</span> allows solvable inference from a dataset, such as by taking the average, derivative, variance, and such. Many of them were straight up linear regressions on handcrafted features (and thus subject to exactly the criticism of Minsky and Papert).</p>
<p>A good example is the <a href="https://en.wikipedia.org/wiki/IBM_alignment_models#Model_1">IBM alignment model 1</a>, which can be trained by expectation-maximization with <em>closed form solution</em> (!). To see the difference, compare it with <span class="citation" data-cites="bahdanauNeuralMachineTranslation2014">(<a href="#ref-bahdanauNeuralMachineTranslation2014" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>, which also learns to align from a corpus, but does not have any closed form solution.</p></div><div id="fn16"><p><sup>16</sup>&nbsp;And if not Krizhevsky and Hinton, how much longer would it have taken? In 2009, Andrew Ng’s research cluster trained a 100M model on GPUs <span class="citation" data-cites="rainaLargescaleDeepUnsupervised2009">(<a href="#ref-rainaLargescaleDeepUnsupervised2009" role="doc-biblioref">Raina, Madhavan, and Ng 2009</a>)</span>, which suggests that the idea was ripe for taking due to the advance in compute and data, and would have happened around 2010 regardless. <a href="https://en.wikipedia.org/wiki/Rain_follows_the_plow">The rain might not follow the plow</a>, but the AI does follow the compute and data.</p></div></div><p>Why must we take such a large detour? My guess is twofold. One, the resemblance to neat symbolic programs is partial. Large amounts of computing done by neural networks is only symbolic in the trivial, messy way. Only a small amount is symbolic in the neat way. Two, because symbolic programs suffer from <a href="https://en.wikipedia.org/wiki/Diseconomies_of_scale">diseconomies of scale</a>. Peering into any large enough software project, be it the Cyc project, or the Linux source code, one feels that it is easier to start anew than to add to it. Perhaps with thousands of years of very patient work and many evolutionary deadends, purely symbolic AI research can succeed in constructing a general intelligence in the elegant style sketched by Minsky. The irony is that symbolic programs do not scale while neural networks scale, the exact opposite of the lesson that Minsky and Papert wished to impart by their book.</p>
<p>As an example, the history of computer vision demonstrates the problem with the symbolic AI approach. It is true that some problems, such as the parity problem or the connectedness problem, cannot be efficiently solved by neural networks. However, do they really matter? Why do we care about them? We don’t care about solving connectedness for its own sake, but because it is supposed to be a necessary step on the way towards general machine vision, of understanding real scenes. But the surprise of history is that general machine vision turned out to be far <em>less</em> about provably detecting edges and cubes and cones in a picture, and far <em>more</em> about having a large dataset. In this sense, it’s Minsky and Papert who were misled by their experiments with building block-playing robots in a block world. It’s their work that could not scale.</p>
</section>
<section id="what-is-left-of-perceptrons" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="what-is-left-of-perceptrons">What is left of <em>Perceptrons</em>?</h3>
<p>I have never seen a piece of work so systematically opposed to <a href="https://gwern.net/scaling-hypothesis">the scaling hypothesis</a>. Reading their theory, I have the feeling that at every turn, I could hear them say, “Neural networks work – if they have less than 100 neurons.”. To their credit, they made falsifiable hypotheses. To their blame, they were almost all proven wrong. Neural networks do scale, to 100 billion and counting. Several standard architectures constitute almost the entirety of neural networks nowadays – MLP, CNN, GNN, LSTM, VAE, and Transformers. Six is quite far from the thousands of architectures they explicitly predicted.</p>
<p>Among all the objections to neural networks in the <em>Perceptrons</em> book, almost all were either disproved (the anti-scaling hypothesis) or made irrelevant (the perceptron learning rule).</p>
<p>Recognizing connectivity is hard and requires a serial program, but that’s fine, because it’s hard for humans too. Learning to solve logical problems is difficult and requires a thousand iterations. Well, it looks inefficient, except that neural networks are still the best we have even 30 years later, so perhaps the XOR problem is just something neural networks have to work hard for. That’s fine – in the worst case, we’ll just let the neural network offload those logical operations to a symbolic program, much like how humans use calculators.</p>
<p>The only legitimate remaining problem is the recognition of symmetry. It is hard for all modern neural networks, including convolutional and fully connected versions.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> In any case, if human brains are neural networks and they can instantly recognize symmetries, then it shows that there is <em>some</em> remaining architectural trick we don’t yet know.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;It might be solved efficiently with a Transformer, but I need to check this.</p></div></div><p>Therefore, out of all the clever mathematics and wise lessons of <em>Perceptrons</em>, we ended up with… just one problem remaining? Minsky and Papert hoped to show that there would be thousands of different problems, each requiring a bespoke algorithm implemented by a bespoke neural network. In this regard, their project has been fully debunked.</p>
</section>
</section>
<section id="appendix-the-three-camps-of-ai" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="appendix-the-three-camps-of-ai">Appendix: The three camps of AI</h2>
<p>In the early days of AI, there were mainly three camps: cybernetics, symbolic systems, and neural networks. In our current age, it seems the other two camps have fallen largely into oblivion. This section gives a brief history and an orienting perspective of their key ideas.</p>
<section id="cybernetic-ai" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cybernetic-ai">Cybernetic AI</h3>
<p>The founding metaphor of the cybernetic camp was that intelligence is adaptive analog signal processing: homeostasis, Fourier transforms, Kalman filtering, PID control, linear predictive coding, and such.</p>
<p>The origin of cybernetics was entangled with the control of machinery in WWII, when you were either the quick or the dead. Norbert Wiener, the mastermind of cybernetics, worked on anti-aircraft gun (AA gun) controllers. As planes flew faster and higher than ever before, AA guns needed to “<a href="https://en.wikipedia.org/wiki/Deflection_(ballistics)">lead the target</a>” to a greater and greater extent. This put a severe strain on the operator of the AA guns. Wiener constructed electromechanical devices that performed linear prediction of the future trajectory of an aircraft based on its past trajectory. As the aircraft was a human-machine system, Wiener modelled both together as a synthetic whole. After the war, Wiener continued to work on cybernetics, using the analogies he had accumulated during the war.</p>
<blockquote class="blockquote">
<p>If humans do not differ from machines from the “scientific standpoint,” it is because the scientific standpoint of the 1940s was one of men machines at war. The man-airplane-radar-predictor-artillery system is closed one in which it appeared possible to replace men by machines and machines by men. To an antiaircraft operator, the enemy really does act like an autocorrelated servomechanism. … In every piece of his writing on cybernetics, from the first technical exposition of the AA predictor before Pearl Harbor up through his essays of the 1960s on science and society, Wiener put feedback in the foreground, returning again and again to the torpedo, the guided missile, and his antiaircraft director. <span class="citation" data-cites="galisonOntologyEnemyNorbert1994">(<a href="#ref-galisonOntologyEnemyNorbert1994" role="doc-biblioref">Galison 1994</a>)</span></p>
</blockquote>
<p>Cybernetics entered the realm of popular consciousness with Wiener’s 1948 bestseller, <em>Cybernetics</em>. In it, we find a curious description of artificial intelligence and self-reproduction from the analog signal processing point of view, detailed in <a href="https://yuxi-liu-wired.github.io/essays/posts/cybernetic-artificial-intelligence/"><em>Cybernetic artificial intelligence</em></a>. The short version is that it was an analog-circuit quine:</p>
<blockquote class="blockquote">
<p>These operations of analysis, synthesis, and automatic self-adjustment of white boxes into the likeness of black boxes … [by] learning, by choosing appropriate inputs for the black and white boxes and comparing them; and in many of these processes, including the method of Professor Gabor, multiplication devices play an important role. While there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. <span class="citation" data-cites="wienerCyberneticsControlCommunication2019">(<a href="#ref-wienerCyberneticsControlCommunication2019" role="doc-biblioref">Wiener 2019, xli</a>)</span></p>
</blockquote>
<p>The cybernetic approach to AI is a strange chapter in the history of AI, filled with machines too <em>sui generis</em> to have followups. It seems to me that there is no issue with building our way to AI one nonlinear filter at a time, except for technical issues, but the technical issues are insurmountable. One day I might write an essay that gives justice to the cybernetic approach, but as this essay is not about cybernetics, we will only give a whirlwind tour of highlights from the cybernetic approach to AI.</p>
<p>In 1948, <a href="https://en.wikipedia.org/wiki/W._Ross_Ashby">Ross Ashby</a> built a “<a href="https://en.wikipedia.org/wiki/Homeostat">homeostat machine</a>”, consisting of four interacting electromechanical controllers. Each controller had some needles that can move in arcs. If one perturbs it, so that the needles move out of their “comfort zones”, the needles would complete an electric circuit, and the controller would start going through every possible setting one by one, until the needles return to their comfort zones.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> The other thing for which Ashby is famous is the “law of requisite variety”, which is equivalent to the theorem that to solve <span class="math inline">\(f(x) = y\)</span>, generically, the <span class="math inline">\(x\)</span> must have at least as many dimensions as the <span class="math inline">\(y\)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;Perhaps <a href="https://en.wikipedia.org/wiki/Useless_machine">Marvin Minsky’s useless machine</a> was a gentle parody of the homeostat machine. As the homeostat was built in 1948 and the useless machine in 1952, the timing checks out.</p></div></div><div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Homeostat_diagram_1.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="slocumRobotExcessMachine2024">(<a href="#ref-slocumRobotExcessMachine2024" role="doc-biblioref">Slocum and Yablonina 2024, fig. 2</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Homeostat_diagram_2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="slocumRobotExcessMachine2024">(<a href="#ref-slocumRobotExcessMachine2024" role="doc-biblioref">Slocum and Yablonina 2024, fig. 3</a>)</span></figcaption>
</figure>
</div>
<p><a href="https://en.wikipedia.org/wiki/Stafford_Beer">Stafford Beer</a> started his professional life as a business consultant, and devised methods to increase steel plant production efficiency. He understood production management as a problem analogous to that of biological homeostasis – a problem solved by the nervous system – and so he managed production by imitating the nervous system<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. He also investigated a wide variety of strange machines, including one that used an entire pond ecosystem as a computer for black-box homeostatic control <span class="citation" data-cites="beerProgressNoteResearch1962">(<a href="#ref-beerProgressNoteResearch1962" role="doc-biblioref">Beer 1962</a>)</span>:</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;Stafford Beer might have meant this literally, according to <span class="citation" data-cites="pickeringScienceUnknowableStafford2004">(<a href="#ref-pickeringScienceUnknowableStafford2004" role="doc-biblioref">Pickering 2004</a>)</span>:</p>
<blockquote class="blockquote">
<p>… it is clear from subsequent developments that the homeostatic system Beer really had in mind was something like the human spinal cord and brain. He never mentioned this in his work on biological computers, but the image that sticks in my mind is that the brain of the cybernetic factory should really have been an unconscious human body, floating in a vat of nutrients and with electronic readouts tapping its higher and lower reflexes …</p>
</blockquote>
</div></div><blockquote class="blockquote">
<p>Why not use an entire ecological system, such as a pond? The theory on which all this thinking is based does not require a knowledge of the black box employed. Accordingly, over the past year, I have been conducting experiments with a large tank or pond. The contents of the tank were randomly sampled from ponds in Derbyshire and Surrey. Currently there are a few of the usual creatures visible to the naked eye (Hydra, Cyclops, Daphnia, and a leech); microscopically there is the expected multitude of micro-organisms. In this tank are suspended four lights, the intensities of which can be varied to fine limits. At other points are suspended photocells with amplifying circuits which give them very high sensitivity. … There is an ecological guarantee that this system stabilizes itself, and is indeed ultra-stable. But, of course, the problem of making it act as a control system is very difficult indeed. I regard the machine as a tending-to-be-stable-but-not-quite homeostat …</p>
</blockquote>
<p>In 1971, he was invited to become the principal architect of <a href="https://en.wikipedia.org/wiki/Project_Cybersyn">Project Cybersyn</a>, which was a nervous system for the Chilean socialist economy, employing “algedonic control” (“algedonic” is Greek for “pain-pleasure”). This project, like president Allende, was shot in the head by the <a href="https://en.wikipedia.org/wiki/1973_Chilean_coup_d'%C3%A9tat">1973 coup</a> that established a free market economy in Chile. After this, he lived as a hermit, though he still published a few books and did occasional business consulting.<span class="citation" data-cites="morozovPlanningMachineProject2014">(<a href="#ref-morozovPlanningMachineProject2014" role="doc-biblioref">Morozov 2014</a>)</span></p>
<p>In the 1950s, <a href="https://en.wikipedia.org/wiki/Gordon_Pask">Gordon Pask</a> constructed electrochemical “sensory organs”. He prepared a dish of acidic metal salt solution (such as <span class="math inline">\(\text{FeSO}_4\)</span>) and then immersed electrodes into it. Metal tends to dissolve in the acidic solution, but applying a voltage through the electrodes pulls metal out of the solution by electrolysis. This allows him to “reward” whatever metallic structure in the dish by applying a voltage. He reported success at growing some simple sensory organs in the dish <span class="citation" data-cites="gordonNaturalHistoryNetworks1959 carianiEvolveEarEpistemological1993">(<a href="#ref-gordonNaturalHistoryNetworks1959" role="doc-biblioref">Gordon 1959</a>; <a href="#ref-carianiEvolveEarEpistemological1993" role="doc-biblioref">Cariani 1993</a>)</span>:</p>
<blockquote class="blockquote">
<p>We have made an ear and we have made a magnetic receptor. The ear can discriminate two frequencies, one of the order of fifty cycles per second and the other on the order of one hundred cycles per second. The ‘training’ procedure takes approximately half a day and once having got the ability to recognize sound at all, the ability to recognize and discriminate two sounds comes more rapidly. … The ear, incidentally, looks rather like an ear. It is a gap in the thread structure in which you have fibrils which resonate at the excitation frequency. <span class="citation" data-cites="gordonNaturalHistoryNetworks1959">(<a href="#ref-gordonNaturalHistoryNetworks1959" role="doc-biblioref">Gordon 1959</a>)</span></p>
</blockquote>
<p>The details of the electrochemical ear are lost, and this line of research had no followups.</p>
<p>A faint echo of Pask’s electrochemical ear was heard in late 1990s, when <a href="https://web.archive.org/web/20231212170749/https://www.damninteresting.com/on-the-origin-of-circuits/">Adrian Thompson used evolutionary algorithm</a> to evolve circuits on <a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">field-programmable gate arrays</a> to tell apart input signals of frequencies <span class="math inline">\(1 \mathrm{~kHz}\)</span> and <span class="math inline">\(10 \mathrm{~kHz}\)</span>. Curiously, some circuits succeeded at the task despite using no master clock signal. He traced this down to the detailed physical properties that digital circuit design was precisely meant to abstract away from. The circuit functioned precisely <em>because</em> the electronic elements were not digital, but analog.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> The circuits’ performance degraded when outside the temperature range in which they evolved in <span class="citation" data-cites="thompsonAnalysisUnconventionalEvolved1999 thompsonExplorationsDesignSpace1999">(<a href="#ref-thompsonAnalysisUnconventionalEvolved1999" role="doc-biblioref">Thompson and Layzell 1999</a>; <a href="#ref-thompsonExplorationsDesignSpace1999" role="doc-biblioref">Thompson, Layzell, and Zebulum 1999</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;It is as if a linear neural network managed to compute a nonlinear function precisely because floating point operations are not perfect.<span class="citation" data-cites="foersterNonlinearComputationDeep2017">(<a href="#ref-foersterNonlinearComputationDeep2017" role="doc-biblioref">Foerster 2017</a>)</span></p></div></div><blockquote class="blockquote">
<p>… at <span class="math inline">\(43.0^{\circ} \mathrm{C}\)</span> the output is not steady at <span class="math inline">\(+5 \mathrm{~V}\)</span> for <span class="math inline">\(\mathrm{F} 1\)</span>, but is pulsing to <span class="math inline">\(0 \mathrm{~V}\)</span> for a small fraction of the time. Conversely, at <span class="math inline">\(23.5^{\circ} \mathrm{C}\)</span> the output is not a steady <span class="math inline">\(0 \mathrm{~V}\)</span> for <span class="math inline">\(\mathrm{F} 2\)</span>, but is pulsing to <span class="math inline">\(+5 \mathrm{~V}\)</span> for a small fraction of the time. This is not surprising: the only time reference that the system has is the natural dynamical behaviour of the components, and properties such as resistance, capacitance and propagation delays are temperature dependent. The circuit operates perfectly over the <span class="math inline">\(10^{\circ} \mathrm{C}\)</span> range of temperatures that the population was exposed to during evolution, and no more could reasonably be expected of it. <span class="citation" data-cites="thompsonEvolvedCircuitIntrinsic1996">(<a href="#ref-thompsonEvolvedCircuitIntrinsic1996" role="doc-biblioref">Thompson 1996</a>)</span></p>
</blockquote>
<p>Continuing the tradition of one-hit wonders, there was no followup work to this.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;I have tried tracking down what Adrian Thompson has been doing since his late 1990s work. It turned out that the hardware evolution <a href="https://web.archive.org/web/20201015175808/https://sites.google.com/site/thompsonevolvablehardware/publications">was his PhD work</a> <span class="citation" data-cites="thompsonHardwareEvolutionAutomatic1998">(<a href="#ref-thompsonHardwareEvolutionAutomatic1998" role="doc-biblioref">Thompson 1998</a>)</span>. He has almost completely dropped off the face of academia. His website at University of Sussex <a href="https://web.archive.org/web/20030402181211/http://www.cogs.susx.ac.uk/users/adrianth/">did not see another update since 2002</a> and is currently dead. His <a href="https://web.archive.org/web/20201015175808/https://sites.google.com/site/thompsonevolvablehardware/publications">minimalistic Google Site</a> was created around 2014, and currently only survives on the Internet Archive. There was also a single <code>gif</code> of the circuit in operation, which I decided to download and <a href="./figure/gen1400_150x113_lossy25.gif">save for posterity</a>.</p></div></div></section>
<section id="symbolic-ai" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="symbolic-ai">Symbolic AI</h3>
<p>The founding metaphor of the symbolic system camp was that intelligence is symbolic manipulation using preprogrammed symbolic rules: logical inference, heuristic tree search, list processing, syntactic trees, and such. The symbolic camp was not strongly centered, though it had key players like Alan Turing, John McCarthy, Herbert Simon, and Marvin Minsky.</p>
<p>The project of symbolic AI is a curious episode in AI history. Despite its early successes such as SHRDLU, LISP, and ELIZA, despite the support of most of AI researchers during 1960–2000, and despite the support of reigning cognitivists in the adjacent field of psychology, it never achieved something as successful as neural networks.</p>
<p>A brief sketch of the greatest project in symbolic AI might give you a feel for the difficulty involved. Have you ever heard of the Cyc doing anything useful in its 39 years of life? Compare that with something even as small as BERT.</p>
<p>In 1984, <a href="https://en.wikipedia.org/wiki/Douglas_Lenat">Douglas Lenat</a> began the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. The Cyc project used a LISP-like symbolic logic language to encode common sense. Even from the vantage point of 1985, it was clear to all that there was a lot of common sense to code in <a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, although few could have predicted that Lenat would persevere at it for over 30 years. In 2016, Lenat finally declared the Cyc project “done” and set about commercializing it.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;They themselves probably underestimated the difficulty. In 1990, they confidently titled a paper “Cyc: A midterm report” <span class="citation" data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">Lenat and Guha 1990</a>)</span>, suggesting that they expected to be done around 1995.</p></div></div><blockquote class="blockquote">
<p>Having spent the past 31 years memorizing an astonishing collection of general knowledge, the artificial-intelligence engine created by Doug Lenat is finally ready to go to work. Lenat’s creation is Cyc, a knowledge base of semantic information designed to give computers some understanding of how things work in the real world. … “Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, the company is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri but less predisposed to foolish misunderstandings. Michael Stewart, a longtime collaborator of Lenat’s and the CEO of Lucid, says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies. <span class="citation" data-cites="knightAI30Years2016">(<a href="#ref-knightAI30Years2016" role="doc-biblioref">Knight 2016</a>)</span></p>
</blockquote>
<p>That was essentially the last we heard from Cyc.</p>
<p>Why has the symbolic AI project failed to scale? It is hard to say, and a definitive answer would not be forthcoming until we have a human level AI as an existence proof of the necessary requirements for scalable AI. However, I can speculate. When looking at this figure from 1985, one is simultaneously filled with respect and sadness, for they were facing impossible odds, and yet they charged right into it.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/cyc_project_ontology.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">Lenat, Prakash, and Shepherd 1985, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>Their “midterm report” only accentuates this sense of tragedy, of seeing them fighting impossible odds, and losing. They saw with clarity that there is no shortcut to intelligence, no “Maxwell’s equations of thought”.</p>
<blockquote class="blockquote">
<p>The majority of work in knowledge representation has dealt with the technicalities of relating predicate calculus to other formalisms and with the details of various schemes for default reasoning. There has almost been an aversion to addressing the problems that arise in actually representing large bodies of knowledge with content. However, deep, important issues must be addressed if we are to ever have a large intelligent knowledge-based program: What ontological categories would make up an adequate set for carving up the universe? How are they related? What are the important facts and heuristics most humans today know about solid objects? And so on. In short, we must bite the bullet.</p>
<p>We don’t believe there is any shortcut to being intelligent, any yet-to-be-discovered Maxwell’s equations of thought, any AI Risc architecture that will yield vast amounts of problem-solving power. Although issues such as architecture are important, no powerful formalism can obviate the need for a lot of knowledge.</p>
<p>By knowledge, we don’t just mean dry, almanac-like or highly domain-specific facts. Rather, most of what we need to know to get by in the real world is prescientific (knowledge that is too commonsensical to be included in reference books; for example, animals live for a single solid interval of time, nothing can be in two places at once, animals don’t like pain), dynamic (scripts and rules of thumb for solving problems) and metaknowledge (how to fill in gaps in the knowledge base, how to keep it organized, how to monitor and switch among problem-solving methods, and so on). Perhaps the hardest truth to face, one that AI has been trying to wriggle out of for 34 years, is that there is probably no elegant, effortless way to obtain this immense knowledge base. Rather, the bulk of the effort must (at least initially) be manual entry of assertion after assertion. <span class="citation" data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">Lenat and Guha 1990</a>)</span></p>
</blockquote>
<p>I was struck by the same sense of ontological vertigo when looking back at Simon and Newell’s <em>Human Problem Solving</em>, a compendium of their work on decomposing human problem solving into symbolic processes:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/human_problem_solving_behavior_graph.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="newellHumanProblemSolving1972">(<a href="#ref-newellHumanProblemSolving1972" role="doc-biblioref">Newell and Simon 1972, 533</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/human_problem_solving_transcript.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="newellHumanProblemSolving1972">(<a href="#ref-newellHumanProblemSolving1972" role="doc-biblioref">Newell and Simon 1972, 534</a>)</span></figcaption>
</figure>
</div>
<p>This sense of vertigo is perhaps best described by Borges in <em>The analytical language of John Wilkins</em> <span class="citation" data-cites="borgesSelectedNonfictions2000">(<a href="#ref-borgesSelectedNonfictions2000" role="doc-biblioref">Borges 2000, 229–32</a>)</span>:</p>
<blockquote class="blockquote">
<p>… we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.</p>
<p>These ambiguities, redundancies, and deficiencies recall those attributed by Dr.&nbsp;Franz Kuhn to a certain Chinese encyclopedia called the Heavenly Emporium of Benevolent Knowledge. In its distant pages it is written that animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained; (d) suckling pigs; (e) mermaids; (f) fabulous ones; (g) stray dogs; (h) those that are included in this classification; (i) those that tremble as if they were mad; (j) in­ numerable ones; (k) those drawn with a very fine camel’s-hair brush; (1) etcetera; (m) those that have just broken the flower vase; (n) those that at a distance resemble flies. The Bibliographical Institute of Brussels also exercises chaos: it has parceled the universe into 1,000 subdivisions, of which number 262 corresponds to the Pope, number 282 to the Roman Catholic Church, number 263 to the Lord’s Day, number 268 to Sunday schools, number 298 to Mormonism, and number 294 to Brahmanism, Buddhism, Shintoism, and Taoism. Nor does it disdain the employment of heterogeneous subdivisions, for example, number 179: “Cruelty to animals. Protection of animals. Dueling and suicide from a moral point of view. Various vices and defects. Various virtues and qualities.”</p>
</blockquote>
</section>
</section>
<section id="appendix-the-chomskyans" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="appendix-the-chomskyans">Appendix: the Chomskyans</h2>
<p>In the field of psychology, the neural network vs Minsky divide corresponds quite well with the behaviorism vs cognitivism divide.</p>
<p>Among the cognitivists, there is no consensus view about the exact role that neural networks must play. Some, like Fodor and Marr, argue that intelligence is just a hardware-independent program, and brains just happen to use neural networks to run the program due to accidents of evolution and biophysics. Others, like Gary Marcus, grant neural networks a more substantial role in constraining the possible programs – that is, each neural network architecture is designed , and each architecture can only run a certain kind of program efficiently – but they still insist that neural networks must have very particular architectures.</p>
<p>Some might allow simple learning algorithms with simple environmental stimulus (the universal grammar approach of Chomsky), while others might allow complex learning algorithms with complex environmental stimulus (the society of mind approach of Minsky), but what unifies the cognitivists is that they are against simple learning algorithms applied to complex environmental stimulus. They called this enemy by many names, such as “radical behaviorism”, “Skinnerism”, “perceptrons”, “radical connectionism”, and now “deep learning”.</p>
<section id="noam-chomsky" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="noam-chomsky">Noam Chomsky</h3>
<p>The cognitivist revolution was led by Noam Chomsky against behaviorism during the 1950s, ending with the victory of cognitivism in “higher psychology”, such as linguistics, though behaviorism survived respectably to this day in other aspects of psychology, such as addiction studies and animal behavior.</p>
<p>In a curious parallel, just as neural networks for AI became popular again in the late 1980s, statistical methods for NLP returned during the same period. The watershed moment was the series of <a href="https://en.wikipedia.org/wiki/IBM_alignment_models">IBM alignment models</a> published in 1993 <span class="citation" data-cites="brownMathematicsStatisticalMachine1993">(<a href="#ref-brownMathematicsStatisticalMachine1993" role="doc-biblioref">Brown et al. 1993</a>)</span>.</p>
<p>In the 1950s, language production was modelled by theoretical machines: finite state automata, stack machines, Markov transition models, and variants thereof. We must understand Chomsky’s two contributions to linguistics. On the first part, he constructed a <a href="https://en.wikipedia.org/wiki/Chomsky_hierarchy">hierarchy</a> of increasingly powerful kinds of language. This hierarchy was in one-to-one correspondence with the hierarchy of theoretical machines, from finite state automata (type-3, or regular) all the way to Turing machines (type-0, or recursively enumerable). On the second part, he rejected statistical language learning and argued for inborn universal grammar.</p>
<p>Chomsky argued, and subsequent linguists have found, that the syntax of all human languages is at the type-2 level, or a <a href="https://en.wikipedia.org/wiki/Context-free_grammar">context-free grammar</a>. None are regular and almost none are context-dependent. Regular languages are modeled by finite state machines and cannot model arbitrarily deep recursion, whereas context-free languages allow for arbitrarily deep recursion such as <a href="https://en.wikipedia.org/wiki/Center_embedding">center embedding</a>. This fact would come into play later.</p>
<p>With the dominance of the Chomsky approach, finite state machines were abandoned, as they are not capable of parsing context-free grammar. You might have heard of the controversy over Pirahã. It is mainly fought over the problem of recursion: does the Pirahã language have recursion or not?<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;Tracing the battle lines, I predicted that Pinker would argue that it must have recursion… and I turned out to be wrong. Pinker argued against Chomsky in this case.</p>
<blockquote class="blockquote">
<p>“There’s a lot of strange stuff going on in the Chomskyan program. He’s a guru, he makes pronouncements that his disciples accept on faith and that he doesn’t feel compelled to defend in the conventional scientific manner. Some of them become accepted within his circle as God’s truth without really being properly evaluated, and, surprisingly for someone who talks about universal grammar, he hasn’t actually done the spadework of seeing how it works in some weird little language that they speak in New Guinea.”</p>
<p>Pinker says that his own doubts about the “Chomskyan program” increased in 2002, when Marc Hauser, Chomsky, and Tecumseh Fitch published their paper on recursion in Science. The authors wrote that the distinctive feature of the human faculty of language, narrowly defined, is recursion. Dogs, starlings, whales, porpoises, and chimpanzees all use vocally generated sounds to communicate with other members of their species, but none do so recursively, and thus none can produce complex utterances of infinitely varied meaning. “Recursion had always been an important part of Chomsky’s theory,” Pinker said. “But in Chomsky Mark II, or Mark III, or Mark VII, he all of a sudden said that the only thing unique to language is recursion. It’s not just that it’s the universal that has to be there; it’s the magic ingredient that makes language possible.” <span class="citation" data-cites="colapintoInterpreter2007">(<a href="#ref-colapintoInterpreter2007" role="doc-biblioref">Colapinto 2007</a>)</span></p>
</blockquote>
</div></div><p>A key principle Chomsky used was the “poverty of stimulus” argument, which he used to argue that humans must have a universal grammar built in at birth, because children cannot possibly learn to speak when they are just a few years old – they cannot possibly have heard and seen enough. For one thing, true recursion can never be learned empirically, because true recursion can only be conclusively proven by observing an infinite number of sentences.</p>
<p>Consider the simple example of the <a href="https://en.wikipedia.org/wiki/Dyck_language">balanced brackets language</a>. A language learner observes sample sentences from the language and tries to infer the language. Suppose the learner sees a sequence <code>(), (()), ((())), (((())))</code>. What can they conclude? That it is the balanced brackets language? So we ask them to construct another sentence, and they confidently write <code>((((()))))</code>, but then we inform them that they have been tricked! The language is the balanced brackets language – except that the brackets only go 4 levels deep. In general, only by seeing all levels of recursion can the balanced brackets language be <em>conclusively</em> learned.</p>
<p>Applied to linguistics, Chomsky claimed that statistical learning cannot learn syntax, and all attempts have been “colossal failures”.</p>
<blockquote class="blockquote">
<p>Just to illustrate, I’ll take one example that was presented back in the 1950s and has become a sort of a classic case because it’s so trivial. ‘Can eagles that fly swim?’ Okay, simple sentence. Everyone understands it. Any young child understands it. There is a question about it. We know that we associate the word ‘can’ with ‘swim,’ not with ‘fly.’ We’re asking ‘Can they swim?’ We’re not asking ‘Can they fly?’ Well, why is that? A natural answer ought to be that you associate ‘can’ with ‘fly.’ After all, ‘fly’ is the word that’s closest to ‘can,’ so why don’t you just take the closest word and interpret it that way? … Well, that property is universal. It holds up in every language. Languages may do it differently, but they’re going to have the same property. It holds in every construction anyone knows, and it’s just a universal property of language.</p>
<p>Well, this particular example has taken on a life of its own. For one thing, it’s a poverty of stimulus problem, like Descartes’ triangle. There’s been a huge effort to show that it’s not a problem, that if you just do a complex statistical analysis of complex data, you’ll find that that’s what the child will determine from the data. The approaches are odd in several respects. First, every one is not only a failure but a colossal failure. I’m not going to talk about that. I actually have a recent paper about it with a computer scientist at MIT, Bob Berwick, where we run through a lot of the current proposals but it’s easy to show that they’re all just wildly wrong. But they keep coming. Almost every issue of the main cognitive science journals has another proposal of this sort, so that’s one odd fact. There are many efforts to show that there is nothing puzzling about it, they’re all colossal failures. <span class="citation" data-cites="chomskyPovertyStimulusUnfinished2010">(<a href="#ref-chomskyPovertyStimulusUnfinished2010" role="doc-biblioref">Chomsky 2010</a>)</span></p>
</blockquote>
<p>At last, a testable hypothesis! I asked GPT-4 to <code>Draw a syntax tree for "Can eagles that fly swim?"</code>, and received this:<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;The code it gave failed on the first try, due to an environment variable issue on the Linux virtual machine it ran on. We fixed it after two more plies of conversation.</p></div></div><div class="callout callout-style-default callout-note callout-titled" title="Code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> Tree</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> svgling</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> SVG, display</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> Tree.fromstring(<span class="st">"""</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">    (SQ</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">        (MD Can)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">        (NP</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">            (NNS eagles)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="st">            (SBAR</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">                (WHNP (WDT that))</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="st">                (S</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">                    (VP (VBP fly))</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">                )</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">            )</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="st">        )</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="st">        (VP (VB swim))</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="st">    )</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>tree_svg <span class="op">=</span> svgling.draw_tree(tree)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>tree_svg.get_svg().save()</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>display(SVG(tree_svg.get_svg()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/can_eagles_that_fly_swim_tree.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The syntax tree for “Can eagles that fly swim?”, drawn by GPT-4.</figcaption>
</figure>
</div>
<p>Chomsky has consistently rejected statistical language learning from the beginning to the end.</p>
<blockquote class="blockquote">
<p>But it must be recognized that the notion of “probability of a sentence” is an entirely useless one, under any known interpretation of this term.</p>
<p><span class="citation" data-cites="chomskyEmpiricalAssumptionsModern1969">(<a href="#ref-chomskyEmpiricalAssumptionsModern1969" role="doc-biblioref">Chomsky 1969</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>It’s true there’s been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success … which I think is novel in the history of science. It interprets success as approximating unanalyzed data.</p>
<p>Chomsky at the MIT150: Brains, Minds and Machines Symposium (2011), quoted in <span class="citation" data-cites="norvigChomskyTwoCultures2017">(<a href="#ref-norvigChomskyTwoCultures2017" role="doc-biblioref">Norvig 2017</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Well, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They’ve achieved zero… GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It’ll use even more energy and achieve exactly nothing, for the same reasons. So there’s nothing to discuss.</p>
<p><a href="https://whimsical.com/question-1-large-language-models-such-as-gpt-3-DJmQ8R5kD8tqvsXxgCN9vK">Machine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding (2022)</a></p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Peter_Norvig">Peter Norvig</a> gave a detailed analysis and a rebuttal in <span class="citation" data-cites="norvigChomskyTwoCultures2017">(<a href="#ref-norvigChomskyTwoCultures2017" role="doc-biblioref">Norvig 2017</a>)</span>.</p>
</section>
<section id="the-chomskyans" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-chomskyans">The Chomskyans</h3>
<p><a href="https://en.wikipedia.org/wiki/Language_identification_in_the_limit#Gold's_theorem">Gold’s theorem about language learning in the limit</a> is occasionally cited in the same context as a justification for the “poverty of stimulus” argument. It appears Chomsky did not regard it as a relevant argument <span class="citation" data-cites="johnsonGoldTheoremCognitive2004">(<a href="#ref-johnsonGoldTheoremCognitive2004" role="doc-biblioref">Johnson 2004</a>)</span>, and I agree with Chomsky in this respect, as Gold’s theorem is extremely generic.</p>
<p>After the second rise of neural networks, there was a bitter controversy that raged in the 1990s but is now essentially forgotten: the past tense debate. On one side were the connectionists, and on the other were the cognitivists, including Steven Pinker and Gary Marcus <span class="citation" data-cites="pinkerFutureTense2002">(<a href="#ref-pinkerFutureTense2002" role="doc-biblioref">Pinker and Ullman 2002</a>)</span>. Tellingly, both Steven Pinker and Gary Marcus sided with the cognitivists. Steven Pinker is best known for his other books such as <em>The Blank Slate</em>, which applies Chomskyan linguistics to general psychology.</p>
<p>Human language exhibits a distinctly fractal-like structure: for every rule, there are exceptions, and for every exception, there are exceptional exceptions, and so on. This is called “quasi-regularity”. Sejnowski in an interview described how quasi-regularity shows up in phonology, and offered a perfect demonstration project for the power of neural networks against the Chomskyans:</p>
<blockquote class="blockquote">
<p>I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they’re doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, “Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do.” <span class="citation" data-cites="rosenfeldTalkingNetsOral2000">(<a href="#ref-rosenfeldTalkingNetsOral2000" role="doc-biblioref">Rosenfeld and Anderson 2000, 324–25</a>)</span></p>
</blockquote>
<p><span class="citation" data-cites="sejnowskiDeepLearningRevolution2018">(<a href="#ref-sejnowskiDeepLearningRevolution2018" role="doc-biblioref">Sejnowski 2018, 75–78</a>)</span> recounts an anecdote about <a href="https://en.wikipedia.org/wiki/Jerry_Fodor">Jerry Fodor</a>, another prominent cognitivist. While Fodor is no longer a hot name in AI nowadays, this anecdote illustrates the Chomsky way of thinking.</p>
<blockquote class="blockquote">
<p>In 1988, I served on a committee for the McDonnell and Pew Foundations that interviewed prominent cognitive scientists and neuroscientists to get their recommendations on how to jump-start a new field called “cognitive neuroscience”. … [Fodor] started by throwing down the gauntlet, “Cognitive neuroscience is not a science and it never will be.” … Fodor explained why the mind had to be thought of as a modular symbol-processing system running an intelligent computer program. [Patricia Churchland] asked him whether his theory also applied to cats. “Yes,” said Fodor, “cats are running the cat program.” But when Mortimer Mishkin, an NIH neuroscientist studying vision and memory, asked him to tell us about discoveries made in his own lab, Fodor mumbled something I couldn’t follow about using event-related potentials in a language experiment. Mercifully, at that moment, a fire drill was called and we all filed outside. Standing in the courtyard, I overheard Mishkin say to Fodor: “Those are pretty small potatoes.” When the drill was over, Fodor had disappeared.</p>
</blockquote>
<p>Minsky was himself dismissive of the Chomskyans:</p>
<blockquote class="blockquote">
<p>… fairly soon ideas that have been brewing in AI since the 1960’s, on making computers understand significant fragments of natural language, will enter and, I’m sure, soon dominate the main stream of Linguistics. (In the era of these memos, it was the students in AI, almost alone, who carried on the quest for meaningful theories of linguistic processes, when most all other language work was stuck in shallow, syntax-oriented, formalisms.)</p>
<p><span class="citation" data-cites="minskyIntroductionCOMTEXMicrofiche1983">(<a href="#ref-minskyIntroductionCOMTEXMicrofiche1983" role="doc-biblioref">Minsky 1983</a>)</span></p>
</blockquote>
<p>Similarly, Gary Marcus has consistently criticized neural network language models since at least 1992 <span class="citation" data-cites="marcusOverregularizationLanguageAcquisition1992">(<a href="#ref-marcusOverregularizationLanguageAcquisition1992" role="doc-biblioref">G. F. Marcus et al. 1992</a>)</span>. His theory of intelligence is fundamentally Chomskyan: neural networks can exhibit intelligence but only if they implement rules for symbolic manipulation.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> Moreover, many symbolic rules must be present at birth, by the poverty of stimulus.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;See <span class="citation" data-cites="marcusAlgebraicMindIntegrating2003">(<a href="#ref-marcusAlgebraicMindIntegrating2003" role="doc-biblioref">G. F. Marcus 2003</a>)</span> for a book-length treatment.</p></div></div><p>For example, here is him saying in 1993:</p>
<blockquote class="blockquote">
<p>Whether children require “negative evidence” (i.e., information about which strings of words are not grammatical sentences) to eliminate their ungrammatical utterances is a central question in language acquisition because, lacking negative evidence, a child would require internal mechanisms to unlearn grammatical errors. … There is no evidence that noisy feedback is required for language learning, or even that noisy feedback exists. Thus internal mechanisms are necessary to account for the unlearning of ungrammatical utterances.</p>
<p><span class="citation" data-cites="marcusNegativeEvidenceLanguage1993">(<a href="#ref-marcusNegativeEvidenceLanguage1993" role="doc-biblioref">G. F. Marcus 1993</a>)</span></p>
</blockquote>
<p>And here is him in 2012, quoting the old past tense debate to urge caution against the hype of AlexNet:</p>
<blockquote class="blockquote">
<p>They learned slowly and inefficiently, and as Steven Pinker and I showed, couldn’t master even some of the basic things that children do, like learning the past tense of regular verbs. By the late nineteen-nineties, neural networks had again begun to fall out of favor. … Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships, and are likely to face challenges in acquiring abstract ideas like “sibling” or “identical to”. They have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. … Norvig is clearly very interested in seeing what Hinton could come up with. But even Norvig didn’t see how you could build a machine that could understand stories using deep learning alone.</p>
<p><span class="citation" data-cites="marcusDeepLearningRevolution2012">(<a href="#ref-marcusDeepLearningRevolution2012" role="doc-biblioref">G. Marcus 2012</a>)</span></p>
</blockquote>
<p>And here is him writing in 2018 dismissing deep learning as not working on the problem of general intelligence, just working on applications:</p>
<blockquote class="blockquote">
<p>Once upon a time, before the fashionable rise of machine learning and “big data”, A.I. researchers tried to understand how complex knowledge could be encoded and processed in computers. This project, known as knowledge engineering… A.I. researchers need to return to that project sooner rather than later, ideally enlisting the help of cognitive psychologists who study the question of how human cognition manages to be endlessly flexible. Today’s dominant approach to A.I. has not worked out. Yes, some remarkable applications have been built from it, including Google Translate and Google Duplex. But the limitations of these applications as a form of intelligence should be a wake-up call.</p>
<p><span class="citation" data-cites="marcusOpinionAIHarder2018">(<a href="#ref-marcusOpinionAIHarder2018" role="doc-biblioref">G. Marcus and Davis 2018</a>)</span></p>
</blockquote>
<p>As it happens, 2018 was the year of Transformer revolution in natural language modeling, with BERT and GPT-1. Never one to give up, he continued with the same criticisms as easily as substituting “Transformers” for “recurrent networks”. Given the track record, he is conveniently predictable,<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> and we can expect nothing less than his recent criticisms of deep learning in general <span class="citation" data-cites="marcusDeepLearningCritical2018">(<a href="#ref-marcusDeepLearningCritical2018" role="doc-biblioref">G. Marcus 2018</a>)</span> and <a href="https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/">large language models in particular</a>, <a href="https://www.theatlantic.com/technology/archive/2023/03/ai-chatbots-large-language-model-misinformation/673376/">repeatedly</a>.</p>


<!-- -->


<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;It would be funny if someone could train a language model to pass the “Gary Marcus test”: impersonate Gary Marcus in a Turing test setup. If such a model were to pass, Marcus would either have to admit that the language model makes sense or accept that what he says is indistinguishable from nonsense.</p>
<p>The same could work for Noam Chomsky – a statistical language model who can editorialize against every one of GPT-5, Gemini 2, GPT-6, etc, as soon as they come out. A <a href="https://en.wikipedia.org/wiki/Performative_contradiction">performative contradiction</a>.</p></div></div></section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bahdanauNeuralMachineTranslation2014" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural <span>Machine Translation</span> by <span>Jointly Learning</span> to <span>Align</span> and <span>Translate</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.
</div>
<div id="ref-beerProgressNoteResearch1962" class="csl-entry" role="listitem">
Beer, Stafford. 1962. <span>“A Progress Note on Research into a Cybernetic Analogue of Fabric.”</span>
</div>
<div id="ref-bernsteinMarvinMinskyVision1981" class="csl-entry" role="listitem">
Bernstein, Jeremy. 1981. <span>“Marvin <span>Minsky</span>’s <span>Vision</span> of the <span>Future</span>.”</span> <em>The New Yorker</em>, December. <a href="https://www.newyorker.com/magazine/1981/12/14/a-i">https://www.newyorker.com/magazine/1981/12/14/a-i</a>.
</div>
<div id="ref-borgesSelectedNonfictions2000" class="csl-entry" role="listitem">
Borges, Jorge Luis. 2000. <em>Selected Non-Fictions</em>. Edited by Eliot Weinberger. New York: Penguin Books. <a href="http://books.google.com/books?id=BpUrAQAAMAAJ">http://books.google.com/books?id=BpUrAQAAMAAJ</a>.
</div>
<div id="ref-breimanReflectionsRefereeingPapers1995" class="csl-entry" role="listitem">
Breiman, Leo. 1995. <span>“Reflections <span>After Refereeing Papers</span> for <span>NIPS</span>.”</span> In <em>The <span>Mathematics Of Generalization</span></em>. CRC Press.
</div>
<div id="ref-brownMathematicsStatisticalMachine1993" class="csl-entry" role="listitem">
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. <span>“The Mathematics of Statistical Machine Translation: <span>Parameter</span> Estimation.”</span>
</div>
<div id="ref-carianiEvolveEarEpistemological1993" class="csl-entry" role="listitem">
Cariani, Peter. 1993. <span>“To Evolve an Ear. <span>Epistemological</span> Implications of Gordon Pask’s Electrochemical Devices.”</span> <em>Systems Research</em> 10 (3): 19–33. <a href="https://doi.org/10/bv3vd8">https://doi.org/10/bv3vd8</a>.
</div>
<div id="ref-chomskyEmpiricalAssumptionsModern1969" class="csl-entry" role="listitem">
Chomsky, Noam. 1969. <span>“Some <span>Empirical Assumptions</span> in <span>Modern Philosophy</span> of <span>Language</span>.”</span> In <em>Philosophy, Science, and Method</em>, edited by Ernest Nagel, Sidney Morgenbesser, Patrick Suppes, and Morton White. St. Martin’s Press.
</div>
<div id="ref-chomskyPovertyStimulusUnfinished2010" class="csl-entry" role="listitem">
———. 2010. <span>“Poverty of <span>Stimulus</span>: <span>Unfinished Business</span>.”</span> Johannes Gutenberg-Universit<span>ä</span>t. <a href="https://web.archive.org/web/20230216033431/https://www.blogs.uni-mainz.de/studgen-stiftung-jgsp-eng/files/2019/01/Mainz_transcript_edited2.pdf">https://web.archive.org/web/20230216033431/https://www.blogs.uni-mainz.de/studgen-stiftung-jgsp-eng/files/2019/01/Mainz_transcript_edited2.pdf</a>.
</div>
<div id="ref-colapintoInterpreter2007" class="csl-entry" role="listitem">
Colapinto, John. 2007. <span>“The Interpreter.”</span> <em>The New Yorker</em>, April, 125. <a href="https://blogs.baruch.cuny.edu/anthropology1001/files/2011/01/NewYorkerPirahaArticle1.pdf">https://blogs.baruch.cuny.edu/anthropology1001/files/2011/01/NewYorkerPirahaArticle1.pdf</a>.
</div>
<div id="ref-dertouzosThresholdLogicSynthesis1965" class="csl-entry" role="listitem">
Dertouzos, Michael. 1965. <em>Threshold Logic: <span>A Synthesis Approach</span></em>. <a href="https://mitpress.mit.edu/9780262040099/">https://mitpress.mit.edu/9780262040099/</a>.
</div>
<div id="ref-dudaPatternClassificationScene1973" class="csl-entry" role="listitem">
Duda, Richard O., and Peter E. Hart. 1973. <em>Pattern Classification and Scene Analysis</em>. New York: Wiley.
</div>
<div id="ref-dudaPatternClassification2001" class="csl-entry" role="listitem">
Duda, Richard O., Peter E. Hart, and David G. Stork. 2001. <em>Pattern Classification</em>. 2nd ed. New York: Wiley.
</div>
<div id="ref-dysonContinuousFunctionsDefined1951" class="csl-entry" role="listitem">
Dyson, Freeman J. 1951. <span>“Continuous Functions Defined on Spheres.”</span> <em>Annals of Mathematics</em>, 534–36. <a href="https://doi.org/10.2307/1969487">https://doi.org/10.2307/1969487</a>.
</div>
<div id="ref-fodorConnectionismCognitiveArchitecture1988" class="csl-entry" role="listitem">
Fodor, Jerry A., and Zenon W. Pylyshyn. 1988. <span>“Connectionism and Cognitive Architecture: <span>A</span> Critical Analysis.”</span> <em>Cognition</em> 28 (1-2): 3–71. <a href="https://doi.org/10.1016/0010-0277(88)90031-5">https://doi.org/10.1016/0010-0277(88)90031-5</a>.
</div>
<div id="ref-foersterNonlinearComputationDeep2017" class="csl-entry" role="listitem">
Foerster, Jakob. 2017. <span>“Nonlinear Computation in Deep Linear Networks.”</span> <a href="https://openai.com/research/nonlinear-computation-in-deep-linear-networks">https://openai.com/research/nonlinear-computation-in-deep-linear-networks</a>.
</div>
<div id="ref-fukushimaVisualFeatureExtraction1969" class="csl-entry" role="listitem">
Fukushima, Kunihiko. 1969. <span>“Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements.”</span> <em>IEEE Transactions on Systems Science and Cybernetics</em> 5 (4): 322–33. <a href="https://ieeexplore.ieee.org/abstract/document/4082265/">https://ieeexplore.ieee.org/abstract/document/4082265/</a>.
</div>
<div id="ref-galisonOntologyEnemyNorbert1994" class="csl-entry" role="listitem">
Galison, Peter. 1994. <span>“The Ontology of the Enemy: <span>Norbert Wiener</span> and the Cybernetic Vision.”</span> <em>Critical Inquiry</em> 21 (1): 228–66. <a href="https://doi.org/10/fpfd3b">https://doi.org/10/fpfd3b</a>.
</div>
<div id="ref-gordonNaturalHistoryNetworks1959" class="csl-entry" role="listitem">
Gordon, Pask. 1959. <span>“The Natural History of Networks.”</span> <em>Proceedings of International Tracts In Computer Science and Technology and Their Application</em> 2: 232–63. <a href="https://www.pangaro.com/pask/Pask-1960-TheNaturalHistoryofNetworks.pdf">https://www.pangaro.com/pask/Pask-1960-TheNaturalHistoryofNetworks.pdf</a>.
</div>
<div id="ref-hubelReceptiveFieldsSingle1959" class="csl-entry" role="listitem">
Hubel, David H., and Torsten N. Wiesel. 1959. <span>“Receptive Fields of Single Neurones in the Cat’s Striate Cortex.”</span> <em>The Journal of Physiology</em> 148 (3): 574. <a href="https://doi.org/10.1113/jphysiol.1959.sp006308">https://doi.org/10.1113/jphysiol.1959.sp006308</a>.
</div>
<div id="ref-johnsonGoldTheoremCognitive2004" class="csl-entry" role="listitem">
Johnson, Kent. 2004. <span>“Gold’s Theorem and Cognitive Science.”</span> <em>Philosophy of Science</em> 71 (4): 571–92. <a href="https://doi.org/10.1086/423752">https://doi.org/10.1086/423752</a>.
</div>
<div id="ref-kanalPatternCategoriesAlternate1993" class="csl-entry" role="listitem">
Kanal, Laveen N. 1993. <span>“On Pattern, Categories, and Alternate Realities.”</span> <em>Pattern Recognition Letters</em> 14 (3): 241–55. <a href="http://lnk.com/prl14.pdf">http://lnk.com/prl14.pdf</a>.
</div>
<div id="ref-knightAI30Years2016" class="csl-entry" role="listitem">
Knight, Will. 2016. <span>“An <span>AI</span> with 30 <span>Years</span>’ <span>Worth</span> of <span>Knowledge Finally Goes</span> to <span>Work</span>.”</span> <em>MIT Technology Review</em>, March. <a href="https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/">https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/</a>.
</div>
<div id="ref-lenatCycMidtermReport1990" class="csl-entry" role="listitem">
Lenat, Douglas B., and Ramanathan V. Guha. 1990. <span>“Cyc: <span>A</span> Midterm Report.”</span> <em>AI Magazine</em> 11 (3): 32–32. <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/842">https://ojs.aaai.org/index.php/aimagazine/article/view/842</a>.
</div>
<div id="ref-lenatCycUsingCommon1985" class="csl-entry" role="listitem">
Lenat, Douglas B., Mayank Prakash, and Mary Shepherd. 1985. <span>“Cyc: <span>Using</span> Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks.”</span> <em>AI Magazine</em> 6 (4): 65–65. <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/510/0">https://ojs.aaai.org/index.php/aimagazine/article/view/510/0</a>.
</div>
<div id="ref-levyHackersHeroesComputer2010" class="csl-entry" role="listitem">
Levy, Steven. 2010. <em>Hackers: <span>Heroes</span> of the <span>Computer Revolution</span></em>. 1st edition. Sebastopol, CA: O’Reilly Media.
</div>
<div id="ref-marcusDeepLearningRevolution2012" class="csl-entry" role="listitem">
Marcus, Gary. 2012. <span>“Is <span>‘<span>Deep Learning</span>’</span> a <span>Revolution</span> in <span>Artificial Intelligence</span>?”</span> <em>The New Yorker</em>, November. <a href="https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence">https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence</a>.
</div>
<div id="ref-marcusDeepLearningCritical2018" class="csl-entry" role="listitem">
———. 2018. <span>“Deep <span>Learning</span>: <span>A Critical Appraisal</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1801.00631">http://arxiv.org/abs/1801.00631</a>.
</div>
<div id="ref-marcusNegativeEvidenceLanguage1993" class="csl-entry" role="listitem">
Marcus, Gary F. 1993. <span>“Negative Evidence in Language Acquisition.”</span> <em>Cognition</em> 46 (1): 53–85. <a href="https://doi.org/10.1016/0010-0277(93)90022-N">https://doi.org/10.1016/0010-0277(93)90022-N</a>.
</div>
<div id="ref-marcusAlgebraicMindIntegrating2003" class="csl-entry" role="listitem">
———. 2003. <em>The Algebraic Mind: <span>Integrating</span> Connectionism and Cognitive Science</em>. MIT press. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=7YpuRUlFLm8C&amp;oi=fnd&amp;pg=PR9&amp;dq=the+algebraic+mind&amp;ots=asKHu3nT8m&amp;sig=mu9Myw9KuXRhKsBa1Ywa0CgKB2g">https://books.google.com/books?hl=en&amp;lr=&amp;id=7YpuRUlFLm8C&amp;oi=fnd&amp;pg=PR9&amp;dq=the+algebraic+mind&amp;ots=asKHu3nT8m&amp;sig=mu9Myw9KuXRhKsBa1Ywa0CgKB2g</a>.
</div>
<div id="ref-marcusOverregularizationLanguageAcquisition1992" class="csl-entry" role="listitem">
Marcus, Gary F., Steven Pinker, Michael Ullman, Michelle Hollander, T. John Rosen, Fei Xu, and Harald Clahsen. 1992. <span>“Overregularization in Language Acquisition.”</span> <em>Monographs of the Society for Research in Child Development</em>, i–178. <a href="https://www.jstor.org/stable/1166115">https://www.jstor.org/stable/1166115</a>.
</div>
<div id="ref-marcusOpinionAIHarder2018" class="csl-entry" role="listitem">
Marcus, Gary, and Ernest Davis. 2018. <span>“Opinion <span></span> <span>A</span>.<span>I</span>. <span>Is Harder Than You Think</span>.”</span> <em>The New York Times</em>, May. <a href="https://www.nytimes.com/2018/05/18/opinion/artificial-intelligence-challenges.html">https://www.nytimes.com/2018/05/18/opinion/artificial-intelligence-challenges.html</a>.
</div>
<div id="ref-mccullochLogicalCalculusIdeas1943" class="csl-entry" role="listitem">
McCulloch, Warren S., and Walter Pitts. 1943. <span>“A Logical Calculus of the Ideas Immanent in Nervous Activity.”</span> <em>The Bulletin of Mathematical Biophysics</em> 5 (4): 115–33. <a href="https://doi.org/10/djsbj6">https://doi.org/10/djsbj6</a>.
</div>
<div id="ref-metzGeniusMakersMavericks2021" class="csl-entry" role="listitem">
Metz, Cade. 2021. <em>Genius Makers: The Mavericks Who Brought <span>A</span>.<span>I</span>. To <span>Google</span>, <span>Facebook</span>, and the World</em>. New York: Dutton, an imprint of Penguin Random House LLC.
</div>
<div id="ref-minskyNeuralanalogueCalculatorBased1952" class="csl-entry" role="listitem">
Minsky, Marvin. 1952. <span>“A Neural-Analogue Calculator Based Upon a Probability Model of Reinforcement.”</span> <em>Harvard University Psychological Laboratories, Cambridge, Massachusetts</em>.
</div>
<div id="ref-minskyTheoryNeuralanalogReinforcement1954" class="csl-entry" role="listitem">
———. 1954. <span>“Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model Problem.”</span> PhD thesis, Princeton University. <a href="https://search.proquest.com/openview/a4ee1c0c78c1b940b9ec121f0e89cef8/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y">https://search.proquest.com/openview/a4ee1c0c78c1b940b9ec121f0e89cef8/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a>.
</div>
<div id="ref-minskyComputationFiniteInfinite1967" class="csl-entry" role="listitem">
———. 1967. <em>Computation: Finite and Infinite Machines</em>. Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-minskyIntroductionCOMTEXMicrofiche1983" class="csl-entry" role="listitem">
———. 1983. <span>“Introduction to the <span>COMTEX</span> Microfiche Edition of the Early <span>MIT Artificial Intelligence Memos</span>.”</span> <em>AI Magazine</em> 4 (1): 19–22. <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/384">https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/384</a>.
</div>
<div id="ref-minskySocietyMind1988" class="csl-entry" role="listitem">
———. 1988. <em>The Society of Mind</em>. 6. Pb-pr. A <span>Touchstone</span> Book. New York: <span>Simon and Schuster</span>.
</div>
<div id="ref-minskyMyUndergraduateThesis2011" class="csl-entry" role="listitem">
———. 2011. <span>“My Undergraduate Thesis in Fixed Point Theorems (70/151).”</span> <a href="https://www.youtube.com/watch?v=k6yRP203UaM">https://www.youtube.com/watch?v=k6yRP203UaM</a>.
</div>
<div id="ref-minskySelectedPublicationsMarvin" class="csl-entry" role="listitem">
———. n.d. <span>“Selected <span>Publications</span> of <span>Marvin Minsky</span>.”</span> Accessed December 25, 2023. <a href="https://web.media.mit.edu/~minsky/bibliography.html">https://web.media.mit.edu/~minsky/bibliography.html</a>.
</div>
<div id="ref-minskyPerceptronsIntroductionComputational1988" class="csl-entry" role="listitem">
Minsky, Marvin, and Seymour Papert. 1988. <em>Perceptrons: An Introduction to Computational Geometry</em>. Expanded ed. Cambridge, Mass: MIT Press.
</div>
<div id="ref-morozovPlanningMachineProject2014" class="csl-entry" role="listitem">
Morozov, Evgeny. 2014. <span>“The Planning Machine. <span>Project Cybersyn</span> and the Origins of the Big Data Nation.”</span> <em>The New Yorker</em>.
</div>
<div id="ref-nagySystemCircuitDesigns1963" class="csl-entry" role="listitem">
Nagy, GEORGE. 1963. <span>“System and Circuit Designs for the Tobermory Perceptron.”</span> <em>OTS, AD</em> 604: 459. <a href="https://apps.dtic.mil/sti/trecms/pdf/AD0607459.pdf">https://apps.dtic.mil/sti/trecms/pdf/AD0607459.pdf</a>.
</div>
<div id="ref-nagyNeuralNetworksthenNow1991" class="csl-entry" role="listitem">
Nagy, George. 1991. <span>“Neural Networks-Then and Now.”</span> <em>IEEE Transactions on Neural Networks</em> 2 (2): 316–18. <a href="https://doi.org/10.1109/72.80343">https://doi.org/10.1109/72.80343</a>.
</div>
<div id="ref-newellHumanProblemSolving1972" class="csl-entry" role="listitem">
Newell, Allen, and Herbert Alexander Simon. 1972. <em>Human Problem Solving</em>.
</div>
<div id="ref-nilssonQuestArtificialIntelligence2009" class="csl-entry" role="listitem">
Nilsson, Nils J. 2009. <em>The <span>Quest</span> for <span>Artificial Intelligence</span></em>. 1st edition. Cambridge ; New York: Cambridge University Press.
</div>
<div id="ref-norvigChomskyTwoCultures2017" class="csl-entry" role="listitem">
Norvig, Peter. 2017. <span>“<span>On Chomsky and the Two Cultures of Statistical Learning</span>.”</span> In <em><span>Berechenbarkeit der Welt? Philosophie und Wissenschaft im Zeitalter von Big Data</span></em>, edited by Wolfgang Pietsch, Jörg Wernecke, and Maximilian Ott, 61–83. Wiesbaden: Springer Fachmedien. <a href="https://doi.org/10.1007/978-3-658-12153-2_3">https://doi.org/10.1007/978-3-658-12153-2_3</a>.
</div>
<div id="ref-olazaranHistoricalSociologyNeural1991" class="csl-entry" role="listitem">
Olazaran, Mikel. 1991. <span>“A Historical Sociology of Neural Network Research.”</span> PhD thesis, The University of Edinburgh. <a href="https://era.ed.ac.uk/handle/1842/20075">https://era.ed.ac.uk/handle/1842/20075</a>.
</div>
<div id="ref-olazaranSociologicalHistoryNeural1993" class="csl-entry" role="listitem">
———. 1993. <span>“A <span>Sociological History</span> of the <span>Neural Network Controversy</span>.”</span> In <em>Advances in <span>Computers</span></em>, edited by Marshall C. Yovits, 37:335–425. Elsevier. <a href="https://doi.org/10.1016/S0065-2458(08)60408-8">https://doi.org/10.1016/S0065-2458(08)60408-8</a>.
</div>
<div id="ref-papertOneAIMany1988" class="csl-entry" role="listitem">
Papert, Seymour. 1988. <span>“One <span>AI</span> or Many?”</span> <em>Daedalus</em>, 1–14. <a href="https://www.jstor.org/stable/20025136">https://www.jstor.org/stable/20025136</a>.
</div>
<div id="ref-papertChildrenMachineRethinking1994" class="csl-entry" role="listitem">
Papert, Seymour A. 1994. <em>The <span>Children</span>’s <span>Machine</span>: <span>Rethinking School In The Age Of The Computer</span></em>. Revised ed. edition. New York: Basic Books.
</div>
<div id="ref-pickeringScienceUnknowableStafford2004" class="csl-entry" role="listitem">
Pickering, Andrew. 2004. <span>“The Science of the Unknowable: <span>Stafford Beer</span>’s Cybernetic Informatics.”</span> <em>Kybernetes</em> 33 (3/4): 499–521. <a href="https://doi.org/10/dqjsk8">https://doi.org/10/dqjsk8</a>.
</div>
<div id="ref-pinkerFutureTense2002" class="csl-entry" role="listitem">
Pinker, Steven, and Michael T. Ullman. 2002. <span>“The Past and Future of the Past Tense.”</span> <em>Trends in Cognitive Sciences</em> 6 (11): 456–63. <a href="https://doi.org/10.1016/S1364-6613(02)01990-3">https://doi.org/10.1016/S1364-6613(02)01990-3</a>.
</div>
<div id="ref-rainaLargescaleDeepUnsupervised2009" class="csl-entry" role="listitem">
Raina, Rajat, Anand Madhavan, and Andrew Y. Ng. 2009. <span>“Large-Scale Deep Unsupervised Learning Using Graphics Processors.”</span> In <em>Proceedings of the 26th <span>Annual International Conference</span> on <span>Machine Learning</span></em>, 873–80. Montreal Quebec Canada: ACM. <a href="https://doi.org/10.1145/1553374.1553486">https://doi.org/10.1145/1553374.1553486</a>.
</div>
<div id="ref-rosenResearchDevelopmentProgram1965" class="csl-entry" role="listitem">
Rosen, Charles A., Nils J. Nilsson, and Milton B. Adams. 1965. <span>“A Research and Development Program in Applications of Intelligent Automata to Reconnaissance-Phase <span>I</span>.”</span> Proposal for {{Research}} ESU 65-1. Stanford Research Institute.
</div>
<div id="ref-rosenblattPrinciplesNeurodynamicsPerceptrons1962" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1962. <em>Principles of Neurodynamics: <span>Perceptrons</span> and the Theory of Brain Mechanisms</em>. Vol. 55. Spartan books Washington, DC. <a href="https://apps.dtic.mil/sti/citations/AD0256582">https://apps.dtic.mil/sti/citations/AD0256582</a>.
</div>
<div id="ref-rosenfeldTalkingNetsOral2000" class="csl-entry" role="listitem">
Rosenfeld, Edward, and James A. Anderson, eds. 2000. <em>Talking <span>Nets</span>: <span>An Oral History</span> of <span>Neural Networks</span></em>. Reprint edition. The MIT Press.
</div>
<div id="ref-rumelhartLearningInternalRepresentations1985" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1985. <span>“Learning Internal Representations by Error Propagation.”</span> Institute for Cognitive Science, University of California, San Diego La <span>…</span>. <a href="https://apps.dtic.mil/sti/citations/ADA164453">https://apps.dtic.mil/sti/citations/ADA164453</a>.
</div>
<div id="ref-rumelhartParallelDistributedProcessing1986" class="csl-entry" role="listitem">
Rumelhart, David E., and James L. McClelland. 1986. <em>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</em>. Computational Models of Cognition and Perception. Cambridge, Mass: MIT Press.
</div>
<div id="ref-sejnowskiDeepLearningRevolution2018" class="csl-entry" role="listitem">
Sejnowski, Terrence J. 2018. <em>The <span>Deep Learning Revolution</span></em>. Illustrated edition. Cambridge, Massachusetts London, England: The MIT Press.
</div>
<div id="ref-singhExaminingSocietyMind2003" class="csl-entry" role="listitem">
Singh, Push. 2003. <span>“Examining the Society of Mind.”</span> <em>Computing and Informatics</em> 22 (6): 521–43. <a href="https://www.cai.sk/ojs/index.php/cai/article/view/467/0">https://www.cai.sk/ojs/index.php/cai/article/view/467/0</a>.
</div>
<div id="ref-slocumRobotExcessMachine2024" class="csl-entry" role="listitem">
Slocum, Brian, and Maria Yablonina. 2024. <span>“Robot Excess: Machine Histories and a Hermeneutics of Movement.”</span> <em>Construction Robotics</em> 8 (2): 18. <a href="https://doi.org/10.1007/s41693-024-00129-7">https://doi.org/10.1007/s41693-024-00129-7</a>.
</div>
<div id="ref-thompsonEvolvedCircuitIntrinsic1996" class="csl-entry" role="listitem">
Thompson, Adrian. 1996. <span>“An Evolved Circuit, Intrinsic in Silicon, Entwined with Physics.”</span> In <em>International <span>Conference</span> on <span>Evolvable Systems</span></em>, 390–405. Springer.
</div>
<div id="ref-thompsonHardwareEvolutionAutomatic1998" class="csl-entry" role="listitem">
———. 1998. <em>Hardware <span>Evolution</span>: <span>Automatic Design</span> of <span>Electronic Circuits</span> in <span>Reconfigurable Hardware</span> by <span>Artificial Evolution</span></em>. 1st edition. Springer.
</div>
<div id="ref-thompsonAnalysisUnconventionalEvolved1999" class="csl-entry" role="listitem">
Thompson, Adrian, and Paul Layzell. 1999. <span>“Analysis of Unconventional Evolved Electronics.”</span> <em>Communications of the ACM</em> 42 (4): 71–79. <a href="https://doi.org/10/bpnp42">https://doi.org/10/bpnp42</a>.
</div>
<div id="ref-thompsonExplorationsDesignSpace1999" class="csl-entry" role="listitem">
Thompson, Adrian, Paul Layzell, and Ricardo Salem Zebulum. 1999. <span>“Explorations in Design Space: <span>Unconventional</span> Electronics Design Through Artificial Evolution.”</span> <em>IEEE Transactions on Evolutionary Computation</em> 3 (3): 167–96. <a href="https://doi.org/10.1109/4235.788489">https://doi.org/10.1109/4235.788489</a>.
</div>
<div id="ref-turkleEpistemologicalPluralismStyles1990" class="csl-entry" role="listitem">
Turkle, Sherry, and Seymour Papert. 1990. <span>“Epistemological <span>Pluralism</span>: <span>Styles</span> and <span>Voices</span> Within the <span>Computer Culture</span>.”</span> <em>Signs: Journal of Women in Culture and Society</em> 16 (1): 128–57. <a href="https://doi.org/10.1086/494648">https://doi.org/10.1086/494648</a>.
</div>
<div id="ref-widrow30YearsAdaptive1990" class="csl-entry" role="listitem">
Widrow, Bernard, and Michael A. Lehr. 1990. <span>“30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation.”</span> <em>Proceedings of the IEEE</em> 78 (9): 1415–42. <a href="https://ieeexplore.ieee.org/abstract/document/58323/">https://ieeexplore.ieee.org/abstract/document/58323/</a>.
</div>
<div id="ref-wienerCyberneticsControlCommunication2019" class="csl-entry" role="listitem">
Wiener, Norbert. 2019. <em>Cybernetics: Or Control and Communication in the Animal and the Machine</em>. Edited by Doug Hill and Sanjoy K. Mitter. Reissue of the 1961 second edition. Cambridge, Massachusetts London, England: The MIT Press.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The Perceptron Controversy"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-01-01"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2024-09-22"</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [AI, history]</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        toc: true</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Connectionism died in the 60s from technical limits to scaling, then resurrected in the 80s after backprop allowed scaling. The Minsky--Papert anti-scaling hypothesis explained, psychoanalyzed, and buried."</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner_2_cropped.png"</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="an">image-alt:</span><span class="co"> "Brutalist architecture representing a single perceptron. A hexagonal building in the center representing the unit itself, with multiple arc legs representing the input weights, and antennas projecting off the top representing the output. High contrast, monochromatic, minimalistic, in the style of vector svg art."</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "likely"</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 7</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Abstract</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>During the 1950s and 1960s, the connectionist and symbolic schools of artificial intelligence competed for researcher allegiance and funding, with the symbolic school winning by 1970. This "first neural network winter" lasted until the rise of the connectionist school in the 1980s.</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>It is often stated that neural networks were killed off by the 1969 publication of *Perceptrons* by Marvin Minsky and Seymour Papert. This story is wrong on multiple accounts:</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Minsky and Papert had been working towards killing off neural networks since around 1965 by speaking at conferences and circulating preprints.</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The mathematical content studies only the behavior of a single perceptron. It does not study multilayer perceptrons.</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>By 1969, most researchers had already left connectionism, frustrated by the lack of progress, as they did not develop backpropagation or deep learning. The last holdout, Frank Rosenblatt, died in 1971.</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>The book achieved its mythical status as the "neural network killer" by its opportune timing, appearing just as the symbolic school achieved dominance. Since the connectionist school was almost empty, it faced little objection at its publication, creating the illusion that it caused the closure of the perceptron controversy.</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>In the 1980s, the perceptron controversy reopened with the rise of connectionism. This time the controversy was bypassed without closure. Connectionists demonstrated that backpropagation with MLP bypassed most of the objections from *Perceptrons*. Minsky and Papert objected that the *lessons* of *Perceptrons* still applied, but their objections and lessons had by then become irrelevant.</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>Minsky and Papert were possibly the most consistent critics of the scaling hypothesis, arguing over decades that neural networks cannot scale beyond mere toy problems. Minsky was motivated by mathematical certainty, as backpropagation--MLP cannot provably find global optima or accomplish any task efficiently, unlike a single perceptron. He rejected all experimental data with large MLP as theory-less data that cannot be extrapolated. Papert was motivated by social justice and epistemological equality. He rejected all scalable uniform architectures, like backpropagation--MLP, as threats to his vision of a society with different but equally valid ways of knowing.</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>As of 2019, essentially all predictions by Minsky and Papert, concerning the non-scalability of neural networks, had been disproven.</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="fu">## The enigma of Marvin Minsky</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>In a 1993 interview, <span class="co">[</span><span class="ot">Robert Hecht-Nielsen</span><span class="co">](https://en.wikipedia.org/wiki/Robert_Hecht-Nielsen)</span> recounted an encounter between <span class="co">[</span><span class="ot">Marvin Minsky</span><span class="co">](https://en.wikipedia.org/wiki/Marvin_Minsky)</span> and the neural network community in the late 1980s<span class="ot">[^minsky-not-the-devil]</span>:</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky had gone to the same New York "science" high school as Frank Rosenblatt, a Cornell psychology Ph.D. whose "perceptron" neural network pattern recognition machine was receiving significant media attention. The wall-to-wall media coverage of Rosenblatt and his machine irked Minsky. One reason was that although Rosenblatt's training was in "soft science," his perceptron work was quite mathematical and quite sound—turf that Minsky, with his "hard science" Princeton mathematics Ph.D., didn't feel Rosenblatt belonged on. Perhaps an even greater problem was the fact that the heart of the perceptron machine was a clever motor-driven potentiometer adaptive element that had been pioneered in the world's first neurocomputer, the "SNARC", which had been designed and built by Minsky several years earlier! In some ways, Minsky's early career was like that of Darth Vader. He started out as one of the earliest pioneers in neural networks but was then turned to the dark side of the force (AI) and became the strongest and most effective foe of his original community. This view of his career history is not unknown to him. When he was invited to give the keynote address at a large neural network conference in the late 1980s to an absolutely rapt audience, he began with the words: "I am not the Devil!" </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, pages 303-305</span><span class="co">]</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="ot">[^minsky-not-the-devil]</span>:</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    This was corroborated by a contemporary news report on the International Conference on Neural Networks of 1988:</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Minsky who has been criticized by many for the conclusions he and Papert make in 'Perceptrons,' opened his defense with the line 'Everybody seems to think I'm the devil.' Then he made the statement, 'I was wrong about Dreyfus too, but I haven't admitted it yet,' which brought another round of applause. (quoted in [@olazaranHistoricalSociologyNeural1991, page 285])</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>However, it appears that he had changed his mind later. As recounted by <span class="co">[</span><span class="ot">Terry Sejnowski</span><span class="co">](https://en.wikipedia.org/wiki/Terry_Sejnowski)</span>:</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I was invited to attend the 2006 Dartmouth Artificial Intelligence Conference, "AI\@50," a look back at the seminal 1956 Summer Research Project on artificial intelligence held at Dartmouth and a look forward to the future of artificial intelligence. ... These success stories had a common trajectory. In the past, computers were slow and only able to explore toy models with just a few parameters. But these toy models generalized poorly to real-world data. When abundant data were available and computers were much faster, it became possible to create more complex statistical models and to extract more features and relationships between the features.</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In his summary talk at the end of the conference, Marvin Minsky started out by saying how disappointed he was both by the talks and by where AI was going. He explained why: "You're not working on the problem of general intelligence. You're just working on applications." ...</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There was a banquet on the last day of AI\@50. At the end of the dinner, the five returning members of the 1956 Dartmouth Summer Research Project on Artificial Intelligence made brief remarks about the conference and the future of AI. In the question and answer period, I stood up and, turning to Minsky, said: "There is a belief in the neural network community that you are the devil who was responsible for the neural network winter in the 1970s. Are you the devil?" Minsky launched into a tirade about how we didn't understand the mathematical limitations of our networks. I interrupted him—"Dr. Minsky, I asked you a yes or no question. Are you, or are you not, the devil?" He hesitated for a moment, then shouted out, "Yes, I am the devil!" </span><span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, pages 256--258</span><span class="co">]</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>What are we to make of the enigma of Minsky? Was he the devil or not?</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="fu">### The intellectual history of Minsky</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>During his undergraduate years, Minsky was deeply impressed by <span class="co">[</span><span class="ot">Andrew Gleason</span><span class="co">](https://en.wikipedia.org/wiki/Andrew_M._Gleason)</span>,<span class="ot">[^andrew-gleason]</span> and decided to work on pure mathematics, resulting in his 1951 undergraduate thesis *A Generalization of Kakutani's Fixed-Point Theorem*, which extended an obscure fixed-point theorem of <span class="co">[</span><span class="ot">Kakutani</span><span class="co">](https://en.wikipedia.org/wiki/Shizuo_Kakutani)</span> -- not <span class="co">[</span><span class="ot">the famous version</span><span class="co">](https://en.wikipedia.org/wiki/Kakutani_fixed-point_theorem)</span>, as Kakutani proved more than one fixed-point theorem.</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a><span class="ot">[^andrew-gleason]</span>:</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    <span class="at">&gt; I asked Gleason how he was going to solve it. Gleason said he had a plan that consisted of three steps, each of which he thought would take him three years to work out. Our conversation must have taken place in 1947, when I was a sophomore. Well, the solution took him only about five more years ... Gleason made me realize for the first time that mathematics was a landscape with discernible canyons and mountain passes, and things like that. In high school, I had seen mathematics simply as a bunch of skills that were fun to master -- but I had never thought of it as a journey and a universe to explore. No one else I knew at that time had that vision, either. </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>::: {#thm-kakutani}</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kakutani's fixed point theorem on the sphere</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>If $f$ is an $\R^2$-valued continuous function on the unit sphere in $\R^3$, then for any side length $r \in (0, \sqrt{3})$, there exist $x_1, x_2, x_3$ on the sphere forming an equilateral triangle with side length $r$, such that $f(x_1) = f(x_2) = f(x_3)$.</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>Equivalently, if $x_1, x_2, x_3$ form an equilateral triangle on the unit sphere, then there exists a rotation $T$ such that $f(T(x_1)) = f(T(x_2)) = f(T(x_3))$.</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>Using knot theory, Minsky proved an extension where $x_1, x_2, x_3$ are three points of a square or a regular pentagon <span class="co">[</span><span class="ot">@minskyMyUndergraduateThesis2011</span><span class="co">]</span>. The manuscript "disappeared" <span class="co">[</span><span class="ot">@minskySelectedPublicationsMarvin</span><span class="co">]</span>.</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I wrote it up and gave it to Gleason. He read it and said, ‘You are a mathematician.' Later, I showed the proof to Freeman Dyson, at the Institute for Advanced Study, and he amazed me with a proof </span><span class="co">[</span><span class="ot">@dysonContinuousFunctionsDefined1951</span><span class="co">]</span><span class="at"> that there must be at least one square that has the same temperature at all four vertices. He had found somewhere in my proof a final remnant of unused logic. </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>He then became interested in neural networks and reinforcement learning, and constructed a very simple electromechanical machine called SNARC.<span class="ot">[^snarc]</span> The SNARC machine is a recurrent neural network that performs reinforcement learning by the Hebbian learning rule. It simulates a mouse running around a maze, while the operator watches an indicator light showing the mouse. The operator can press a button as a reward signal, which would cause an electric motor to turn a chain. The chain is clutched to rheostats that connect the neurons, with the stretch of the clutch being proportional to the charge in a capacitor. During the operation of the neural network, the capacitor charges up if there is neural co-activation on the connection, and decays naturally, thus serving as a short-term memory. When the reward button is pressed, the clutches turn by an amount proportional to the co-activation of neural connections, thereby completing the Hebbian learning.</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="ot">[^snarc]: </span>It was published as <span class="co">[</span><span class="ot">@minskyNeuralanalogueCalculatorBased1952</span><span class="co">]</span>, but the document is not available online, and I could only piece together a possible reconstruction from the fragments of information.</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>Minsky was impressed by how well it worked. The machine was designed to simulate one mouse, but by some kind of error it simulated multiple mice, and yet it still worked.</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The rats actually interacted with one another. If one of them found a good path, the others would tend to follow it. We sort of quit science for a while to watch the machine. ... In those days, even a radio set with twenty tubes tended to fail a lot. I don't think we ever debugged our machine completely, but that didn't matter. By having this crazy random design, it was almost sure to work, no matter how you built it. </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>This was the last we saw of Minsky's work with random neural networks. He had crossed the Rubicon, away from the land of brute reason and into the land of genuine insight.</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I had the naïve idea that if one could build a big enough network, with enough memory loops, it might get lucky and acquire the ability to envision things in its head. ... Even today, I still get letters from young students who say, ‘Why are you people trying to program intelligence? Why don't you try to find a way to build a nervous system that will just spontaneously create it?' Finally, I decided that either this was a bad idea or it would take thousands or millions of neurons to make it work, and I couldn't afford to try to build a machine like that. </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>For his PhD thesis, Minsky worked on the mathematical theory of McCulloch--Pitts neural networks. In style, it was a fine piece of classical mathematics <span class="co">[</span><span class="ot">@minskyTheoryNeuralanalogReinforcement1954</span><span class="co">]</span>. Minsky would go on to write <span class="co">[</span><span class="ot">@minskyComputationFiniteInfinite1967, Chapter 3</span><span class="co">]</span>, still the best introduction to McCulloch--Pitts neural networks.</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky's doctoral dissertation in mathematics from Princeton in 1954 was a theoretical and experimental study of computing with neural networks. He had even built small networks from electronic parts to see how they behaved. The story I heard when I was a graduate student at Princeton in physics was that there wasn't anyone in the Mathematics Department who was qualified to assess his dissertation, so they sent it to the mathematicians at the Institute for Advanced Study in Princeton who, it was said, talked to God. The reply that came back was, "If this isn't mathematics today, someday it will be," which was good enough to earn Minsky his PhD. </span><span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, page 259</span><span class="co">]</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>Reading the story, I recalled <span class="co">[</span><span class="ot">"Sussman attains enlightenment"</span><span class="co">](https://web.archive.org/web/20231223082954/http://www.catb.org/jargon/html/koans.html)</span>, a <span class="co">[</span><span class="ot">hacker koan</span><span class="co">](https://simple.wikipedia.org/wiki/Hacker_koan)</span> about Minsky and his student <span class="co">[</span><span class="ot">Sussman</span><span class="co">](https://en.wikipedia.org/wiki/Gerald_Jay_Sussman)</span> <span class="ot">[^sussman-hacker-koan]</span>:</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. "What are you doing?", asked Minsky. "I am training a randomly wired neural net to play Tic-Tac-Toe" Sussman replied. "Why is the net wired randomly?", asked Minsky. "I do not want it to have any preconceptions of how to play", Sussman said. Minsky then shut his eyes. "Why do you close your eyes?", Sussman asked his teacher. "So that the room will be empty." At that moment, Sussman was enlightened.</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a><span class="ot">[^sussman-hacker-koan]</span>:</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>    This is based on a true story.</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; ... Sussman told Minsky that he was using a certain randomizing technique in his program because he didn't want the machine to have any preconceived notions. Minsky said, "Well, it has them, it's just that you don't know what they are." It was the most profound thing Gerry Sussman had ever heard. And Minsky continued, telling him that the world is built a certain way, and the most important thing we can do with the world is avoid randomness, and figure out ways by which things can be planned. [@levyHackersHeroesComputer2010, pages 110-111]</span></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>As for Sussman, I knew him for two things: writing the <span class="co">[</span><span class="ot">*SICP*</span><span class="co">](https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs)</span> book, and being the coordinator of the infamous *summer vision project* that was to construct "a significant part of a visual system" in a single summer, using only undergraduate student researchers. A brief read of his "<span class="co">[</span><span class="ot">reading list</span><span class="co">](https://web.archive.org/web/20231007210930/http://aurellem.org/thoughts/html/sussman-reading-list.html)</span>" shows where his loyalties lie: firmly in <span class="co">[</span><span class="ot">the school of neats</span><span class="co">](https://en.wikipedia.org/wiki/Neats_and_scruffies)</span>.</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, page 28</span><span class="co">]</span> recounts the background of the summer vision project:</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the 1960s, the MIT AI Lab received a large grant from a military research agency to build a robot that could play Ping-Pong. I once heard a story that the principal investigator forgot to ask for money in the grant proposal to build a vision system for the robot, so he assigned the problem to a graduate student as a summer project. I once asked Marvin Minsky whether the story was true. He snapped back that I had it wrong: "We assigned the problem to undergraduate students."</span></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>After rejecting neural networks, Minsky became a leading researcher in AI. His style of AI is typically described as "symbolic AI", although a more accurate description would be *The Society of Mind* (SoM). Minsky developed the SoM in the 1960s and 1970s with his long-time collaborator, <span class="co">[</span><span class="ot">Seymour Papert</span><span class="co">](#sec-seymour-papert)</span>, inspired by their difficulty with building robots, and published the definitive account in <span class="co">[</span><span class="ot">@minskySocietyMind1988</span><span class="co">]</span>. The SoM thesis states that "any brain, machine, or other thing that has a mind must be composed of smaller things that cannot think at all".</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>Stated in this way, it *seems* patently compatible with neural networks, but only on the surface. Minsky concretely described how he expected a Society of Mind to work, based on his attempts at making Builder, a robot that can play with blocks:</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Both my collaborator, Seymour Papert, and I had long desired to combine a mechanical hand, a television eye, and a computer into a robot that could build with children's building-blocks. It took several years for us and our students to develop Move, See, Grasp, and hundreds of other little programs we needed to make a working Builder-agency. I like to think that this project gave us glimpses of what happens inside certain parts of children's minds when they learn to "play" with simple toys. The project left us wondering if even a thousand microskills would be enough to enable a child to fill a pail with sand. It was this body of experience, more than anything we'd learned about psychology, that led us to many ideas about societies of mind.</span></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; To do those first experiments, we had to build a mechanical Hand, equipped with sensors for pressure and touch at its fingertips. Then we had to interface a television camera with our computer and write programs with which that Eye could discern the edges of the building-blocks. It also had to recognize the Hand itself. When those programs didn't work so well, we added more programs that used the fingers' feeling-sense to verify that things were where they visually seemed to be. Yet other programs were needed to enable the computer to move the Hand from place to place while using the Eye to see that there was nothing in its way. We also had to write higher-level programs that the robot could use for planning what to do—and still more programs to make sure that those plans were actually carried out. To make this all work reliably, we needed programs to verify at every step (again by using Eye and Hand) that what had been planned inside the mind did actually take place outside—or else to correct the mistakes that occurred. ... Thousands and, perhaps, millions of little processes must be involved in how we anticipate, imagine, plan, predict, and prevent—and yet all this proceeds so automatically that we regard it as "ordinary common sense." </span><span class="co">[</span><span class="ot">@minskySocietyMind1988, Section 2.5</span><span class="co">]</span></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a><span class="al">![Marvin Minsky and Builder the robot. It's hard to tell who is having more fun here.](figure/minsky_robot_arm_blocks.jpg)</span></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>From the concrete description, as well as the many attractive illustrations in the book, it is clear that Minsky intended the "Society of Mind" to be a uniform computing substrate (silicon or carbon) upon which millions of little symbolic programs are running, each capable of running some specific task, each describable by a distinct and small piece of symbolic program. They cannot be mere perceptrons in a uniform block of neural network, or mere logic gates in a uniform block of CPU.</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>In his 1988 book, Minsky described dozens of these heterogeneous components he thought might make up a Society of Mind. However, the precise details are not relevant,<span class="ot">[^society-of-mind-details]</span> as he freely admitted that they are conjectured. He was only adamant about the overarching scheme: heterogeneous little separated components, not a homogeneous big connected piece.</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a><span class="ot">[^society-of-mind-details]: </span>The conjectured components included "K-lines", "nomes", "nemes", "frames", "frame-arrays", etc. Although Minsky meant for this SoM project to last a very long time, building up to general intelligence brick by brick, my literature search shows that there had been no new development since the 2000s, so the overview <span class="co">[</span><span class="ot">@singhExaminingSocietyMind2003</span><span class="co">]</span> still represents the SOTA of SoM.</span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>Perhaps a modern reincarnation of such an idea would be the dream of Internet agents operating in a digital economy, populated by agents performing simple tasks like spam filtering, listening for the price of petroleum, etc. Some agents would interface with reality, while others would interface with agents. Some agents are organized at a higher level into DAOs, created by a small committee of simple "manager agents" serving as the interface and coordinators for other agents. DAOs can interface with other DAOs through little speaker-agents, which consist of a simple text filter for the torrent of information and then outsource to text-weaving agents to compose the actual messages they send out.</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a><span class="fu">### Seymour Papert {#sec-seymour-papert}</span></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Seymour Papert</span><span class="co">](https://en.wikipedia.org/wiki/Seymour_Papert)</span>, the long-time collaborator of Minsky, was the second author of *Perceptrons*. To unlock the enigma of Minsky, we must look into Papert's past as well.</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>In 1958, after earning a doctorate in mathematics, he met <span class="co">[</span><span class="ot">Jean Piaget</span><span class="co">](https://en.wikipedia.org/wiki/Jean_Piaget)</span> and became his pupil for four years. This experience had a formative effect on Papert. Piaget's work was an important influence on the <span class="co">[</span><span class="ot">constructivism philosophy in education</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Constructivism_(philosophy_of_education)), and Papert would go on to bring constructivism from books to classrooms. He was particularly hopeful that computers can realize the constructivist dream of unlocking the kaleidoscopic creativity that a child can construct.</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>The main theme of Jean Piaget's work was developmental psychology -- how children's understanding of the world changes as they grow up. What goes on in their mind as they progressively understand that things fall down, what dogs are, and that solid steel sinks but hollow steel might float? Piaget discovered that children did not simply start with a blank sheet of paper and gradually fill in sketchy details of the true model. Instead they constructed little models of small facets of reality that would be modified or completely replaced as they encounter new phenomena that their old models cannot explain. In this way, Piaget claimed that children are "little scientists".</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>A small example illustrates the idea. When children see that a leaf floats on water, but a stone sinks, they add a rule "Soft things float, while hard things sink.". Then, they see that a hard plastic boat floats too, so they add a rule "Except hard and light things also float.". Then, they see that a large boat also floats, so they rewrite the entire model to "Flat-bottomed things float, while small-bottomed things sink.". And so on.</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>There are conservative and radical ways of using Piaget's research for pedagogy. The conservative approach involves studying how children construct their scientific theories and identifying a sequence of evidence to present to these young scientists so they can quickly reach scientific orthodoxy. For example, we might show children videos of curling and air hockey, then let them play with an air hockey table, following this with guided exercises, so they race through <span class="co">[</span><span class="ot">animism</span><span class="co">](https://en.wikipedia.org/wiki/Animism)</span>, <span class="co">[</span><span class="ot">Aristotelian physics</span><span class="co">](https://en.wikipedia.org/wiki/Aristotelian_physics)</span>, <span class="co">[</span><span class="ot">impetus theory</span><span class="co">](https://en.wikipedia.org/wiki/Theory_of_impetus)</span>, etc, and end up with Newton's laws of motion.</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>The radical way is to decenter the orthodoxy and let a thousand heterodoxies bloom. Why go for the orthodoxy, when the <span class="co">[</span><span class="ot">Duhem–Quine thesis</span><span class="co">](https://en.wikipedia.org/wiki/Duhem%E2%80%93Quine_thesis)</span> tells us that evidence is never enough to constrain us to only one orthodoxy? And given that objectively no theory deserves the high name of "orthodoxy", how did the scientific "orthodoxy" become dominant? A critical analysis of the history shows that its dominance over Aboriginal and woman ways of knowing is merely a historical accident due to an alliance with <span class="co">[</span><span class="ot">the hegemonic reason of the metropole over the periphery</span><span class="co">](https://en.wikipedia.org/wiki/World-systems_theory)</span>.<span class="ot">[^decolonialization]</span></span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a><span class="ot">[^decolonialization]: </span>This is not a joke, since <span class="co">[</span><span class="ot">decolonial studies</span><span class="co">](https://en.wikipedia.org/wiki/Decoloniality)</span> literally begin with this assumption.</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>Papert went with the radical way.</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>After four years of study under Piaget, he arrived in MIT in 1963, and began working with Minsky on various topics, including the <span class="co">[</span><span class="ot">Logo Turtle robot</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Turtle_(robot)), and the *Perceptrons* book. The computer revolution was starting, and Papert saw computers as a way to bring radical constructivism to children.</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>In the real world, phenomena are limited by nature, and aspiring little heterodoxy-builders are limited by their ability to construct theories and check their consequences. In the computer world, every child could program and construct "microworlds" from their own theories. Thus, computers would bring constructivism to the classroom. Furthermore, the constructed world inside computers could then flow out to the physical world via robots. This is why Papert worked on both *Logo* the programming language and *Logo* the turtle robots. In his words, he intended to fight "instructionism" with "constructionism" by bringing the power of the computer to every child, so that they would grow up to be "<span class="co">[</span><span class="ot">bricoleurs</span><span class="co">](https://en.wikipedia.org/wiki/The_Savage_Mind)</span>", working with whatever little tool they have available doing whatever is necessary to accomplish little things. This is a vital piece in his overarching project of epistemological pluralism, to liberate heterodoxical ways of knowing <span class="co">[</span><span class="ot">@papertChildrenMachineRethinking1994, Chapter 7</span><span class="co">]</span>:</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Traditional education codifies what it thinks citizens need to know and sets out to feed children this "fish." Constructionism is built on the assumption that children will do best by finding ("fishing") for themselves the specific knowledge they need ... it is as well to have good fishing lines, which is why we need computers, and to know the location of rich waters, which is why we need to develop a large range of mathetically rich activities or "microworlds."</span></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... School math, like the ideology, though not necessarily the practice, of modern science, is based on the ideal of generality -- the single, universally correct method that will work for all problems and for all people. Bricolage is a metaphor for the ways of the old-fashioned traveling tinker, the jack-of-all-trades who knocks on the door offering to fix whatever is broken. Faced with a job, the tinker rummages in his bag of assorted tools to find one that will fit the problem at hand and, if one tool does nor work for the job, simply tries another without ever being upset in the slightest by the lack of generality. The basic tenets of bricolage as a methodology for intellectual activity are: Use what you've got, improvise, make do. And for the true bricoleur, the tools in the bag will have been selected over a long time by a process determined by more than pragmatic utility. These mental tools will be as well worn and comfortable as the physical tools of the traveling tinker; they will give a sense of the familiar, of being at ease with oneself ...</span></span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Kitchen math provides a clear demonstration of bricolage in its seamless connection with a surrounding ongoing activity that provides the tinker's bag of tricks and tools. The opposite of bricolage would be to leave the "cooking microworld" for a "math world," to work the fractions problem using a calculator or, more likely in this case, mental arithmetic. But the practitioner of kitchen math, as a good bricoleur, does not stop cooking and turn to math; on the contrary, the mathematical manipulations of ingredients would be indistinguishable to an outside observer from the culinary manipulations.</span></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... The traditional epistemology is based on the proposition, so closely linked to the medium of text-written and especially printed. Bricolage and concrete thinking always existed but were marginalized in scholarly contexts by the privileged position of text. As we move into the computer age and new and more dynamic media emerge, this will change.</span></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>According to Papert, his project is **epistemological pluralism**, or promoting different ways of knowing:</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The diversity of approaches to programming suggests that equal access to even the most basic elements of computation requires accepting the validity of multiple ways of knowing and thinking, an epistemological pluralism. Here we use the word epistemology in a sense closer to Piaget's than to the philosopher's. In the traditional usage, the goal of epistemology is to inquire into the nature of knowledge and the conditions of its validity; and only one form of knowledge, the propositional, is taken to be valid. The step taken by Piaget in his definition of *epistemologie genetique* was to eschew inquiry into the "true" nature of knowledge in favor of a comparative study of the diverse nature of different kinds of knowledge, in his case the kinds encountered in children of different ages. We differ from Piaget on an important point, however. Where he saw diverse forms of knowledge in terms of stages to a finite end point of formal reason, we see different approaches to knowledge as styles, **each equally valid on its own terms**.</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... The development of a new computer culture would require more than environments where there is permission to work with highly personal approaches. It would require a new social construction of the computer, with a new set of intellectual and emotional values more like those applied to harpsichords than hammers. Since, increasingly, computers are the tools people use to write, to design, to play with ideas and shapes and images, they should be addressed with a language that reflects the full range of human experiences and abilities. Changes in this direction would necessitate the reconstruction of our cultural assumptions about formal logic as the "law of thought." This point brings us full circle to where we began, with the assertion that **epistemological pluralism is a necessary condition for a more inclusive computer culture**.</span></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@turkleEpistemologicalPluralismStyles1990</span><span class="co">]</span></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>The project of epistemological pluralism erupted into public consciousness during the "<span class="co">[</span><span class="ot">Science Wars</span><span class="co">](https://en.wikipedia.org/wiki/Science_wars)</span>" of 1990s. After that, it had stayed rather quiet.</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a><span class="fu">## The perceptron controversy</span></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connectionism, 1945--1970</span></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>In the early days, there were several centers of connectionist research, clustered around <span class="co">[</span><span class="ot">Frank Rosenblatt</span><span class="co">](https://en.wikipedia.org/wiki/Frank_Rosenblatt)</span>, <span class="co">[</span><span class="ot">Bernard Widrow</span><span class="co">](https://en.wikipedia.org/wiki/Bernard_Widrow)</span>, and the <span class="co">[</span><span class="ot">Stanford Research Institute</span><span class="co">](https://en.wikipedia.org/wiki/SRI_International)</span> (SRI). Out of those centers of research, Minsky and Papert targeted mostly Rosenblatt's research.</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>Frank Rosenblatt's research had three modes: mathematical theory, experiments on bespoke machines, such as the Mark I Perceptron and the Tobermory, and experiments on serial digital computers, usually IBM machines. He was strongly inclined to building two-layered perceptron machines where the first layer was *fixed* 0-1 weights, and only the second layer contained real-valued weights learned by the perceptron learning rule. This is precisely the abstract model of the perceptron machine used by Minsky and Papert.</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>After 4 years of research, he published a summary of his work in <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962</span><span class="co">]</span>. In the book, he noted that there were many problems that the perceptron machines could not learn well. As summarized in <span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991, pages 116--121</span><span class="co">]</span>,</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... two stimuli (presented one after another) had to occupy nearly the same area of the retina in order to be classified as similar. ... The lack of an adequate preprocessing system meant that a set of association units had to be dedicated to the recognition of each possible object, and this created an excessively large layer of association units in the perceptron. ... Other problems were excessive learning time, excessive dependence on external evaluation (supervision), and lack of ability to separate essential parts in a complex environment. Rosenblatt (1962, pp. 309-310) included the 'figure-ground' or 'connectedness' problem in this last point.</span></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; &gt; A number of perceptrons analyzed in the preceding chapters have been analyzed in a purely formal way, yielding equations which are not readily translated into numbers. This is particularly true in the case of the four-layer and cross-coupled systems, where the generality of the equations is reflected in the obscurity of their implications. ... The previous questions </span><span class="co">[</span><span class="ot">from the first to the twelfth</span><span class="co">]</span><span class="at"> are all in the nature of 'mopping-up' operations in areas where some degree of performance is known to be possible . . . </span><span class="co">[</span><span class="ot">However,</span><span class="co">]</span><span class="at"> the problems of figure-ground separation (or recognition of unity) and topological relation recognition represent new territory, against which few inroads have been made." (Rosenblatt, 1962a, pp. 580-581)</span></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>Almost every one of these problems was specifically targeted by the *Perceptrons* book. For example, the difficulty of testing for "connectedness" was a centerpiece of the entire book, the difficulty of recognizing symmetry was studied by "stratification" and shown to have exponentially growing coefficients (Chapter 7), the requirement for "had to occupy nearly the same area of the retina" was targeted by studies on the limitations of "diameter-limited perceptrons" (Chapter 8), the "figure-ground problem" was targeted by showing "recognition-in-context" has infinite order (Section 6.6), the "generality of the equations is reflected in the obscurity of their implications" was targeted by comments such as "if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head" (Section 13.2), etc.</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>Bernard Widrow worked mostly in collaboration with <span class="co">[</span><span class="ot">Marcian Hoff</span><span class="co">](https://en.wikipedia.org/wiki/Marcian_Hoff)</span>. Their work is detailed in my essay <span class="co">[</span><span class="ot">*The Backstory of Backpropagation*</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/#bernard-widrow-and-marcian-hoff)</span>. In short, they first developed a least-mean-square gradient descent method to train a single perceptron, then proceeded to two-layered perceptrons and predictably failed to develop backpropagation, as the activation function is not differentiable. Thereafter, Widrow gave up neural networks until learning of backpropagation in the 1980s.</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Widrow and his students developed uses for the Adaline and Madaline. Early applications included, among others, speech and pattern recognition, weather forecasting, and adaptive controls. Work then switched to adaptive filtering and adaptive signal processing after attempts to develop learning rules for networks with multiple adaptive layers were unsuccessful. ... After 20 years of research in adaptive signal processing, the work in Widrow's laboratory has once again returned to neural networks.</span></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@widrow30YearsAdaptive1990</span><span class="co">]</span></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; At the time that Hoff left, about 1965 or 1966, we had already had lots of troubles with neural nets. My enthusiasm had dropped. But we were beginning to have successful adaptive filters, in other words, finding good applications. ... So we stopped, basically stopped on neural nets, and began on adaptive antennas very strongly. </span></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Interview with Widrow, quoted in </span><span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991, pages 129--130</span><span class="co">]</span></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>SRI had a strong AI program, with luminaries such as <span class="co">[</span><span class="ot">Nils Nilsson</span><span class="co">](https://en.wikipedia.org/wiki/Nils_John_Nilsson)</span>, <span class="co">[</span><span class="ot">Charles Rosen</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Charles_Rosen_(scientist)), <span class="co">[</span><span class="ot">Duda</span><span class="co">](https://en.wikipedia.org/wiki/Richard_O._Duda)</span>, and <span class="co">[</span><span class="ot">Hart</span><span class="co">](https://en.wikipedia.org/wiki/Peter_E._Hart)</span>. At first they worked on a series of systems, MINOS I to III.</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; When I first interviewed for a position at SRI in 1961, I was warned by one researcher there against joining research on neural networks. Such research, he claimed, was “premature,” and my involvement in it could damage my reputation.</span>  </span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@nilssonQuestArtificialIntelligence2009, chapter 24.2</span><span class="co">]</span></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>MINOS II was representative of the whole series. Made in 1962, it had 3 layers: input, hidden, output, but only one was trainable. The input-to-hidden layer consists of 100 photomasks. That is, given an input image, that image is filtered through a mask, and the light is focussed by a convex lens to a single photosensitive pixel. If the light level exceeds a threshold, it is a 1. Else, it is a 0. Repeat this 100 times, one for each mask, and we have converted an image to 100 binary bits.</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Applying a list of photomasks on an input image, resulting in a list of binary bits that featurizes the image, which a perceptron will be able to classify. [@minskyPerceptronsIntroductionComputational1988, Figure 13.1]</span><span class="co">](figure/photomask_featurizer_Minsky_Papert_1988_fig_13_1.png)</span></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>In the hidden-to-output layer, they ran the standard perceptron learning rule. Presumably, they used *hand-designed photomasks* because they also had no better training method than the perceptron learning rule. Since they used 0-1 activation functions like everyone else, they were frustrated by the same problem of *not doing backpropagation*, so they switched to symbolic AI techniques around 1965.</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The weight matrix of the MINOS II pattern recognition system, consisting of a grid of magnetic cores. The magnetization of a core is proportional to its matrix weight. Matrix weights that are quite literally weighty. The weight matrices are swappable. [@rosenResearchDevelopmentProgram1965]</span><span class="co">](figure/MINOS_II.png)</span></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>In 1973, Duda and Hart published the famous "Duda and Hart" book on pattern classification <span class="co">[</span><span class="ot">@dudaPatternClassificationScene1973</span><span class="co">]</span>. The book contained two halves. The first half was statistical: Bayes, nearest neighbors, perceptron, clustering, etc. The second half was on scene analysis, a symbolic-AI method for computer vision. Indeed, <span class="co">[</span><span class="ot">Minsky and Papert promoted it as superior to perceptron networks</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#chapter-13)</span>.<span class="ot">[^duda-hart-2]</span> <span class="co">[</span><span class="ot">Shakey the robot</span><span class="co">](https://en.wikipedia.org/wiki/Shakey_the_robot)</span>, built between 1966 and 1972, was a *tour de force* of scene analysis, and it could move around a mock-up of an office building, pushing around cubes along the way. Its program was written in LISP, the staple programming language for symbolic AI.</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a><span class="ot">[^duda-hart-2]</span>:</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>    It is instructive to compare the first edition with the second, published in 2001 <span class="co">[</span><span class="ot">@dudaPatternClassification2001</span><span class="co">]</span>. It had become almost completely statistical. There were new chapters on neural networks, Boltzmann machines, decision trees, and so on. In contrast, scene analysis was completely removed.</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a><span class="in">    It says something about the obsolescence of scene analysis even in 2001, as Duda and Hart deleted half of their most famous book just to avoid talking about it. In fact, the only mention of "scene analysis" is a condemnation:</span></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Some of the earliest work on three-dimensional object recognition relied on complex grammars which described the relationships of corners and edges, in block structures such arches and towers. It was found that such systems were very brittle; they failed whenever there were errors in feature extraction, due to occlusion and even minor misspecifications of the model. For the most part, then, grammatical methods have been abandoned for object recognition and scene analysis. [@dudaPatternClassification2001, section 8.8]</span></span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I got very interested for a while in the problem of training more than one layer of weights, and was not able to make very much progress on that problem. ... When we stopped the neural net studies at SRI, research money was running out, and we began looking for new ideas. (Nilsson, interview)</span></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; About 1965 or 1966 we decided that we were more interested in the other artificial intelligence techniques. ... Our group never solved the problem of training more than one layer of weights in an automatic fashion. We never solved that problem. That was most critical. Everybody was aware of that problem. (Rosen, interview)</span></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991, pages 131--133</span><span class="co">]</span></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Bill Ridgway (one of Bernard Widrow's Stanford students) adjusted weights in the first layer of what he called a MADALINE. We had a similar scheme for adjusting weights in the first layer of the MINOS II and MINOS III neural network machines at SRI. Others used various statistical techniques to set weight values. But what stymied us all was how to change weights in more than one layer of multilayer networks. (I recall Charles Rosen, the leader of our group, sitting in his office with yellow quadrille tablets hand-simulating his ever-inventive schemes for making weight changes; none seemed to work out.)</span></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@nilssonQuestArtificialIntelligence2009, Chapter 29.4</span><span class="co">]</span></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a><span class="fu">### The perceptron controversy, 1960s</span></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the middle nineteen-sixties, Papert and Minsky set out to kill the Perceptron, or, at least, to establish its limitations -- a task that Minsky felt was a sort of social service they could perform for the artificial-intelligence community. </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a>Although the book was published only in 1969, close to the end of the perceptron controversy, the influence of Minsky and Papert had been felt years earlier as they attended conferences and disseminated their ideas through talks and preprints, sometimes quarreling on stage. Both sides had their motivations and the conflict was real.</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In order to show the extent of the perceptron controversy, it is interesting to repeat some of the rhetorical expressions that were used in it: 'many remember as great spectator sport the quarrels Minsky and Rosenblatt had;' 'Rosenblatt irritated a lot of people;' 'Rosenblatt was given to steady and extravagant statements about the performance of his machine;' 'Rosenblatt was a press agent's dream, a real medicine man;' 'to hear Rosenblatt tell it, his machine was capable of fantastic things;' 'they disparaged everything Rosenblatt did, and most of what ONR did in supporting him;' 'a pack of happy bloodhounds;' 'Minsky knocked the hell out of our perceptron business;' 'Minsky and his crew thought that Rosenblatt's work was a waste of time, and Minsky certainly thought that our work at SRI was a waste of time;' 'Minsky and Papert set out to kill the perceptron, it was a sort of social service they could perform for the Al community;' 'there was some hostility;' 'we became involved with a somewhat therapeutic compulsion;' 'a misconception that would threaten to haunt artificial intelligence;' 'the mystique surrounding such machines.' These rhetorical expressions show the extent (the heat) of the perceptron controversy beyond doubt. </span><span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991, page 112</span><span class="co">]</span></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a>Charles Rosen of SRI recalls:</span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky and his crew thought that Frank Rosenblatt's work was a waste of time, and they certainly thought that our work at SRI was a waste of time. Minsky really didn't believe in perceptrons, he didn't think it was the way to go. I know he knocked the hell out of our perceptron business. </span><span class="co">[</span><span class="ot">@olazaranSociologicalHistoryNeural1993, page 622</span><span class="co">]</span></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a>When *Perceptrons* was finally published in 1969, the connectionist camp was already deserted. The SRI group had switched to symbolic AI projects; Widrow's group had switched to adapting single perceptrons to adaptive filtering; Frank Rosenblatt was still labouring, isolated, with dwindling funds, until his early death in 1971.<span class="ot">[^memory-of-rosenblatt]</span></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a><span class="ot">[^memory-of-rosenblatt]: </span>The 1972 reprinting of *Perceptrons* included a handwritten note, "In memory of Frank Rosenblatt". This was not an ironic dedication, as Minsky and Rosenblatt were personally friendly, although their research paradigms had been fighting for dominance.</span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>During the last days of Rosenblatt, he worked on a massive expansion of the Mark I Perceptron, the Tobermory (1961--1967). Named after a talking cat, it was built for speech recognition. It had 4 layers with 45-1600-1000-12 neurons, and 12,000 adjustable weights implemented with tape-wound magnetic cores. As usual for Rosenblatt, these adjustable weights are all in the last layer ($12000 = 1000 \times 12$). By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines. <span class="co">[</span><span class="ot">@nagyNeuralNetworksthenNow1991</span><span class="co">]</span> Indeed, often during the history of neural networks, someone would think "this calls for a purpose-made computer" and a few years later, Moore's law obsoleted their effort. A kind of hardware bitter lesson.</span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Tobermory schematic. [@nagySystemCircuitDesigns1963]</span><span class="co">](figure/Tobermory.png)</span></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@olazaranHistoricalSociologyNeural1991</span><span class="co">]</span> gathered evidence that the publication of *Perceptrons* was not the cause but a "marker event" for the end of the perceptron controversy and the ascendancy of the symbolic AI school. The book was not the neural network killer, but its epitaph.</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a><span class="fu">### Connectionist retrospectives, 1990s</span></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a>Following the resurgence of connectionism in the 1980s, Anderson and Rosenfeld conducted interviews with prominent connectionists throughout the 1990s, compiled in <span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span>. The perceptron controversy is mentioned several times. Reading the interviews gives one a distinct feeling of <span class="co">[</span><span class="ot">*Rashomon*</span><span class="co">](https://en.wikipedia.org/wiki/In_a_Grove)</span>. The same events are recounted from multiple perspectives. I will excerpt some of the most important ones for the essay.</span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a>Jack D. Cowan gave an "eyewitness account" of Minsky and Papert's role in the controversy, before the publication of the book in 1969.</span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ER: I'm curious about one thing. You said that Minsky and Papert first presented their notions about exclusive-OR in the *Perceptron* work </span><span class="co">[</span><span class="ot">in a 1965 conference</span><span class="co">]</span><span class="at">.</span></span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; JC: Well, they first presented their notions about the limitations of perceptrons and what they could and couldn't do.</span></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ER: They hadn't gotten to exclusive-OR yet?</span></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; JC: They had, but that wasn't a central issue for them. The essential issue was, suppose you had diameter-limited receptive fields in a perceptron, what could it compute?</span></span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ER: How was that received at that first conference?</span></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; JC: Both of them were quite persuasive speakers, and it was well received. What came across was the fact that you had to put some structure into the perceptron to get it to do anything, but there weren't a lot of things it could do. The reason was that it didn't have hidden units. It was clear that without hidden units, nothing important could be done, and they claimed that the problem of programming the hidden units was not solvable. They discouraged a lot of research and that was wrong. ... Everywhere there were people working on perceptrons, but they weren't working hard on them. Then along came Minsky and Papert's preprints that they sent out long before they published their book. There were preprints circulating in which they demolished Rosenblatt's claims for the early perceptrons. In those days, things really did damp down. There's no question that after '62 there was a quiet period in the field.</span></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ER: Robert Hecht-Nielsen has told me stories that long before Minsky and Papert ever committed anything to a paper that they delivered at a conference or published anywhere, they were going down to ARPA and saying, "You know, this is the wrong way to go. It shouldn't be a biological model; it should be a logical model."</span></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; JC: I think that's probably right. In those days they were really quite hostile to neural networks. I can remember having a discussion with Seymour ... in the '60s. We were talking about visual illusions. He felt that they were all higher-level effects that had nothing to do with neural networks as such. They needed a different, a top-down approach to understand. By then he had become a real, a true opponent of neural networks. I think Marvin had the same feelings as well. To some extent, David Marr had those feelings too. After he got to the AI lab, I think he got converted to that way of thinking. Then Tommy Poggio essentially persuaded him otherwise.</span></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Teuvo Kohonen</span><span class="co">](https://en.wikipedia.org/wiki/Teuvo_Kohonen)</span> seemed also angry at the <span class="co">[</span><span class="ot">Chomskyan school</span><span class="co">](https://en.wikipedia.org/wiki/Cognitive_revolution)</span>, for reasons I sketched out in the <span class="co">[</span><span class="ot">appendix on the Chomskyans</span><span class="co">](#appendix-the-chomskyans)</span>.</span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I was one of the people suffering from Minsky and Papert's book </span><span class="co">[</span><span class="ot">Perceptrons</span><span class="co">]</span><span class="at"> because it went roughly this way: you start telling somebody about your work, and this visitor or whoever you talk to says, "Don't you know that this area is dead?" It is something like what we experienced in the pattern recognition society when everything started to be structural and grammatical and semantic and so on. If somebody said, "I'm doing research on the statistical pattern recognition," then came this remark, "Hey, don't you know that is a dead idea already?"</span></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Michael A. Arbib</span><span class="co">](https://en.wikipedia.org/wiki/Michael_A._Arbib)</span> thought the book did not cause the neural network winter, but rather caused by the change in funding.</span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky and Papert basically said that if you limit your networks to one layer in depth, then, unless you have very complicated individual neurons, you can't do very much. This is not too surprising. ... Many people see the book as what killed neural nets, but I really don't think that's true. I think that the funding priorities, the fashions in computer science departments, had shifted the emphasis away from neural nets to the more symbolic methods of AI by the time the book came out. I think it was more that a younger generation of computer scientists who didn't know the earlier work may have used the book as justification for sticking with "straight AI" and ignoring neural nets.</span></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a>Bernard Widrow concurred.</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I looked at that book, and I saw that they'd done some serious work here, and there was some good mathematics in this book, but I said, "My God, what a hatchet job." I was so relieved that they called this thing the perceptron rather than the Adaline because actually what they were mostly talking about was the Adaline, not the perceptron. I felt that they had sufficiently narrowly defined what the perceptron was, that they were able to prove that it could do practically nothing. Long, long, long before that book, I was already successfully adapting Madaline </span><span class="sc">\[</span><span class="at">Madaline = many Adalines</span><span class="sc">\]</span><span class="at">, which is a whole bunch of neural elements. All this worry and agony over the limitations of linear separability, which is the main theme of the book, was long overcome.</span></span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We had already stopped working on neural nets. As far as I knew, there wasn't anybody working on neural nets when that book came out. I couldn't understand what the point of it was, why the hell they did it. But I know how long it takes to write a book. I figured that they must have gotten inspired to write that book really early on to squelch the field, to do what they could to stick pins in the balloon. But by the time the book came out, the field was already gone. There was just about nobody doing it.</span></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">James A. Anderson</span><span class="co">]</span>(https://en.wikipedia.org/wiki/James_A._Anderson_(cognitive_scientist)) pointed out that during the "winter", neural networks survived outside of AI.</span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This was during the period sometimes called the neural network dark ages, after the Minsky and Papert book on perceptrons had dried up most of the funding for neural networks in engineering and computer science. Neural networks continued to be developed by psychologists, however, because they turned out to be effective models in psychology ... What happened during the dark ages was that the ideas had moved away from the highly visible areas of big science and technology into areas of science that did not appear in the newspapers.</span></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">David Rumelhart</span><span class="co">](https://en.wikipedia.org/wiki/David_Rumelhart)</span> had nice things to say about Minsky, with no trace of bitterness. It is understandable as he only started working in neural networks years after the controversy died down.</span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I always had one course that was like a free course in which I would choose a book of the year and teach out of that. In 1969, I think it was, or maybe '70, I chose Perceptrons by Minsky and Papert as the book of the year. We then carefully went through it and read it in a group. ... This was my most in-depth experience with things related to neural networks, or what were later called neural networks. I was quite interested in Minsky in those days because he also had another book which was called, I think, Semantic Information Processing. That book was a collection, including an article by Ross Quillian. It was a collection of dissertations from his graduate students. In a way, it was Minsky who led me to read about the perceptron more than anybody else.</span></span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>Regarding Robert Hecht-Nielsen, we have already seen his belief that Minsky was "Darth Vader" and possibly "the Devil". Unsurprisingly, he was the most embittered, and placed the blame for the 1970s neural network winter squarely on the publication of *Perceptrons*.</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By the mid-1970s, Minsky and his colleagues (notably Seymour Papert) began to take actions designed to root out neural networks and ensure large and, in their view, richly deserved funding for AI research by getting the money currently being "wasted" on neural networks, and more to boot, redirected. They did two things. First, Minsky and Papert began work on a manuscript designed to discredit neural network research. Second, they attended neural network and "bionics" conferences and presented their ever-growing body of mathematical results being compiled in their manuscript to what they later referred to as "the doleful responses" of members of their audiences.</span></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; At the heart of this effort was Minsky and Papert's growing manuscript, which they privately circulated for comments. The technical approach they took in the manuscript was based on a mathematical theorem discovered and proven some years earlier—ironically, by a strong supporter of Rosenblatt—that the perceptron was incapable of ever implementing the "exclusive-OR" </span><span class="co">[</span><span class="ot">X-OR</span><span class="co">]</span><span class="at"> logic function. What Minsky and Papert and their colleagues did was elaborate and bulk up this idea to book length by devising many variants of this theorem. Some, such as a theorem showing that single-layer perceptrons, of many varied types, cannot compute topological connectedness, are quite clever. To this technical fabric, they wove in what amounted to a personal attack on Rosenblatt. This was the early form of their crusade manifesto.</span></span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Later, on the strong and wise advice of colleagues, they expunged the vitriol. They didn't quite get it all, as a careful reading will show. They did a complete flip-flop, dedicating the book to Rosenblatt! As their colleagues sensed it would, this apparently "objective" evaluation of perceptrons had a much more powerful impact than the original manuscript with its unseemly personal attack would have. Of course, in reality, the whole thing was intended, from the outset, as a book-length damnation of Rosenblatt's work and many of its variants in particular, and, by implication, all other neural network research in general.</span></span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Minsky and Papert's book, Perceptrons, worked. The field of neural networks was discredited and destroyed. The book and the associated conference presentations created a new conventional wisdom at DARPA and almost all other research sponsorship organizations that some MIT professors have proven mathematically that neural networks cannot ever do anything interesting. The chilling effect of this episode on neural network research lasted almost twenty years.</span></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a><span class="fu">## The message of *Perceptrons*</span></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a>Minsky described how he and Papert felt impelled to write the book by a "therapeutic compulsion", since they were "appalled" by the influence perceptrons:</span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Both of the present authors (first independently and later together) became involved with a somewhat therapeutic compulsion: to dispel what we feared to be the first shadows of a "holistic" or "Gestalt" misconception that would threaten to haunt the fields of engineering and artificial intelligence as it had earlier haunted biology and psychology. For this, and for a variety of more practical and theoretical goals, we set out to find something about the range and limitations of perceptrons.</span></span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988, page 20</span><span class="co">]</span></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Our first formal presentation of the principal results in this book was at an American Mathematical Society symposium on Mathematical Aspects of Computer Science in April 1966. At this time we could prove that $\psi_{CONNECTED}$ was not of finite order and conjectured that the same must be true of the apparently "global" predicates of symmetry and twins described in §7.3. </span></span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For the rest of 1966 the matter rested there. We were pleased and encouraged by the enthusiastic reception by many colleagues at the A.M.S. meeting and no less so by the doleful reception of a similar presentation at a Bionics meeting. However, we were now involved in establishing at M.I.T. an artificial intelligence laboratory largely devoted to real "seeing machines", and gave no attention to perceptrons until we were jolted by attending an I.E.E.E. Workshop on Pattern Recognition in Puerto Rico early in 1967.</span></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Appalled at the persistent influence of perceptrons (and similar ways of thinking) on practical pattern recognition, we determined to set out our work as a book.</span></span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988, page 242</span><span class="co">]</span></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>This 1966 encounter had been corroborated in other places.<span class="ot">[^1966-encounter]</span> According to Kanal, the workshop was in fact held in 1966 October 24--26, and that only 5 out of 30 papers were about neural networks for pattern recognition. I suppose for Minsky and Papert, 5 out of 30 is 5 too many. <span class="co">[</span><span class="ot">@kanalPatternCategoriesAlternate1993</span><span class="co">]</span></span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1966-encounter]</span>: </span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; In 1966, a few dozen researchers traveled to Puerto Rico, gathering at the Hilton hotel in San Juan. They met to discuss the latest advances in what was then called "pattern recognition"--technology that could identify patterns in images and other data. Whereas Rosenblatt viewed the Perceptron as a model of the brain, others saw it as a means of pattern recognition... Rosenblatt didn't even travel to Puerto Rico. Inside the Hilton, the tension emerged when a young scientist named John Munson addressed the conference. Munson worked at SRI... There, alongside a larger team of researchers, he was trying to build a neural network that could read handwritten characters, not just printed letters, and with his presentation at the conference, he aimed to show the progress of this research. But when Munson finished the lecture and took questions from the floor, Minsky made himself heard. "How can an intelligent young man like you," he asked, "waste your time with something like this?" ... "This is an idea with no future." [@metzGeniusMakersMavericks2021]</span></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a>The book has become a true classic: everybody wants to have read and nobody wants to read. Taking the opposite approach, I *have* read the book, despite not wanting to read it.</span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a>Its content can be neatly divided into a greater and a lesser half. The greater half is a mathematical monograph on which functions can be implemented by a single perceptron with fixed featurizers, and the lesser half is a commentary on the wider implications of the monograph. The impact of the work is precisely reversed: most of the impact comes from the commentary derived from the results, and effectively no impact comes from the mathematical results themselves.</span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>Despite this imbalance, the mathematical work is substantial, and the perceptron controversy turns critically on the pliable interpretation sprouting from the solid structure. Therefore, I have detailed the mathematical content in a separate essay, <span class="co">[</span><span class="ot">*Reading Perceptrons*</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/)</span>, to which I refer occasionally to gloss their interpretation.</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a><span class="fu">### Minsky and Papert struck back</span></span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a>In the 1980s, neural networks rose again to prominence under the name of "connectionism", prompting an eventual response from Minsky and Papert. The *Perceptrons* book was reissued in 1988, with new chapters dedicated to rejecting connectionism. They took the 1986 two-volume work of *Parallel Distributed Processing* (*PDP*), especially <span class="co">[</span><span class="ot">@rumelhartLearningInternalRepresentations1985</span><span class="co">] [^rumelhart-1985]</span>, as the representative of connectionism, and made specific objections to them.</span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a><span class="ot">[^rumelhart-1985]: </span>This paper was reprinted in <span class="co">[</span><span class="ot">@rumelhartParallelDistributedProcessing1986, volume 1, chapter 8</span><span class="co">]</span>, in which Minsky and Papert read it. This paper is often cited for the backpropagation algorithm, which I have discussed in <span class="co">[</span><span class="ot">*The Backstory of Backpropagation*</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/)</span>.</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a>In the prologue, they staked their claim thus: Connectionism is a mistake engendered by a new generation of researchers ignorant of history; though the theorems of the *Perceptrons* book apply to only a single perceptron, the *lessons* extend to all neural networks. To back up the claim, they made specific technical, historical, and philosophical objections, all with the central goal of showing that **homogeneous neural networks cannot scale**.</span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... when we found that little of significance had changed since 1969, when the book was first published, we concluded that it would be more useful to keep the original text (with its corrections of 1972) and add an epilogue, so that the book could still be read in its original form. One reason why progress has been so slow in this field is that researchers unfamiliar with its history have continued to make many of the same mistakes that others have made before them.</span></span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... there has been little clear-cut change in the conceptual basis of the field. The issues that give rise to excitement today seem much the same as those that were responsible for previous rounds of excitement. ... many contemporary experimenters assume that, because the perceptron networks discussed in this book are not exactly the same as those in use today, these theorems no longer apply. Yet, as we will show in our epilogue, most of the *lessons* of the theorems still apply.</span></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a>In an earlier interview, Minsky reiterated his belief that the proper place of perceptrons is solving *tiny* problems with *tiny* perceptron networks.</span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... for certain purposes the Perceptron was actually very good. I realized that to make one all you needed in principle was a couple of molecules and a membrane. So after being irritated with Rosenblatt for overclaiming, and diverting all those people along a false path, I started to realize that for what you get out of it -- the kind of recognition it can do -- it is such a simple machine that it would be astonishing if nature did not make use of it somewhere. It may be that one of the best things a neuron can have is a tiny Perceptron, since you get so much from it for so little. You can't get one big Perceptron to do very much, but for some things it remains one of the most elegant and simple learning devices I know of. </span><span class="co">[</span><span class="ot">@bernsteinMarvinMinskyVision1981</span><span class="co">]</span></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>They also urged all AI researchers to adopt the Society of Mind hypothesis, or else face the charge of being unreflective or of drawing lines where none exists. It seems to me that Minsky wrote most of the prologue and epilogue, because in <span class="co">[</span><span class="ot">Papert's solo paper</span><span class="co">](#sec-papert-struck-back)</span>, he went considerably further with sociological interpretation.</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This broad division makes no sense to us, because these attributes are largely independent of one another; for example, the very same system could combine symbolic, analogical, serial, continuous, and localized aspects. Nor do many of those pairs imply clear opposites; at best they merely indicate some possible extremes among some wider range of possibilities. And although many good theories begin by making distinctions, we feel that in subjects as broad as these there is less to be gained from sharpening bound­aries than from seeking useful intermediates.</span></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... Are there inherent incompatibilities between those connectionist and symbolist views? The answer to that depends on the extent to which one regards each separate connectionist scheme as a self-standing system. If one were to ask whether any particular, homogeneous network could serve as a model for a brain, the answer (we claim) would be, clearly. No. But if we consider each such network as a possible model for a part of a brain, then those two overviews are complementary. This is why we see no reason to choose sides.</span></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... Most researchers tried to bypass </span><span class="sc">\[</span><span class="at">the technical objections</span><span class="sc">\]</span><span class="at">, either by ignoring them or by using brute force or by trying to discover powerful and generally applicable methods. Few researchers tried to use them as guides to thoughtful research. We do not believe that any completely general solution to them can exist ...</span></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a>We now proceed to the epilogue and its arguments.</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1980s connectionism is not that different</span></span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>They speculated on the reason for the revival of neural networks. Was it because of the development of backpropagation, multilayer networks, and faster computers? *Emphatically not*. In fact, 1980s connectionists were not different from the 1960s connectionists. It is only the ignorance of history that made them think otherwise. In both periods, connectionism was focused on making small-scale experiments and then extrapolating to the largest scale, without mathematical theorems to justify the extrapolation. In both periods, connectionism failed (or would fail) to scale beyond toy problems.</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; most of the theorems in this book are explicitly about machines with a single layer of adjustable connection weights. But this does not imply (as many modern connectionists assume) that our conclusions don't apply to multilayered machines. To be sure, those proofs no longer apply unchanged, because their antecedent conditions have changed. But the phenomena they describe will often still persist. One must examine them, case by case.</span></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... the situation in the early 1960s: Many people were impressed by the fact that initially unstructured networks composed of very simple devices could be made to perform many interesting tasks -- by processes that could be seen as remarkably like some forms of learning. A different fact seemed to have impressed only a few people: While those networks did well on certain tasks and failed on certain other tasks, there was no theory to explain what made the difference -- particularly when they seemed to work well on small ("toy") problems but broke down with larger problems of the same kind. Our goal was to develop analytic tools to give us better ideas about what made the difference.</span></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a><span class="fu">#### There is no silver bullet in machine learning</span></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a>There are no general algorithms and there are no general problems. There are only particular algorithm-problem pairs. An algorithm-problem pair can be a good fit, or a bad fit. The parity problem is a bad fit with a neural network trained by backpropagation, but it is a good fit with a Turing machine.</span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>There is no general and effective algorithm. Either the algorithm is so general that it is as useless as "just try every algorithm" akin to Ross Ashby's homeostat, or it is useful but not general. This general lesson is similar to Gödel's speedup theorem, Blum's speedup theorem, the no free lunch theorem, etc.</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Clearly, the procedure can make but a finite number of errors before it hits upon a solution. It would be hard to justify the term "learning" for a machine that so relentlessly ignores its experience. The content of the perceptron convergence theorem must be that it yields a better learning procedure than this simple homeostat. Yet the problem of relative speeds of learning of perceptrons and other devices has been almost entirely neglected. </span><span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988, Section 11.7</span><span class="co">]</span></span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Arthur Samuel</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist))'s checker learning algorithm encountered two fundamental problems: credit assignment and inventing novel features. Those two problems are not just for the checker AI, but for *all* AI. There are *no* universal and effective solutions to credit assignment, and there are *no* universally effective solutions to inventing novel features. There could be universal but impractical solutions, such as backpropagation on homogeneous neural networks, Solomonoff induction, trying every Turing machine, etc. There could be practical but not universal solutions, which is precisely what populates the Society of Mind in human brains.</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Rosenblatt's credit-assignment method turned out to be as effective as any such method could be. When the answer is obtained, in effect, by adding up the contributions of many processes that have no significant interactions among themselves, then the best one can do is reward them in proportion to how much each of them contributed.</span></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Several kinds of evidence impel us toward this view. One is the great variety of different and specific functions embodied in the brain's biology. Another is the similarly great variety of phenomena in the psychology of intelligence. And from a much more abstract viewpoint, we cannot help but be impressed with the practical limitations of each "general" scheme that has been proposed -- and with the theoretical opacity of questions about how they behave when we try to scale their applications past the toy problems for which they were first conceived.</span></span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a><span class="fu">#### There is no efficient way to train homogeneous, high-order networks</span></span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>They ask the reader to think back to the <span class="co">[</span><span class="ot">lesson of the parity predicate from Chapter 10</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#exm-parity-order)</span>: Even though it is learnable by a two-layered perceptron network, it would involve weights exponential in the input pixel count, and therefore take a very long time to learn. They expect this to generalize, so that *any* problem that require *some* perceptron in the network to have receptive field of size $\Omega(|R|^\alpha)$, necessarily require that perceptron to have coefficients growing like $2^{\Omega(|R|^\alpha)}$, and therefore taking $2^{\Omega(|R|^\alpha)}$ steps to train.</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We could extend them either by scaling up small connectionist models or by combining small-scale networks into some larger organization. In the first case, we would expect to encounter theoretical obstacles to maintaining GD's effectiveness on larger, deeper nets. And despite the reputed efficacy of other alleged remedies for the deficiencies of hill-climbing, such as "annealing," we stay with our research conjecture that no such procedures will work very well on large-scale nets, except in the case of problems that turn out to be of low order in some appropriate sense.</span></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The second alternative is to employ a variety of smaller networks rather than try to scale up a single one. And if we choose (as we do) to move in that direction, then our focus of concern as theoretical psychologists must turn toward the organizing of small nets into effective large systems.</span></span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a><span class="fu">#### There is no effective use for homogeneous, high-order networks</span></span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a>Fully connected networks, or indeed any neural network without a strong constraint on "order" or "receptive field", would hopelessly confuse itself with its own echoes as soon as it scales up, unless it has sufficient "insulation", meaning almost-zero connection weights, such that it effectively splits into a large number of small subnets. That is, a large fully connected network is useless anyway unless it already decomposes into many tiny networks arranged in a Society of Mind.</span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Certain parallel computations are by their nature synergistic and cooperative: each part makes the others easier. But the </span><span class="co">[</span><span class="ot">And/Or of theorem 4.0</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#thm-and-or)</span><span class="at"> shows that under other circumstances, attempting to make the same network perform two simple tasks at the same time leads to a task that has a far greater order of difficulty. In those sorts of circumstances, there will be a clear advantage to having mechanisms, not to connect things together, but to keep such tasks apart. How can this be done in a connectionist net?</span></span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... a brain is not a single, uniformly structured network. Instead, each brain contains hundreds of different types of machines, interconnected in specific ways which predestine that brain to become a large, diverse society of partially specialized agencies.</span></span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Gradient descent cannot escape local minima</span></span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a>Gradient descent, backpropagation, and all other hill-climbing algorithms are all vulnerable to getting trapped in local optima, and therefore they cannot work -- except in problem-architecture pairs where the loss landscape of this *particular* problem, for this *particular* architecture, using this *particular* loss function, on this *particular* dataset, is a single bump whose width is shorter than this *particular* learning rate.</span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a>Gradient descent is just a form of hill-climbing, when the hill is differentiable. The perceptron learning algorithm can be interpreted as a hill-climbing algorithm too, as it makes localized decision to make one step in this direction or that, one error-signal at a time (Section 11.7). Therefore, the generic ineffectiveness of perceptron learning suggests that gradient descent is also generically ineffective and **cannot scale**. It does not even have a convergence theorem, so in that sense it's *worse* than perceptron learning algorithm.<span class="ot">[^worse-than-perceptron]</span></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We were very pleased to discover (see section 11.6) that PC </span><span class="sc">\[</span><span class="at">Perceptron Convergence theorem</span><span class="sc">\]</span><span class="at"> could be represented as hill-climbing; however, that very fact led us to wonder whether such procedures could dependably be generalized, even to the limited class of multilayer machines that we named Gamba perceptrons. The situation seems not to have changed much -- we have seen no contemporary connectionist publication that casts much new theoretical light on the situation. Then why has GD </span><span class="sc">\[</span><span class="at">Gradient Descent</span><span class="sc">\]</span><span class="at"> become so popular in recent years? ... we fear that its reputation also stems from unfamiliarity with the manner in which hill-climbing methods deteriorate when confronted with larger-scale problems. ... Indeed, GD can fail to find a solution when one exists, so in that narrow sense it could be considered *less* powerful than PC.</span></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a><span class="ot">[^worse-than-perceptron]</span>:</span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>    This claim is astonishing, now that we see how powerful backpropagation works, and how the perceptron learning rule had crippled neural network research for 30 years. We can understand their sentiments by remembering that they, like most of the academic community in computer science, favored the certainty of mathematical theorems over mere empirical success. <span class="co">[</span><span class="ot">Leo Breiman</span><span class="co">](https://en.wikipedia.org/wiki/Leo_Breiman)</span> observed that academic statistics had been hamstrung by the same grasp over mathematical certainty, and thus over 95\% of its publications were useless. <span class="co">[</span><span class="ot">@breimanReflectionsRefereeingPapers1995</span><span class="co">]</span></span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stochastic gradient descent cannot see through the noise</span></span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; So far as we could tell, every experiment described in </span><span class="co">[</span><span class="ot">@rumelhartLearningInternalRepresentations1985</span><span class="co">]</span><span class="at"> involved making a complete cycle through all possible input situations before making any change in weights. Whenever this is feasible, it completely eliminates sampling noise—and then even the most minute correlations can become reliably detectable, be­ cause the variance is zero. But no person or animal ever faces situations that are so simple and arranged in so orderly a manner as to provide such cycles of teaching examples. Moving from small to large problems will often demand this transition from exhaustive to statistical sampling, and we suspect that in many realistic situations the resulting sampling noise would mask the signal completely. We suspect that many who read the connectionist literature are not aware of this phenomenon, which dims some of the prospects of successfully applying certain learning procedures to large-scale problems.</span></span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Differentiable activation is just a hack</span></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a>Using differentiable activations for neural networks is an artificial trick of questionable future. It makes the learned boolean functions imprecise, and only appears to redeem itself by allowing backpropagation. However, backpropagation is a dead-end because it will not scale. It is better to look for a method that can directly train multilayer perceptron networks with discrete activation functions.</span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The trick is to replace the threshold function for each unit with a monotonic and differentiable function ... However, we suspect that this smoothing trick may entail a large (and avoidable) cost when the predicate to be learned is actually a composition of linear threshold functions. There ought to be a more efficient alternative based on how much each weight must be changed, for each stimulus, to make the local input sum cross the threshold.</span></span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We conjecture that learning XOR for larger numbers of variables will become increasingly intractable as we increase the numbers of input variables, because by its nature the underlying parity function is absolutely uncorrelated with any function of fewer variables. Therefore, there can exist no useful correlations among the outputs of the lower-order units involved in computing it, and that leads us to suspect that there is little to gain from following whatever paths are indicated by the artificial introduction of smoothing functions that cause partial derivatives to exist.</span></span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Connectionists have no theory, so they should not extrapolate from experiments</span></span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the past few years, many experiments have demonstrated that various new types of learning machines, composed of multiple layers of perceptron-like elements, can be made to solve many kinds of small-scale prob­lems. Some of those experimenters believe that these performances can be economically extended to larger problems without encountering the limitations we have shown to apply to single­ layer perceptrons. Shortly, we shall take a closer look at some of those results and see that much of what we learned about simple perceptrons will still remain quite pertinent.</span></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a>Without a mathematical theory, experimental data cannot be extrapolated. If neural networks happen to work well on a problem, it merely shows that the problem is a good fit for this *particular* architecture trained in this *particular* way at this *particular* scale, not anything more general than that.</span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; As the field of connectionism becomes more mature, the quest for a general solution to all learning problems will evolve into an understanding of which types of learning processes are likely to work on which classes of problems. And this means that, past a certain point, we won't be able to get by with vacuous generalities about hill-climbing. We will really need to know a great deal more about the nature of those surfaces for each specific realm of problems that we want to solve.</span></span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... the learning procedure required 1,208 cycles through each of the 64 possible examples -- a total of 77,312 trials (enough to make us wonder if the time for this procedure to determine suitable coefficients increases exponentially with the size of the retina). *PDP* does not address this question. What happens when the retina has 100 elements? If such a network required on the order of $2^{200}$ trials to learn. most observers would lose interest.</span></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Connectionist experiments can be extrapolated to show that they do not scale</span></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a>Though lacking a theory of their own on the operation of multilayer perceptrons, Minsky and Papert proceeded to interpret the connectionist experiment data as showing that neural networks would fail to scale.<span class="ot">[^extrapolation-bias]</span></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a><span class="ot">[^extrapolation-bias]: </span>Without a mathematical theory of what neural networks can do, extrapolating from their behavior at small scales to the large scale is impossible and only reflect the bias behind those who make the extrapolation.</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a>Connectionists demonstrated that two-layered perceptrons, where both layers were trainable, bypassed the limits described in *Perceptrons*. For example, <span class="co">[</span><span class="ot">@rumelhartLearningInternalRepresentations1985</span><span class="co">]</span> showed that several problems unsolvable by a single perceptron -- XOR, parity, symmetry, etc -- were solved by a two-layered neural network.</span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Page 253, Figure 2. Redrawn from [@rumelhartLearningInternalRepresentations1985]</span><span class="co">](figure/perceptrons_page_253.png)</span></span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>While the connectionist authors saw the result as a hopeful sign, Minsky and Papert interpreted it as showing that the experiments wouldn't scale, because the coefficients appeared to grow exponentially -- in just the way they proved in <span class="co">[</span><span class="ot">Chapter 7</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/#exm-line-symmetry)</span>.</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In *PDP* it is recognized that the lower-level coefficients appear to be growing exponentially, yet no alarm is expressed about this. In fact, anyone who reads section 7.3 should recognize such a network as employing precisely the type of computational structure that we called stratification.</span></span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; although certain problems can easily by solved by perceptrons on small scales, the computational costs become prohibitive when the problem is scaled up. The authors of PDP seem not to recognize that the coefficients of this symmetry machine confirm that thesis, and celebrate this performance on a toy problem as a success rather than asking whether it could become a profoundly "bad" form of behavior when scaled up to problems of larger size.</span></span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a><span class="fu">### Papert struck back {#sec-papert-struck-back}</span></span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a>While it appears that Minsky was the main author for the new prologue and epilogue, Papert solo-authored <span class="co">[</span><span class="ot">@papertOneAIMany1988</span><span class="co">]</span>, an essay that gave the controversy a uniquely Papert-styled spin. It is an extensive reframing of the perceptron controversy into a social and philosophical issue, with the prediction of ultimate victory for epistemological pluralism:</span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The field of artificial intelligence is currently divided into what seem to be several competing paradigms ... for mechanisms with a universal application. I do not foresee the future in terms of an ultimate victory for any of the present contenders. What I do foresee is a change of frame, away from the search for universal mechanisms. I believe that we have much more to learn from studying the differences, rather than the sameness, of kinds of knowing.</span></span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a>He diagnosed the source of the philosophical error as a "category error".</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There is the same mistake on both sides: the category error of supposing that the existence of a common mechanism provides both an explanation and a unification of all systems, however complex, in which this mechanism might play a central role.</span></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Artificial intelligence, like any other scientific enterprise, had built a scientific culture... more than half of our book is devoted to "pro-perceptron" findings about some very surprising and hitherto unknown things that perceptrons can do. But in a culture set up for global judgment of mechanisms, being understood can be a fate as bad as death. A real understanding of what a mechanism can do carries too much implication about what it cannot do... The same trait of universalism leads the new generation of connectionists to assess their own microlevel experiments, such as Exor, as a projective screen for looking at the largest macroissues in the philosophy of mind. The category error analogous to seeking explanations of the tiger's stripes in the structure of DNA is not an isolated error. It is solidly rooted in AI's culture.</span></span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>He then discussed the compute-first interpretation, a "<span class="co">[</span><span class="ot">bitter lesson</span><span class="co">](https://web.archive.org/web/20190314204621/http://www.incompleteideas.net/IncIdeas/BitterLesson.html)</span>" for the 1980s, before rejecting it.</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the olden days of Minsky and Papert, neural networking models were hopelessly limited by the puniness of the computers available at the time and by the lack of ideas about how to make any but the simplest networks learn. Now things have changed. Powerful, massively parallel computers can implement very large nets, and new learning algorithms can make them learn. ...</span></span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I don't believe it. The influential recent demonstrations of new networks all run on small computers and could have been done in 1970 with ease. Exor is a "toy problem" run for study and demonstration, but the examples discussed in the literature are still very small. Indeed, Minsky and I, in a more technical discussion of this history (added as a new prologue and epilogue to a reissue of Perceptrons), suggest that the entire structure of recent connectionist theories might be built on quicksand: it is all based on toy-sized problems with no theoretical analysis to show that performance will be maintained when the models are scaled up to realistic size. The connectionist authors fail to read our work as a warning that networks, like "brute force" programs based on search procedures, scale very badly.</span></span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>Consider Exor, a certain neural network he picked out of the pages of *PDP*, which learned to perform the infamous XOR task, but only after 2232 examples. Was it slow, or fast? A proper judgment requires a mathematical understanding of the algorithm-problem fit. By extension, to properly judge whether neural networks were good for any specific problem, one must first mathematically understand the fit. He insinuated that the connectionists who were confident that their neural networks were more than a sterile extension of the perceptron did not do their math, unlike he and Minsky.</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; instead of asking whether nets are good, we asked what they are good for. The focus of enquiry shifted from generalities about kinds of machines to specifics about kinds of tasks. From this point of view, Exor raises such questions as: Which tasks would be learned faster and which would be learned even more slowly by this machine? Can we make a theory of tasks that will explain why 2,232 repetitions were needed in this particular act of learning?</span></span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... Minsky and I both knew perceptrons extremely well. We had worked on them for many years before our joint project of under standing their limits was conceived... I was left with a deep respect for the extraordinary difficulty of being sure of what a computational system can or cannot do. I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds.</span></span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Interjection<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a>    I wonder at people who seem so secure in their intuitive convictions, or their less-than-rigorous rhetorical arguments, about computers, neural nets, or human minds:</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting "learning theorem" for the multilayered machine will be found. [@minskyPerceptronsIntroductionComputational1988, page 232]</span></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a>What, then, explains the rise of connectionism? Since Papert reframed the fall of perceptron socially, it only stands to reason he would reframe the rise of connectionism as the rise of a social myth caused by other social myths, *not* by the increase in computing power or new algorithms like backpropagation, convolutional networks, and such. For one, the computing powers used by the breakthrough connectionist models like NETtalk were already within reach even in the 1960s.<span class="ot">[^nettalk]</span> For another, he and Minsky were firm in their conviction that any uniform architecture must scale very badly and that no amount of computing or algorithmic advancement could be anything more than a sterile extension.</span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a><span class="ot">[^nettalk]</span>:</span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a>    NETtalk, a neural network with 3 layers and 18,629 weights, is entirely within reach for the 1960s. Its dataset was built in weeks *by hand*, and its training took a single night on a <span class="co">[</span><span class="ot">Ridge computer</span><span class="co">](https://en.wikipedia.org/wiki/Ridge_Computers)</span> that is close to a <span class="co">[</span><span class="ot">VAX 11/780</span><span class="co">](https://en.wikipedia.org/wiki/VAX-11#VAX-11/780)</span>. Now, VAX 11/780 has $\sim 1 \;\rm{MFLOP/sec}$, so NETtalk took $\sim 10^{11}\;\rm{FLOP}$ to train. <span class="co">[</span><span class="ot">During the 1960s, typical workstations</span><span class="co">](https://yuxi-liu-wired.github.io/docs/posts/1998-hans-moravec/#appendix-dataset)</span> have a computing power of $\sim 0.11 \;\rm{MIPS}$, so NETtalk could be trained in a month.</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; We then used the 20,000-word Brown Corpus and assigned phonemes, as well as stress marks, to each of letters. The alignment of the letters and sounds took weeks, but, once the learning started, the network absorbed the whole corpus in a single night. [@sejnowskiDeepLearningRevolution2018, page 115]</span></span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; I had picked up a Ridge computer, made by a company that is now defunct, but it had the power of a VAX 11/780 which at that time was the standard candle of computer power. ... We had a real computer, and we had a real algorithm, and we looked for a do-able project in language. ... I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they're doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, "[Chomsky](#appendix-the-chomskyans) worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do."</span></span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt;</span></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; ... In retrospect it was an ideal choice for a problem. It was difficult with conventional techniques, and it was not clear that the network could handle it. ... We knew back then there were many local minima in the network, and we knew we were getting trapped. The surprise was that this did not prevent the network from finding good solutions. [@rosenfeldTalkingNetsOral2000, pages 324--325]</span></span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Massively parallel supercomputers do play an important role in the connectionist revival. But I see it as a cultural rather than a technical role, another example of a sustaining myth. Connectionism does not use the new computers as physical machines; it derives strength from the "computer in the mind," from its public's largely nontechnical awareness of supercomputers. I see connectionism's relationship to biology in similar terms. Although its models use biological metaphors, they do not depend on technical findings in biology any more than they do on modern supercomputers. ... I also see a more subtle, but not less relevant, cultural resonance. This is a generalized turn away from the hard-edged rationalism of the time connectionism last went into eclipse and a resurgent attraction to more holistic ways of thinking.</span></span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rebuttal to Minsky and Papert</span></span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting the XOR problem</span></span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a>When I first heard about the first neural network winter and the *Perceptrons* book, I was deeply confused by the story. The story went that "*Perceptrons* proved that the XOR problem is unsolvable by a single perceptron, a result that caused researchers to abandon neural networks". How could it possibly *cause* researchers to abandon the field, unless it was news to them? But anyone could see that a single perceptron could only separate linearly separable points, and therefore the XOR problem is unsolvable by a single perceptron. When I first heard the story, I immediately saw why XOR was unsolvable by one perceptron, then took a few minutes to design a two-layered perceptron network that solved the XOR problem. I then noted that the NAND problem is solvable by a single perceptron, after which I immediately knew that perceptron networks are universal <span class="co">[</span><span class="ot">since the NAND gate is</span><span class="co">](https://en.wikipedia.org/wiki/Functional_completeness)</span>.</span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a>If a high school student could bypass the XOR problem in a few minutes, how could it possibly have been news to the researchers in 1969?</span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a>When I started researching neural networks properly, the standard story about the XOR problem became more nonsensical the more I learned. The 1943 paper by McCulloch and Pitts <span class="co">[</span><span class="ot">@mccullochLogicalCalculusIdeas1943</span><span class="co">]</span> *already* said that their neural networks were equivalent in power to Turing machines. Marvin Minsky's 1954 PhD thesis <span class="co">[</span><span class="ot">@minskyTheoryNeuralanalogReinforcement1954</span><span class="co">]</span> develops an entire computer theory out of McCulloch--Pitts neural networks.</span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a>On the electrical engineering side, perceptron networks were studied under the name of "linear threshold logic" by electrical engineers since the 1950s, who clearly would not have bothered if they could not even make an XOR gate out of it. In fact, in a standard reference from 1965, there are chapters on "Single-Threshold-Element Synthesis by Iteration" -- learning a single perceptron by the perceptron learning algorithm -- and "Network Synthesis" -- which does not imply machine learning, but rather hand-designing perceptron networks. <span class="co">[</span><span class="ot">@dertouzosThresholdLogicSynthesis1965</span><span class="co">]</span></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a>What is going on?</span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a>I believe the story got completely garbled during the teaching process. I am all for changing history for the sake of understanding -- history is made for the winners, not the winners made for history -- but the standard story about the XOR problem is nonsensical, as I have shown. So how did the story come about?</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a>I believe this is because *Perceptrons* contained a host of problems that their restricted form of perceptron machines could not do. The simplest one is the XOR problem. Teachers who just wanted to spend two minutes on the first neural network winter and move on, grabbed this XOR problem and pretended that it was the actual cause of it.<span class="ot">[^mantis-chariot]</span></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a><span class="ot">[^mantis-chariot]</span>:</span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a>    A mantis was crawling on the wheel of a slowly moving train. It gloated, "I am the prime mover of the train!". When the caterpillar asked it to prove so, it jumped down and waved its arms in front of the train, which promptly crushed it.</span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a><span class="in">    This is my retelling of the Taoist story of [螳臂當車](https://en.wiktionary.org/wiki/%E8%9E%B3%E8%87%82%E7%95%B6%E8%BB%8A#Chinese).</span></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a>There is one thing left to explain: what is the significance of the XOR problem to the neural network researchers back in the days? It was clearly significant for something, as when the connectionists rose in the 1980s, one of the first things they did was to check that they could solve the XOR problem. Rumelhart read the *Perceptrons* book very carefully in 1970; it inspired him to go into neural network research, entirely missing its intended message. <span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, page 273</span><span class="co">]</span> After he developed backpropagation around 1982, he immediately tried to train an MLP on the XOR problem.</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; When I first did the X-OR problem, it took a thousand iterations to solve it. If we thought that was the way it was going to go and that we were going to scale up to a hundred thousand input patterns, my God, we wouldn't live long enough to see the results. But that's not the way it's gone. That problem turned out to be an anomaly. The scaling is about linear. We haven't hit any exponential curves yet. </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000</span><span class="co">]</span></span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a>What is the significance of the XOR problem? In the context of the neural network research in the 1960s, the significance becomes clear. Nobody knew how to simultaneously adapt two or more layers well.</span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a>Before 1962, Rosenblatt had studied both theoretically and experimentally "four-layer perceptron with adaptive preterminal network", which means a perceptron network with three layers: the first layer random and fixed, and the second and third layers learned <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962, Chapter 16</span><span class="co">]</span>. However, it had not a single derivative in it. The second layer was learned by the Hebbian learning rule, and the third layer was by the perceptron learning rule.</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a>Meanwhile, during the early 1960s, Widrow and Hoff trained a single perceptron with gradient descent, then proceeded to try every trick *except* gradient descent to train a two-layered perceptron network. They gave up and parted ways. Hoff went on to co-invent the microprocessor at Intel, while Widrow applied a single perceptron to adaptive filter design, revolutionizing electrical engineering in the process. These and more of the ridiculous backstory can be read in [*The Backstory of Backpropagation*](https://yuxi-liu-wired.github.io/essays/posts/backstory-of-backpropagation/).</span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a>In short, due to a variety of unfortunate developments, people spent about twenty years (1950--1970) failing to find an effective algorithm for training the pre-final layers of neural networks. They could train the final layer either by the perceptron learning rule of Rosenblatt or by the Widrow--Hoff rule of gradient descent on the squared error, but that was the extent of the learning they could get the neural networks to do.</span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a>Consider a two-layered neural network. The second layer is easy to learn. What should happen to the first layer? Rosenblatt's solution was mainly just randomization because he mistakenly believed that the retina was randomly wired to the visual cortex, and he believed in emulating nature. Maybe Rosenblatt was working with the standard knowledge of neuroscience in his time, so he could not have known that neural connections were anything but random -- the first of the Hubel and Wiesel papers was published only in 1959. However, it seems that Rosenblatt simply had a strong attachment to randomization, as <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962</span><span class="co">]</span> cites <span class="co">[</span><span class="ot">@hubelReceptiveFieldsSingle1959</span><span class="co">]</span> several times, yet he still randomized the first layer for most experiments in the book. Rosenblatt had also experimented with Hebbian learning <span class="co">[</span><span class="ot">@rosenblattPrinciplesNeurodynamicsPerceptrons1962, Section 16.1</span><span class="co">]</span>, but since he did not use this method extensively, I infer that it did not work well.</span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a>Widrow's solution was the MADALINE I rule -- a complicated hack and a dead end. Without an effective method to train the first layer, those who worked on two-layered neural networks had only two choices: either randomize the first layer or design it by hand. Both choices played right into the hands of Minsky and Papert.</span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a>Seen from the viewpoint of the second layer, the first layer is the featurizer for the raw input. It is intuitively clear that, unless the raw input is featurized and the features are adapted to the problem, the second layer will not be able to solve the problem.</span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a>The XOR problem requires two layers. Furthermore, if the first layer is not wired correctly, the second layer will not be able to solve it either.</span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a>Put yourself in the place of a 1960s connectionist. How do you solve the XOR problem by a perceptron network? Well, not a single perceptron, as it's impossible. Not with three layers, because two layers are sufficient, and you already have enough problems with two layers. So, two layers.</span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a>How to train it? You know only how to fix the first layer and train the second. How do you fix the first layer? Do you randomize it? Unless you use many hidden perceptrons, this will fail with high probability. Do you design it by hand? But then, Minsky and Papert would interject, "You see, you cannot substitute thinking by tabula-rasa learning! You need some intelligent design to get it to work! The network needs the right representations in the hidden layer, and you cannot expect it to learn the representation from a vacuous generality like the fully connected multilayer perceptron, unless you did not get the lesson from our book. You must design it by hand.".</span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">As an example of the kind of MLP that Minsky approves: a hand-designed deep network [@fukushimaVisualFeatureExtraction1969]. Here is someone who actually tried to understand the problem instead of just tossing it to the computer and expect things to magically work.</span><span class="co">](figure/Fukushima_1969.png)</span></span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a>Not to give up, you try one of the hacks like the MADALINE I learning rule, or the Hebbian learning rule, but they are extremely fiddly and unable to learn most of the time unless you tune them just right, and it seems to require a different tuning for problems even slightly more complex than the XOR problem. Minsky and Papert interject again, "You see, there is no universal learning algorithm! You need a bespoke learning algorithm for each problem!".</span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a>And so we stood at the impasse of the 1960s. If only we had tried an activation function, any activation function, other than the dreaded 0-1 activation function...</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a><span class="al">![A summary of the XOR argument in a flowchart.](figure/XOR_problem_flowchart.png)</span></span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a><span class="fu">### Where did they go wrong?</span></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a>Brains are neural networks in hardware -- in this regard, there is no controversy since the 1900s. Intelligence is what happens in the brain. This is the occasion for small controversies from the "embodiment cognition" or "externalism" school, like those of James Gibson and Rodney Brooks, but none that has led to anything substantial yet. Therefore, most people agree that intelligence is something that neural networks do, including those people who are otherwise dismissive of neural networks like Minsky and Papert.</span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a>The abstract of a key anti-connectionist paper <span class="co">[</span><span class="ot">@fodorConnectionismCognitiveArchitecture1988</span><span class="co">]</span> makes the point that the brain is symbolic at the "cognitive level", and only beneath that level it is connectionist. Interpreted with sufficient charity, this hypothesis is unfalsifiable. None disputes that the brain is connectionist, and the operation of any hardware is symbolic if you use enough symbols to approximate the real numbers. However, at this level of charity, the hypothesis is also useless, therefore we must interpret less charitably.</span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a>What did they really mean? They concretely rejected "Parallel Distributed Processing", and claimed that trained neural networks work if and only if they implement approximations to symbolic programs, where each symbolic variable is represented locally by a small group of neurons (thus not "distributed"), and the variables are processed serially layer by layer through the network (thus not "parallel"). Further, the symbolic programs they approximate are not any kind of symbolic programs (otherwise we fall back to the trivial claim), but symbolic programs that people tend to write, things that on the small scale resemble subroutines and command line scripts, and on the large scale resemble operating systems and <span class="co">[</span><span class="ot">the Cyc project</span><span class="co">](https://en.wikipedia.org/wiki/Cyc)</span>.</span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a>At this level, it is quantifiable and thus scientifically testable. However, scientific hypotheses become political disputes when large amounts of money or social justice is on the line. We can consider an alternative history with an alternative Minsky and Papert. In this history, they put this in the epilogue:</span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Our mathematical results indicate that we need multilayer perceptrons as well as efficient methods for training them. Furthermore, simple estimates show that brain-level intelligence likely require computing power up to 10 orders of magnitude larger than currently available, suggesting the need for special hardware boards.</span></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We also need to explore alternative architectures capable of correlating global information without using all-to-all connections. Perhaps they should have a two-level structure, with a meta-network generating weights for the network, or perhaps more generic mechanisms for multiplicative interactions. Certain inherently serial operations, such as the connectivity predicate, suggest that there must be 'serial mode interfaces' allowing neural networks to call external subroutines. It is a live scientific question whether the number of external subroutines can be kept small. Perhaps a hundred or so would suffice, or perhaps it would turn out that even large neural networks are incapable of most commonsense tasks, in which case the *Society of Mind* hypothesis would be more viable. However, we consider this an empirical question that can only be answered by attempting to scale up neural networks and seeing what they might do, as *a priori* estimates of computational difficulty is close to impossible.</span></span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a>What distinguishes the two possible Minsky--Paperts? Not the facts present, but their prescientific commitments. Minsky's commitment to elegant mathematics and simple programming structures led him to insist on things for which he could prove theorems -- and to denounce empirical methods, especially if large sums of money might be "misdirected" to large-scale neural network machines. Papert, committed to epistemological pluralism, had no choice but to insist on computers that resembled his ideal society -- and to denounce any uniform computational structure as flattening, enframing, and reproducing the hegemonic ideology of universalism.</span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a>For Papert and Minsky specifically, their claim to be "pro-perceptron" is a sophistry intended to shift the narrative on the perceptron controversy, as they only approved perceptrons with a single layer of learnable parameters. In other words, they were only *pro-useless-perceptron*. They were trying to kill the project of general large-scale perceptrons, which both Frank Rosenblatt and the new connectionists in the 1980s were working towards.</span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) our intuitive judgment that the extension is sterile. Per­haps some powerful convergence theorem will be discovered, or some profound reason for the failure to produce an interesting "learning theorem" for the multilayered machine will be found. </span><span class="co">[</span><span class="ot">@minskyPerceptronsIntroductionComputational1988, page 232</span><span class="co">]</span></span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a>The irony is that decades later, despite the general neglect of neural networks, they quickly overtook symbolic or statistical AI<span class="ot">[^classical-statistical]</span> as soon as compute and data price fell so low that they had to appear. And so in 2012, Alex Krizhevsky cobbled together 2 GPUs and train a neural network that outperformed every symbolic or statistical AI.<span class="ot">[^if-not-krizhevsky]</span> There are large homogeneous neural networks that work, and there are hints that some of them have small groups of neurons representing symbolic concepts, some of which are engaged in serial computation across the layers. However, to find these hints of symbolic programs, we had to take a large detour through the brute reason of uniform neural network architecture, uniform GPU architecture, and uniform training objectives.</span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a><span class="ot">[^classical-statistical]</span>:</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a>    More precisely, classical-statistical AI, with fixed parameters, handcrafted features, and solvable models. A classical-statistical model is constructed as some form of $p_\theta(y|x)$, where $\theta$ are the parameters, $x$ are the inputs, and $y$ are the outputs.</span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a><span class="in">    The difference from neural networks is that for classical-statistical models, $p_\theta$ allows solvable inference from a dataset, such as by taking the average, derivative, variance, and such. Many of them were straight up linear regressions on handcrafted features (and thus subject to exactly the criticism of Minsky and Papert).</span></span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a><span class="in">    A good example is the [IBM alignment model 1](https://en.wikipedia.org/wiki/IBM_alignment_models#Model_1), which can be trained by expectation-maximization with *closed form solution* (!). To see the difference, compare it with [@bahdanauNeuralMachineTranslation2014], which also learns to align from a corpus, but does not have any closed form solution.</span></span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a><span class="ot">[^if-not-krizhevsky]: </span>And if not Krizhevsky and Hinton, how much longer would it have taken? In 2009, Andrew Ng's research cluster trained a 100M model on GPUs <span class="co">[</span><span class="ot">@rainaLargescaleDeepUnsupervised2009</span><span class="co">]</span>, which suggests that the idea was ripe for taking due to the advance in compute and data, and would have happened around 2010 regardless. <span class="co">[</span><span class="ot">The rain might not follow the plow</span><span class="co">](https://en.wikipedia.org/wiki/Rain_follows_the_plow)</span>, but the AI does follow the compute and data.</span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a>Why must we take such a large detour? My guess is twofold. One, the resemblance to neat symbolic programs is partial. Large amounts of computing done by neural networks is only symbolic in the trivial, messy way. Only a small amount is symbolic in the neat way. Two, because symbolic programs suffer from <span class="co">[</span><span class="ot">diseconomies of scale</span><span class="co">](https://en.wikipedia.org/wiki/Diseconomies_of_scale)</span>. Peering into any large enough software project, be it the Cyc project, or the Linux source code, one feels that it is easier to start anew than to add to it. Perhaps with thousands of years of very patient work and many evolutionary deadends, purely symbolic AI research can succeed in constructing a general intelligence in the elegant style sketched by Minsky. The irony is that symbolic programs do not scale while neural networks scale, the exact opposite of the lesson that Minsky and Papert wished to impart by their book.</span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a>As an example, the history of computer vision demonstrates the problem with the symbolic AI approach. It is true that some problems, such as the parity problem or the connectedness problem, cannot be efficiently solved by neural networks. However, do they really matter? Why do we care about them? We don't care about solving connectedness for its own sake, but because it is supposed to be a necessary step on the way towards general machine vision, of understanding real scenes. But the surprise of history is that general machine vision turned out to be far *less* about provably detecting edges and cubes and cones in a picture, and far *more* about having a large dataset. In this sense, it's Minsky and Papert who were misled by their experiments with building block-playing robots in a block world. It's their work that could not scale.</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is left of *Perceptrons*?</span></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a>I have never seen a piece of work so systematically opposed to <span class="co">[</span><span class="ot">the scaling hypothesis</span><span class="co">](https://gwern.net/scaling-hypothesis)</span>. Reading their theory, I have the feeling that at every turn, I could hear them say, "Neural networks work -- if they have less than 100 neurons.". To their credit, they made falsifiable hypotheses. To their blame, they were almost all proven wrong. Neural networks do scale, to 100 billion and counting. Several standard architectures constitute almost the entirety of neural networks nowadays -- MLP, CNN, GNN, LSTM, VAE, and Transformers. Six is quite far from the thousands of architectures they explicitly predicted.</span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a>Among all the objections to neural networks in the *Perceptrons* book, almost all were either disproved (the anti-scaling hypothesis) or made irrelevant (the perceptron learning rule).</span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a>Recognizing connectivity is hard and requires a serial program, but that's fine, because it's hard for humans too. Learning to solve logical problems is difficult and requires a thousand iterations. Well, it looks inefficient, except that neural networks are still the best we have even 30 years later, so perhaps the XOR problem is just something neural networks have to work hard for. That's fine -- in the worst case, we'll just let the neural network offload those logical operations to a symbolic program, much like how humans use calculators.</span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a>The only legitimate remaining problem is the recognition of symmetry. It is hard for all modern neural networks, including convolutional and fully connected versions.<span class="ot">[^symmetry-transformer]</span> In any case, if human brains are neural networks and they can instantly recognize symmetries, then it shows that there is *some* remaining architectural trick we don't yet know.</span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a><span class="ot">[^symmetry-transformer]: </span>It might be solved efficiently with a Transformer, but I need to check this.</span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a>Therefore, out of all the clever mathematics and wise lessons of *Perceptrons*, we ended up with... just one problem remaining? Minsky and Papert hoped to show that there would be thousands of different problems, each requiring a bespoke algorithm implemented by a bespoke neural network. In this regard, their project has been fully debunked.</span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a><span class="fu">## Appendix: The three camps of AI</span></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a>In the early days of AI, there were mainly three camps: cybernetics, symbolic systems, and neural networks. In our current age, it seems the other two camps have fallen largely into oblivion. This section gives a brief history and an orienting perspective of their key ideas.</span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cybernetic AI</span></span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a>The founding metaphor of the cybernetic camp was that intelligence is adaptive analog signal processing: homeostasis, Fourier transforms, Kalman filtering, PID control, linear predictive coding, and such.</span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a>The origin of cybernetics was entangled with the control of machinery in WWII, when you were either the quick or the dead. Norbert Wiener, the mastermind of cybernetics, worked on anti-aircraft gun (AA gun) controllers. As planes flew faster and higher than ever before, AA guns needed to "<span class="co">[</span><span class="ot">lead the target</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Deflection_(ballistics))" to a greater and greater extent. This put a severe strain on the operator of the AA guns. Wiener constructed electromechanical devices that performed linear prediction of the future trajectory of an aircraft based on its past trajectory. As the aircraft was a human-machine system, Wiener modelled both together as a synthetic whole. After the war, Wiener continued to work on cybernetics, using the analogies he had accumulated during the war.</span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; If humans do not differ from machines from the "scientific standpoint," it is because the scientific standpoint of the 1940s was one of men machines at war. The man-airplane-radar-predictor-artillery system is closed one in which it appeared possible to replace men by machines and machines by men. To an antiaircraft operator, the enemy really does act like an autocorrelated servomechanism. ... In every piece of his writing on cybernetics, from the first technical exposition of the AA predictor before Pearl Harbor up through his essays of the 1960s on science and society, Wiener put feedback in the foreground, returning again and again to the torpedo, the guided missile, and his antiaircraft director. </span><span class="co">[</span><span class="ot">@galisonOntologyEnemyNorbert1994</span><span class="co">]</span></span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a>Cybernetics entered the realm of popular consciousness with Wiener's 1948 bestseller, *Cybernetics*. In it, we find a curious description of artificial intelligence and self-reproduction from the analog signal processing point of view, detailed in [*Cybernetic artificial intelligence*](https://yuxi-liu-wired.github.io/essays/posts/cybernetic-artificial-intelligence/). The short version is that it was an analog-circuit quine:</span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; These operations of analysis, synthesis, and automatic self-adjustment of white boxes into the likeness of black boxes ... </span><span class="sc">\[</span><span class="at">by</span><span class="sc">\]</span><span class="at"> learning, by choosing appropriate inputs for the black and white boxes and comparing them; and in many of these processes, including the method of Professor Gabor, multiplication devices play an important role. While there are many approaches to the problem of multiplying two functions electrically, this task is not technically easy. On the one hand, a good multiplier must work over a large range of amplitudes. On the other hand, it must be so nearly instantaneous in its operation that it will be accurate up to high frequencies. Gabor claims for his multiplier a frequency range running to about 1,000 cycles. </span><span class="co">[</span><span class="ot">@wienerCyberneticsControlCommunication2019, page xli</span><span class="co">]</span></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a>The cybernetic approach to AI is a strange chapter in the history of AI, filled with machines too *sui generis* to have followups. It seems to me that there is no issue with building our way to AI one nonlinear filter at a time, except for technical issues, but the technical issues are insurmountable. One day I might write an essay that gives justice to the cybernetic approach, but as this essay is not about cybernetics, we will only give a whirlwind tour of highlights from the cybernetic approach to AI.</span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a>In 1948, <span class="co">[</span><span class="ot">Ross Ashby</span><span class="co">](https://en.wikipedia.org/wiki/W._Ross_Ashby)</span> built a "<span class="co">[</span><span class="ot">homeostat machine</span><span class="co">](https://en.wikipedia.org/wiki/Homeostat)</span>", consisting of four interacting electromechanical controllers. Each controller had some needles that can move in arcs. If one perturbs it, so that the needles move out of their "comfort zones", the needles would complete an electric circuit, and the controller would start going through every possible setting one by one, until the needles return to their comfort zones.<span class="ot">[^shannon-useless-machine]</span> The other thing for which Ashby is famous is the "law of requisite variety", which is equivalent to the theorem that to solve $f(x) = y$, generically, the $x$ must have at least as many dimensions as the $y$.</span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@slocumRobotExcessMachine2024, figure 2</span><span class="co">]</span>](figure/Homeostat_diagram_1.png)</span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@slocumRobotExcessMachine2024, figure 3</span><span class="co">]</span>](figure/Homeostat_diagram_2.png)</span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a><span class="ot">[^shannon-useless-machine]: </span>Perhaps <span class="co">[</span><span class="ot">Marvin Minsky's useless machine</span><span class="co">](https://en.wikipedia.org/wiki/Useless_machine)</span> was a gentle parody of the homeostat machine. As the homeostat was built in 1948 and the useless machine in 1952, the timing checks out.</span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Stafford Beer</span><span class="co">](https://en.wikipedia.org/wiki/Stafford_Beer)</span> started his professional life as a business consultant, and devised methods to increase steel plant production efficiency. He understood production management as a problem analogous to that of biological homeostasis -- a problem solved by the nervous system -- and so he managed production by imitating the nervous system<span class="ot">[^matrix-brain-of-the-firm]</span>. He also investigated a wide variety of strange machines, including one that used an entire pond ecosystem as a computer for black-box homeostatic control <span class="co">[</span><span class="ot">@beerProgressNoteResearch1962</span><span class="co">]</span>:</span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Why not use an entire ecological system, such as a pond? The theory on which all this thinking is based does not require a knowledge of the black box employed. Accordingly, over the past year, I have been conducting experiments with a large tank or pond. The contents of the tank were randomly sampled from ponds in Derbyshire and Surrey. Currently there are a few of the usual creatures visible to the naked eye (Hydra, Cyclops, Daphnia, and a leech); microscopically there is the expected multitude of micro-organisms. In this tank are suspended four lights, the intensities of which can be varied to fine limits. At other points are suspended photocells with amplifying circuits which give them very high sensitivity. ... There is an ecological guarantee that this system stabilizes itself, and is indeed ultra-stable. But, of course, the problem of making it act as a control system is very difficult indeed. I regard the machine as a tending-to-be-stable-but-not-quite homeostat ...</span></span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a><span class="ot">[^matrix-brain-of-the-firm]</span>:</span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a>    Stafford Beer might have meant this literally, according to <span class="co">[</span><span class="ot">@pickeringScienceUnknowableStafford2004</span><span class="co">]</span>:</span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; ... it is clear from subsequent developments that the homeostatic system Beer really had in mind was something like the human spinal cord and brain. He never mentioned this in his work on biological computers, but the image that sticks in my mind is that the brain of the cybernetic factory should really have been an unconscious human body, floating in a vat of nutrients and with electronic readouts tapping its higher and lower reflexes ...</span></span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a>In 1971, he was invited to become the principal architect of <span class="co">[</span><span class="ot">Project Cybersyn</span><span class="co">](https://en.wikipedia.org/wiki/Project_Cybersyn)</span>, which was a nervous system for the Chilean socialist economy, employing "algedonic control" ("algedonic" is Greek for "pain-pleasure"). This project, like president Allende, was shot in the head by the <span class="co">[</span><span class="ot">1973 coup</span><span class="co">](https://en.wikipedia.org/wiki/1973_Chilean_coup_d'%C3%A9tat)</span> that established a free market economy in Chile. After this, he lived as a hermit, though he still published a few books and did occasional business consulting.<span class="co">[</span><span class="ot">@morozovPlanningMachineProject2014</span><span class="co">]</span></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a>In the 1950s, <span class="co">[</span><span class="ot">Gordon Pask</span><span class="co">](https://en.wikipedia.org/wiki/Gordon_Pask)</span> constructed electrochemical "sensory organs". He prepared a dish of acidic metal salt solution (such as $\text{FeSO}_4$) and then immersed electrodes into it. Metal tends to dissolve in the acidic solution, but applying a voltage through the electrodes pulls metal out of the solution by electrolysis. This allows him to "reward" whatever metallic structure in the dish by applying a voltage. He reported success at growing some simple sensory organs in the dish <span class="co">[</span><span class="ot">@gordonNaturalHistoryNetworks1959; @carianiEvolveEarEpistemological1993</span><span class="co">]</span>:</span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We have made an ear and we have made a magnetic receptor. The ear can discriminate two frequencies, one of the order of fifty cycles per second and the other on the order of one hundred cycles per second. The 'training' procedure takes approximately half a day and once having got the ability to recognize sound at all, the ability to recognize and discriminate two sounds comes more rapidly. ... The ear, incidentally, looks rather like an ear. It is a gap in the thread structure in which you have fibrils which resonate at the excitation frequency. </span><span class="co">[</span><span class="ot">@gordonNaturalHistoryNetworks1959</span><span class="co">]</span></span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a>The details of the electrochemical ear are lost, and this line of research had no followups.</span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a>A faint echo of Pask's electrochemical ear was heard in late 1990s, when <span class="co">[</span><span class="ot">Adrian Thompson used evolutionary algorithm</span><span class="co">](https://web.archive.org/web/20231212170749/https://www.damninteresting.com/on-the-origin-of-circuits/)</span> to evolve circuits on <span class="co">[</span><span class="ot">field-programmable gate arrays</span><span class="co">](https://en.wikipedia.org/wiki/Field-programmable_gate_array)</span> to tell apart input signals of frequencies $1 \mathrm{~kHz}$ and $10 \mathrm{~kHz}$. Curiously, some circuits succeeded at the task despite using no master clock signal. He traced this down to the detailed physical properties that digital circuit design was precisely meant to abstract away from. The circuit functioned precisely *because* the electronic elements were not digital, but analog.<span class="ot">[^thompson-analog-signal]</span> The circuits' performance degraded when outside the temperature range in which they evolved in <span class="co">[</span><span class="ot">@thompsonAnalysisUnconventionalEvolved1999; @thompsonExplorationsDesignSpace1999</span><span class="co">]</span>.</span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a><span class="ot">[^thompson-analog-signal]: </span>It is as if a linear neural network managed to compute a nonlinear function precisely because floating point operations are not perfect.<span class="co">[</span><span class="ot">@foersterNonlinearComputationDeep2017</span><span class="co">]</span></span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... at $43.0^{\circ} \mathrm{C}$ the output is not steady at $+5 \mathrm{~V}$ for $\mathrm{F} 1$, but is pulsing to $0 \mathrm{~V}$ for a small fraction of the time. Conversely, at $23.5^{\circ} \mathrm{C}$ the output is not a steady $0 \mathrm{~V}$ for $\mathrm{F} 2$, but is pulsing to $+5 \mathrm{~V}$ for a small fraction of the time. This is not surprising: the only time reference that the system has is the natural dynamical behaviour of the components, and properties such as resistance, capacitance and propagation delays are temperature dependent. The circuit operates perfectly over the $10^{\circ} \mathrm{C}$ range of temperatures that the population was exposed to during evolution, and no more could reasonably be expected of it. </span><span class="co">[</span><span class="ot">@thompsonEvolvedCircuitIntrinsic1996</span><span class="co">]</span></span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a>Continuing the tradition of one-hit wonders, there was no followup work to this.<span class="ot">[^adrian-thompson]</span></span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a><span class="ot">[^adrian-thompson]: </span>I have tried tracking down what Adrian Thompson has been doing since his late 1990s work. It turned out that the hardware evolution <span class="co">[</span><span class="ot">was his PhD work</span><span class="co">](https://web.archive.org/web/20201015175808/https://sites.google.com/site/thompsonevolvablehardware/publications)</span> <span class="co">[</span><span class="ot">@thompsonHardwareEvolutionAutomatic1998</span><span class="co">]</span>. He has almost completely dropped off the face of academia. His website at University of Sussex <span class="co">[</span><span class="ot">did not see another update since 2002</span><span class="co">](https://web.archive.org/web/20030402181211/http://www.cogs.susx.ac.uk/users/adrianth/)</span> and is currently dead. His <span class="co">[</span><span class="ot">minimalistic Google Site</span><span class="co">](https://web.archive.org/web/20201015175808/https://sites.google.com/site/thompsonevolvablehardware/publications)</span> was created around 2014, and currently only survives on the Internet Archive. There was also a single <span class="in">`gif`</span> of the circuit in operation, which I decided to download and <span class="co">[</span><span class="ot">save for posterity</span><span class="co">](./figure/gen1400_150x113_lossy25.gif)</span>.</span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a><span class="fu">### Symbolic AI</span></span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a>The founding metaphor of the symbolic system camp was that intelligence is symbolic manipulation using preprogrammed symbolic rules: logical inference, heuristic tree search, list processing, syntactic trees, and such. The symbolic camp was not strongly centered, though it had key players like Alan Turing, John McCarthy, Herbert Simon, and Marvin Minsky.</span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a>The project of symbolic AI is a curious episode in AI history. Despite its early successes such as SHRDLU, LISP, and ELIZA, despite the support of most of AI researchers during 1960--2000, and despite the support of reigning cognitivists in the adjacent field of psychology, it never achieved something as successful as neural networks.</span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a>A brief sketch of the greatest project in symbolic AI might give you a feel for the difficulty involved. Have you ever heard of the Cyc doing anything useful in its 39 years of life? Compare that with something even as small as BERT.</span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>In 1984, <span class="co">[</span><span class="ot">Douglas Lenat</span><span class="co">](https://en.wikipedia.org/wiki/Douglas_Lenat)</span> began the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. The Cyc project used a LISP-like symbolic logic language to encode common sense. Even from the vantage point of 1985, it was clear to all that there was a lot of common sense to code in <span class="ot">[^cyc-midterm-report]</span>, although few could have predicted that Lenat would persevere at it for over 30 years. In 2016, Lenat finally declared the Cyc project "done" and set about commercializing it.</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Having spent the past 31 years memorizing an astonishing collection of general knowledge, the artificial-intelligence engine created by Doug Lenat is finally ready to go to work. Lenat's creation is Cyc, a knowledge base of semantic information designed to give computers some understanding of how things work in the real world. ... "Part of the reason is the doneness of Cyc," explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. "Not that there's nothing else to do," he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, the company is developing a personal assistant equipped with Cyc's general knowledge. This could perhaps lead to something similar to Siri but less predisposed to foolish misunderstandings. Michael Stewart, a longtime collaborator of Lenat's and the CEO of Lucid, says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.  </span><span class="co">[</span><span class="ot">@knightAI30Years2016</span><span class="co">]</span></span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a><span class="ot">[^cyc-midterm-report]: </span>They themselves probably underestimated the difficulty. In 1990, they confidently titled a paper "Cyc: A midterm report" <span class="co">[</span><span class="ot">@lenatCycMidtermReport1990</span><span class="co">]</span>, suggesting that they expected to be done around 1995.</span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a>That was essentially the last we heard from Cyc.</span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a>Why has the symbolic AI project failed to scale? It is hard to say, and a definitive answer would not be forthcoming until we have a human level AI as an existence proof of the necessary requirements for scalable AI. However, I can speculate. When looking at this figure from 1985, one is simultaneously filled with respect and sadness, for they were facing impossible odds, and yet they charged right into it.</span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@lenatCycUsingCommon1985, Figure 1</span><span class="co">]</span>](figure/cyc_project_ontology.png)</span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a>Their "midterm report" only accentuates this sense of tragedy, of seeing them fighting impossible odds, and losing. They saw with clarity that there is no shortcut to intelligence, no "Maxwell's equations of thought".</span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The majority of work in knowledge representation has dealt with the technicalities of relating predicate calculus to other formalisms and with the details of various schemes for default reasoning. There has almost been an aversion to addressing the problems that arise in actually representing large bodies of knowledge with content. However, deep, important issues must be addressed if we are to ever have a large intelligent knowledge-based program: What ontological categories would make up an adequate set for carving up the universe? How are they related? What are the important facts and heuristics most humans today know about solid objects? And so on. In short, we must bite the bullet.</span></span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We don't believe there is any shortcut to being intelligent, any yet-to-be-discovered Maxwell's equations of thought, any AI Risc architecture that will yield vast amounts of problem-solving power. Although issues such as architecture are important, no powerful formalism can obviate the need for a lot of knowledge.</span></span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By knowledge, we don't just mean dry, almanac-like or highly domain-specific facts. Rather, most of what we need to know to get by in the real world is prescientific (knowledge that is too commonsensical to be included in reference books; for example, animals live for a single solid interval of time, nothing can be in two places at once, animals don't like pain), dynamic (scripts and rules of thumb for solving problems) and metaknowledge (how to fill in gaps in the knowledge base, how to keep it organized, how to monitor and switch among problem-solving methods, and so on). Perhaps the hardest truth to face, one that AI has been trying to wriggle out of for 34 years, is that there is probably no elegant, effortless way to obtain this immense knowledge base. Rather, the bulk of the effort must (at least initially) be manual entry of assertion after assertion. </span><span class="co">[</span><span class="ot">@lenatCycMidtermReport1990</span><span class="co">]</span></span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a>I was struck by the same sense of ontological vertigo when looking back at Simon and Newell's *Human Problem Solving*, a compendium of their work on decomposing human problem solving into symbolic processes:</span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@newellHumanProblemSolving1972, page 533</span><span class="co">]</span>](figure/human_problem_solving_behavior_graph.png)</span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@newellHumanProblemSolving1972, page 534</span><span class="co">]</span>](figure/human_problem_solving_transcript.png)</span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a>This sense of vertigo is perhaps best described by Borges in *The analytical language of John Wilkins* <span class="co">[</span><span class="ot">@borgesSelectedNonfictions2000, pages 229--232</span><span class="co">]</span>:</span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller's earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.</span></span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; These ambiguities, redundancies, and deficiencies recall those attributed by Dr. Franz Kuhn to a certain Chinese encyclopedia called the Heavenly Emporium of Benevolent Knowledge. In its distant pages it is written that animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained; (d) suckling pigs; (e) mermaids; (f) fabulous ones; (g) stray dogs; (h) those that are included in this classification; (i) those that tremble as if they were mad; (j) in­ numerable ones; (k) those drawn with a very fine camel's-hair brush; (1) etcetera; (m) those that have just broken the flower vase; (n) those that at a distance resemble flies. The Bibliographical Institute of Brussels also exercises chaos: it has parceled the universe into 1,000 subdivisions, of which number 262 corresponds to the Pope, number 282 to the Roman Catholic Church, number 263 to the Lord's Day, number 268 to Sunday schools, number 298 to Mormonism, and number 294 to Brahmanism, Buddhism, Shintoism, and Taoism. Nor does it disdain the employment of heterogeneous subdivisions, for example, number 179: "Cruelty to animals. Protection of animals. Dueling and suicide from a moral point of view. Various vices and defects. Various virtues and qualities."</span></span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a><span class="fu">## Appendix: the Chomskyans</span></span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a>In the field of psychology, the neural network vs Minsky divide corresponds quite well with the behaviorism vs cognitivism divide.</span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a>Among the cognitivists, there is no consensus view about the exact role that neural networks must play. Some, like Fodor and Marr, argue that intelligence is just a hardware-independent program, and brains just happen to use neural networks to run the program due to accidents of evolution and biophysics. Others, like Gary Marcus, grant neural networks a more substantial role in constraining the possible programs -- that is, each neural network architecture is designed , and each architecture can only run a certain kind of program efficiently -- but they still insist that neural networks must have very particular architectures.</span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a>Some might allow simple learning algorithms with simple environmental stimulus (the universal grammar approach of Chomsky), while others might allow complex learning algorithms with complex environmental stimulus (the society of mind approach of Minsky), but what unifies the cognitivists is that they are against simple learning algorithms applied to complex environmental stimulus. They called this enemy by many names, such as "radical behaviorism", "Skinnerism", "perceptrons", "radical connectionism", and now "deep learning".</span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a><span class="fu">### Noam Chomsky</span></span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a>The cognitivist revolution was led by Noam Chomsky against behaviorism during the 1950s, ending with the victory of cognitivism in "higher psychology", such as linguistics, though behaviorism survived respectably to this day in other aspects of psychology, such as addiction studies and animal behavior.</span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a>In a curious parallel, just as neural networks for AI became popular again in the late 1980s, statistical methods for NLP returned during the same period. The watershed moment was the series of <span class="co">[</span><span class="ot">IBM alignment models</span><span class="co">](https://en.wikipedia.org/wiki/IBM_alignment_models)</span> published in 1993 <span class="co">[</span><span class="ot">@brownMathematicsStatisticalMachine1993</span><span class="co">]</span>.</span>
<span id="cb2-703"><a href="#cb2-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-704"><a href="#cb2-704" aria-hidden="true" tabindex="-1"></a>In the 1950s, language production was modelled by theoretical machines: finite state automata, stack machines, Markov transition models, and variants thereof. We must understand Chomsky's two contributions to linguistics. On the first part, he constructed a <span class="co">[</span><span class="ot">hierarchy</span><span class="co">](https://en.wikipedia.org/wiki/Chomsky_hierarchy)</span> of increasingly powerful kinds of language. This hierarchy was in one-to-one correspondence with the hierarchy of theoretical machines, from finite state automata (type-3, or regular) all the way to Turing machines (type-0, or recursively enumerable). On the second part, he rejected statistical language learning and argued for inborn universal grammar.</span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a>Chomsky argued, and subsequent linguists have found, that the syntax of all human languages is at the type-2 level, or a <span class="co">[</span><span class="ot">context-free grammar</span><span class="co">](https://en.wikipedia.org/wiki/Context-free_grammar)</span>. None are regular and almost none are context-dependent. Regular languages are modeled by finite state machines and cannot model arbitrarily deep recursion, whereas context-free languages allow for arbitrarily deep recursion such as <span class="co">[</span><span class="ot">center embedding</span><span class="co">](https://en.wikipedia.org/wiki/Center_embedding)</span>. This fact would come into play later.</span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a>With the dominance of the Chomsky approach, finite state machines were abandoned, as they are not capable of parsing context-free grammar. You might have heard of the controversy over Pirahã. It is mainly fought over the problem of recursion: does the Pirahã language have recursion or not?<span class="ot">[^pinker-piraha]</span></span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a><span class="ot">[^pinker-piraha]</span>:</span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a>    Tracing the battle lines, I predicted that Pinker would argue that it must have recursion... and I turned out to be wrong. Pinker argued against Chomsky in this case.</span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; "There's a lot of strange stuff going on in the Chomskyan program. He's a guru, he makes pronouncements that his disciples accept on faith and that he doesn't feel compelled to defend in the conventional scientific manner. Some of them become accepted within his circle as God's truth without really being properly evaluated, and, surprisingly for someone who talks about universal grammar, he hasn't actually done the spadework of seeing how it works in some weird little language that they speak in New Guinea."</span></span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt;</span></span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Pinker says that his own doubts about the "Chomskyan program" increased in 2002, when Marc Hauser, Chomsky, and Tecumseh Fitch published their paper on recursion in Science. The authors wrote that the distinctive feature of the human faculty of language, narrowly defined, is recursion. Dogs, starlings, whales, porpoises, and chimpanzees all use vocally generated sounds to communicate with other members of their species, but none do so recursively, and thus none can produce complex utterances of infinitely varied meaning. "Recursion had always been an important part of Chomsky's theory," Pinker said. "But in Chomsky Mark II, or Mark III, or Mark VII, he all of a sudden said that the only thing unique to language is recursion. It's not just that it's the universal that has to be there; it's the magic ingredient that makes language possible." [@colapintoInterpreter2007]</span></span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a>A key principle Chomsky used was the "poverty of stimulus" argument, which he used to argue that humans must have a universal grammar built in at birth, because children cannot possibly learn to speak when they are just a few years old -- they cannot possibly have heard and seen enough. For one thing, true recursion can never be learned empirically, because true recursion can only be conclusively proven by observing an infinite number of sentences.</span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a>Consider the simple example of the <span class="co">[</span><span class="ot">balanced brackets language</span><span class="co">](https://en.wikipedia.org/wiki/Dyck_language)</span>. A language learner observes sample sentences from the language and tries to infer the language. Suppose the learner sees a sequence <span class="in">`(), (()), ((())), (((())))`</span>. What can they conclude? That it is the balanced brackets language? So we ask them to construct another sentence, and they confidently write <span class="in">`((((()))))`</span>, but then we inform them that they have been tricked! The language is the balanced brackets language -- except that the brackets only go 4 levels deep. In general, only by seeing all levels of recursion can the balanced brackets language be *conclusively* learned.</span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a>Applied to linguistics, Chomsky claimed that statistical learning cannot learn syntax, and all attempts have been "colossal failures".</span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Just to illustrate, I'll take one example that was presented back in the 1950s and has become a sort of a classic case because it's so trivial. 'Can eagles that fly swim?' Okay, simple sentence. Everyone understands it. Any young child understands it. There is a question about it. We know that we associate the word 'can' with 'swim,' not with 'fly.' We're asking 'Can they swim?' We're not asking 'Can they fly?' Well, why is that? A natural answer ought to be that you associate 'can' with 'fly.' After all, 'fly' is the word that's closest to 'can,' so why don't you just take the closest word and interpret it that way? ... Well, that property is universal. It holds up in every language. Languages may do it differently, but they're going to have the same property. It holds in every construction anyone knows, and it's just a universal property of language.</span></span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Well, this particular example has taken on a life of its own. For one thing, it's a poverty of stimulus problem, like Descartes' triangle. There's been a huge effort to show that it's not a problem, that if you just do a complex statistical analysis of complex data, you'll find that that's what the child will determine from the data. The approaches are odd in several respects. First, every one is not only a failure but a colossal failure. I'm not going to talk about that. I actually have a recent paper about it with a computer scientist at MIT, Bob Berwick, where we run through a lot of the current proposals but it's easy to show that they're all just wildly wrong. But they keep coming. Almost every issue of the main cognitive science journals has another proposal of this sort, so that's one odd fact. There are many efforts to show that there is nothing puzzling about it, they're all colossal failures. </span><span class="co">[</span><span class="ot">@chomskyPovertyStimulusUnfinished2010</span><span class="co">]</span></span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a>At last, a testable hypothesis! I asked GPT-4 to <span class="in">`Draw a syntax tree for "Can eagles that fly swim?"`</span>, and received this:<span class="ot">[^gpt-4-eagle-sentence]</span></span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a><span class="ot">[^gpt-4-eagle-sentence]: </span>The code it gave failed on the first try, due to an environment variable issue on the Linux virtual machine it ran on. We fixed it after two more plies of conversation.</span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Code" collapse="true"}</span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> Tree</span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>)</span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> svgling</span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> SVG, display</span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> Tree.fromstring(<span class="st">"""</span></span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a><span class="st">    (SQ</span></span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a><span class="st">        (MD Can)</span></span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a><span class="st">        (NP</span></span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a><span class="st">            (NNS eagles)</span></span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a><span class="st">            (SBAR</span></span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a><span class="st">                (WHNP (WDT that))</span></span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a><span class="st">                (S</span></span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a><span class="st">                    (VP (VBP fly))</span></span>
<span id="cb2-749"><a href="#cb2-749" aria-hidden="true" tabindex="-1"></a><span class="st">                )</span></span>
<span id="cb2-750"><a href="#cb2-750" aria-hidden="true" tabindex="-1"></a><span class="st">            )</span></span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a><span class="st">        )</span></span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a><span class="st">        (VP (VB swim))</span></span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a><span class="st">    )</span></span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>)</span>
<span id="cb2-755"><a href="#cb2-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-756"><a href="#cb2-756" aria-hidden="true" tabindex="-1"></a>tree_svg <span class="op">=</span> svgling.draw_tree(tree)</span>
<span id="cb2-757"><a href="#cb2-757" aria-hidden="true" tabindex="-1"></a>tree_svg.get_svg().save()</span>
<span id="cb2-758"><a href="#cb2-758" aria-hidden="true" tabindex="-1"></a>display(SVG(tree_svg.get_svg()))</span>
<span id="cb2-759"><a href="#cb2-759" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-760"><a href="#cb2-760" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-761"><a href="#cb2-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-762"><a href="#cb2-762" aria-hidden="true" tabindex="-1"></a><span class="al">![The syntax tree for "Can eagles that fly swim?", drawn by GPT-4.](figure/can_eagles_that_fly_swim_tree.svg)</span></span>
<span id="cb2-763"><a href="#cb2-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-764"><a href="#cb2-764" aria-hidden="true" tabindex="-1"></a>Chomsky has consistently rejected statistical language learning from the beginning to the end.</span>
<span id="cb2-765"><a href="#cb2-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-766"><a href="#cb2-766" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; But it must be recognized that the notion of "probability of a sentence" is an entirely useless one, under any known interpretation of this term.</span>  </span>
<span id="cb2-767"><a href="#cb2-767" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-768"><a href="#cb2-768" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@chomskyEmpiricalAssumptionsModern1969</span><span class="co">]</span>  </span>
<span id="cb2-769"><a href="#cb2-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-770"><a href="#cb2-770" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data. </span></span>
<span id="cb2-771"><a href="#cb2-771" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-772"><a href="#cb2-772" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Chomsky at the MIT150: Brains, Minds and Machines Symposium (2011), quoted in </span><span class="co">[</span><span class="ot">@norvigChomskyTwoCultures2017</span><span class="co">]</span></span>
<span id="cb2-773"><a href="#cb2-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-774"><a href="#cb2-774" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Well, first we should ask the question whether large language models have achieved anything, anything, in this domain? Answer: No! They've achieved zero... GPT-4 is coming along, which is supposed to going to have a trillion parameters. It will be exactly the same. It'll use even more energy and achieve exactly nothing, for the same reasons. So there's nothing to discuss.</span></span>
<span id="cb2-775"><a href="#cb2-775" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-776"><a href="#cb2-776" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">Machine Learning Street Talk: The Ghost in the Machine and the Limits of Human Understanding (2022)</span><span class="co">](https://whimsical.com/question-1-large-language-models-such-as-gpt-3-DJmQ8R5kD8tqvsXxgCN9vK)</span>  </span>
<span id="cb2-777"><a href="#cb2-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-778"><a href="#cb2-778" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Peter Norvig</span><span class="co">](https://en.wikipedia.org/wiki/Peter_Norvig)</span> gave a detailed analysis and a rebuttal in <span class="co">[</span><span class="ot">@norvigChomskyTwoCultures2017</span><span class="co">]</span>.</span>
<span id="cb2-779"><a href="#cb2-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-780"><a href="#cb2-780" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Chomskyans</span></span>
<span id="cb2-781"><a href="#cb2-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-782"><a href="#cb2-782" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Gold's theorem about language learning in the limit</span><span class="co">](https://en.wikipedia.org/wiki/Language_identification_in_the_limit#Gold's_theorem)</span> is occasionally cited in the same context as a justification for the "poverty of stimulus" argument. It appears Chomsky did not regard it as a relevant argument <span class="co">[</span><span class="ot">@johnsonGoldTheoremCognitive2004</span><span class="co">]</span>, and I agree with Chomsky in this respect, as Gold's theorem is extremely generic.</span>
<span id="cb2-783"><a href="#cb2-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-784"><a href="#cb2-784" aria-hidden="true" tabindex="-1"></a>After the second rise of neural networks, there was a bitter controversy that raged in the 1990s but is now essentially forgotten: the past tense debate. On one side were the connectionists, and on the other were the cognitivists, including Steven Pinker and Gary Marcus <span class="co">[</span><span class="ot">@pinkerFutureTense2002</span><span class="co">]</span>. Tellingly, both Steven Pinker and Gary Marcus sided with the cognitivists. Steven Pinker is best known for his other books such as *The Blank Slate*, which applies Chomskyan linguistics to general psychology.</span>
<span id="cb2-785"><a href="#cb2-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-786"><a href="#cb2-786" aria-hidden="true" tabindex="-1"></a>Human language exhibits a distinctly fractal-like structure: for every rule, there are exceptions, and for every exception, there are exceptional exceptions, and so on. This is called "quasi-regularity". Sejnowski in an interview described how quasi-regularity shows up in phonology, and offered a perfect demonstration project for the power of neural networks against the Chomskyans:</span>
<span id="cb2-787"><a href="#cb2-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-788"><a href="#cb2-788" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After I went to the library and found a book on text-to-speech that had probably one thousand rules in it. It was just lists of rules and exceptions. After every rule, there were dozens of exceptions. I figured that either they're doing it the wrong way, or this is a really tough problem. We considered finding something that was simpler after consulting the experts in linguistics. They said, "Chomsky worked on the problem, and it is well beyond the capability of anything that you could imagine trying to do." </span><span class="co">[</span><span class="ot">@rosenfeldTalkingNetsOral2000, pages 324-325</span><span class="co">]</span></span>
<span id="cb2-789"><a href="#cb2-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-790"><a href="#cb2-790" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@sejnowskiDeepLearningRevolution2018, pages 75--78</span><span class="co">]</span> recounts an anecdote about <span class="co">[</span><span class="ot">Jerry Fodor</span><span class="co">](https://en.wikipedia.org/wiki/Jerry_Fodor)</span>, another prominent cognitivist. While Fodor is no longer a hot name in AI nowadays, this anecdote illustrates the Chomsky way of thinking.</span>
<span id="cb2-791"><a href="#cb2-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-792"><a href="#cb2-792" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In 1988, I served on a committee for the McDonnell and Pew Foundations that interviewed prominent cognitive scientists and neuroscientists to get their recommendations on how to jump-start a new field called "cognitive neuroscience". ... </span><span class="sc">\[</span><span class="at">Fodor</span><span class="sc">\]</span><span class="at"> started by throwing down the gauntlet, "Cognitive neuroscience is not a science and it never will be." ... Fodor explained why the mind had to be thought of as a modular symbol-processing system running an intelligent computer program. </span><span class="sc">\[</span><span class="at">Patricia Churchland</span><span class="sc">\]</span><span class="at"> asked him whether his theory also applied to cats. "Yes," said Fodor, "cats are running the cat program." But when Mortimer Mishkin, an NIH neuroscientist studying vision and memory, asked him to tell us about discoveries made in his own lab, Fodor mumbled something I couldn't follow about using event-related potentials in a language experiment. Mercifully, at that moment, a fire drill was called and we all filed outside. Standing in the courtyard, I overheard Mishkin say to Fodor: "Those are pretty small potatoes." When the drill was over, Fodor had disappeared.</span></span>
<span id="cb2-793"><a href="#cb2-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-794"><a href="#cb2-794" aria-hidden="true" tabindex="-1"></a>Minsky was himself dismissive of the Chomskyans:</span>
<span id="cb2-795"><a href="#cb2-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-796"><a href="#cb2-796" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... fairly soon ideas that have been brewing in AI since the 1960's, on making computers understand significant fragments of natural language, will enter and, I'm sure, soon dominate the main stream of Linguistics. (In the era of these memos, it was the students in AI, almost alone, who carried on the quest for meaningful theories of linguistic processes, when most all other language work was stuck in shallow, syntax-oriented, formalisms.)</span></span>
<span id="cb2-797"><a href="#cb2-797" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-798"><a href="#cb2-798" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@minskyIntroductionCOMTEXMicrofiche1983</span><span class="co">]</span></span>
<span id="cb2-799"><a href="#cb2-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-800"><a href="#cb2-800" aria-hidden="true" tabindex="-1"></a>Similarly, Gary Marcus has consistently criticized neural network language models since at least 1992 <span class="co">[</span><span class="ot">@marcusOverregularizationLanguageAcquisition1992</span><span class="co">]</span>. His theory of intelligence is fundamentally Chomskyan: neural networks can exhibit intelligence but only if they implement rules for symbolic manipulation.<span class="ot">[^gary-marcus-algebraic-mind]</span> Moreover, many symbolic rules must be present at birth, by the poverty of stimulus.</span>
<span id="cb2-801"><a href="#cb2-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-802"><a href="#cb2-802" aria-hidden="true" tabindex="-1"></a>For example, here is him saying in 1993:</span>
<span id="cb2-803"><a href="#cb2-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-804"><a href="#cb2-804" aria-hidden="true" tabindex="-1"></a><span class="ot">[^gary-marcus-algebraic-mind]: </span>See <span class="co">[</span><span class="ot">@marcusAlgebraicMindIntegrating2003</span><span class="co">]</span> for a book-length treatment.</span>
<span id="cb2-805"><a href="#cb2-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-806"><a href="#cb2-806" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Whether children require "negative evidence" (i.e., information about which strings of words are not grammatical sentences) to eliminate their ungrammatical utterances is a central question in language acquisition because, lacking negative evidence, a child would require internal mechanisms to unlearn grammatical errors. ... There is no evidence that noisy feedback is required for language learning, or even that noisy feedback exists. Thus internal mechanisms are necessary to account for the unlearning of ungrammatical utterances. </span></span>
<span id="cb2-807"><a href="#cb2-807" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-808"><a href="#cb2-808" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@marcusNegativeEvidenceLanguage1993</span><span class="co">]</span></span>
<span id="cb2-809"><a href="#cb2-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-810"><a href="#cb2-810" aria-hidden="true" tabindex="-1"></a>And here is him in 2012, quoting the old past tense debate to urge caution against the hype of AlexNet:</span>
<span id="cb2-811"><a href="#cb2-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-812"><a href="#cb2-812" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; They learned slowly and inefficiently, and as Steven Pinker and I showed, couldn't master even some of the basic things that children do, like learning the past tense of regular verbs. By the late nineteen-nineties, neural networks had again begun to fall out of favor. ... Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships, and are likely to face challenges in acquiring abstract ideas like "sibling" or "identical to". They have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. ... Norvig is clearly very interested in seeing what Hinton could come up with. But even Norvig didn't see how you could build a machine that could understand stories using deep learning alone.</span></span>
<span id="cb2-813"><a href="#cb2-813" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb2-814"><a href="#cb2-814" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@marcusDeepLearningRevolution2012</span><span class="co">]</span></span>
<span id="cb2-815"><a href="#cb2-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-816"><a href="#cb2-816" aria-hidden="true" tabindex="-1"></a>And here is him writing in 2018 dismissing deep learning as not working on the problem of general intelligence, just working on applications:</span>
<span id="cb2-817"><a href="#cb2-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-818"><a href="#cb2-818" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Once upon a time, before the fashionable rise of machine learning and "big data", A.I. researchers tried to understand how complex knowledge could be encoded and processed in computers. This project, known as knowledge engineering... A.I. researchers need to return to that project sooner rather than later, ideally enlisting the help of cognitive psychologists who study the question of how human cognition manages to be endlessly flexible. Today's dominant approach to A.I. has not worked out. Yes, some remarkable applications have been built from it, including Google Translate and Google Duplex. But the limitations of these applications as a form of intelligence should be a wake-up call.</span></span>
<span id="cb2-819"><a href="#cb2-819" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb2-820"><a href="#cb2-820" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@marcusOpinionAIHarder2018</span><span class="co">]</span></span>
<span id="cb2-821"><a href="#cb2-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-822"><a href="#cb2-822" aria-hidden="true" tabindex="-1"></a>As it happens, 2018 was the year of Transformer revolution in natural language modeling, with BERT and GPT-1. Never one to give up, he continued with the same criticisms as easily as substituting "Transformers" for "recurrent networks". Given the track record, he is conveniently predictable,<span class="ot">[^llm-simulation]</span> and we can expect nothing less than his recent criticisms of deep learning in general <span class="co">[</span><span class="ot">@marcusDeepLearningCritical2018</span><span class="co">]</span> and <span class="co">[</span><span class="ot">large language models in particular</span><span class="co">](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)</span>, <span class="co">[</span><span class="ot">repeatedly</span><span class="co">](https://www.theatlantic.com/technology/archive/2023/03/ai-chatbots-large-language-model-misinformation/673376/)</span>.</span>
<span id="cb2-823"><a href="#cb2-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-824"><a href="#cb2-824" aria-hidden="true" tabindex="-1"></a><span class="ot">[^llm-simulation]</span>:</span>
<span id="cb2-825"><a href="#cb2-825" aria-hidden="true" tabindex="-1"></a>    It would be funny if someone could train a language model to pass the "Gary Marcus test": impersonate Gary Marcus in a Turing test setup. If such a model were to pass, Marcus would either have to admit that the language model makes sense or accept that what he says is indistinguishable from nonsense.</span>
<span id="cb2-826"><a href="#cb2-826" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-827"><a href="#cb2-827" aria-hidden="true" tabindex="-1"></a><span class="in">    The same could work for Noam Chomsky -- a statistical language model who can editorialize against every one of GPT-5, Gemini 2, GPT-6, etc, as soon as they come out. A [performative contradiction](https://en.wikipedia.org/wiki/Performative_contradiction).</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>