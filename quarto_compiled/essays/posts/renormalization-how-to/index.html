<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2024-04-11">
<meta name="description" content="Survival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics.">

<title>How to do Renormalization – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="How to do Renormalization – Yuxi on the Wired">
<meta property="og:description" content="Survival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/renormalization-how-to/figure/banner/banner_2.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="768">
<meta property="og:image:width" content="1232">
<meta property="og:image:alt" content="A metaphor for real-space renormalization, where the building on the left is coarse-grained to the building on right. The facade of a wide brutalist architecture. On the left, the windows are small and the floors are close together. On the right, the windows are large and the floors are wide apart. High contrast, monochromatic, minimalistic, in the flat style of vector svg art., illustration, conceptual art.">
<meta name="twitter:title" content="How to do Renormalization – Yuxi on the Wired">
<meta name="twitter:description" content="Survival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/renormalization-how-to/figure/banner/banner_2.png">
<meta name="twitter:image-height" content="768">
<meta name="twitter:image-width" content="1232">
<meta name="twitter:image:alt" content="A metaphor for real-space renormalization, where the building on the left is coarse-grained to the building on right. The facade of a wide brutalist architecture. On the left, the windows are small and the floors are close together. On the right, the windows are large and the floors are wide apart. High contrast, monochromatic, minimalistic, in the flat style of vector svg art., illustration, conceptual art.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">How to do Renormalization</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Survival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">math</div>
                <div class="quarto-category">physics</div>
                <div class="quarto-category">scaling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 11, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">July 19, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-logistic-map-rn-on-mathbbr" id="toc-the-logistic-map-rn-on-mathbbr" class="nav-link" data-scroll-target="#the-logistic-map-rn-on-mathbbr">The logistic map: RN on <span class="math inline">\(\mathbb{R}\)</span></a>
  <ul class="collapse">
  <li><a href="#the-logistic-map" id="toc-the-logistic-map" class="nav-link" data-scroll-target="#the-logistic-map">The logistic map</a></li>
  <li><a href="#universality" id="toc-universality" class="nav-link" data-scroll-target="#universality">Universality</a></li>
  <li><a href="#the-self-similarity-equation" id="toc-the-self-similarity-equation" class="nav-link" data-scroll-target="#the-self-similarity-equation">The self-similarity equation</a></li>
  <li><a href="#lessons" id="toc-lessons" class="nav-link" data-scroll-target="#lessons">Lessons</a></li>
  </ul></li>
  <li><a href="#ising-model-and-friends-rn-on-a-grid" id="toc-ising-model-and-friends-rn-on-a-grid" class="nav-link" data-scroll-target="#ising-model-and-friends-rn-on-a-grid">Ising model and friends: RN on a grid</a>
  <ul class="collapse">
  <li><a href="#percolation" id="toc-percolation" class="nav-link" data-scroll-target="#percolation">Percolation</a></li>
  <li><a href="#kadanoff-blocking" id="toc-kadanoff-blocking" class="nav-link" data-scroll-target="#kadanoff-blocking">Kadanoff blocking</a></li>
  <li><a href="#the-ising-model" id="toc-the-ising-model" class="nav-link" data-scroll-target="#the-ising-model">The Ising model</a></li>
  <li><a href="#kadanoff-decimation-take-0" id="toc-kadanoff-decimation-take-0" class="nav-link" data-scroll-target="#kadanoff-decimation-take-0">Kadanoff decimation, take 0</a></li>
  <li><a href="#sec-kadanoff-decimation-2d-ising" id="toc-sec-kadanoff-decimation-2d-ising" class="nav-link" data-scroll-target="#sec-kadanoff-decimation-2d-ising">Kadanoff decimation, take 1 (abortive)</a></li>
  <li><a href="#migdal-bond-moving-trick" id="toc-migdal-bond-moving-trick" class="nav-link" data-scroll-target="#migdal-bond-moving-trick">Migdal bond-moving trick</a></li>
  <li><a href="#sec-note-onsager-ising-2d" id="toc-sec-note-onsager-ising-2d" class="nav-link" data-scroll-target="#sec-note-onsager-ising-2d">Side note: Onsager’s solution of Ising model in 2D</a></li>
  <li><a href="#bonus-edgeworth-series" id="toc-bonus-edgeworth-series" class="nav-link" data-scroll-target="#bonus-edgeworth-series">Bonus: Edgeworth series</a></li>
  <li><a href="#bonus-generalized-central-limit-theorem" id="toc-bonus-generalized-central-limit-theorem" class="nav-link" data-scroll-target="#bonus-generalized-central-limit-theorem">Bonus: Generalized central limit theorem</a></li>
  </ul></li>
  <li><a href="#wilsons-nobel-prize-rn-in-momentum-space" id="toc-wilsons-nobel-prize-rn-in-momentum-space" class="nav-link" data-scroll-target="#wilsons-nobel-prize-rn-in-momentum-space">Wilson’s Nobel Prize: RN in momentum space</a>
  <ul class="collapse">
  <li><a href="#field-theory-continuous-ising-model" id="toc-field-theory-continuous-ising-model" class="nav-link" data-scroll-target="#field-theory-continuous-ising-model">Field theory: continuous Ising model</a></li>
  <li><a href="#field-theory-in-general" id="toc-field-theory-in-general" class="nav-link" data-scroll-target="#field-theory-in-general">Field theory, in general</a></li>
  <li><a href="#rn-flow-in-the-space-of-field-theories" id="toc-rn-flow-in-the-space-of-field-theories" class="nav-link" data-scroll-target="#rn-flow-in-the-space-of-field-theories">RN flow in the space of field theories</a></li>
  <li><a href="#rn-in-momentum-space" id="toc-rn-in-momentum-space" class="nav-link" data-scroll-target="#rn-in-momentum-space">RN in momentum space</a></li>
  <li><a href="#bonus-how-to-publish-in-quantum-field-theory" id="toc-bonus-how-to-publish-in-quantum-field-theory" class="nav-link" data-scroll-target="#bonus-how-to-publish-in-quantum-field-theory">Bonus: How to publish in quantum field theory</a></li>
  </ul></li>
  <li><a href="#a-bag-of-intuitions" id="toc-a-bag-of-intuitions" class="nav-link" data-scroll-target="#a-bag-of-intuitions">A bag of intuitions</a>
  <ul class="collapse">
  <li><a href="#sec-power-law-two-exponential-parents" id="toc-sec-power-law-two-exponential-parents" class="nav-link" data-scroll-target="#sec-power-law-two-exponential-parents">Power laws are born of two exponential parents</a></li>
  <li><a href="#rn-is-a-journey-in-the-space-of-possible-theories" id="toc-rn-is-a-journey-in-the-space-of-possible-theories" class="nav-link" data-scroll-target="#rn-is-a-journey-in-the-space-of-possible-theories">RN is a journey in the space of possible theories</a></li>
  <li><a href="#sec-symmetries-determine-theory-space" id="toc-sec-symmetries-determine-theory-space" class="nav-link" data-scroll-target="#sec-symmetries-determine-theory-space">Symmetries determine the shape of theory-space</a></li>
  <li><a href="#droplets-inside-droplets" id="toc-droplets-inside-droplets" class="nav-link" data-scroll-target="#droplets-inside-droplets">Droplets inside droplets</a></li>
  <li><a href="#sec-more-is-different" id="toc-sec-more-is-different" class="nav-link" data-scroll-target="#sec-more-is-different">More is different</a></li>
  <li><a href="#mesophysics-why-are-things-interesting-between-the-large-and-the-small" id="toc-mesophysics-why-are-things-interesting-between-the-large-and-the-small" class="nav-link" data-scroll-target="#mesophysics-why-are-things-interesting-between-the-large-and-the-small">Mesophysics: Why are things interesting between the large and the small?</a></li>
  <li><a href="#sec-universality" id="toc-sec-universality" class="nav-link" data-scroll-target="#sec-universality">Universality: The details don’t matter</a></li>
  </ul></li>
  
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Renormalization (RN) theory is a powerful tool for understanding the behavior of physical systems at different length scales. It allows us to explain why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models.</p>
<p>RN theory is based on the idea of self-similarity – that a system looks the same at different length scales. For example, a coastline looks the same whether you are looking at it from a satellite or from a boat. Similarly, a magnet looks the same whether you are looking at it with a microscope or with your naked eye.</p>
<p>The basic idea of RN is to start with a microscopic description of a system and then to coarse-grain it. That is, to average over the details of the system at a smaller length scale to obtain a description of the system at a larger length scale. This process can be repeated, leading to a sequence of descriptions of the system at increasingly larger length scales.</p>
<p>The key insight of RN theory is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. This is known as the universality hypothesis. Universality explains why systems that are very different at the microscopic level can have the same behavior at the macroscopic level. For example, water, oil, and electricity all exhibit the same percolation behavior near the percolation threshold, even though they are very different at the microscopic level.</p>
<p>Universality also justifies the use of toy models to study physical systems. Toy models are simplified models that capture the essential features of a system without including all of the details. For example, the Ising model is a toy model of a magnet that captures the essential features of ferromagnetism without including all of the details of the interactions between the atoms.</p>
<p>In this essay, we will explore the basic ideas of RN theory and see how it can be used to understand a variety of physical systems.</p>
<p><img src="figure/banner/thumbnail_1.png" class="img-fluid"></p>
</section>
<section id="the-logistic-map-rn-on-mathbbr" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-logistic-map-rn-on-mathbbr">The logistic map: RN on <span class="math inline">\(\mathbb{R}\)</span></h2>
<p>Let’s first study the logistic map, the simplest nontrivial example of renormalization that I know of. This section is based on <a href="https://en.wikipedia.org/wiki/Logistic_map">Wikipedia</a>.</p>
<section id="the-logistic-map" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-logistic-map">The logistic map</h3>
<p>Consider a function <span class="math inline">\(f_r(x)=r x(1-x)\)</span>, and we want to study what happens when we iterate the map many times. The map might fall into a fixed point, a fixed cycle, or chaos. We can see all those cases in its <a href="https://en.wikipedia.org/wiki/Bifurcation_diagram">bifurcation diagram</a>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Logistic_Bifurcation_map_High_Resolution.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The bifurcation diagram of the logistic map. <a href="https://en.wikipedia.org/wiki/File:Logistic_Bifurcation_map_High_Resolution.png">Source</a></figcaption>
</figure>
</div>
<p>When the map falls into a stable fixed cycle of length <span class="math inline">\(n\)</span>, we would find that the graph of <span class="math inline">\(f_r^n\)</span> and the graph of <span class="math inline">\(x \mapsto x\)</span> intersect at <span class="math inline">\(n\)</span> points, and the slope of the graph of <span class="math inline">\(f_r^n\)</span> is bounded in <span class="math inline">\((-1, +1)\)</span> at those intersections.</p>
<p>For example, when <span class="math inline">\(r=3.0\)</span>, we find that there is only a single intersection, at which point the slope is exactly <span class="math inline">\(+1\)</span>, indicating that it is a stable single fixed point, but is about to undergo a bifurcation.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/period_doubling_bifurcation.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The relationship between <span class="math inline">\(x_{n+2}\)</span> and <span class="math inline">\(x_{n}\)</span> as <span class="math inline">\(r\)</span> increases from <span class="math inline">\(2.7\)</span> to <span class="math inline">\(3.3\)</span>. Before the period doubling bifurcation occurs. The orbit converges to a single fixed point where the graph of <span class="math inline">\(f_r^2\)</span> intersects the diagonal line. As <span class="math inline">\(r\)</span> reaches <span class="math inline">\(3\)</span>, the intersection pitchforks into three. The middle intersection becomes unstable, but the two neighboring intersections remain stable.</figcaption>
</figure>
</div>
<p>As <span class="math inline">\(r\)</span> increases to beyond <span class="math inline">\(r=3.0\)</span>, the intersection point splits to two, which is a period doubling. For example, when <span class="math inline">\(r=3.4\)</span>, there are three intersection points, with the middle one unstable, and the two others stable.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Logistic_iterates_3.4.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">When <span class="math inline">\(r=3.4\)</span>, there are three intersection points, with the middle one unstable, and the two others stable. <a href="https://en.wikipedia.org/wiki/File:Logistic_iterates_3.4.svg">Source</a></figcaption>
</figure>
</div>
<p>As <span class="math inline">\(r\)</span> approaches <span class="math inline">\(r=3.45\)</span>, another period-doubling occurs in the same way. The period-doublings occur more and more frequently, until at a certain <span class="math inline">\(r \approx 3.56994567\)</span>, the period doublings become infinite, and the map becomes chaotic. This is the period-doubling route to chaos.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Logistic_iterates_3.56994567.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">When <span class="math inline">\(r \approx 3.56994567\)</span>, there are infinitely many intersections, and we have arrived at chaos via the period-doubling route. <a href="https://en.wikipedia.org/wiki/File:Logistic_iterates_with_r%3D3.56994567.svg">Source</a></figcaption>
</figure>
</div>
<p>Something remarkable happens when we superimpose the graphs of <span class="math inline">\(f_r, f_r^2, f_r^4, \dots\)</span> when <span class="math inline">\(r\)</span> is at the critical point <span class="math inline">\(3.5699\dots\)</span>. We see that each iteration of the graph seems to resemble itself, except that it is scaled and rotated by 180 degrees. We can naturally guess that <span class="math inline">\(f_r^{\infty}\)</span> converges to a certain function that is infinitely jagged, such that it exactly resembles itself when scaled and rotated; that is, it is a fractal.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Logistic_iterates_together_r=3.56994567.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The graphs of <span class="math inline">\(f_r, f_r^2, f_r^4, \dots\)</span> when <span class="math inline">\(r\)</span> is at the critical point.</figcaption>
</figure>
</div>
<p>As <span class="math inline">\(r\)</span> approaches the critical value, we can see how the graph of <span class="math inline">\(f_r^\infty\)</span> takes on more and more details, and at the critical point, becomes a perfect fractal.</p>
<figure class="figure">
<video controls="" width="100%">
<source src="figure/Logistic_map_approaching_the_scaling_limit.mp4" type="video/mp4">
</video>
<figcaption>
The graphs of <span class="math inline">\(f_r, f_r^2, f_r^4, \dots\)</span> as <span class="math inline">\(r\)</span> approaches the critical point from below.
</figcaption>
</figure>
</section>
<section id="universality" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="universality">Universality</h3>
<p>Looking at the bifurcation diagram, we can see a region, starting just after <span class="math inline">\(r = 3.8\)</span>, where there is a clear “window” with period <span class="math inline">\(3\)</span> bursting out of a sea of chaos. The window then bifurcates repeatedly, to stable cycles of periods <span class="math inline">\(6, 12, 24, \dots\)</span> until it all collapses back into the chaos again at around <span class="math inline">\(r \approx 3.8494344\)</span>. Though this is a different place, the bifurcation diagram looks suspiciously similar to the previous case.</p>
<p>Not only that, if we look at the movie of <span class="math inline">\(f_r^\infty\)</span> as <span class="math inline">\(r\)</span> approaches this critical point, we again see the same jagged shape.</p>
<video controls="" width="100%">
<source src="figure/Logistic_map_approaching_the_period-3_scaling_limit.mp4" type="video/mp4">
</video>
<p>Are we seeing some kind of <em>universal</em> feature of period-doubling routes to chaos? Is this a general pattern independent of the details of how exactly the logistic map is defined? What if we change to another dynamical system completely different?</p>
<p>For example, we can consider the gauss map <span class="math inline">\(x_{n+1} = \exp(-\alpha x^2_n)+\beta\)</span>. For a fixed <span class="math inline">\(\alpha\)</span>, we can plot the bifurcation graph as we vary <span class="math inline">\(\beta\)</span>. Though it looks different, the two bifurcation graphs have a clear resemblance. This is an instance of <strong>universality</strong>, for which we will see <a href="#sec-universality">again and again later</a>. If <span class="math inline">\(f_r\)</span> is a family of curves with parabolic tops<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, then it will bifurcate just like the logistic curve.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Rigorously, we can describe it as follows. If <span class="math inline">\(F: \mathbb{R}^2 \to \mathbb{R}\)</span> is smooth, and for all <span class="math inline">\(r \in \mathbb{R}\)</span>, the function <span class="math inline">\(F(r, \cdot): \mathbb{R}\to \mathbb{R}\)</span> has a single global maximum, at which point <span class="math inline">\(\partial_x^2 F(r, x) &lt; 0\)</span>, then its bifurcation diagram looks the same as that of the logistic map, and it will have the same two scaling exponents <span class="math inline">\(\alpha, \delta\)</span>, to be calculated below.</p></div></div><div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/bifurcation_gauss_logistic.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The bifurcation graphs of gauss map with <span class="math inline">\(\alpha = 5\)</span> and the logistic map.</figcaption>
</figure>
</div>
<p>More to the issue at hand: why do the two graphs look similar?</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/logistic_bifurcation_graph_fractal.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The bifurcation graph is self-similar. <a href="https://ja.wikipedia.org/wiki/ファイル:ロジスティック写像の窓の自己相似.png">Source</a></figcaption>
</figure>
</div>
</section>
<section id="the-self-similarity-equation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-self-similarity-equation">The self-similarity equation</h3>
<p>Recall that we said the limit of <span class="math inline">\(f^\infty_r\)</span> should be self-similar, in the sense that if we iterate it twice, then rotate and scale it by a factor, we get back the same function. That is, it should be a solution to the <strong>self-similarity equation</strong>.</p>
<p><span class="math display">\[
f(x) = -\alpha f\left(f\left(\frac{x}{-\alpha} \right)\right)
\]</span></p>
<p>In words, if we scale up the graph for <span class="math inline">\(f^2\)</span> by <span class="math inline">\(\alpha &gt; 0\)</span>, and then rotate by 180 degrees, we get back the graph for <span class="math inline">\(f\)</span>. Why should this be the case?</p>
<p>Well, look back at the previous diagram of the first bifurcation:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/period_doubling_bifurcation.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The relationship between <span class="math inline">\(x_{n+2}\)</span> and <span class="math inline">\(x_{n}\)</span> as <span class="math inline">\(r\)</span> increases from <span class="math inline">\(2.7\)</span> to <span class="math inline">\(3.3\)</span>.</figcaption>
</figure>
</div>
<p>The graph of <span class="math inline">\(f\)</span> is a parabola, and the graph of <span class="math inline">\(f^2\)</span> began by looking like a parabola, but its top eventually collapses like an overbaked pie, down and down, until it punches right through the <span class="math inline">\(y = x\)</span> diagonal line again and … wait, wait, it looks like a little parabola again, except this time, it’s rotated by 180 degrees! And so we can run the same argument on <span class="math inline">\(f^2\)</span>, and conclude that the graph of <span class="math inline">\((f^2)^2 = f^4\)</span> would collapse <em>upwards</em>, punching right through the <span class="math inline">\(y=x\)</span> diagonal line again, and then history repeats with <span class="math inline">\(f^8, f^{16}, \dots\)</span>, repeating the same story again &amp; again, but smaller &amp; smaller.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;As a wise <a href="https://en.wikipedia.org/wiki/Lewis_Fry_Richardson">meteorologist</a> <a href="https://en.wikipedia.org/wiki/Siphonaptera_(poem)">has said</a>:</p>
<blockquote class="blockquote">
<p>Big whorls have little whorls, Which feed on their velocity; And little whorls have lesser whorls, And so on to viscosity.</p>
</blockquote>
<p>And indeed, chaos theory, fractals, and RN all made early appearances in the study of turbulence.</p></div></div><div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/logistics_scaling_invariant_function.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The center of <span class="math inline">\(f\)</span> rises, until the center of <span class="math inline">\(f^2\)</span> collapses, causing a bifurcation at <span class="math inline">\(r_1\)</span>. As <span class="math inline">\(r\)</span> increases beyond that point, the center of <span class="math inline">\(f^2\)</span> looks like a parabola again, looking like <span class="math inline">\(f\)</span> in miniature. Thus, as <span class="math inline">\(r\)</span> increases further, <span class="math inline">\(f^4\)</span> will eventually undergo a bifurcation that is similar to the previous bifurcation, except in miniature. <span class="citation" data-cites="strogatzNonlinearDynamicsChaos2015">(<a href="#ref-strogatzNonlinearDynamicsChaos2015" role="doc-biblioref">Strogatz 2015, 388</a>)</span></figcaption>
</figure>
</div>
<p>By eye-balling the curve, we see that <span class="math inline">\(f\)</span> should be an even function, and that in a neighborhood of its centerpoint, . Also, since the <span class="math inline">\(f^2\)</span> can be graphically calculated by doing the cobweb diagram with the graph of <span class="math inline">\(f\)</span>, it does not matter if we first scale up the graph of <span class="math inline">\(f\)</span> by a factor of <span class="math inline">\(r\)</span> to <span class="math inline">\(F\)</span>, then double it to <span class="math inline">\(F^2\)</span>, or if we first double it to <span class="math inline">\(f^2\)</span>, then scale its graph. We would get back the same thing. Thus, without loss of generality, we can scale <span class="math inline">\(f\)</span> such that <span class="math inline">\(f(0) = 1\)</span>.</p>
<p>So, our task is to solve the following equation:</p>
<p><span class="math display">\[
\begin{cases}
f(x) = -\alpha f\left(f\left(\frac{x}{-\alpha} \right)\right)\\
f(x) = 1 - a_2 x^2 + a_4 x^4 + \dots
\end{cases}
\]</span></p>
<p>We can solve the equation numerically as the fixed point. We would start with <span class="math inline">\(f(x) = 1-x^2\)</span>, then guess a good <span class="math inline">\(\alpha\)</span> and repeatedly apply <span class="math inline">\(f \mapsto -\alpha f\left(f\left(\frac{x}{-\alpha} \right)\right)\)</span>. If we picked <span class="math inline">\(\alpha\)</span> correctly, we would have gotten the right result, as shown:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Logistic_scaling_limit_critical.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">At the point of chaos <span class="math inline">\(r^*=3.5699 \dots\)</span>, as we repeat the functional equation iteration <span class="math inline">\(f(x) \mapsto-\alpha f(f(-x / \alpha))\)</span> with <span class="math inline">\(\alpha=2.5029 \ldots\)</span>, we find that the map does converge to a limit. <a href="https://en.wikipedia.org/wiki/File:Logistic_scaling_limit,_r%3D3.56994567.svg">Source</a></figcaption>
</figure>
</div>
<p>If <span class="math inline">\(\alpha\)</span> is not correct, the iterates would not converge; instead, they would have a zooming effect that looks cool.</p>
<video controls="" width="100%">
<source src="figure/Logistic_scaling_with_varying_scaling_factor.webm" type="video/webm">
</video>
<div class="callout callout-style-default callout-note callout-titled" title="Solving the equation at order 2">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solving the equation at order 2
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>At order 2, we approximate by <span class="math inline">\(f(x) \approx 1 - a_2 x^2\)</span> and ignore all higher-order terms. This gives us two equations for two unknowns:</p>
<p><span class="math display">\[
\begin{cases}
1-a_2 = \frac{1}{-\alpha} \\
\frac{2a_2^2}{\alpha} = a_2
\end{cases}
\]</span></p>
<p>It has two solutions. One solution has <span class="math inline">\(\alpha &lt; 0\)</span>, which we know is unphysical. The other one is</p>
<p><span class="math display">\[
\begin{cases}
\alpha = 1 + \sqrt{3} \approx 2.732 \\
a_2 = \frac{1 + \sqrt{3}}{2} \approx 1.366
\end{cases}
\]</span></p>
</div>
</div>
</div>
<p>What happens if we are not <em>exactly</em> at the fixed point but start slightly off? Let’s say we start with a function <span class="math inline">\(f_0(x) = 1 - a_{2,0}x^2\)</span>, where <span class="math inline">\(a_{2,0} = a_2^* + \Delta\)</span>, where <span class="math inline">\(a_2^*\)</span> is the fixed point, and <span class="math inline">\(\Delta\)</span> is small but nonzero. Here, we should think of the space of possible functions. Each point in this space is a possible scaling limit, but if we start a bit too small, we fall into boredom, and if we start a bit too high, we fall into chaos. Start just right, and we harvest a beautiful fractal.</p>
<p>After one iteration, we have <span class="math inline">\(f_1(x) = -\alpha_0 f_0(f_0(x/(-\alpha_0)))\)</span>, where <span class="math inline">\(\alpha_0\)</span> was fixed by <span class="math inline">\(f_1(0) = 1\)</span>. This gives us</p>
<p><span class="math display">\[
\begin{cases}
\alpha_0 = \frac{1}{-1+a_{2, 0}} \\
\frac{2a_{2, 0}^2}{\alpha_0} = a_{2, 1}
\end{cases}
\]</span></p>
<p>That is, we have the <strong>renormalization flow equation</strong></p>
<p><span class="math display">\[
2a_{2, 0}^2(a_{2, 0}-1)= a_{2, 1}
\]</span></p>
<p>We can plot the space of all possible <span class="math inline">\(f(x)\)</span> as a line, like</p>
<p><span class="math display">\[1-0x^2, 1-0.5 x^2, 1-x^2, 1-1.5x^2, \dots\]</span></p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rn_flow_feigenbaum_2.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">RN flow diagram of the self-similarity map at order 2.</figcaption>
</figure>
</div>
<p>This is a 1-dimensional slice of the space of all possible <span class="math inline">\(f\)</span> (the space of theories). Then, the effect of repeatedly applying the self-similarity map is to iterate the map <span class="math inline">\(a_2 \mapsto 2a_{2}^2(a_{2}-1)\)</span>. If we are precisely at the fixed-point <span class="math inline">\(a_2^*\)</span>, then we are not going anywhere, but if we are not exactly there, then since the slope of <span class="math inline">\(a_2 \mapsto 2a_{2}^2(a_{2}-1)\)</span> is <span class="math inline">\(\delta \approx 5.73\)</span> at that point, we would get farther and farther away:</p>
<p><span class="math display">\[
f_0 = 1-(a_2^* + \Delta)x^2, \quad f_1 = 1-(a_2^* + \delta\Delta)x^2, \quad f_1 = 1-(a_2^* + \delta^2\Delta)x^2, \quad \dots
\]</span></p>
<p>and after <span class="math inline">\(\log_\delta(\frac{0.1}{\Delta})\)</span>, we would be at roughly <span class="math inline">\(1-(a_2^* \pm 0.1)x^2\)</span>, which is when we can finally notice that we are <em>obviously</em> no longer in the neighborhood of the fixed point anymore. If we start at <span class="math inline">\(a_2^* + \Delta/\delta\)</span>, then we can sustain the illusion for one more iteration. Similarly, if we start at <span class="math inline">\(a_2^* + \Delta/\delta^n\)</span>, then we can sustain the illusion for <span class="math inline">\(n\)</span> more iterations.</p>
<p>Now, thinking back to what the logistic map says, we understand what we have discovered: The graph of <span class="math inline">\(f_{r^* - \Delta}\)</span> is similar to the graph of <span class="math inline">\(f_{r^* - \Delta/\delta}^2\)</span> scaled by <span class="math inline">\(-\alpha\)</span>. If we let <span class="math inline">\(r_1, r_2, r_3, \dots\)</span> be the points at which the logistic map splits into a stable cycle of period <span class="math inline">\(2^1, 2^2, 2^3, \dots\)</span>, then we have <span class="math inline">\(r_{n} \approx r^* - \Delta/\delta^{n}\)</span>, and so we have:</p>
<p><span class="math display">\[
\frac{r^* - r_n}{r^* - r_{n+1}} \to \delta
\]</span></p>
<p>This is usually spoken in this way: the intervals between two bifurcations shrinks at a rate of <span class="math inline">\(\delta\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/logistic_bifurcation_graph_fractal_delta.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The bifurcation diagram for the logistic map. As the bifurcations approach the point of chaos, the interval between two bifurcations gets shorter and shorter, at a rate of <span class="math inline">\(\delta\)</span> per bifurcation. <a href="https://ja.wikipedia.org/wiki/ファイル:ロジスティック写像、周期倍分岐カスケードの軌道図.png">Source</a></figcaption>
</figure>
</div>
<p><span class="math inline">\(\delta\)</span> is called <strong>Feigenbaum’s first constant</strong>, and <span class="math inline">\(\alpha\)</span> is <strong>Feigenbaum’s second constant</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Solving the equation at order 4">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Solving the equation at order 4
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Similarly, we can solve the equation at order 4 by plugging in <span class="math inline">\(f(x) \approx 1 - a_2 x^2 + a_4 x^4\)</span>, obtaining 3 equations for 3 variables:</p>
<p><span class="math display">\[
\begin{cases}
1-a_2+a_4 = \frac{1}{-\alpha} \\
\frac{2a_2^2 - 4a_2a_4}{\alpha} = a_2 \\
\frac{a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2)}{-\alpha^2} = a_4
\end{cases}
\]</span></p>
<p>To solve this numerically, first guess a solution from the previous one, <span class="math inline">\(\alpha \approx 2.732, a_2 \approx 1.366\)</span>, then plug into the first equation to get <span class="math inline">\(a_4 \approx 0\)</span>. Then, standard numerical root-finding gives</p>
<p><span class="math display">\[
\begin{cases}
\alpha \approx 2.534 \\
a_2 \approx 1.522 \\
a_4 \approx 0.128
\end{cases}
\]</span></p>
</div>
</div>
</div>
<p>We can also make the same argument using a flow in theory-space, except now we are doing it over a 2-dimensional slice of it. The flow map is</p>
<p><span class="math display">\[
F(a_2, a_4) = \left(
   (2a_2^2 - 4a_2a_4)(-1+a_2 - a_4), -(a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2))(-1+a_2 - a_4)^3
\right)
\]</span></p>
<p>At the fixed-point <span class="math inline">\((a_2, a_4) = (1.522, 0.128)\)</span>, the Jacobian matrix is <span class="math display">\[
\nabla F = \begin{bmatrix}
6.0506 &amp; -6.2524 \\
1.2621 &amp; -1.6909
\end{bmatrix}
\]</span></p>
<p>This matrix has eigenvalues of <span class="math inline">\(4.843, -0.483\)</span>, so it is a saddle point, with <span class="math inline">\(\delta = 4.843\)</span>. The flow and the eigenvectors <span class="math inline">\((0.982, 0.190), (0.691, 0.723)\)</span> are plotted below.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rn_flow_feigenbaum_4.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">RN flow diagram of the self-similarity map at order 4. The two eigenvector directions at the fixed point are plotted as dashed lines.</figcaption>
</figure>
</div>
<p>In summary:</p>
<table class="caption-top table">
<caption>The solution to the self-similarity equation, at increasingly high orders of approximation.</caption>
<thead>
<tr class="header">
<th></th>
<th>Order 2</th>
<th>4</th>
<th><span class="math inline">\(\infty\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(a_2\)</span></td>
<td>1.366</td>
<td>1.522</td>
<td>1.530</td>
</tr>
<tr class="even">
<td><span class="math inline">\(a_4\)</span></td>
<td></td>
<td>0.128</td>
<td>0.105</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha\)</span></td>
<td>2.732</td>
<td>2.534</td>
<td>2.503</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\delta\)</span></td>
<td>5.73</td>
<td>4.843</td>
<td>4.669</td>
</tr>
</tbody>
</table>
</section>
<section id="lessons" class="level3">
<h3 class="anchored" data-anchor-id="lessons">Lessons</h3>
<p>Even in this tiny problem, we can already draw several lessons, which will appear again and again in RN:</p>
<ul>
<li>We assume a function is self-similar, and calculate from there.</li>
<li>Self-similarity is a transform on a function (or “theory”).</li>
<li>We often need to use a “fudge factor” like <span class="math inline">\(\alpha\)</span> to make sure that the transformed function does not collapse to zero, for trivial reasons.</li>
<li>If we repeatedly apply the self-similarity transform on a function, we would obtain a scaling limit, a perfectly self-similar object – a fractal.</li>
<li>In the space of all possible theories, the self-similarity transform creates a flow-field in the theory-space. The interesting fixed-points of the flow-field are its saddle points.</li>
<li>The largest eigenvalue of the saddle point describes what happens when you are close to the saddle point, but not quite there.</li>
<li>Bravely calculate using the cheapest approximation you can think of. It often gets you within 50% of the right answer.</li>
<li>But if you want accuracy, you can always use a computer and calculate many orders higher.</li>
</ul>
<p><img src="figure/banner/thumbnail_2.png" class="img-fluid"></p>
</section>
</section>
<section id="ising-model-and-friends-rn-on-a-grid" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="ising-model-and-friends-rn-on-a-grid">Ising model and friends: RN on a grid</h2>
<p>This section studies RN on a grid, using what is called <strong>real space RN</strong>, in contrast to <strong>momentum space RN</strong>. Real space RN is a garden of tricks, useful and intuitive, but not as powerful as momentum space RN. If you find the kind of mathematical trickery in this section fun, look at <span class="citation" data-cites="kadanoffStatisticalPhysicsStatics1999 burkhardtRealSpaceRenormalization1982">(<a href="#ref-kadanoffStatisticalPhysicsStatics1999" role="doc-biblioref">Kadanoff 1999b, chap. 14</a>; <a href="#ref-burkhardtRealSpaceRenormalization1982" role="doc-biblioref">Burkhardt and Leeuwen 1982</a>)</span> for more.</p>
<blockquote class="blockquote">
<p>This paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed – as I will explain briefly – to <strong>run rapidly into insuperable difficulties</strong> and interest faded. In retrospect, however, Kadanoff’s scaling picture embodied important features eventually seen to be basic to Wilson’s conception of the full renormalization group.</p>
<p><span class="citation" data-cites="fisherRenormalizationGroupTheory1998">(<a href="#ref-fisherRenormalizationGroupTheory1998" role="doc-biblioref">Fisher 1998</a>)</span></p>
</blockquote>
<p>I recommend that you open these and play as you follow along:</p>
<ul>
<li><a href="https://www.complexity-explorables.org/explorables/i-sing-well-tempered/">Complexity Explorables | I sing well-tempered</a></li>
<li><a href="https://www.ibiblio.org/e-notes/Perc/ising1k.htm">Ising model</a> by <a href="https://www.ibiblio.org/e-notes/Perc/contents.htm">Evgeny Demidov</a>.</li>
</ul>
<section id="percolation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="percolation">Percolation</h3>
<p>Percolation is about randomly punching holes in a material until it falls apart. In the simplest setting, the material is a square lattice <span class="math inline">\(\mathbb{Z}^2\)</span>, and each site (vertex) is either open or closed.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Open means there is a hole there, and closed means the site is intact. Sites are open or closed randomly and independently with probability <span class="math inline">\(p\)</span>. We are interested in whether there is an infinite connected cluster of open sites.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Percolation on <span class="math inline">\(\mathbb{Z}\)</span> is trivial, even more trivial than Ising model on <span class="math inline">\(\mathbb{Z}\)</span>. The trouble is the same: you can only go from one point to another point by one route, and if at any point on the route, you are stopped, then that’s the end – you can’t get there by any other way. Thus, long-range interactions decay exponentially with distance, which means no power law, no phase transition, no critical point. See a <a href="#sec-power-law-two-exponential-parents">later section</a>.</p></div></div><p>This is a model for a porous material: for example, water seeping through the ground. If we have a layer of rock, then groundwater can seep through if there is a single connected path from top to bottom. A layer of rock can be thought of as a grid, with little cracks between grid-points. According to percolation theory, at a “critical probability”, suddenly we have arbitrarily large connected clusters of cracks, and so water can seep arbitrarily far in the rock – it is all or nothing.</p>
<p>That is, there is a sharp transition: if <span class="math inline">\(p\)</span> is small, then there is no infinite cluster of open sites, and the water cannot go through; but if <span class="math inline">\(p\)</span> is large, then there is an infinite cluster of open sites, and water can go through. The critical value <span class="math inline">\(p_c\)</span> is about <span class="math inline">\(0.5927...\)</span>. See <span class="citation" data-cites="grimmettPercolation1999">(<a href="#ref-grimmettPercolation1999" role="doc-biblioref">Grimmett 1999</a>)</span> for more details about percolation.</p>
<p>To use Kadanoff blocking for percolation, the first step is to coarse grain the lattice. We group the sites into blocks of <span class="math inline">\(3 \times 3\)</span> and call a block open if there is a path of open sites connecting the left and right sides of the block. Otherwise, the block is closed.</p>
<p>The next step is to define a new percolation model on the coarse-grained lattice, but this is a little trickier than in the Ising model, because there is no obvious way to map the parameters of the original model to the parameters of the new model. We need to find a new probability <span class="math inline">\(p'\)</span> such that the new model on the coarse-grained lattice has the same behavior as the original model on the fine lattice. In particular, we want the probability of having an infinite cluster of open sites to be the same in both models.</p>
<p>It turns out that there is no exact way to do this, but we can make an approximation. One way to do this is to use Monte Carlo simulations to estimate the probability of having an infinite cluster for different values of <span class="math inline">\(p'\)</span>.</p>
</section>
<section id="kadanoff-blocking" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="kadanoff-blocking">Kadanoff blocking</h3>
<p>This section based on <span class="citation" data-cites="simkinReinventingWillis2011">(<a href="#ref-simkinReinventingWillis2011" role="doc-biblioref">Simkin and Roychowdhury 2011, sec. 10</a>)</span> and <span class="citation" data-cites="stinchcombeIntroductionScalingConcepts1991">(<a href="#ref-stinchcombeIntroductionScalingConcepts1991" role="doc-biblioref">Stinchcombe 1991</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/ising model.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">When an Ising model is at the critical temperature <span class="math inline">\(T_c\)</span>, coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. <span class="citation" data-cites="sethnaCourseCracklingNoise2007">(<a href="#ref-sethnaCourseCracklingNoise2007" role="doc-biblioref">Sethna 2007, fig. 13</a>)</span>.</figcaption>
</figure>
</div>
<p>The idea of Kadanoff blocking is simple: “This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again.”</p>
<p>Given a hexagonal grid, you make <span class="math inline">\(p\)</span> of them black and the rest white. What is the critical <span class="math inline">\(p\)</span> where you get a percolation (infinitely big black island)? To do this by blocking, we regard a triangle as a large spin, and “merge” the three spins on a triangle to one single spin. If two or three spins are black, then the whole spin is also black, otherwise, the whole spin is white.</p>
<p>Then, after one blocking operation, the lattice length increases from <span class="math inline">\(l\)</span> to <span class="math inline">\(\sqrt 3 l\)</span> , and the renormalized occupation probability changes from <span class="math inline">\(p\)</span> to <span class="math inline">\(p^3 + 3p^2(1-p) = p^2(3-2p)\)</span>. That is, we have the RN flow equation</p>
<p><span class="math display">\[p' = p^3 + 3p^2(1-p) = p^2(3-2p)\]</span></p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/hexagonal_decimation.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Kadanoff blocking on a triangular lattice. <span class="citation" data-cites="simkinReinventingWillis2011">(<a href="#ref-simkinReinventingWillis2011" role="doc-biblioref">Simkin and Roychowdhury 2011, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>The equilibrium point is <span class="math inline">\(p=1/2\)</span>. This is the percolation probability. Let the reduced probability be <span class="math inline">\(\bar p = p-1/2\)</span>. We find that one iteration of the RN flow makes <span class="math inline">\(\bar p_{n+1} = \frac 32 \bar p - 2 \bar p^3\)</span> which is <span class="math inline">\(\approx \frac 32 \bar p_n\)</span> for small values of <span class="math inline">\(\bar p\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rn_flow_hexagonal_grid_1.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">RN flow for Kadanoff blocking on a triangular lattice.</figcaption>
</figure>
</div>
<p>Suppose we start with <span class="math inline">\(\bar p_0\)</span> , and we perform <span class="math inline">\(N\)</span> repeats of RN to reach some constant <span class="math inline">\(\Delta \bar p\)</span> (for example, 0.001), then</p>
<p><span class="math display">\[N = \frac{\ln \Delta \bar p - \ln \bar p_0}{\ln \frac 32}\]</span></p>
<p>during which time, the lattice length has increased by</p>
<p><span class="math display">\[3^{\frac 12 N} \propto \bar p_0^{-\frac{\ln 3}{2\ln \frac 32}}\]</span></p>
<p>Since at constant <span class="math inline">\(\Delta \bar p\)</span>, the lattice has a certain <em>characteristic</em> look-and-feel with a certain <em>characteristic</em> size for its clusters, we find that the <em>characteristic</em> length of its clusters is <span class="math inline">\(\propto (p-1/2)^{-\nu}\)</span>, where<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;It amuses me to no end that the word <em>characteristic</em> is something chemists use a lot. Physicists do it too, sure, with their “characteristic length”, “characteristic height”, “characteristic temperature”, and such, but it is abstract. You rarely need to actually check a cake’s characteristic length against a standard cake. However, when you are doing chemistry, and you need to check a chemical’s <em>characteristic smell</em>, then you are out of luck.</p>
<blockquote class="blockquote">
<p>I’m saddened to report that the chemical literature contains descriptions of dimethylcadmium’s smell. Whoever provided these reports was surely exposed to far more of the vapor than common sense would allow … its odor is variously described as “foul”, “unpleasant”, “metallic”, “disagreeable”, and (wait for it) “characteristic”, which is an adjective that shows up often in the literature with regard to smells, and almost always makes a person want to punch whoever thought it was useful. … if you’re working with organocadmium derivatives and smell something nasty, but nasty in a new, exciting way that you’ve never quite smelled before, then you can probably assume the worst. <span class="citation" data-cites="loweThingsWonWork2013">(<a href="#ref-loweThingsWonWork2013" role="doc-biblioref">Lowe 2013</a>)</span></p>
</blockquote>
</div></div><p><span class="math display">\[\nu = \frac{\ln 3}{2\ln \frac 32} \approx 1.355\]</span></p>
<p>The actual exponent is believed to be <span class="math inline">\(\nu = 4/3\)</span>, so we are only 1.6% off. Very good for such a cheap calculation!</p>
</section>
<section id="the-ising-model" class="level3">
<h3 class="anchored" data-anchor-id="the-ising-model">The Ising model</h3>
<p>Ferromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks – an eternity on the atomic level?</p>
<p>In 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until <span class="citation" data-cites="peierlsIsingModelFerromagnetism1936">(<a href="#ref-peierlsIsingModelFerromagnetism1936" role="doc-biblioref">Peierls 1936</a>)</span> showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See <span class="citation" data-cites="dombCriticalPhenomenaBrief1985">(<a href="#ref-dombCriticalPhenomenaBrief1985" role="doc-biblioref">Domb 1985</a>)</span> for some more historical details.</p>
<p>The Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature <span class="math inline">\(T\)</span> makes the atoms jiggle and be random.</p>
<p>There are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span>. Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is</p>
<p><span class="math display">\[
H(s) = -J \sum_{i, j} s_i s_{j}
\]</span></p>
<p>where <span class="math inline">\(J&gt;0\)</span> is the strength of interaction between neighboring atoms, <span class="math inline">\(s_i\)</span> represents the spin at site <span class="math inline">\(i\)</span>, and the summation is over neighboring pairs of <span class="math inline">\(i, j\)</span>. For example, if we have Ising model on the integer line <span class="math inline">\(\mathbb{Z}\)</span>, then <span class="math inline">\(H = -J \sum_{i} s_i s_{i+1}\)</span>. Similarly, if we have an Ising model on the square grid <span class="math inline">\(\mathbb{Z}^2\)</span>, then <span class="math inline">\(H = -J \sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})\)</span>.</p>
<p>We are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity <span class="math inline">\(Z\)</span>, called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating <span class="math inline">\(Z\)</span> at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, <span class="math inline">\(Z\)</span> is defined as</p>
<p><span class="math display">\[
Z := \sum_{s} e^{-\beta H(s)}
\]</span></p>
<p>where <span class="math inline">\(\beta = 1/T\)</span> is the inverse of the temperature <span class="math inline">\(T\)</span>. The summation is over all possible configurations of the system. For example, if the Ising model is over <span class="math inline">\(\{0, 1, 2, \dots, 99\}\)</span>, then the summation is over <span class="math inline">\(\{-1, +1\}^{100}\)</span>.</p>
<p>The temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the <span class="math inline">\(J\)</span> like this:</p>
<p><span class="math display">\[Z = \sum_{s} e^{-\beta H(s)}, \quad H(s) = -J_0 \sum_{i, j} s_i s_{j}, \quad J_0 = J/T\]</span></p>
<p>After we finish the calculation, we should write the temperature explicitly again in order to interpret what we have found.</p>
</section>
<section id="kadanoff-decimation-take-0" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="kadanoff-decimation-take-0">Kadanoff decimation, take 0</h3>
<p>Like ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.</p>
<p>Let us consider the Ising model on the integer line <span class="math inline">\(\mathbb{Z}\)</span> with periodic boundary conditions. That is, we have <span class="math inline">\(2n\)</span> spins <span class="math inline">\(s_0, s_1, \dots, s_{2n-1}\)</span> arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:</p>
<p><span class="math display">\[
\begin{aligned}
Z &amp;= \sum_{s_0, s_1, \dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \dots + s_{2n-1}s_0)} \\
  &amp;= \sum_{s_0, s_1, \dots, s_{2n-2}} \sum_{s_1, s_3, \dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \dots + s_{2n-1}s_0)} \\
  &amp;\overset{\mathrm{hopefully}}{=} \sum_{s_0, s_2, \dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \dots + s_{2n-2}s_0)} \\
\end{aligned}
\]</span></p>
<p>where we have hopefully written the <span class="math inline">\(J'\)</span>. It is our ardent hope that there exists some <span class="math inline">\(J'\)</span>, somewhere in the universe, that will make the equation come true.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Physicists call it “ansatz”, but I prefer to say it like this:</p>
<blockquote class="blockquote">
<p>The author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.</p>
<p>— <a href="https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths">Yu Tsun</a>, professor of English at Deutsch-Chinesische Hochschule.</p>
</blockquote>
</div></div><p>Because each even spin can only reach its nearest-neighboring even spins <em>via</em> the odd spin between them, the partition function splits cleanly:</p>
<p><span class="math display">\[
Z = \sum_{s_0, s_2, \dots} \left(\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\right) \left(\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\right) \cdots
\]</span></p>
<p>Thus, our wish would be fulfilled if <span class="math inline">\(\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}\)</span> for all 4 possible choices of <span class="math inline">\(s_0, s_2\)</span>. Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>+</th>
<th>-</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>+</td>
<td><span class="math inline">\(e^{J'} = 2 \cosh(2J)\)</span></td>
<td><span class="math inline">\(e^{-J'} = 2\)</span></td>
</tr>
<tr class="even">
<td>-</td>
<td><span class="math inline">\(e^{-J'} = 2\)</span></td>
<td><span class="math inline">\(e^{J'} = 2 \cosh(2J)\)</span></td>
</tr>
</tbody>
</table>
<p>Immediately we see a problem: if both <span class="math inline">\(e^{J'} = 2 \cosh(2J)\)</span> and <span class="math inline">\(e^{-J'} = 2\)</span> hold, then <span class="math inline">\(1 = 4 \cosh(2J)\)</span>. This means that we have to introduce a “fudge factor” again. Does that remind you of the <span class="math inline">\(\alpha\)</span> from the logistic map calculation? It should. Add in the fudge factor <span class="math inline">\(k\)</span> with:</p>
<p><span class="math display">\[
Z = k^{n} \sum_{s_0, s_2, \dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \dots + s_{2n-2}s_0)}, \quad \sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}
\]</span></p>
<p>Now the solution is: <span class="math inline">\(J' = \frac 12 \ln \cosh(2J), k = 2\sqrt{\cosh(2J)}\)</span>. Again, we see that the RN flow equation</p>
<p><span class="math display">\[
J \mapsto \frac 12 \ln \cosh(2J)
\]</span></p>
<p>But this time, the RN flow has only a single stable fixed point at <span class="math inline">\(J = 0\)</span>. Not only that, since <span class="math inline">\(\frac 12 \ln \cosh(2J) \approx J^2\)</span> if <span class="math inline">\(J\)</span> is small, if we decimate for <span class="math inline">\(n\)</span> times, we would end up with <span class="math inline">\(J' \sim J^{2^n}\)</span>. What does this mean?</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/rn_flow_1d_ising.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The RN flow for the 1D Ising model has only one fixed point at zero.</figcaption>
</figure>
</div>
<p>Suppose we start with a magnet with interaction strength <span class="math inline">\(J\)</span>, then after we zoom out for <span class="math inline">\(n\)</span> times, where <span class="math inline">\(n\)</span> is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites <span class="math inline">\(0, 2^n, 2\times 2^n, 3 \times 2^n, \dots\)</span>. Our RN flow calculation states that the strength of interaction between <span class="math inline">\(s_0\)</span> and <span class="math inline">\(s_{2^n}\)</span> are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for <span class="math inline">\(s_0\)</span> to interact with <span class="math inline">\(s_{2^n}\)</span> is if they laboriously go, bond-by-bond, across all the <span class="math inline">\(2^n\)</span> bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks… holding a paintbrush. You rapidly run out of all bonding strength.</p>
<p>In particular, it shows that no matter how strong <span class="math inline">\(J\)</span> is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This disappointment led Ising to abandon this model of magnetism back in 1925.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you are ambitious, you can try doing the same RN analysis for a “ladder” made of two <span class="math inline">\(\mathbb{Z}\)</span> put side-by-side. If you have “a lot of time” like Onsager, you would be <a href="#sec-note-onsager-ising-2d">on your way to solving the Ising model in 2 dimensions</a>. I have not worked this out, because I don’t have a lot of time, but it would involve a 4-by-4 matrix. See <a href="https://web.archive.org/web/20120713215413/http://micro.stanford.edu/~caiwei/me334/Chap12_Ising_Model_v04.pdf">page 12</a> for a sketch of solution.</p>
</div>
</div>
</section>
<section id="sec-kadanoff-decimation-2d-ising" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-kadanoff-decimation-2d-ising">Kadanoff decimation, take 1 (abortive)</h3>
<p>Now that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:</p>
<p><span class="math display">\[
Z = \sum_s e^{-H}, \quad H = -J \sum_{i, j \text{ are neighbors}} s_i s_j
\]</span></p>
<p>where again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Kadanoff decimation.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><a href="https://phas.ubc.ca/~berciu/TEACHING/PHYS502/PROJECTS/21-Jonah.pdf">Figure source</a></figcaption>
</figure>
</div>
<p>However, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from <span class="math inline">\((-1, 0)\)</span> to a nearest-neighbor <span class="math inline">\((0, +1)\)</span> via the to-be-decimated atom <span class="math inline">\((0, 0)\)</span>, but also go to the next-nearest neighbor <span class="math inline">\((+1, 0)\)</span>. Not only that, the square of 4 spins neighboring <span class="math inline">\((0, 0)\)</span> are connected via <span class="math inline">\((0, 0)\)</span>, so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:</p>
<p><span class="math display">\[
-H' = J_{nn} \sum_{i, j \text{ are nn}} s_i s_j + J_{nnn} \sum_{i, j \text{ are nnn}} s_i s_j + J_{\square} \sum_{i, j, k, l \text{ are }\square} s_i s_j s_k s_l
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Exact solution">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exact solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let the grid have <span class="math inline">\(N\)</span> sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case – it does not matter when <span class="math inline">\(N\)</span> is large. The partition function of the system is</p>
<p><span class="math display">\[
Z = \sum_{s \in \{-1, +1\}^\text{grid}} \exp\left(J \sum_{ i, j \text{ are nn}} s_i s_j\right)
\]</span></p>
<p>Now, decimate half of the grid, and leave behind the other half. As derived in <span class="citation" data-cites="marisTeachingRenormalizationGroup1978">(<a href="#ref-marisTeachingRenormalizationGroup1978" role="doc-biblioref">Maris and Kadanoff 1978</a>)</span>, we have</p>
<p><span class="math display">\[
Z = f(J)^{N/2} \sum_{s \in \{-1, +1\}^\text{decimated grid}} \exp\left(J_{nn} \sum_{ i, j \text{ are nn}} s_i s_j + J_{nn} \sum_{ i, j \text{ are nnn}} s_i s_j + J_{\square} \sum_{ i, j, k, l \text{ are }\square} s_i s_j s_k s_l\right)
\]</span></p>
<p>where <span class="math display">\[
\begin{aligned}
f(J) &amp;= 2 \cosh ^{1 / 2}(2 J) \cosh ^{1 / 8}(4 J) \\
J_{nn} &amp;= (1 / 4) \ln \cosh (4 J) \\
J_{nnn} &amp;= (1 / 8) \ln \cosh (4 J) \\
J_{\square} &amp;= (1 / 8) \ln \cosh (4 J)-(1 / 2) \ln \cosh (2 J)
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why don’t we have the triangle terms like <span class="math inline">\(J_\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}\)</span>?</p>
<p>Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.</p>
</div>
</div>
<p>Well, that’s fine, you might say, what’s the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet <em>infinitely many</em> interaction terms! How so?</p>
<p>Look at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, <span class="math inline">\((-1, 0)\)</span> and <span class="math inline">\((0, +1)\)</span> are mediated by <span class="math inline">\((0, 0)\)</span>, so they must have an interaction term. No problem, that’s the nn term, written as <span class="math inline">\(J_{nn}s_{(-1, 0)}s_{(0, +1)}\)</span>. Similarly, <span class="math inline">\((-1, 0)\)</span> and <span class="math inline">\((+1, 0)\)</span> are mediated by <span class="math inline">\((0, 0)\)</span>, so they must have an interaction term. That’s the nnn term, written as <span class="math inline">\(J_{nnn}s_{(-1, 0)}s_{(+1, 0)}\)</span>. Do you see it now?</p>
<p>We can connect <span class="math inline">\((-1, 0)\)</span> and <span class="math inline">\((3, 0)\)</span> via <span class="math inline">\((0, 0), (1, -1), (2, 0)\)</span>. Not only that, we can connect them by <em>infinitely many</em> possible routes: <span class="math inline">\((0, 0), (1, -1), (2, 0), (3, -1)\)</span>, and <span class="math inline">\((0, 0), (1, -1), (2, 0), (3, 1)\)</span>, etc. And that’s not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.</p>
<p>We can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.</p>
<blockquote class="blockquote">
<p>These calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager’s values to with­ in about 0.2%.</p>
<p><span class="citation" data-cites="wilsonProblemsPhysicsMany1979">(<a href="#ref-wilsonProblemsPhysicsMany1979" role="doc-biblioref">Wilson 1979</a>)</span></p>
</blockquote>
</section>
<section id="migdal-bond-moving-trick" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="migdal-bond-moving-trick">Migdal bond-moving trick</h3>
<p>Before we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Migdal bond-moving.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The Migdal bond-moving trick. <span class="citation" data-cites="kadanoffStatisticalPhysicsStatics1999">(<a href="#ref-kadanoffStatisticalPhysicsStatics1999" role="doc-biblioref">Kadanoff 1999b, fig. 14.2</a>)</span>.</figcaption>
</figure>
</div>
<p>We perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as <span class="math inline">\(J_x, J_y\)</span>. The RN flow of this step is</p>
<p><span class="math display">\[
(J_x, J_y) \mapsto \left(2 J_x, \frac 12 \ln \cosh(2 J_x)\right)
\]</span></p>
<p>After doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:</p>
<p><span class="math display">\[
(J_x, J_y) \mapsto (f_1(J_x), f_2(J_y))
\]</span></p>
<p>where <span class="math inline">\(f_1(x) = \ln\cosh(2x)\)</span> and <span class="math inline">\(f_2(x) = \frac 12 \ln\cosh(4x)\)</span>. This RN flow has saddle point <span class="math inline">\((0.609, 0.305)\)</span>. If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead <span class="math inline">\((J_x, J_y) \mapsto (f_2(J_x), f_1(J_y))\)</span>, with saddle point <span class="math inline">\((0.305, 0.609)\)</span>. Well, the true saddle point of the whole system should have equal <span class="math inline">\(J_x, J_y\)</span>, since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: <span class="math inline">\((0.609 + 0.305)/2 = 0.457\)</span>.</p>
<p>According to Onsager’s exact solution, the true critical point is <span class="math inline">\(J_c = \frac{\ln(1+\sqrt 2)}{2} = 0.4407\)</span>. So by this simple trick, we have already gotten within 3.7% of the true answer.</p>
<p>A small improvement is to find the “double fixed point”. That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to</p>
<p><span class="math display">\[x = f_1(f_2(x)); \quad x = f_2(f_1(x))\]</span></p>
<p>Now, these have different solutions, so we do the obvious thing and take their midpoint. This gives <span class="math inline">\(0.4417\)</span>. And with that simple idea, I got within 0.23% of the true answer.</p>
<p>We can also calculate the scaling exponent, like how we found <span class="math inline">\(\delta\)</span> for the logistic map. The average gradient at the fixed point is <span class="math inline">\(\frac{(f_1\circ f_2)' + (f_2\circ f_1)'}{2}\)</span> which is <span class="math inline">\(2.7633\)</span>, corresponding to a length scaling exponent of <span class="math inline">\(\nu = \frac{\ln 4}{\ln 2.7633} = 1.364\)</span>, which is well off the real value of 1.</p>
<div id="fig-migdal-rn-flow" class="quarto-layout-panel page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-migdal-rn-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="figure/Migdal_trick_ising.svg" class="img-fluid figure-img"></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="figure/Migdal_trick_ising_zoom.svg" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-migdal-rn-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Two RN flows on the <span class="math inline">\((J_x, J_y)\)</span> plane found by Migdal bond-moving, and the saddle points thereof.
</figcaption>
</figure>
</div>
</section>
<section id="sec-note-onsager-ising-2d" class="level3">
<h3 class="anchored" data-anchor-id="sec-note-onsager-ising-2d">Side note: Onsager’s solution of Ising model in 2D</h3>
<p>So far, we have been doing it for the Ising model on <span class="math inline">\(\mathbb{Z}\)</span>. But it’s clear that we can also do it for two <span class="math inline">\(\mathbb{Z}\)</span>s put side by side like a ladder. Each “rung” of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a <span class="math inline">\(2\times 2\)</span> matrix, we have a <span class="math inline">\(4 \times 4\)</span> matrix.</p>
<p>For that matter, we can do it for any finite number of those <span class="math inline">\(\mathbb{Z}\)</span> put together. We can then imagine doing that for such ladders with widths <span class="math inline">\(2, 3, 4, 5, 6, \dots\)</span>, then discover a pattern, and take the limit. If this works, we would solve the Ising model on <span class="math inline">\(\mathbb{Z}^2\)</span>.</p>
<p>Arduous as it sounds, this is exactly how Lars Onsager arrived at his solution for the Ising model on <span class="math inline">\(\mathbb{Z}^2\)</span>. He calculated up to ladders with width 6, diagonalizing matrices of size <span class="math inline">\(64\times 64\)</span> in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:</p>
<blockquote class="blockquote">
<p>In 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults….</p>
<p>In March, 1965… I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a <span class="math inline">\(2 \times \infty\)</span>, then a <span class="math inline">\(3 \times \infty\)</span>, then a <span class="math inline">\(4 \times \infty\)</span> lattice. He then went on to a <span class="math inline">\(5 \times \infty\)</span> lattice, for which the transfer matrix is <span class="math inline">\(32 \times 32\)</span> in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the <span class="math inline">\(6 \times \infty\)</span> case, and eventually diagonalized the <span class="math inline">\(64 \times 64\)</span> matrix, finding that all the eigenvalues were of the form <span class="math inline">\(e^{\pm \gamma_1 \pm \gamma_2 \pm \gamma_3 \pm \gamma_4 \pm \gamma_5 \pm \gamma_6}\)</span>. That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper.</p>
<p><span class="citation" data-cites="yangSelectedPapers194519802005">(<a href="#ref-yangSelectedPapers194519802005" role="doc-biblioref">Yang 2005, 11–13</a>)</span></p>
</blockquote>
<p>I also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:</p>
<blockquote class="blockquote">
<p>a long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result [spontaneous magnetization of the Ising model]</p>
<p><span class="citation" data-cites="yangSelectedPapers194519802005">(<a href="#ref-yangSelectedPapers194519802005" role="doc-biblioref">Yang 2005, 12</a>)</span></p>
</blockquote>
</section>
<section id="bonus-edgeworth-series" class="level3">
<h3 class="anchored" data-anchor-id="bonus-edgeworth-series">Bonus: Edgeworth series</h3>
<p>In a typical intermediate probability course taught to graduate students, the central limit theorem is taught in two lines: For any probability distribution with finite variance, check how its characteristic function varies under sum-then-average operations, and check that it converges to the characteristic function of the gaussian distribution.</p>
<p>This simple argument is the seeds to functional renormalization theory. This section is based on <span class="citation" data-cites="sethnaStatisticalMechanicsEntropy2021">(<a href="#ref-sethnaStatisticalMechanicsEntropy2021" role="doc-biblioref">Sethna 2021</a>, exercise 12.11)</span>.</p>
<p>If we have a sequence of random variables <span class="math inline">\(X_1, X_2, \dots\)</span>, then we can do the double-then-average trick repeatedly:</p>
<p><span class="math display">\[
X_1, \frac{X_1 + X_2}{\sqrt 2}, \frac{\frac{X_1 + X_2}{\sqrt 2} + \frac{X_3 + X_4}{\sqrt 2}}{\sqrt 2}, \dots
\]</span></p>
<p>Now, consider Fourier transforms to the probability density function</p>
<p><span class="math display">\[
\tilde\rho(k) := \int_\mathbb{R}e^{-ikx} \rho(x) dx
\]</span></p>
<p>If <span class="math inline">\(X, X'\)</span> are sampled from the pdf <span class="math inline">\(\rho\)</span>, then <span class="math inline">\(\frac{X+X'}{\sqrt 2}\)</span> has the pdf <span class="math inline">\(\rho'\)</span> such that</p>
<p><span class="math display">\[\tilde{\rho'}(k) = \tilde\rho(k/\sqrt 2)^2\]</span></p>
<p>Thus, we define an operator by <span class="math inline">\(R[\tilde f](k) = \tilde f(k/\sqrt 2)^2\)</span>, and the problem of central limit theorem is finding the fixed points of <span class="math inline">\(R\)</span> and their local stability.</p>
<div class="callout callout-style-default callout-note callout-titled" title="derivation">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
derivation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The pdf of <span class="math inline">\(\mathcal N(\mu, \sigma^2)\)</span> transforms to <span class="math inline">\(e^{-ik\mu - \frac 12 \sigma^2 k^2}\)</span>, so <span class="math inline">\(\mathcal N(0, \sigma^2)\)</span> is a fixed point of <span class="math inline">\(R\)</span>.</p>
<p>Let <span class="math inline">\(\rho^*\)</span> be the pdf for <span class="math inline">\(\mathcal N(0, 1)\)</span>, then the question is to find the local linear expansion of <span class="math inline">\(R\)</span> near <span class="math inline">\(\tilde \rho^*\)</span>. That is, we want to find the eigenvectors of <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[
R[\tilde \rho^* + \epsilon f] = \tilde \rho^* + \lambda\epsilon f
\]</span></p>
<p>Now, <span class="math inline">\(\tilde f_n := (ik)^n \tilde \rho^*(k)\)</span> has eigenvalue <span class="math inline">\(1/2^{\frac{n-2}{2}}\)</span>. Since these <span class="math inline">\(\tilde f_0, \tilde f_1, \dots\)</span> allow for Taylor expansion, these exhaust all possible eigenvectors of <span class="math inline">\(R\)</span> near <span class="math inline">\(\tilde \rho^*\)</span>.</p>
<p>Consider a generic function <span class="math inline">\((1+a_0) \tilde f_0 + a_1 \tilde f_1 + \cdots\)</span>. It corresponds to a pdf</p>
<p><span class="math display">\[
\rho = (1+a_0) f_0 + a_1 f_1 + \cdots
\]</span></p>
<p><span class="math display">\[
f_n(x) = \frac{1}{2\pi} \int_\mathbb{R}e^{ikx}\tilde f_n(k) dk = \partial_x^n \rho^*(x) = \rho^*(x) He_n(-x/\sigma)/\sigma^n
\]</span></p>
<p><span class="math display">\[
He_0 = 1, He_1 = x, He_2 = x^2 - 1, He_3 = x^3 - 3x, \dots
\]</span></p>
<p>Those are the <a href="https://en.wikipedia.org/wiki/Hermite_polynomials">probabilist’s Hermite polynomial</a>.</p>
<p>The only dangers to convergence are <span class="math inline">\(f_n\)</span> where <span class="math inline">\(n = 0, 1, 2\)</span>, since the other terms have eigenvalue less than 1. The effect of adding a small amount of <span class="math inline">\(f_0, f_1, f_2\)</span> to <span class="math inline">\(\rho^*\)</span> is to change it from <span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x^2}{2\sigma^2}}\)</span> to</p>
<p><span class="math display">\[
\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x^2}{2\sigma^2} + \ln a_0 - a_1 \frac{x}{\sigma^2} + a_2 \frac{x^2 - \sigma^2 }{\sigma^4}}
\]</span></p>
<p>So we see that the effect of <span class="math inline">\(a_0\)</span> is to normalize the probability mass to 1, the effect of <span class="math inline">\(a_1\)</span> is to shift the mean, and <span class="math inline">\(a_2\)</span> is to change the variance.</p>
</div>
</div>
</div>
<p>Therefore, if we always subtract the mean and dividing by the variance, then the first three perturbation terms <span class="math inline">\(f_0, f_1, f_2\)</span> are zeroed out, leaving the other terms decaying to zero.</p>
<p>Normalize <span class="math inline">\(X\)</span> to have mean 0 and variance 1, then we find that the pdf of <span class="math inline">\(2^{-n}\sum_{i=1}^{2^n}X_i\)</span> is</p>
<p><span class="math display">\[
\rho_{2^n} = \rho^*(1 + 2^{-n/2}a_3 He_3(-x) + 2^{-n}a_4 He_4(-x) + \cdots)
\]</span></p>
<p>where</p>
<p><span class="math display">\[e^{\frac 12 k^2}\tilde \rho(k) = 1 + a_3 (ik)^3+ a_4 (ik)^4 + \cdots\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="example: Bernoulli variable">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
example: Bernoulli variable
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The Bernoulli variable <span class="math inline">\(\frac 12(\delta_{-1} + \delta_{+1})\)</span> has</p>
<p><span class="math display">\[e^{\frac 12 k^2}\tilde \rho(k) = e^{\frac 12 k^2}\cos k = 1 - \frac{k^4}{12} - \frac{k^6}{45} + \cdots\]</span></p>
<p>giving</p>
<p><span class="math display">\[
a_4 = -\frac{1}{12}, a_6 = \frac{1}{45}, \dots
\]</span></p>
<p><span class="math display">\[
\rho_{2^n}(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac 12 x^2} \left[1 -\frac{1}{12} 2^{-n} He_4(-x) + \frac{1}{45} 2^{-2n} He_6(-x) + \cdots\right]
\]</span></p>
<p>The exact result is</p>
<p><span class="math display">\[
\rho_{2^n} = \sum_{i=0}^{2^n} \frac{1}{2^{2^n}} \binom{2^n}{i}  \delta_{\frac{-2^n + 2i}{2^{n/2}}}
\]</span></p>
<p>which reduces to the previous result by Stirling approximation.</p>
</div>
</div>
</div>
</section>
<section id="bonus-generalized-central-limit-theorem" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="bonus-generalized-central-limit-theorem">Bonus: Generalized central limit theorem</h3>
<p>This section is based on <span class="citation" data-cites="amirElementaryRenormalizationgroupApproach2020">(<a href="#ref-amirElementaryRenormalizationgroupApproach2020" role="doc-biblioref">Amir 2020</a>)</span>.</p>
<p>Consider three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto <span class="math inline">\(\mu=0.7\)</span>. We notice two facts:</p>
<ul>
<li>The random walks are self-similar. A small section has the same look-and-feel as a large section of it.</li>
<li>Different random walks have very different characters. The Gaussian walk appears smoother, while the Cauchy and Pareto walks display more dramatic jumps and bursts, reflecting the heavier tails of their respective distributions.</li>
</ul>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/generalized_random_walks_CLT.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto <span class="math inline">\(\mu=0.7\)</span></figcaption>
</figure>
</div>
<p>When we see self-similarity, we think RN. Can we use RN to study random walks? Yes.</p>
<p>Let’s take a fresh look at the central limit theorem. It says that if <span class="math inline">\(X_1, X_2, \dots\)</span> are IID samples from a distribution with finite mean <span class="math inline">\(E[X]\)</span> and variance <span class="math inline">\(V[X]\)</span>, then <span class="math inline">\(\frac{(X_1 + \dots + X_n) - n E[X]}{\sqrt{n V[X]}}\)</span> converges to the standard normal distribution. If we think about it from the RN point of view, we can decompose each <span class="math inline">\(X\)</span> into a sum of two random variables: <span class="math inline">\(X_i = A_i + Z_i\)</span>, where <span class="math inline">\(Z_i\)</span> is a normal distribution with the same mean and variance, and <span class="math inline">\(A_i\)</span> is the “noise” part of it. Each <span class="math inline">\(A_i\)</span> might be overpowering, but when we repeatedly coarse-grain by taking a bunch of <span class="math inline">\(X_i\)</span>, and adding them up (a lossy operation!), we would eventually destroy all traces of what cannot survive coarse-graining, and leaving behind a fixed-point of coarse-graining.</p>
<p>We define the following letters:</p>
<ul>
<li><span class="math inline">\(X_1, X_2, \dots\)</span> are IID random variables, with characteristic function <span class="math inline">\(\phi_X(t)= E[e^{itX}]\)</span>.</li>
<li><span class="math inline">\(S_n = X_1 + \dots + X_n\)</span>.</li>
<li><span class="math inline">\(a_n, b_n\)</span> are two sequences of real numbers, such that <span class="math inline">\(\frac{S_n - b_n}{a_n}\)</span> converges in distribution to a nontrivial random variable <span class="math inline">\(Z\)</span> with characteristic function <span class="math inline">\(\phi(t)\)</span>.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Deriving the field equation by RN">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deriving the field equation by RN
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Since <span class="math inline">\(\frac{X_1 + \dots + X_n - b_n}{a_n}\)</span> converges in distribution to a nontrivial random variable, the sequence <span class="math inline">\(a_n\)</span> must diverge to infinity. For, if the sequence <span class="math inline">\(a_n\)</span> is bounded, then for large enough <span class="math inline">\(n\)</span> , the sum <span class="math inline">\(X_1 + \dots + X_n\)</span> would spread wider and wider, and dividing it by <span class="math inline">\(a_n\)</span> cannot keep it together.</p>
<p>Let <span class="math inline">\(Z\)</span> be a random variable with characteristic function <span class="math inline">\(\phi\)</span>. By assumption, <span class="math inline">\((S_n- b_n)/a_n\)</span> is approximately distributed like <span class="math inline">\(Z\)</span> , that is, <span class="math inline">\(S_n\)</span> is approximately distributed as <span class="math inline">\(a_nZ + b_n\)</span>. Thus,</p>
<p><span class="math display">\[\phi_{S_n}(t) \approx e^{ib_n t}\phi(a_nt )\]</span></p>
<p>Given <span class="math inline">\(1 \ll n \ll N\)</span> , we can compute <span class="math inline">\(\phi_{S_N}\)</span> in two ways: adding it up as <span class="math inline">\(N\)</span> copies of <span class="math inline">\(X\)</span> , or adding it up as <span class="math inline">\(N/n\)</span> copies of <span class="math inline">\(S_n\)</span>. Both should give us the same result. That is: <span class="math display">\[\phi_{S_N} = \phi_X^N = \phi_{S_n}^{N/n}\]</span></p>
<p>However, since <span class="math inline">\(n\)</span> is very large, we have the approximations <span class="math inline">\(\phi_{S_n}(t) \approx e^{ib_n t}\phi(a_nt )\)</span>. Thus, we have</p>
<p><span class="math display">\[
\ln \phi_{S_N}(t) \approx \frac{N}{n}(ib_n t + \ln\phi(a_n t))
\]</span></p>
<p>Note how we have an exponent of the form <span class="math inline">\(Nf(n)\)</span> , where <span class="math inline">\(N\)</span> is a very large number, and <span class="math inline">\(n\)</span> is a number that is small compared to it. This is a common pattern in RN calculation.</p>
<p>Since <span class="math inline">\(n\)</span> is small compared to <span class="math inline">\(N\)</span> , but large compared to <span class="math inline">\(1\)</span> , we can pretend that it’s a continuous variable, and take derivative of it. Since the left side is independent of <span class="math inline">\(n\)</span> , the derivative should be zero:</p>
<p><span class="math display">\[
\partial_n \frac{N}{n}(ib_n t + \ln\phi(a_n t)) = 0
\]</span></p>
<p>Simplifying it, and substituting <span class="math inline">\(t\)</span> for <span class="math inline">\(a_n t\)</span> , we get the <strong>field equation</strong></p>
<p><span class="math display">\[\frac{\phi'(t)}{\phi(t)}t - \ln \phi(t) \frac{a_n}{n \partial_n  a_n} + it\partial_n (b_n/n) \frac{n}{\partial_n a_n} = 0\]</span></p>
</div>
</div>
</div>
<p>Thus, we have obtained the field equation:</p>
<p><span class="math display">\[\frac{\phi'(t)}{\phi(t)}t -\frac{a_n}{n \partial_n  a_n} \ln \phi(t)  + \frac{n\partial_n (b_n/n)}{\partial_n a_n} it = 0\]</span></p>
<p>which we can solve by standard mathematical analysis without any more use of RN, so we don’t do those. You can read <span class="citation" data-cites="amirElementaryRenormalizationgroupApproach2020">(<a href="#ref-amirElementaryRenormalizationgroupApproach2020" role="doc-biblioref">Amir 2020</a>)</span> if you are interested.</p>
<p>However, there is a problem: If we have a “field” equation, what is the “field”? Well, here is one way to think of it.</p>
<p>Imagine a line of atoms, at locations <span class="math inline">\(1, 2, 3, \dots\)</span>. Each atom has a height <span class="math inline">\(X_1, X_2, X_3, \dots\)</span>. Now, we can coarse-grain the system by a factor of <span class="math inline">\(4\)</span>, by defining</p>
<p><span class="math display">\[Y_1 = \frac{X_1 + \dots + X_4 - b_4}{a_4}, \quad Y_2 = \frac{X_5 + \dots + X_8 - b_4}{a_4}, \quad \dots\]</span></p>
<p>from which we can perform another coarse-graining by a factor of <span class="math inline">\(100\)</span>, ending up with a coarse-grain by a factor of <span class="math inline">\(400\)</span>. Now, if the system has a nontrivial scaling limit, then this should give us the same result as doing a coarse-graining by <span class="math inline">\(5\)</span>, then by <span class="math inline">\(80\)</span>, or first <span class="math inline">\(6\)</span> then <span class="math inline">\(67\)</span>. This is the RN argument we used here.</p>
<p>Now, since <span class="math inline">\(S_n \approx a_n Z + b_n\)</span>, we see that <span class="math inline">\(b_n\)</span> can be thought of as the coarse-grained height of the height field, and <span class="math inline">\(a_n\)</span> as the coarse-grained jaggedness of the height-field. Then, the field equation describes how the two numbers vary according to <span class="math inline">\(n\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Exercise: extreme value distribution">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise: extreme value distribution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The maximum of random variables often has a nontrivial scaling limit as well. That is, there exists some sequence <span class="math inline">\(a_n, b_n\)</span> such that <span class="math inline">\(\frac{\max(X_1, \dots, X_n) - b_n}{a_n}\)</span> converges to a nontrivial distribution with cumulative distribution function (CDF) <span class="math inline">\(F\)</span>.</p>
<p>Let <span class="math inline">\(F_X\)</span> be the CDF of <span class="math inline">\(X\)</span>; then we have <span class="math inline">\(F_{\max(X_1, \dots, X_N)}(t) = F_{\max(X_1)}(t)^{N}\)</span>. Now, derive the field equation by an RN argument.</p>
<p>Answer: <span class="math inline">\(\partial_n \frac 1n \ln F(\frac{t-b_n}{a_n}) = 0\)</span>.</p>
</div>
</div>
</div>
<p><img src="figure/banner/thumbnail_7.png" class="img-fluid"></p>
</section>
</section>
<section id="wilsons-nobel-prize-rn-in-momentum-space" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="wilsons-nobel-prize-rn-in-momentum-space">Wilson’s Nobel Prize: RN in momentum space</h2>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/firefox-renormalization-group-theory.jpeg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Evolution of Firefox icon under RN flow in frequency space. The high-spatial-frequency details are summed away, leaving low-frequency broad brushstrokes.</figcaption>
</figure>
</div>
<p><strong>RN in momentum space</strong>, or <strong>RN in frequency space</strong>, is the most widely used method of RN nowadays. It is called “momentum” because it was first done by quantum field theorists to study subatomic particles, and in quantum mechanics, the spatial frequency of a particle-wave is proportional its momentum: <span class="math inline">\(p = \hbar k\)</span>, for which <a href="https://en.wikipedia.org/wiki/Louis_de_Broglie">de Broglie</a> was awarded a Nobel Prize in 1929.</p>
<p>In the 1970s, <a href="https://en.wikipedia.org/wiki/Kenneth_G._Wilson">Kenneth Wilson</a> invented RN in momentum space and used it to solve many problems. He was awarded the 1982 Nobel Prize in Physics for this work.</p>
<p>Unfortunately, unlike RN in real space, RN in momentum space is extremely verbose, requiring pages and pages of symbols. Instead of subjecting you to the horrible experience, I will sketch out the big ideas only.</p>
<blockquote class="blockquote">
<p>There remained the possibility that there might be smaller but still infinite quantities left over. No one had the patience needed to calculate whether these theories were actually completely finite. It was reckoned it would take a good student two hundred years, and how would you know he hadn’t made a mistake on the second page? Still, up to 1985, most people believed that most supersymmetric supergravity theories would be free of infinities.</p>
<p><span class="citation" data-cites="hawkingUniverseNutshell2001">(<a href="#ref-hawkingUniverseNutshell2001" role="doc-biblioref">Hawking 2001, 52</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>To know for sure whether a Feynman diagram with three virtual graviton loops produces infinite quantities, we would need to evaluate <span class="math inline">\(10^{20}\)</span> terms. By five loops, a diagram spawns <span class="math inline">\(10^{30}\)</span> terms … The unitarity method has completely changed the situation … What would have taken the Feynman technique <span class="math inline">\(10^{20}\)</span> terms, we can now do with dozens. … we found that the 1980s era speculations were wrong. Quantities that seemed destined to be infinite are in fact finite. Supergravity is not as nonsensical as physicists thought. In concrete terms, it means that quantum fluctuations of space and time are much more innocuous in supergravity than previously imagined. If you ply us with fine wine, you might catch us speculating that some version of it might be the long sought quantum theory of gravity.</p>
<p><span class="citation" data-cites="bernLoopsTreesSearch2012">(<a href="#ref-bernLoopsTreesSearch2012" role="doc-biblioref">Bern, Dixon, and Kosower 2012</a>)</span></p>
</blockquote>
<section id="field-theory-continuous-ising-model" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="field-theory-continuous-ising-model">Field theory: continuous Ising model</h3>
<p>Arguably the first field theory was hydrodynamics. Working in the era just after Newton, Euler and Lagrange understood water as a block of infinitely many tiny mass-points. Because each point is so tiny, they do not study the velocity of individual particles of water, but study the velocity <em>field</em> of the entire block of water.</p>
<p>The Ising model, with its grid of spins, provides a clear example of this continuum limit. As the grid of spins grow large, the individuals blur together into a field, similar to deriving hydrodynamics from particle dynamics. Let’s take a concrete example, of Ising model on the square grid <span class="math inline">\(\mathbb{Z}^2\)</span>.</p>
<p>Initially, the system’s energy and partition function are represented as a sum over individual spins:</p>
<p><span class="math display">\[
Z = \sum_{s: \mathbb{Z}^2 \to \{-1, +1\}} e^{-H(s)}, \quad H(s) = -J \sum_{i, j \text{ are nearest neighbors}} s_i s_j
\]</span></p>
<p>Recall how, <a href="#sec-kadanoff-decimation-2d-ising">after two Kadanoff decimations</a>, all forms of spin-spin interactions are unlocked, and so we arrive at an energy function in the most general form:</p>
<p><span class="math display">\[
Z = \sum_{s: \mathbb{Z}^2 \to \{-1, +1\}} e^{-H(s)}, \quad H(s) = -\sum_{\text{configuration }C} \sum_{i_1, i_2, \dots \text{ are configured like }C} J_C s_{i_1}s_{i_2}\cdots
\]</span></p>
<p>We can convert the summation into an integral, and suggestively write it as <span class="math inline">\(Z = \int_{s: \mathbb{Z}^2 \to \{-1, +1\}} e^{-H(s)} ds\)</span>.</p>
<p>In the continuous limit, the discrete field of spins <span class="math inline">\(s: \mathbb{Z}^2 \to \{-1, +1\}\)</span> blurs into a continuous field of spins <span class="math inline">\(\phi: \mathbb{R}^2 \to \mathbb{R}\)</span>. The energy <span class="math inline">\(H(s)\)</span> becomes <span class="math inline">\(S[\phi]\)</span>, a functional<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> an <span class="math inline">\(\phi(x)\)</span> and its gradients. This gives us</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;A “functional” is nothing but a special kind of function. Specifically, it is a function of type <span class="math inline">\(\text{function} \to \text{number}\)</span>. We use square brackets in <span class="math inline">\(S[\phi]\)</span>, not round brackets like <span class="math inline">\(S(\phi)\)</span>, because it is conventional for functionals to use square brackets, not round brackets. It is written as <span class="math inline">\(S[\phi]\)</span> rather than <span class="math inline">\(H[\phi]\)</span>, and called “action”, because of some old historical usage in variational calculus (as in “the principle of least action”).</p></div></div><p><span class="math display">\[
Z = \int_{\mathbb{R}^2 \to \mathbb{R}} D[\phi] \; e^{-S[\phi]}
\]</span></p>
</section>
<section id="field-theory-in-general" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="field-theory-in-general">Field theory, in general</h3>
<p>Consider a mattress as an analogy. Let the state The state of the mattress is determined by the height and velocity of each point. That is, the energy function of the mattress comprises two quadratic terms: height of each point; height differences between nearby points. We assume the mattress points are massless, so that we have no kinetic energy. We can write the energy of the mattress schematically as</p>
<p><span class="math display">\[
\text{energy} = H(\phi) = \frac 12 K_0 \sum_i \phi_i^2 + \frac 12 K_1 \sum_{i, j \text{ are neighbors}} (\phi_i - \phi_j)^2
\]</span></p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/mattress_field_theory.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A mattress in space. The energy of the mattress is determined by the height of each point, and the height differences between neighboring points.</figcaption>
</figure>
</div>
<p>If the mattress is held in an atmosphere of temperature <span class="math inline">\(T\)</span>, then its state becomes uncertain, following a Boltzmann distribution, just like how a pollen’s position in hot water becomes uncertain, due to Brownian motion.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> The probability that you would find the mattress in state <span class="math inline">\(\phi\)</span> is then <span class="math inline">\(e^{-H(\phi)/T}/Z\)</span>, where <span class="math inline">\(Z\)</span> is the partition function again:</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;When statistical field theory gets too scary, I call it “hot water theory” to make it sound nicer.</p></div></div><p><span class="math display">\[
Z = \int d\phi e^{- H(\phi)/T}
\]</span></p>
<p>In the continuum limit, this analogy leads us to statistical field theory.</p>
<p>Calculating useful quantities within statistical field theory often involves complex mathematical techniques. This complexity arises from the need to compute the partition function, which involves integrating over all possible states of the system. In the case of the mattress analogy, the partition function involves an integral over all possible height fields, leading to the use of path integrals, written like <span class="math inline">\(\int_{\phi: \mathbb{R}^2 \to \mathbb{R}} D[\phi] e^{-S[\phi]/T}\)</span>.</p>
<p>Integrating over <span class="math inline">\(\mathbb{R}^{10^{23}}\)</span> would be bad enough, let along integrating over <span class="math inline">\(\mathbb{R}^{\mathbb{R}^{2}}\)</span>, and yet the miracle is that this can be done, and can be done in a way that matches experiments.</p>
<p>Statistical field theory employs various tricks to calculate the partition function or its limits. A common approach involves alternating between discrete and continuous representations of the field for calculations.</p>
<p>Interestingly, statistical field theory is almost isomorphic with quantum field theory. The general idea is that if you take the dimension of time in a quantum field theory over space of dimension <span class="math inline">\(n\)</span>, and do a substitution <span class="math inline">\(t \mapsto it\)</span>, you somehow end up with a statistical field theory over space of dimension <span class="math inline">\(n+1\)</span>. This is the <a href="https://en.wikipedia.org/wiki/Wick_rotation">Wick rotation</a>.</p>
<p>For example, the 1D Ising model is analogous to a particle in a double well. Whereas the single particle might switch from left to right after time <span class="math inline">\(\Delta T\)</span>, the Ising model chain might switch from <span class="math inline">\(+1\)</span> to <span class="math inline">\(-1\)</span> after space <span class="math inline">\(\Delta L\)</span>. Because of quantum tunneling, it is impossible to confine a particle in one side of the well – it will always jump to the other side, and the jumping probability is on the order of <span class="math inline">\(1 - e^{-kt}\)</span> for some constant <span class="math inline">\(k\)</span>. This corresponds to the fact that there is no way to “freeze” an Ising model in one dimension. Even at low temperatures, the system cannot be entirely frozen. Long stretches of “up” spins can suddenly flip to “down” spins, no matter how cold the chain gets. Similarly, in the quantum analogy, the particle can tunnel between the two wells, no matter how high the barrier between them gets.</p>
<p>Generally, 1D statistical fields lack phase transitions, just like how a single particle with finitely many states would always quantum-tunnel between states, no matter how cold it gets. Conversely, since the 2D Ising model can be frozen, we naturally suspect that the quantum field theory on one dimension should have some kind of phase transition.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;I don’t know what it is, but perhaps Bose–Einstein condensation? I mean, I just flipped through <span class="citation" data-cites="herbutModernApproachCritical2007">(<a href="#ref-herbutModernApproachCritical2007" role="doc-biblioref">Herbut 2007</a>)</span> and the book suggests that this is it. Don’t quote me on this.</p></div></div><p>You get another perspective by noting that magnetism requires spontaneous symmetry breaking: you have more spins pointing up than down, even though the underlying energy function does not distinguish up from down. No symmetry breaking, no phase transition. And since the quantum states of a single particle can always tunnel between each other, it cannot fall into a symmetry-breaking state. However, since magnets can exist in 2D, we see that <a href="#sec-more-is-different">spontaneous symmetry breaking</a> can occur for a 1D line of quantum particles evolving through time.</p>
</section>
<section id="rn-flow-in-the-space-of-field-theories" class="level3">
<h3 class="anchored" data-anchor-id="rn-flow-in-the-space-of-field-theories">RN flow in the space of field theories</h3>
<p>Think back to the Ising model on <span class="math inline">\(\mathbb{R}^2\)</span>. What is its action <span class="math inline">\(S[\phi]\)</span>? If we were mathematically omnipotent, then we can simply perform RN flow on the discrete Ising model, and just find its fixed point, which should hopefully tell us what <span class="math inline">\(S\)</span> is. But we can’t even perform a single RN flow. What to do?</p>
<p>Well, by <a href="#sec-universality">universality</a>, we can start with some very different discrete Ising model and end up with the same continuum limit after renormalizing enough times. Why can’t we start with some very different <em>continuous</em> Ising model, discretize it to a discrete Ising model, then renormalize it again until we are back to a continuous Ising model? And if we can do that, why can’t we renormalize directly in the space of all continuous Ising models? We can start with whatever Ising field theory we can write down, and then just repeatedly renormalize it. By universality, we will end up in an interesting place, no matter where we started.</p>
<p>But before we do that, we have to construct the space of possible Ising field theories. As Wilson would say, it is <a href="#sec-symmetries-determine-theory-space">all about the symmetries</a>.</p>
<p>There are two kinds of symmetries: the symmetry without, and the symmetry within. For the Ising field <span class="math inline">\(\phi: \mathbb{R}^2 \to \mathbb{R}\)</span>, we have Euclidean symmetry for <span class="math inline">\(\mathbb{R}^2\)</span>, and up-down symmetry for <span class="math inline">\(\mathbb{R}\)</span>. To ensure Euclidean geometry, the action <span class="math inline">\(S[\phi]\)</span> should not explicitly depend on the position. That is, if we take some <span class="math inline">\(\phi\)</span>, and translate it by <span class="math inline">\(\delta\)</span>, then we must have</p>
<p><span class="math display">\[
S[\phi] = S[x \mapsto \phi(x + \delta)]
\]</span></p>
<p>Similarly for reflections and rotations of the field. This shows that <span class="math inline">\(S\)</span> must involve only terms like <span class="math inline">\(\phi, \nabla \phi, \nabla^2 \phi, \nabla \phi \cdot \nabla \phi\)</span>, etc.</p>
<p>To account for the symmetry within – the up-down symmetry – we must have <span class="math inline">\(S[\phi] = S[-\phi]\)</span>. This shows that <span class="math inline">\(S\)</span> must have only even-ordered terms</p>
<p>Under these assumptions, you can convince yourself that the most generic form of <span class="math inline">\(S\)</span> is</p>
<p><span class="math display">\[
S[\phi] = \int d x\left[\frac{1}{2} \nabla \phi \cdot \nabla \phi+\frac{1}{2} \mu^2 \phi^2+g \phi^4+\cdots\right]
\]</span></p>
<p>where we removed an irrelevant constant term independent of <span class="math inline">\(\phi\)</span>, and picked the scale of length so that the coefficient for <span class="math inline">\(\frac{1}{2} \nabla \phi \cdot \nabla \phi\)</span> is one.</p>
<p>This is the space of all possible Ising field theories. Each Ising field theory is completely specified if we specify the real numbers <span class="math inline">\(\mu, g, \dots\)</span>. Doing RN would then consist of taking one Ising field theory specified by some <span class="math inline">\(\mu, g, \dots\)</span>, then renormalize it to some other Ising field theory specified by <span class="math inline">\(\mu', g', \dots\)</span>. We can then find the fixed points in the theory-space, and say, “These are the most interesting theories. Let’s calculate their properties, scaling exponents, and look-and-feel.”</p>
</section>
<section id="rn-in-momentum-space" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="rn-in-momentum-space">RN in momentum space</h3>
<p>Consider an Ising field <span class="math inline">\(\phi: \mathbb{R}^2 \to \mathbb{R}\)</span>. It is of course possible to directly renormalize the field in real space: we blur it a bit, then zoom out. However, this turns out to be very hard to calculate with. Instead, it is much easier to renormalize the field in frequency space.</p>
<p>To “blur and zoom out” in real space – what does it look like if we take a Fourier transform? It would look like we are removing some high-frequency vibrations, then expand the vibrations so that low-frequency vibrations become high-frequency vibrations.</p>
<p>Now we are ready to meet RN in momentum space.</p>
<p>Let <span class="math inline">\(\tilde \phi\)</span> denote the Fourier transform of a field. Now, solve<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> for <span class="math inline">\(\tilde S\)</span>, so that <span class="math inline">\(S[\phi] = \tilde S[\tilde\phi]\)</span> for all <span class="math inline">\(\phi\)</span> in theory-space.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;This is typically called “Fourier-transforming the operator”. For example, the Fourier transform of the gradient operator <span class="math inline">\(\nabla\)</span> is <span class="math inline">\(ik\)</span>, where <span class="math inline">\(k\)</span> is the wave vector.</p></div></div><p><span class="math display">\[
Z = \int D[\phi] e^{-S[\phi]} = \int D[\tilde\phi] e^{-\tilde S[\tilde\phi]}
\]</span></p>
<p>Next, we restrict the domain of integration from all frequencies, to only frequencies below an upper limit <span class="math inline">\(\Lambda\)</span>. This is called “frequency cut-off”<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;This frequency cut-off is often called “ultraviolet cutoff”, because we throw away all frequencies that are too high, and for light, ultraviolet light is high-frequency. That’s it. It started as a hack meant to remove the annoying infinities that crop up everywhere in quantum field theory. You might have heard mysterious whispers of how renormalization “cuts off infinities” and makes them “normal again”. In fact, this is where the horrendous name “re-normalization” came from. If I had any choice in the matter, I would have called it “re-scaling”. At least that would be more descriptive.</p>
<p>For that matter, “renormalization group theory” is also a terrible name. To an applied physicist, the name “group theory” is abstract, inspiring fear and uncertainty. To a mathematician, the name “group theory” is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining.</p>
<p>In modern QFT, this compromise has been converted into a triumph. Instead of a hack to remove infinities, it is now interpreted as a necessary fact about the world. To see high-frequency features in space, we have to be able to probe it. To probe it, we need to fire some high-frequency particles at a space. High-frequency particles have high energy, and the higher the energy gets, the more complicated the interaction gets, and in this way, the quantum field theory actually changes depending on our choice of frequency-cutoff.</p>
<blockquote class="blockquote">
<p>Wilson’s analysis takes just the opposite point of view, that any quantum field theory is defined fundamentally with a cutoff A that has some physical significance. In statistical mechanical applications, this momentum scale is the inverse atomic spacing. In QED and other quantum field theories appropriate to elementary particle physics, the cutoff would have to be associated with some fundamental graininess of spacetime, perhaps a result of quantum fluctuations in gravity. … whatever this scale is, it lies far beyond the reach of present-day experiments. The argument we have just given shows that this circumstance explains the renormalizability of QED and other quantum field theories of particle interactions. Whatever the Lagrangian of QED was at its fundamental scale, as long as its couplings are sufficiently weak, it must be described at the energies of our experiments by a renormalizable effective Lagrangian.</p>
<p><span class="citation" data-cites="peskinIntroductionQuantumField1995">(<a href="#ref-peskinIntroductionQuantumField1995" role="doc-biblioref">Peskin and Schroeder 1995, 402–3</a>)</span></p>
</blockquote>
</div></div><p><span class="math display">\[
Z \approx \int_{\tilde \phi(k) \text{ is nonzero only for }\|k \| \leq \Lambda} D[\tilde\phi] e^{-\tilde S[\tilde\phi]}
\]</span></p>
<p>Next, we pick some small number <span class="math inline">\(\epsilon\)</span>, and integrate away all frequencies on a shell:</p>
<p><span class="math display">\[
\begin{aligned}
Z &amp;\approx \int_{\tilde \phi^- \text{ is nonzero only for }\|k \| \leq (1-\epsilon)\Lambda} D[\tilde \phi^-] \left(\int_{\tilde\phi^+ \text{ is nonzero only for }(1-\epsilon)\Lambda \leq \|k \| \leq \Lambda} D[\tilde\phi^+] e^{-\tilde S[\tilde\phi^-, \tilde\phi^+]}\right) \\
&amp;\overset{\mathrm{hopefully}}{=} \int_{\tilde \phi^- \text{ is nonzero only for }\|k \| \leq (1-\epsilon)\Lambda} D[\tilde \phi^-] e^{-\tilde S'[\tilde \phi^-]}
\end{aligned}
\]</span></p>
<p>This gives us some renormalized action <span class="math inline">\(\tilde S'\)</span> in frequency space. Do an inverse Fourier transform to obtain <span class="math inline">\(S'\)</span>, then scale space down by <span class="math inline">\((1-\epsilon)\)</span>, to obtain the fully renormalized action <span class="math inline">\(S''\)</span>. The RN flow is then defined by</p>
<p><span class="math display">\[
S \mapsto S''
\]</span></p>
<p>Since <span class="math inline">\(\epsilon\)</span> is small, we can make it infinitesimal, to obtain the RN flow (a real flow this time!)</p>
<p><span class="math display">\[
S \mapsto S + \epsilon F[S]
\]</span></p>
<p>where <span class="math inline">\(F\)</span> denotes the RN flow field in theory-space. It is a functional of <span class="math inline">\(S\)</span>, since the flow field differs for each theory in theory-space.</p>
<p>This is as far as we are going to discuss the RN in momentum space. Any more and I would be writing a textbook on QFT, and you are better served by a proper textbook like <span class="citation" data-cites="zeeQuantumFieldTheory2023 zeeQuantumFieldTheory2010">(<a href="#ref-zeeQuantumFieldTheory2023" role="doc-biblioref">Zee 2023</a>, <a href="#ref-zeeQuantumFieldTheory2010" role="doc-biblioref">2010</a>)</span>, etc.</p>
</section>
<section id="bonus-how-to-publish-in-quantum-field-theory" class="level3">
<h3 class="anchored" data-anchor-id="bonus-how-to-publish-in-quantum-field-theory">Bonus: How to publish in quantum field theory</h3>
<ol type="1">
<li><p>Work through a textbook and learn RN in momentum space.</p></li>
<li><p>Learn group theory and group representation theory.</p></li>
<li><p>Write down many groups with some nice geometry, like <span class="math inline">\(SO(3)\)</span>.</p></li>
<li><p>Construct a group out of those. For example, <span class="math inline">\(G = SO(4) \rtimes SU(3) \times SU(2) \times U(1)\)</span>. The group <span class="math inline">\(G\)</span> should have around 10–20 dimensions, but if you are a string theory enthusiast, then 500 dimensions is perfectly fine.</p></li>
<li><p>Construct another group <span class="math inline">\(H\)</span>.</p></li>
<li><p>Pick a nice space <span class="math inline">\(X\)</span>. For example, <span class="math inline">\(X = \mathbb{R}^4 \times \mathbb{C}^6\)</span>. It must be a space that <span class="math inline">\(H\)</span> can act upon.</p></li>
<li><p>Pick another space <span class="math inline">\(Y\)</span>. It must be a space that <span class="math inline">\(G\)</span> can act upon.</p></li>
<li><p>Construct the most generic possible functional of type <span class="math inline">\(S: (X \to Y) \to \mathbb{C}\)</span> that is still compatible with the two symmetry groups <span class="math inline">\(G, H\)</span>.</p></li>
<li><p>Spend the next month doing RN calculations about <span class="math inline">\(Z := \int_{\phi: X \to Y} D[\phi] \; e^{-S[\phi]}\)</span>, probably with Feynman diagrams scribbled everywhere.</p></li>
<li><p>Type it up in LaTeX.</p></li>
<li><p>Suffer through peer review, or just put it up on arXiv.</p></li>
</ol>
<p><img src="figure/banner/thumbnail_4.png" class="img-fluid"></p>
</section>
</section>
<section id="a-bag-of-intuitions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-bag-of-intuitions">A bag of intuitions</h2>
<p>After the long, hard slog at mathematics, we can take a break, take stock of what we have learned, and make some philosophical reflections. I guarantee that you will find at least one sentence here that you can proclaim with style at a party.</p>
<section id="sec-power-law-two-exponential-parents" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-power-law-two-exponential-parents">Power laws are born of two exponential parents</h3>
<p>Why is it that a critical point is typically surrounded by a power law? One intuition is that at a critical point, two exponentials are matched exactly – an exponentially decaying interaction strength and an exponentially increasing number of interaction paths – and a power law is born in their collision.</p>
<p>Consider the Ising model on the plane. Fix an origin <span class="math inline">\(0\)</span> , and we ask, how strong is the correlation between the origin <span class="math inline">\(0\)</span> and a point that is at distance <span class="math inline">\((n, n)\)</span> away from the origin, where <span class="math inline">\(n\)</span> is large?</p>
<p>Well, the two points are correlated because they are connected by chains of spins. The more chains there are, the stronger the correlation, but the longer each chain is, the weaker the correlation. Since the correlation along each chain is exponentially weak, we can crudely pretend that all the correlations can be added.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> As a good approximation, we consider only the shortest chains, which are of length <span class="math inline">\(2n\)</span>. By Stirling approximation, there are <span class="math inline">\({2n \choose n} \sim \frac{4^{n}}{\sqrt{n\pi }}\)</span> such chains. We can think of spin at origin as <span class="math inline">\(x_{(0,0)} + z_1 + z_2 + \cdots\)</span>, and the spin at <span class="math inline">\((n, n)\)</span> as <span class="math inline">\(x_{(n,n)} + z_1 + z_2 + \cdots\)</span>, where <span class="math inline">\(z_1, z_2,...\)</span> are random variables that are responsible for creating the correlations between the two spins along each chain.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Every biologist knows intuitively that weak correlations are additive. This is why, for instance, we can predict height accurately by a simple linear sum of the genes correlated with height, ignoring pairwise, triple-wise, and higher-order interactions. Indeed, there is a common pattern in biology: If you have a developmental process of type <span class="math inline">\(\text{many genes}\to \mathbb{R}\)</span>, where <span class="math inline">\(\mathbb{R}\)</span> stands for some kind of real-valued trait, like probability of diabetes, then the developmental process is pretty much just a linear map. <a href="https://en.wikipedia.org/wiki/Epistasis">Epistasis</a>, defanged. Heritability, vindicated.</p>
<p>Why? One possibility is that “even nature does not laugh at the difficulties of integration”. That is, even natural evolution will be hopelessly confused by a fully nonlinear developmental process, so such a process is unadaptive, <a href="https://en.wikipedia.org/wiki/Evolvability">un-evolvable</a>, a tightly woven ball of spaghetti code where a single base-pair change collapses the whole thing. No, evolution favors those that are evolvable, which usually means a linear function.</p></div></div><p>Now, each chain contributes a weak correlation that decays exponentially with distance. We can assume the chains do not interact. Along each chain, we have a 1D Ising model. The covariance between two neighboring spins is</p>
<p><span class="math display">\[Cov(s_0, s_1) = E[s_0s_1] - \underbrace{E[s_0]E[s_1]}_{\text{=0}} = Pr(s_0 = s_1 ) - Pr(s_0 \neq s_1 ) = \tanh(\beta J)\]</span></p>
<p>Now, we need a trick.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> If you think a bit, you would see that whether <span class="math inline">\(s_0 = s_1\)</span> is independent of whether <span class="math inline">\(s_1 = s_2\)</span>. Thus,</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;If you don’t like the trick, then you can use the transfer matrix method. We have no use for the transfer matrix, so we don’t do it.</p></div></div><p><span class="math display">\[Cov(s_0, s_2) = E[s_0 s_2] = E[(s_0 s_1) (s_1 s_2)] = Cov(s_0, s_1) Cov(s_1, s_2) = \tanh(\beta J)^2\]</span></p>
<p>And since the chain has length <span class="math inline">\(2n\)</span>, the correlation contributed by the chain is <span class="math inline">\(\tanh^{2n}(\beta J)\)</span>. The total correlation is</p>
<p><span class="math display">\[\sim \frac{4^{n}}{\sqrt{n\pi }} \tanh^{2n}(\beta J)\]</span></p>
<p>The two terms are exactly balanced when <span class="math inline">\(\beta J = \tanh^{-1}(1/2) = 0.549\dots\)</span>. In fact, the exact result is <span class="math inline">\(\beta J = 0.44\dots\)</span> , so our crude estimate is only 25% too high.</p>
<p>Now, right at the critical point, the correlation is <span class="math inline">\(\sim (n\pi)^{-1/2}\)</span> , so we see that when the exponential decay in correlation is exactly matched with the exponential growth in possible paths, the remaining polynomial decay comes to the fore. Notice that we have also estimated one of the Ising critical exponents: <span class="math inline">\(\nu = 1/2\)</span>. The actual answer is <span class="math inline">\(1\)</span>.</p>
<p>Similarly, with <span class="math display">\[\binom{kn}{n, \dots n}\sim \frac{k^{kn}}{n^{\frac{k-1}2}}\frac{k^{1/2}}{(2\pi)^{\frac{k-1}2}}\]</span></p>
<p>we can estimate that the Ising model in <span class="math inline">\(\mathbb{Z}^k\)</span> has a critical <span class="math inline">\(\beta J \approx \tanh^{-1}(1/k)\)</span> and critical exponent <span class="math inline">\(\nu = \frac{k-1}2\)</span>. It turns out that for all dimensions <span class="math inline">\(\geq 4\)</span>, we have the exact result of <span class="math inline">\(\nu = 1/2\)</span>. This can be derived by “mean field theory”, which we are not going to discuss.</p>
<section id="stevens-power-law" class="level4">
<h4 class="anchored" data-anchor-id="stevens-power-law">Stevens’ power law</h4>
<p>As a side note, this “two exponents lead to a power law” is known in psychophysics as Stevens’ power law <span class="citation" data-cites="stevensNeuralEventsPsychophysical1970">(<a href="#ref-stevensNeuralEventsPsychophysical1970" role="doc-biblioref">Stevens 1970</a>)</span>.</p>
<p>Consider the case where the brain needs to respond to the presence of a stimulus (e.g., a sound, a smell, etc.) with intensity <span class="math inline">\(I\)</span>. The response intensity (such as in the height of jumping, or a verbal report, or wincing of the face) is <span class="math inline">\(R\)</span>. Stevens found that for many kinds of stimulus, <span class="math inline">\(R \propto I^k\)</span> for some exponent <span class="math inline">\(k\)</span> that depends on the type of stimulus and response.</p>
<p>Stevens conjectured that the number of neurons firing <span class="math inline">\(N\)</span> is proportional to the log of intensity of stimulus <span class="math inline">\(I\)</span>, and that <span class="math inline">\(N\)</span> is also proportional to the log of intensity of response <span class="math inline">\(R\)</span>. Thus, we have</p>
<p><span class="math display">\[
k_I \ln I = N = k_R \ln R
\]</span></p>
<p>for two constants <span class="math inline">\(k_I, k_R\)</span>, which implies that <span class="math inline">\(R = I^{k_I/k_R}\)</span>, a power law. In this way, a small number of neurons can allow us to perceive and react to a wide range of stimuli intensities – for example, the physical brightness between noon and a starlit moonless night is more than <span class="math inline">\(10^{8}\)</span>, and yet the optical nerve, with only <span class="math inline">\(10^{6}\)</span> neurons <span class="citation" data-cites="evangelouAnatomyRetinaOptic2016">(<a href="#ref-evangelouAnatomyRetinaOptic2016" role="doc-biblioref">Evangelou and Alrawashdeh 2016</a>)</span>, can comfortably accommodate them both.</p>
<blockquote class="blockquote">
<p>At the Ciba Symposium in 1966, there was a general discussion on the topic “Linearity of transmission along the perceptual pathway”. In that discussion, and elsewhere at the symposium, Sir John Eccles turned forceful attention to the question of whether the sense organ could adequately account for the nonlinearity in the coupling between stimulus and sensation, leaving the central nervous system with the task of performing only linear transformations. He observed that “there is no great impediment to the idea that… the transfer functions across the synaptic mechanism are approximately linear.” To which Professor Mountcastle added, “The interesting point for me here is the great importance that we must now place upon the transducer process itself, at the periphery.”</p>
<p><span class="citation" data-cites="stevensNeuralEventsPsychophysical1970">(<a href="#ref-stevensNeuralEventsPsychophysical1970" role="doc-biblioref">Stevens 1970</a>)</span></p>
</blockquote>
</section>
</section>
<section id="rn-is-a-journey-in-the-space-of-possible-theories" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="rn-is-a-journey-in-the-space-of-possible-theories">RN is a journey in the space of possible theories</h3>
<p>So far, we have seen again and again the common refrain of “the space of theories” and “moving to another theory”. It is time to make this clear. The space of possible theories is defined by the symmetry of the physical system, and the Renormalization Group (RG) flow defines a journey in this space.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/fisher_1998_fig_4.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="fisherRenormalizationGroupTheory1998">(<a href="#ref-fisherRenormalizationGroupTheory1998" role="doc-biblioref">Fisher 1998, fig. 4</a>)</span></figcaption>
</figure>
</div>
<p>Consider a generic RG flow in a generic space of theories. diagram for a system with two coupling constants <span class="math inline">\(K\)</span> and <span class="math inline">\(y\)</span>. Each point in the diagram represents a theory, and the arrows indicate the direction of the RG flow. The fixed points, where the arrows converge, correspond to theories that are scale-invariant.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/fisher_1998_fig_5.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="fisherRenormalizationGroupTheory1998">(<a href="#ref-fisherRenormalizationGroupTheory1998" role="doc-biblioref">Fisher 1998, fig. 5</a>)</span></figcaption>
</figure>
</div>
<p>The above figure shows the RG flow for the Ising model, a simple model of ferromagnetism. The fixed points correspond to the paramagnetic phase (<span class="math inline">\(K = 0\)</span>) and the ferromagnetic phase (<span class="math inline">\(K = K_c\)</span>). The critical point, where the two phases meet, is at <span class="math inline">\(K = K_c\)</span>.</p>
<p>The RG flow provides a way to understand the behavior of a system at different length scales. As we zoom out, the system flows towards a fixed point. The fixed point describes the long-distance behavior of the system.</p>
<p>For example, in the Ising model, as we zoom out, the system flows towards either the paramagnetic or the ferromagnetic fixed point, depending on the initial value of <span class="math inline">\(K\)</span>. If <span class="math inline">\(K &lt; K_c\)</span>, the system flows towards the paramagnetic fixed point, and the spins become disordered at long distances. If <span class="math inline">\(K &gt; K_c\)</span>, the system flows towards the ferromagnetic fixed point, and the spins become ordered at long distances.</p>
<p>What defines the space of possible theories? The symmetry of the physical system.</p>
</section>
<section id="sec-symmetries-determine-theory-space" class="level3">
<h3 class="anchored" data-anchor-id="sec-symmetries-determine-theory-space">Symmetries determine the shape of theory-space</h3>
<p>The Ising model on <span class="math inline">\(\mathbb{Z}^2\)</span> is not a single theory, but an entire infinite-dimensional space of possible theories. Each Ising model can be specified by all coupling strengths for all possible spin configurations – nearest neighbors, next-nearest neighbors, four in a square, etc. However, they must follow the symmetries. We can’t have three-in-a-triangle, because switching up and down gives us the same energy. We can’t have <span class="math inline">\(Js_{(0, 0)}s_{(1, 0)}\)</span> different from <span class="math inline">\(J's_{(1, 0)}s_{(2, 0)}\)</span>, because translating the whole plane by <span class="math inline">\((1, 0)\)</span> gives us the same energy.</p>
<p>This is true in general: Symmetries determine the shape of theory-space.</p>
<p>Conversely, if two physical systems are constrained under the same symmetries, then their behavior are the same near the critical point, because their renormalization flows are the same.</p>
<p>The following table shows many theory-spaces with their corresponding symmetries.</p>
<table class="caption-top table">
<caption>Table of theory-spaces with their corresponding symmetries. Reproduced from <span class="citation" data-cites="wilsonProblemsPhysicsMany1979">(<a href="#ref-wilsonProblemsPhysicsMany1979" role="doc-biblioref">Wilson 1979</a>)</span>.</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(d\)</span>, the underlying space’s dimensions</th>
<th><span class="math inline">\(n\)</span>, the internal degrees of freedom</th>
<th>Theoretical Model</th>
<th>Physical System</th>
<th>Order Parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>1</td>
<td>Two dimensions</td>
<td>Adsorbed films</td>
<td>Surface density</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td>XY model in two dimensions</td>
<td>Helium-4 films</td>
<td>Amplitude of superfluid phase</td>
</tr>
<tr class="odd">
<td></td>
<td>3</td>
<td>Heisenberg model in two dimensions</td>
<td></td>
<td>Magnetization</td>
</tr>
<tr class="even">
<td>&gt;2</td>
<td>∞</td>
<td>“Spherical” model</td>
<td>None</td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>0</td>
<td>Self-avoiding random walk</td>
<td>Conformation of long-chain polymers</td>
<td>Density of chain ends</td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td>Ising model in three dimensions</td>
<td>Uniaxial ferromagnet</td>
<td>Magnetization</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td>Fluid near a critical point</td>
<td>Density difference between phases</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>Mixture of liquids near consolute point</td>
<td>Concentration difference</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td>Alloy near order-disorder transition</td>
<td>Concentration difference</td>
</tr>
<tr class="even">
<td></td>
<td>2</td>
<td>XY model in three dimensions</td>
<td>Planar ferromagnet</td>
<td>Magnetization</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td>Helium 4 near superfluid transition</td>
<td>Amplitude of superfluid phase</td>
</tr>
<tr class="even">
<td></td>
<td>3</td>
<td>Heisenberg model in three dimensions</td>
<td>Isotropic ferromagnet</td>
<td>Magnetization</td>
</tr>
<tr class="odd">
<td>≤4</td>
<td>-2</td>
<td></td>
<td>None</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>32</td>
<td>Quantum chromodynamics</td>
<td>Quarks bound in protons, neutrons, etc.</td>
<td></td>
</tr>
</tbody>
</table>
<p>As a particular example, with <span class="math inline">\(d=3, n=1\)</span>, we see that water-vapor mixture has the same critical behavior as copper-zinc alloy, or Ising model on three dimensions. We can think of vapor as just up-spin, and liquid as just down-spin. Next time you find yourself meditating upon a magnet, try reimagining it as a kettle of water, about to boil over.</p>
<table class="caption-top table">
<caption>Examples of <span class="math inline">\(d=3, n=1\)</span> systems.</caption>
<thead>
<tr class="header">
<th>physical system</th>
<th>site types</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>uniaxial magnet</td>
<td>up / down</td>
<td></td>
</tr>
<tr class="even">
<td>fluid</td>
<td>has atom / no atom</td>
<td></td>
</tr>
<tr class="odd">
<td>brass crystal</td>
<td>zinc / copper</td>
<td></td>
</tr>
<tr class="even">
<td>simple lattice field theory</td>
<td>has particle / no particle</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="droplets-inside-droplets" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="droplets-inside-droplets">Droplets inside droplets</h3>
<blockquote class="blockquote">
<p>Big whirls have little whirls that feed on their velocity,<br>
and little whirls have lesser whirls and so on to viscosity.</p>
</blockquote>
<p>I <em>especially</em> recommend playing with the <a href="https://www.ibiblio.org/e-notes/Perc/ising1k.htm">interactive Ising model</a> by <a href="https://www.ibiblio.org/e-notes/Perc/contents.htm">Evgeny Demidov</a> while reading this section.</p>
<p>We can interpret an Ising model over <span class="math inline">\(\mathbb{Z}^3\)</span> as a mixture of liquid and gaseous water. Each site can either be in a liquid state or in a vapor state. We start below the critical temperature, so that the bonding strength <span class="math inline">\(J\)</span> is just slightly larger than the critical <span class="math inline">\(J_c\)</span>. The system must make a free choice to make between being a liquid ocean with tiny islands of liquid or a vapor ocean with tiny islands of liquid. Both choices are equally good in our toy model. Since liquid wants to be in contact with liquid and vapor with vapor, the system must decide. For the sake of intuition, we say that we have an ocean of liquid with tiny droplets of vapor inside.</p>
<p>Now we increase the temperature towards the critical temperature. As we get hotter, thermal fluctuations become larger. In the ocean of liquid, a thermal fluctuation produces a droplet of vapor. This droplet secretes other material of the same density, and it grows larger and larger.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/kadanoff_1999_fig_1_3.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Vapor droplets inside an ocean of liquid. <span class="citation" data-cites="kadanoffOrderChaosII1999">(<a href="#ref-kadanoffOrderChaosII1999" role="doc-biblioref">Kadanoff 1999a, 298</a>)</span></figcaption>
</figure>
</div>
<p>However, as criticality is approached, the energetic cost in making a fluctuation approaches zero. The energetic penalty in creating droplets gets smaller and smaller. So, the droplets can grow large, until it becomes infinite right at the critical point.</p>
<p>Furthermore, each droplet itself is a nearly-critical system, so fluctuations appear within the droplets, in a delicate fractal.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/kadanoff_1999_fig_1_4.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Droplets inside droplets inside droplets… <span class="citation" data-cites="kadanoffOrderChaosII1999">(<a href="#ref-kadanoffOrderChaosII1999" role="doc-biblioref">Kadanoff 1999a, 299</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="sec-more-is-different" class="level3">
<h3 class="anchored" data-anchor-id="sec-more-is-different">More is different</h3>
<p>Like Wilson, <a href="https://en.wikipedia.org/wiki/Philip_W._Anderson">Philip Anderson</a> is a physicist who has won a Nobel Prize for his work on surprising things that happen when many particles interact. His Nobel Prize was awarded for his work on magnetism and “<a href="https://en.wikipedia.org/wiki/Anderson_localization">Anderson localization</a>”. Consider a pure piece of metal. An electron wave can vibrate freely across it from side to side, like a wave on a perfectly uniform infinite ocean. Now if we dope the metal with impurities, like planting wave-breakers in the ocean, then as the amount of impurity increases, an electron wave would suddenly become trapped, and the metal would become an insulator.</p>
<p>In a famous paper <span class="citation" data-cites="andersonMoreDifferentBroken1972">(<a href="#ref-andersonMoreDifferentBroken1972" role="doc-biblioref">Anderson 1972</a>)</span>, he interprets many-body physics like a philosopher (like what I’ve been doing in this whole section). He called the “constructionist hypothesis” the view that science divides neatly into fundamental laws and applications of those. He countered this view by proposing that scale and complexity create distinct stages in nature. Each stage necessitates new laws, concepts, and generalizations, and requires just as much ingenuity as the other stages. For example, psychology is not merely applied biology, biology is not simply applied chemistry, and condensed matter physics is not only “device engineering”.</p>
<p>Anderson proposed that new stages appear because of “broken symmetries”. Consider the sugar molecule. Though the sugar molecule is governed by quantum mechanics, which does not distinguish left from right, in our world we mostly only see sugar in one chirality. Sometime in the distant past, the symmetry was broken, and we are living in the consequences of its history.</p>
<p>More concretely, consider cars driving on a road. Why do we drive on the right instead of the left? Left or right, it’s better (“lower energy”) if everyone agrees. If people disagree, then it’s chaos (“higher energy”), so in the past, a coin was flipped, and we are here. In a parallel universe, we are driving on the left instead of the right. This is clear in the many-worlds interpretation of quantum mechanics:</p>
<p><span class="math display">\[
\ket{\Psi_{\text{world}}} = \frac{1}{\sqrt{2}} \ket{\Psi_{\text{world where we drive on the left}}} + \frac{1}{\sqrt{2}} \ket{\Psi_{\text{world where we drive on the right}}}
\]</span></p>
<p>The state of the entire multiverse <span class="math inline">\(\ket{\Psi_{\text{world}}}\)</span> is symmetric, but any observer has to fall into one of the sub-states, where the symmetry no longer holds.</p>
<p>However, Anderson pointedly did not explain what allows the stages to have laws largely independent of each other. Suppose that we have explained how some symmetries of the quantum-mechanical level break on the biochemistry level; we still have many questions. Why does the biochemistry level still have simple laws? And why these biochemical laws, but not others? Why are quantum mechanical laws and the everyday physics of frogs and cats so different, without the laws “bleeding into each other”?</p>
</section>
<section id="mesophysics-why-are-things-interesting-between-the-large-and-the-small" class="level3">
<h3 class="anchored" data-anchor-id="mesophysics-why-are-things-interesting-between-the-large-and-the-small">Mesophysics: Why are things interesting between the large and the small?</h3>
<p>The space of all theories is big, but most of it is rather uninteresting. Consider the humble Ising model on <span class="math inline">\(\mathbb{Z}^2\)</span>. Too much interaction and you get a block of spins all pointing in one direction. Too little interaction and you get a gas of spins pointing noisily in all directions. Only right at the critical point do we get interesting behavior in all possible scales. Not only that, the critical point is very delicate.</p>
<p>If you take an Ising model at <span class="math inline">\(J\)</span> that is just a bit above <span class="math inline">\(J_c\)</span> and zoom out, then by the RN flow equation, the effective <span class="math inline">\(J\)</span> would keep increasing, and it becomes more and more uniform until it’s a perfect shade of black/white at <span class="math inline">\(J = +\infty\)</span>. Conversely, if you start with <span class="math inline">\(J\)</span> slightly below <span class="math inline">\(J_c\)</span> and keep zooming out, <span class="math inline">\(J\)</span> would approach <span class="math inline">\(0\)</span> and everything would become a uniform shade of gray, with the spins pointing up and down with no regard for any other spin. Balanced on a knife’s edge is <span class="math inline">\(J = J_c\)</span>, where there is interesting behavior at all levels of zooming.</p>
<figure class="figure">
<video controls="" width="100%">
<source src="figure/renormalization_group_douglas_ashton.webm" type="video/webm">
</video>
<figcaption>
Zooming in and out of the Ising model. Video by Douglas Ashton, taken from <a href="https://blog.dougmet.net/2012/04/the-renormalisation-group/">The Renormalisation Group | dougmet-dot-net</a>.
</figcaption>
</figure>
<p>But what keeps <span class="math inline">\(J = J_c\)</span>?</p>
<p>Why is it that we are surprised by the quantum mechanics in the microscopic world? Because daily life in the mesoscopic world does not betray its origin from the microscopic world. The details has been renormalized away. But if that’s the case, how come our world is neither a homogeneous block of spins all pointing up nor a hot mess of spins unrelated to every other spin? Why is the mesoscopic world interesting?</p>
<p>Look around you. The world is interesting, with power laws, fractal patterns, and details at all scales. You never see a pencil standing on its end without a hand keeping it there. What keeps the mesoscopic world in its critical place?</p>
<p>One answer is that most of the interestingness did not come from criticality. However, if there exists <em>some</em> criticality in the mesoscopic world, and there does not seem to be an intelligent agent keeping the criticality there, then we have a mystery. This is the question that launched a thousand papers, including the famed <a href="https://en.wikipedia.org/wiki/Self-organized_criticality">self-organized criticality</a> paper <span class="citation" data-cites="bakSelforganizedCriticalityExplanation1987">(<a href="#ref-bakSelforganizedCriticalityExplanation1987" role="doc-biblioref">Bak, Tang, and Wiesenfeld 1987</a>)</span>, itself launching a thousand papers. The idea is typically illustrated by the forest fire model.</p>
<p>Consider the standard percolation model on a square grid. Each point might be occupied (a “tree” grows there) or unoccupied (empty plot of land). Randomly, lightning falls, and if it hits a tree, the tree catches on fire and the fire spreads to any neighboring trees. The process ends when there are no more burning trees.</p>
<p>As the proportion <span class="math inline">\(p\)</span> of a site being occupied (“tree density”) changes, we see a phase transition. For low <span class="math inline">\(p &lt; p_c\)</span>, there is no percolation, and so the fire quickly dies out. For high <span class="math inline">\(p &gt; p_c\)</span>, there is percolation, and so the fire spreads across the entire grid. The process automatically balances the system at the critical value <span class="math inline">\(p_c\)</span>, the system is poised between these two regimes, and the burnt-out patches can be of any size, following a power-law distribution. In this way, the delicate critical point in the standard percolation model has been transformed to a robust critical point.</p>
<blockquote class="blockquote">
<p>Nature shows an amazing variety of length scales: There is the Hubble radius of the universe, <span class="math inline">\(10^{10}\)</span> light years or so and the radius of our own solar system, <span class="math inline">\(10^{11}\)</span> meters roughly, and us-two meters perhaps, and an atom <span class="math inline">\(-10^{-10}\)</span> meters in radius, and a proton <span class="math inline">\(10^{-16}\)</span> meters, and the characteristic length of quantum gravity-which involves another factor of about <span class="math inline">\(10^{20}\)</span>.</p>
<p>How these vastly different lengths arise is a very interesting and fundamental question…. However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks [at] any event which takes place between the scale of the lattice constant [the spacing between molecules or spins] and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths. <span class="citation" data-cites="kadanoffStatisticalPhysicsStatics1999">(<a href="#ref-kadanoffStatisticalPhysicsStatics1999" role="doc-biblioref">Kadanoff 1999b, 251</a>)</span></p>
</blockquote>
<p>In a footnote, Kadanoff suggested that self-organized criticality might explain forms of stable criticality we see around us. Kadanoff wrote the words in 1999, near the end of the 1980s–90s chaos theory boom. Since then, the self-organized criticality theory has fallen by the wayside, like fractal compression, mirrorshades, and large-folio printed pages of fractal art. The modern evaluation is that while it was oversold by Per Bak, and certainly could not explain <em>all</em> critical phenomena <span class="citation" data-cites="bakHowNatureWorks1996">(<a href="#ref-bakHowNatureWorks1996" role="doc-biblioref">Bak 1996</a>)</span>, it can explain some of them <span class="citation" data-cites="watkins25YearsSelforganized2016">(<a href="#ref-watkins25YearsSelforganized2016" role="doc-biblioref">Watkins et al. 2016, sec. 8</a>)</span>.</p>
</section>
<section id="sec-universality" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-universality">Universality: The details don’t matter</h3>
<p>Or: Why elephants don’t know quantum mechanics, but don’t need to know it either.</p>
<p>In the early 20th century, material scientists noticed the remarkable phenomenon of “corresponding states”. As first reported by <span class="citation" data-cites="guggenheimPrincipleCorrespondingStates1945">(<a href="#ref-guggenheimPrincipleCorrespondingStates1945" role="doc-biblioref">Guggenheim 1945</a>)</span>, scientists measured the density <span class="math inline">\(\rho\)</span> of many substances near their liquid-vapor critical point. They fixed pressure and increased temperature <span class="math inline">\(T\)</span> around the critical temperature <span class="math inline">\(T_c\)</span>. As they plotted the relation between <span class="math inline">\(T\)</span> and <span class="math inline">\(\rho\)</span>, rescaled by critical temperature <span class="math inline">\(T_c\)</span> and density at critical point <span class="math inline">\(\rho_c\)</span>, remarkably, all the substances fell onto a single curve.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/herbut_2007_corresponding_states.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">High-resolution reprint of <span class="citation" data-cites="guggenheimPrincipleCorrespondingStates1945">(<a href="#ref-guggenheimPrincipleCorrespondingStates1945" role="doc-biblioref">Guggenheim 1945, fig. 2</a>)</span> in <span class="citation" data-cites="herbutModernApproachCritical2007">(<a href="#ref-herbutModernApproachCritical2007" role="doc-biblioref">Herbut 2007, 13</a>)</span>.</figcaption>
</figure>
</div>
<p>Despite the diversity of intermolecular forces, the phase transition behavior of a wide variety of gasses follows a universal pattern.</p>
<p>We see this pattern over and over again in physics. To give another example, imagine water flowing through a porous rock, oil through sand, or electricity through a random network of resistors. These systems, seemingly completely unrelated, share the same underlying mathematical structure and exhibit universal behavior near the percolation threshold. This universality arises because the details of the microscopic interactions become irrelevant at larger scales, and the system’s behavior is governed by the collective properties of its components.</p>
<p>It is often noted that quantum mechanics is unintuitive, because the mesoscopic physics is so different from the microscopic physics, so we had evolved to intuit the mesoscopic world, and not the microscopic world. But why is it possible to ignore quantum mechanics? Elephants don’t need to know, or don’t care about, the Standard Model of particle physics.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> They don’t know and don’t need to know, because there is no natural selective pressure favoring elephants that have intuitions about quantum mechanics. When they walk, they push dirt around. When water flows, it pushes water around.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;Nor do they play chess. <span class="citation" data-cites="brooksElephantsDonPlay1990">(<a href="#ref-brooksElephantsDonPlay1990" role="doc-biblioref">Brooks 1990</a>)</span></p></div></div><p>For example, a cup of water is a composite of H2O molecules, so it should be studied by quantum mechanics. However, zooming out and out, we would find that the system falls into one of several possible fixed points (liquid, gas, solid) or critical points (boiling point, freezing point, triple point). In each case, the quantum-mechanical messiness has been renormalized away.</p>
<p>In general, the lesson is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. The benefit of RN is that it saves us from the effort of understanding the microscopic details. On the flip-side, the details do matter if we are far from the critical or fixed point.</p>
<p>This is an overarching theme in renormalization theory, which might be called the <strong>universality hypothesis</strong>:</p>
<blockquote class="blockquote">
<p>All phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed. <span class="citation" data-cites="kadanoffOrderChaosII1999">(<a href="#ref-kadanoffOrderChaosII1999" role="doc-biblioref">Kadanoff 1999a, 273</a>)</span></p>
</blockquote>
<p>Furthermore, universality justifies our toy models as “serious play”. When I was first learning statistical mechanics, I was terribly confused by it. “They can’t be serious – do they think I’m stupid? How could the Ising model possibly be relevant to real magnets?” But the universality hypothesis justifies Ising models as serious toy models. Even if they are completely different from real magnets when far from the critical point, as we approach the critical point, their behavior becomes <em>exactly equal</em> at the limit.</p>
<p>RN theory explains why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up at the same destination as the toy model. <span class="citation" data-cites="battermanUniversalityRGExplanations2019">(<a href="#ref-battermanUniversalityRGExplanations2019" role="doc-biblioref">Batterman 2019</a>)</span></p>
<blockquote class="blockquote">
<p>We may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. … [A] good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details. <span class="citation" data-cites="fisherScalingUniversalityRenormalization1983">(<a href="#ref-fisherScalingUniversalityRenormalization1983" role="doc-biblioref">Fisher 1983, 47</a>)</span></p>
</blockquote>
<section id="koan-sociophysics" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="koan-sociophysics">Koan: Sociophysics</h4>
<blockquote class="blockquote">
<p>“The details don’t matter.” said them triumphantly as they declared their independence from biophysics.</p>
<p>“‘The details don’t matter.’” said them mockingly as they declared their insurrection against sociophysics.</p>
</blockquote>
<p><strong>Yuxi’s Comment:</strong><a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> The traditional approach of historians, going back to the days of “kings and battles”, is to run to personality theory and the individual acts, when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups, and then attempts to explain events, actions, and so on. However, for truly complicated systems that these days are studied under the name of “sociophysics”, this is a hopeless task; furthermore, the questions it answers are not even the right ones. The modern theorist would rather explain how the stable features of the problem are <em>invariant</em> under different assumptions of what individual people do, and arise from features of their interactions. Indeed, if one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;Inspired by Tolstoy’s <em>War and Peace</em>.</p>
<blockquote class="blockquote">
<p>The movement of humanity, arising as it does from innumerable arbitrary human wills, is continuous. To understand the laws of this continuous movement is the aim of history. But to arrive at these laws, resulting from the sum of all those human wills, man’s mind postulates arbitrary and disconnected units. The first method of history is to take an arbitrarily selected series of continuous events and examine it apart from others, though there is and can be no beginning to any event, for one event always flows uninterruptedly from another.</p>
<p>The second method is to consider the actions of some one man – a king or a commander – as equivalent to the sum of many individual wills; whereas the sum of individual wills is never expressed by the activity of a single historic personage.</p>
<p>Historical science in its endeavor to draw nearer to truth continually takes smaller and smaller units for examination. But however small the units it takes, we feel that to take any unit disconnected from others, or to assume a beginning of any phenomenon, or to say that the will of many men is expressed by the actions of any one historic personage, is in itself false.</p>
</blockquote>
</div></div><p><strong><a href="https://en.wikipedia.org/wiki/The_Gateless_Barrier">Mumon’s Comment</a>:</strong> Historians search for the king’s motive and the general’s ambition. They build castles of personality, moats of actions, yet understand nothing of the war. The great black spider traps its preys with a net so dense that nothing is lost after it has digested all of them.</p>
<p><img src="figure/banner/thumbnail_3.png" class="img-fluid"></p>
</section>
</section>
</section>



<div id="quarto-appendix" class="default"><section id="appendix" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">Appendix</h2><div class="quarto-appendix-contents">

<p>If only I understood what this is saying, then I would have written it in. <span class="citation" data-cites="mehtaExactMappingVariational2014">(<a href="#ref-mehtaExactMappingVariational2014" role="doc-biblioref">Mehta and Schwab 2014</a>)</span></p>


<!-- -->


</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-amirElementaryRenormalizationgroupApproach2020" class="csl-entry" role="listitem">
Amir, Ariel. 2020. <span>“An Elementary Renormalization-Group Approach to the Generalized Central Limit Theorem and Extreme Value Distributions.”</span> <em>Journal of Statistical Mechanics: Theory and Experiment</em> 2020 (1): 013214. <a href="https://doi.org/10.1088/1742-5468/ab5b8c">https://doi.org/10.1088/1742-5468/ab5b8c</a>.
</div>
<div id="ref-andersonMoreDifferentBroken1972" class="csl-entry" role="listitem">
Anderson, Philip W. 1972. <span>“More Is Different: Broken Symmetry and the Nature of the Hierarchical Structure of Science.”</span> <em>Science</em> 177 (4047): 393–96. <a href="https://doi.org/10.1126/science.177.4047.393">https://doi.org/10.1126/science.177.4047.393</a>.
</div>
<div id="ref-bakHowNatureWorks1996" class="csl-entry" role="listitem">
Bak, Per. 1996. <em>How <span>Nature Works</span>: The <span>Science</span> of <span>Self-Organized Criticality</span>.</em> New York, NY: Springer. <a href="https://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5555394">https://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=5555394</a>.
</div>
<div id="ref-bakSelforganizedCriticalityExplanation1987" class="csl-entry" role="listitem">
Bak, Per, Chao Tang, and Kurt Wiesenfeld. 1987. <span>“Self-Organized Criticality: <span>An</span> Explanation of the 1/f Noise.”</span> <em>Physical Review Letters</em> 59 (4): 381–84. <a href="https://doi.org/10/bh9v3d">https://doi.org/10/bh9v3d</a>.
</div>
<div id="ref-battermanUniversalityRGExplanations2019" class="csl-entry" role="listitem">
Batterman, Robert W. 2019. <span>“Universality and <span>RG</span> Explanations.”</span> <em>Perspectives on Science</em> 27 (1): 26–47. <a href="https://direct.mit.edu/posc/article-abstract/27/1/26/15429">https://direct.mit.edu/posc/article-abstract/27/1/26/15429</a>.
</div>
<div id="ref-bernLoopsTreesSearch2012" class="csl-entry" role="listitem">
Bern, Zvi, Lance J. Dixon, and David A. Kosower. 2012. <span>“Loops, Trees and the Search for New Physics.”</span> <em>Scientific American</em> 306 (5): 34–41. <a href="https://www.jstor.org/stable/26014420">https://www.jstor.org/stable/26014420</a>.
</div>
<div id="ref-brooksElephantsDonPlay1990" class="csl-entry" role="listitem">
Brooks, Rodney A. 1990. <span>“Elephants Don’t Play Chess.”</span> <em>Robotics and Autonomous Systems</em> 6 (1-2): 3–15. <a href="https://doi.org/10/bk6">https://doi.org/10/bk6</a>.
</div>
<div id="ref-burkhardtRealSpaceRenormalization1982" class="csl-entry" role="listitem">
Burkhardt, T. W., and J. M. J. van Leeuwen, eds. 1982. <em>Real-<span>Space Renormalization</span></em>. Springer.
</div>
<div id="ref-dombCriticalPhenomenaBrief1985" class="csl-entry" role="listitem">
Domb, C. 1985. <span>“Critical Phenomena: <span>A</span> Brief Historical Survey.”</span> <em>Contemporary Physics</em> 26 (1): 49–72. <a href="https://doi.org/10.1080/00107518508210738">https://doi.org/10.1080/00107518508210738</a>.
</div>
<div id="ref-evangelouAnatomyRetinaOptic2016" class="csl-entry" role="listitem">
Evangelou, Nikos, and Omar S. M. Alrawashdeh. 2016. <span>“Anatomy of the <span>Retina</span> and the <span>Optic Nerve</span>.”</span> In <em>Optical <span>Coherence Tomography</span> in <span>Multiple Sclerosis</span>: <span>Clinical Applications</span></em>, edited by Axel Petzold, 3–19. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-20970-8_2">https://doi.org/10.1007/978-3-319-20970-8_2</a>.
</div>
<div id="ref-fisherScalingUniversalityRenormalization1983" class="csl-entry" role="listitem">
Fisher, Michael E. 1983. <span>“Scaling, Universality and Renormalization Group Theory.”</span> <em>Critical Phenomena (Stellenbosch 1982)</em>, 1–139. <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=g7glCQAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=Scaling,+Universality+and+Renormalization+Group+Theory&amp;ots=f_pqWPfi-N&amp;sig=xsx4lUrrioVQCL1H0g_WQ7eeRmI">https://books.google.com/books?hl=en&amp;lr=&amp;id=g7glCQAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=Scaling,+Universality+and+Renormalization+Group+Theory&amp;ots=f_pqWPfi-N&amp;sig=xsx4lUrrioVQCL1H0g_WQ7eeRmI</a>.
</div>
<div id="ref-fisherRenormalizationGroupTheory1998" class="csl-entry" role="listitem">
———. 1998. <span>“Renormalization Group Theory: <span>Its</span> Basis and Formulation in Statistical Physics.”</span> <em>Reviews of Modern Physics</em> 70 (2): 653–81. <a href="https://doi.org/10.1103/RevModPhys.70.653">https://doi.org/10.1103/RevModPhys.70.653</a>.
</div>
<div id="ref-grimmettPercolation1999" class="csl-entry" role="listitem">
Grimmett, Geoffrey. 1999. <em>Percolation</em>. 2nd ed. Grundlehren Der Mathematischen <span>Wissenschaften</span> 321. Berlin ; New York: Springer.
</div>
<div id="ref-guggenheimPrincipleCorrespondingStates1945" class="csl-entry" role="listitem">
Guggenheim, Edward A. 1945. <span>“The Principle of Corresponding States.”</span> <em>The Journal of Chemical Physics</em> 13 (7): 253–61. <a href="https://doi.org/10.1063/1.1724033">https://doi.org/10.1063/1.1724033</a>.
</div>
<div id="ref-hawkingUniverseNutshell2001" class="csl-entry" role="listitem">
Hawking, Stephen W. 2001. <em>The Universe in a Nutshell</em>. London: Bantam Books.
</div>
<div id="ref-herbutModernApproachCritical2007" class="csl-entry" role="listitem">
Herbut, Igor. 2007. <em>A Modern Approach to Critical Phenomena</em>. Cambridge, UK ; New York: Cambridge University Press.
</div>
<div id="ref-kadanoffOrderChaosII1999" class="csl-entry" role="listitem">
Kadanoff, Leo P. 1999a. <em>From Order to Chaos <span>II</span>: <span>Essays</span>: Critical, Chaotic and Otherwise</em>. World <span>Scientific</span> Series on Nonlinear Science <span>Series A</span>, <span>Monographs</span> and Treatises 32. Singapore: World Scientific.
</div>
<div id="ref-kadanoffStatisticalPhysicsStatics1999" class="csl-entry" role="listitem">
———. 1999b. <em>Statistical Physics: Statics, Dynamics and Renormalization</em>. Repr. Singapore: World Scientific.
</div>
<div id="ref-loweThingsWonWork2013" class="csl-entry" role="listitem">
Lowe, Derek. 2013. <span>“Things <span>I Won</span>’t <span>Work With</span>: <span>Dimethylcadmium</span>.”</span> Blog. <em>In the Pipeline</em>. <a href="https://www.science.org/content/blog-post/things-i-won-t-work-dimethylcadmium">https://www.science.org/content/blog-post/things-i-won-t-work-dimethylcadmium</a>.
</div>
<div id="ref-marisTeachingRenormalizationGroup1978" class="csl-entry" role="listitem">
Maris, Humphrey J., and Leo P. Kadanoff. 1978. <span>“Teaching the Renormalization Group.”</span> <em>American Journal of Physics</em> 46 (6): 652–57. <a href="https://faculty.kfupm.edu.sa/phys/imnasser/Phase_transition/Maris_Kadanoff.pdf">https://faculty.kfupm.edu.sa/phys/imnasser/Phase_transition/Maris_Kadanoff.pdf</a>.
</div>
<div id="ref-mehtaExactMappingVariational2014" class="csl-entry" role="listitem">
Mehta, Pankaj, and David J. Schwab. 2014. <span>“An Exact Mapping Between the <span>Variational Renormalization Group</span> and <span>Deep Learning</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1410.3831">http://arxiv.org/abs/1410.3831</a>.
</div>
<div id="ref-peierlsIsingModelFerromagnetism1936" class="csl-entry" role="listitem">
Peierls, Rudolf. 1936. <span>“On <span>Ising</span>’s Model of Ferromagnetism.”</span> In <em>Mathematical <span>Proceedings</span> of the <span>Cambridge Philosophical Society</span></em>, 32:477–81. Cambridge University Press. <a href="https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/on-isings-model-of-ferromagnetism/C0584C5711BC3D25830B63A4C2F09609">https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/on-isings-model-of-ferromagnetism/C0584C5711BC3D25830B63A4C2F09609</a>.
</div>
<div id="ref-peskinIntroductionQuantumField1995" class="csl-entry" role="listitem">
Peskin, Michael Edward, and Daniel V. Schroeder. 1995. <em>An Introduction to Quantum Field Theory</em>. Reading, Mass: Addison-Wesley Pub. Co.
</div>
<div id="ref-sethnaCourseCracklingNoise2007" class="csl-entry" role="listitem">
Sethna, James P. 2007. <span>“Course 6 <span>Crackling</span> Noise and Avalanches: <span>Scaling</span>, Critical Phenomena, and the Renormalization Group.”</span> <em>Les Houches</em> 85: 257–88. <a href="https://www.sciencedirect.com/science/article/pii/S0924809907800138">https://www.sciencedirect.com/science/article/pii/S0924809907800138</a>.
</div>
<div id="ref-sethnaStatisticalMechanicsEntropy2021" class="csl-entry" role="listitem">
———. 2021. <em>Statistical Mechanics: Entropy, Order Parameters, and Complexity</em>. Second Edition. Oxford Master Series in Statistical, Computational, and Theoretical Physics. Oxford, United Kingdom ; New York, NY: Oxford University Press.
</div>
<div id="ref-simkinReinventingWillis2011" class="csl-entry" role="listitem">
Simkin, M. V., and V. P. Roychowdhury. 2011. <span>“Re-Inventing <span>Willis</span>.”</span> <em>Physics Reports</em> 502 (1): 1–35. <a href="https://doi.org/10.1016/j.physrep.2010.12.004">https://doi.org/10.1016/j.physrep.2010.12.004</a>.
</div>
<div id="ref-stevensNeuralEventsPsychophysical1970" class="csl-entry" role="listitem">
Stevens, S. S. 1970. <span>“Neural <span>Events</span> and the <span>Psychophysical Law</span>.”</span> <em>Science</em> 170 (3962): 1043–50. <a href="https://doi.org/10.1126/science.170.3962.1043">https://doi.org/10.1126/science.170.3962.1043</a>.
</div>
<div id="ref-stinchcombeIntroductionScalingConcepts1991" class="csl-entry" role="listitem">
Stinchcombe, R. B. 1991. <span>“Introduction to <span>Scaling Concepts</span>.”</span> In <em>Scaling <span>Phenomena</span> in <span>Disordered Systems</span></em>, edited by Roger Pynn and Arne Skjeltorp, 13–30. Boston, MA: Springer US. <a href="https://doi.org/10.1007/978-1-4757-1402-9_2">https://doi.org/10.1007/978-1-4757-1402-9_2</a>.
</div>
<div id="ref-strogatzNonlinearDynamicsChaos2015" class="csl-entry" role="listitem">
Strogatz, Steven H. 2015. <em>Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering</em>. Second edition. Boulder, CO: Westview Press, a member of the Perseus Books Group.
</div>
<div id="ref-watkins25YearsSelforganized2016" class="csl-entry" role="listitem">
Watkins, Nicholas W., Gunnar Pruessner, Sandra C. Chapman, Norma B. Crosby, and Henrik J. Jensen. 2016. <span>“25 <span>Years</span> of <span class="nocase">Self-organized Criticality</span>: <span>Concepts</span> and <span>Controversies</span>.”</span> <em>Space Science Reviews</em> 198 (1): 3–44. <a href="https://doi.org/10.1007/s11214-015-0155-x">https://doi.org/10.1007/s11214-015-0155-x</a>.
</div>
<div id="ref-wilsonProblemsPhysicsMany1979" class="csl-entry" role="listitem">
Wilson, Kenneth G. 1979. <span>“Problems in <span>Physics</span> with <span>Many Scales</span> of <span>Length</span>.”</span> <em>Scientific American</em> 241 (2): 158–79. <a href="https://doi.org/10.1038/scientificamerican0879-158">https://doi.org/10.1038/scientificamerican0879-158</a>.
</div>
<div id="ref-yangSelectedPapers194519802005" class="csl-entry" role="listitem">
Yang, Chen Ning. 2005. <em>Selected Papers (1945-1980), with Commentary</em>. 2005 ed. World <span>Scientific</span> Series in 20th Century Physics, v. 36. Hackensack, NJ: World Scientific.
</div>
<div id="ref-zeeQuantumFieldTheory2010" class="csl-entry" role="listitem">
Zee, Anthony. 2010. <em>Quantum <span>Field Theory</span> in a <span>Nutshell</span></em>. 2nd ed. Princeton, N.J: Princeton University Press.
</div>
<div id="ref-zeeQuantumFieldTheory2023" class="csl-entry" role="listitem">
———. 2023. <em>Quantum <span>Field Theory</span>, as <span>Simply</span> as <span>Possible</span></em>. Princeton, New Jersey: Princeton University Press.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "How to do Renormalization"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-04-11"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2024-07-19"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [math, physics, scaling]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    resources:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        - "figure/**"</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Survival guide for renormalization (RN) theory. How to do it in real space and frequency space. What it all means. Should have something for everyone, from mathematicians to physicists to outsiders, from serious series to pretty pictures. Intended audience: those with two years of undergrad mathematics."</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner/banner_2.png"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">image-alt:</span><span class="co"> "A metaphor for real-space renormalization, where the building on the left is coarse-grained to the building on right. The facade of a wide brutalist architecture. On the left, the windows are small and the floors are close together. On the right, the windows are large and the floors are wide apart. High contrast, monochromatic, minimalistic, in the flat style of vector svg art., illustration, conceptual art."</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "certain"</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 3</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>Renormalization (RN) theory is a powerful tool for understanding the behavior of physical systems at different length scales. It allows us to explain why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>RN theory is based on the idea of self-similarity -- that a system looks the same at different length scales. For example, a coastline looks the same whether you are looking at it from a satellite or from a boat. Similarly, a magnet looks the same whether you are looking at it with a microscope or with your naked eye.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>The basic idea of RN is to start with a microscopic description of a system and then to coarse-grain it. That is, to average over the details of the system at a smaller length scale to obtain a description of the system at a larger length scale. This process can be repeated, leading to a sequence of descriptions of the system at increasingly larger length scales.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>The key insight of RN theory is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. This is known as the universality hypothesis. Universality explains why systems that are very different at the microscopic level can have the same behavior at the macroscopic level. For example, water, oil, and electricity all exhibit the same percolation behavior near the percolation threshold, even though they are very different at the microscopic level.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Universality also justifies the use of toy models to study physical systems. Toy models are simplified models that capture the essential features of a system without including all of the details. For example, the Ising model is a toy model of a magnet that captures the essential features of ferromagnetism without including all of the details of the interactions between the atoms.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>In this essay, we will explore the basic ideas of RN theory and see how it can be used to understand a variety of physical systems.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/banner/thumbnail_1.png)</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## The logistic map: RN on $\R$</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>Let's first study the logistic map, the simplest nontrivial example of renormalization that I know of. This section is based on <span class="co">[</span><span class="ot">Wikipedia</span><span class="co">](https://en.wikipedia.org/wiki/Logistic_map)</span>.</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### The logistic map</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>Consider a function $f_r(x)=r x(1-x)$, and we want to study what happens when we iterate the map many times. The map might fall into a fixed point, a fixed cycle, or chaos. We can see all those cases in its <span class="co">[</span><span class="ot">bifurcation diagram</span><span class="co">](https://en.wikipedia.org/wiki/Bifurcation_diagram)</span>.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The bifurcation diagram of the logistic map. [Source](https://en.wikipedia.org/wiki/File:Logistic_Bifurcation_map_High_Resolution.png)</span><span class="co">](figure/Logistic_Bifurcation_map_High_Resolution.png)</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>When the map falls into a stable fixed cycle of length $n$, we would find that the graph of $f_r^n$ and the graph of $x \mapsto x$ intersect at $n$ points, and the slope of the graph of $f_r^n$ is bounded in $(-1, +1)$ at those intersections.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>For example, when $r=3.0$, we find that there is only a single intersection, at which point the slope is exactly $+1$, indicating that it is a stable single fixed point, but is about to undergo a bifurcation.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="al">![The relationship between $x_{n+2}$ and $x_{n}$ as $r$ increases from $2.7$ to $3.3$. Before the period doubling bifurcation occurs. The orbit converges to a single fixed point where the graph of $f_r^2$ intersects the diagonal line. As $r$ reaches $3$, the intersection pitchforks into three. The middle intersection becomes unstable, but the two neighboring intersections remain stable.](figure/period_doubling_bifurcation.png)</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>As $r$ increases to beyond $r=3.0$, the intersection point splits to two, which is a period doubling. For example, when $r=3.4$, there are three intersection points, with the middle one unstable, and the two others stable.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">When $r=3.4$, there are three intersection points, with the middle one unstable, and the two others stable. [Source](https://en.wikipedia.org/wiki/File:Logistic_iterates_3.4.svg)</span><span class="co">](figure/Logistic_iterates_3.4.svg)</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>As $r$ approaches $r=3.45$, another period-doubling occurs in the same way. The period-doublings occur more and more frequently, until at a certain $r \approx 3.56994567$, the period doublings become infinite, and the map becomes chaotic. This is the period-doubling route to chaos.</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">When $r \approx 3.56994567$, there are infinitely many intersections, and we have arrived at chaos via the period-doubling route. [Source](https://en.wikipedia.org/wiki/File:Logistic_iterates_with_r%3D3.56994567.svg)</span><span class="co">](figure/Logistic_iterates_3.56994567.svg)</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>Something remarkable happens when we superimpose the graphs of $f_r, f_r^2, f_r^4, \dots$ when $r$ is at the critical point $3.5699\dots$. We see that each iteration of the graph seems to resemble itself, except that it is scaled and rotated by 180 degrees. We can naturally guess that $f_r^{\infty}$ converges to a certain function that is infinitely jagged, such that it exactly resembles itself when scaled and rotated; that is, it is a fractal.</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="al">![The graphs of $f_r, f_r^2, f_r^4, \dots$ when $r$ is at the critical point.](figure/Logistic_iterates_together_r=3.56994567.svg)</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>As $r$ approaches the critical value, we can see how the graph of $f_r^\infty$ takes on more and more details, and at the critical point, becomes a perfect fractal.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">video</span><span class="ot"> controls width</span><span class="op">=</span><span class="st">100%</span><span class="dt">&gt;</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">source</span><span class="ot"> src</span><span class="op">=</span><span class="st">"figure/Logistic_map_approaching_the_scaling_limit.mp4"</span><span class="ot"> type</span><span class="op">=</span><span class="st">"video/mp4"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">video</span><span class="dt">&gt;</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span>The graphs of $f_r, f_r^2, f_r^4, \dots$ as $r$ approaches the critical point from below.<span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="fu">### Universality</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>Looking at the bifurcation diagram, we can see a region, starting just after $r = 3.8$, where there is a clear "window" with period $3$ bursting out of a sea of chaos. The window then bifurcates repeatedly, to stable cycles of periods $6, 12, 24, \dots$ until it all collapses back into the chaos again at around $r \approx 3.8494344$. Though this is a different place, the bifurcation diagram looks suspiciously similar to the previous case.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>Not only that, if we look at the movie of $f_r^\infty$ as $r$ approaches this critical point, we again see the same jagged shape.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">video</span><span class="ot"> controls width</span><span class="op">=</span><span class="st">100%</span><span class="dt">&gt;</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">source</span><span class="ot"> src</span><span class="op">=</span><span class="st">"figure/Logistic_map_approaching_the_period-3_scaling_limit.mp4"</span><span class="ot"> type</span><span class="op">=</span><span class="st">"video/mp4"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">video</span><span class="dt">&gt;</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>Are we seeing some kind of *universal* feature of period-doubling routes to chaos? Is this a general pattern independent of the details of how exactly the logistic map is defined? What if we change to another dynamical system completely different?</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>For example, we can consider the gauss map $x_{n+1} = \exp(-\alpha x^2_n)+\beta$. For a fixed $\alpha$, we can plot the bifurcation graph as we vary $\beta$. Though it looks different, the two bifurcation graphs have a clear resemblance. This is an instance of **universality**, for which we will see <span class="co">[</span><span class="ot">again and again later</span><span class="co">](#sec-universality)</span>. If $f_r$ is a family of curves with parabolic tops<span class="ot">[^feigenbaum-universality]</span>, then it will bifurcate just like the logistic curve.</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ot">[^feigenbaum-universality]: </span>Rigorously, we can describe it as follows. If $F: \R^2 \to \R$ is smooth, and for all $r \in \R$, the function $F(r, \cdot): \R \to \R$ has a single global maximum, at which point $\partial_x^2 F(r, x) &lt; 0$, then its bifurcation diagram looks the same as that of the logistic map, and it will have the same two scaling exponents $\alpha, \delta$, to be calculated below.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="al">![The bifurcation graphs of gauss map with $\alpha = 5$ and the logistic map.](figure/bifurcation_gauss_logistic.png)</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>More to the issue at hand: why do the two graphs look similar?</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The bifurcation graph is self-similar. [Source](https://ja.wikipedia.org/wiki/ファイル:ロジスティック写像の窓の自己相似.png)</span><span class="co">](figure/logistic_bifurcation_graph_fractal.png)</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="fu">### The self-similarity equation</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>Recall that we said the limit of $f^\infty_r$ should be self-similar, in the sense that if we iterate it twice, then rotate and scale it by a factor, we get back the same function. That is, it should be a solution to the **self-similarity equation**.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>f(x) = -\alpha f\left(f\left(\frac{x}{-\alpha} \right)\right)</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>In words, if we scale up the graph for $f^2$ by $\alpha &gt; 0$, and then rotate by 180 degrees, we get back the graph for $f$. Why should this be the case?</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>Well, look back at the previous diagram of the first bifurcation:</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="al">![The relationship between $x_{n+2}$ and $x_{n}$ as $r$ increases from $2.7$ to $3.3$.](figure/period_doubling_bifurcation.png)</span></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>The graph of $f$ is a parabola, and the graph of $f^2$ began by looking like a parabola, but its top eventually collapses like an overbaked pie, down and down, until it punches right through the $y = x$ diagonal line again and ... wait, wait, it looks like a little parabola again, except this time, it's rotated by 180 degrees! And so we can run the same argument on $f^2$, and conclude that the graph of $(f^2)^2 = f^4$ would collapse *upwards*, punching right through the $y=x$ diagonal line again, and then history repeats with $f^8, f^{16}, \dots$, repeating the same story again <span class="sc">\&amp;</span> again, but smaller <span class="sc">\&amp;</span> smaller.<span class="ot">[^again-again-smaller-smaller]</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The center of $f$ rises, until the center of $f^2$ collapses, causing a bifurcation at $r_1$. As $r$ increases beyond that point, the center of $f^2$ looks like a parabola again, looking like $f$ in miniature. Thus, as $r$ increases further, $f^4$ will eventually undergo a bifurcation that is similar to the previous bifurcation, except in miniature. [@strogatzNonlinearDynamicsChaos2015, page 388]</span><span class="co">](figure/logistics_scaling_invariant_function.png)</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a><span class="ot">[^again-again-smaller-smaller]</span>:</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>    As a wise <span class="co">[</span><span class="ot">meteorologist</span><span class="co">](https://en.wikipedia.org/wiki/Lewis_Fry_Richardson)</span> <span class="co">[</span><span class="ot">has said</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Siphonaptera_(poem)):</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Big whorls have little whorls,</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Which feed on their velocity;</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; And little whorls have lesser whorls,</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; And so on to viscosity.</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="in">    And indeed, chaos theory, fractals, and RN all made early appearances in the study of turbulence.</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>By eye-balling the curve, we see that $f$ should be an even function, and that in a neighborhood of its centerpoint, . Also, since the $f^2$ can be graphically calculated by doing the cobweb diagram with the graph of $f$, it does not matter if we first scale up the graph of $f$ by a factor of $r$ to $F$, then double it to $F^2$, or if we first double it to $f^2$, then scale its graph. We would get back the same thing. Thus, without loss of generality, we can scale $f$ such that $f(0) = 1$.</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>So, our task is to solve the following equation:</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>f(x) = -\alpha f\left(f\left(\frac{x}{-\alpha} \right)\right)<span class="sc">\\</span></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>f(x) = 1 - a_2 x^2 + a_4 x^4 + \dots</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>We can solve the equation numerically as the fixed point. We would start with $f(x) = 1-x^2$, then guess a good $\alpha$ and repeatedly apply $f \mapsto -\alpha f\left(f\left(\frac{x}{-\alpha} \right)\right)$. If we picked $\alpha$ correctly, we would have gotten the right result, as shown:</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">At the point of chaos $r^*=3.5699 \dots$, as we repeat the functional equation iteration $f(x) \mapsto-\alpha f(f(-x / \alpha))$ with $\alpha=2.5029 \ldots$, we find that the map does converge to a limit. [Source](https://en.wikipedia.org/wiki/File:Logistic_scaling_limit,_r%3D3.56994567.svg)</span><span class="co">](figure/Logistic_scaling_limit_critical.svg)</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>If $\alpha$ is not correct, the iterates would not converge; instead, they would have a zooming effect that looks cool.</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">video</span><span class="ot"> controls width</span><span class="op">=</span><span class="st">100%</span><span class="dt">&gt;</span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">source</span><span class="ot"> src</span><span class="op">=</span><span class="st">"figure/Logistic_scaling_with_varying_scaling_factor.webm"</span><span class="ot"> type</span><span class="op">=</span><span class="st">"video/webm"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">video</span><span class="dt">&gt;</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Solving the equation at order 2" collapse="true" }</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>At order 2, we approximate by $f(x) \approx 1 - a_2 x^2$ and ignore all higher-order terms. This gives us two equations for two unknowns:</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>1-a_2 = \frac{1}{-\alpha} <span class="sc">\\</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>\frac{2a_2^2}{\alpha} = a_2</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>It has two solutions. One solution has $\alpha &lt; 0$, which we know is unphysical. The other one is </span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>\alpha = 1 + \sqrt{3} \approx 2.732 <span class="sc">\\</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>a_2 = \frac{1 + \sqrt{3}}{2} \approx 1.366</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>What happens if we are not *exactly* at the fixed point but start slightly off? Let's say we start with a function $f_0(x) = 1 - a_{2,0}x^2$, where $a_{2,0} = a_2^* + \Delta$, where $a_2^*$ is the fixed point, and $\Delta$ is small but nonzero. Here, we should think of the space of possible functions. Each point in this space is a possible scaling limit, but if we start a bit too small, we fall into boredom, and if we start a bit too high, we fall into chaos. Start just right, and we harvest a beautiful fractal.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>After one iteration, we have $f_1(x) = -\alpha_0 f_0(f_0(x/(-\alpha_0)))$, where $\alpha_0$ was fixed by $f_1(0) = 1$. This gives us</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>\alpha_0 = \frac{1}{-1+a_{2, 0}} <span class="sc">\\</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>\frac{2a_{2, 0}^2}{\alpha_0} = a_{2, 1}</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>That is, we have the **renormalization flow equation**</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>2a_{2, 0}^2(a_{2, 0}-1)= a_{2, 1}</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>We can plot the space of all possible $f(x)$ as a line, like </span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>$$1-0x^2, 1-0.5 x^2, 1-x^2, 1-1.5x^2, \dots$$</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="al">![RN flow diagram of the self-similarity map at order 2.](figure/rn_flow_feigenbaum_2.svg)</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>This is a 1-dimensional slice of the space of all possible $f$ (the space of theories). Then, the effect of repeatedly applying the self-similarity map is to iterate the map $a_2 \mapsto 2a_{2}^2(a_{2}-1)$. If we are precisely at the fixed-point $a_2^*$, then we are not going anywhere, but if we are not exactly there, then since the slope of $a_2 \mapsto 2a_{2}^2(a_{2}-1)$ is $\delta \approx 5.73$ at that point, we would get farther and farther away:</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>f_0 = 1-(a_2^* + \Delta)x^2, \quad f_1 = 1-(a_2^* + \delta\Delta)x^2, \quad f_1 = 1-(a_2^* + \delta^2\Delta)x^2, \quad \dots</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>and after $\log_\delta(\frac{0.1}{\Delta})$, we would be at roughly $1-(a_2^* \pm 0.1)x^2$, which is when we can finally notice that we are *obviously* no longer in the neighborhood of the fixed point anymore. If we start at $a_2^* + \Delta/\delta$, then we can sustain the illusion for one more iteration. Similarly, if we start at $a_2^* + \Delta/\delta^n$, then we can sustain the illusion for $n$ more iterations.</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>Now, thinking back to what the logistic map says, we understand what we have discovered: The graph of $f_{r^* - \Delta}$ is similar to the graph of $f_{r^* - \Delta/\delta}^2$ scaled by $-\alpha$. If we let $r_1, r_2, r_3, \dots$ be the points at which the logistic map splits into a stable cycle of period $2^1, 2^2, 2^3, \dots$, then we have $r_{n} \approx r^* - \Delta/\delta^{n}$, and so we have:</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>\frac{r^* - r_n}{r^* - r_{n+1}} \to \delta</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>This is usually spoken in this way: the intervals between two bifurcations shrinks at a rate of $\delta$. </span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The bifurcation diagram for the logistic map. As the bifurcations approach the point of chaos, the interval between two bifurcations gets shorter and shorter, at a rate of $\delta$ per bifurcation. [Source](https://ja.wikipedia.org/wiki/ファイル:ロジスティック写像、周期倍分岐カスケードの軌道図.png)</span><span class="co">](figure/logistic_bifurcation_graph_fractal_delta.png)</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>$\delta$ is called **Feigenbaum's first constant**, and $\alpha$ is **Feigenbaum's second constant**.</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Solving the equation at order 4" collapse="true" }</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>Similarly, we can solve the equation at order 4 by plugging in $f(x) \approx 1 - a_2 x^2 + a_4 x^4$, obtaining 3 equations for 3 variables:</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>1-a_2+a_4 = \frac{1}{-\alpha} <span class="sc">\\</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>\frac{2a_2^2 - 4a_2a_4}{\alpha} = a_2 <span class="sc">\\</span></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>\frac{a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2)}{-\alpha^2} = a_4</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>To solve this numerically, first guess a solution from the previous one, $\alpha \approx 2.732, a_2 \approx 1.366$, then plug into the first equation to get $a_4 \approx 0$. Then, standard numerical root-finding gives </span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>\alpha \approx 2.534 <span class="sc">\\</span></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>a_2 \approx 1.522 <span class="sc">\\</span> </span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>a_4 \approx 0.128</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>We can also make the same argument using a flow in theory-space, except now we are doing it over a 2-dimensional slice of it. The flow map is</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>F(a_2, a_4) = \left(</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>   (2a_2^2 - 4a_2a_4)(-1+a_2 - a_4), -(a_4(4a_4+6a_2^2) - a_2(2a_4 + a_2^2))(-1+a_2 - a_4)^3</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>\right)</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>At the fixed-point $(a_2, a_4) = (1.522, 0.128)$, the Jacobian matrix is</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>\nabla F = \begin{bmatrix}</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>6.0506 &amp; -6.2524 <span class="sc">\\</span></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>1.2621 &amp; -1.6909</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>This matrix has eigenvalues of $4.843, -0.483$, so it is a saddle point, with $\delta = 4.843$. The flow and the eigenvectors $(0.982, 0.190), (0.691, 0.723)$ are plotted below.</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a><span class="al">![RN flow diagram of the self-similarity map at order 4. The two eigenvector directions at the fixed point are plotted as dashed lines.](figure/rn_flow_feigenbaum_4.svg)</span></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>In summary:</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>    <span class="pp">|</span> Order  2 <span class="pp">|</span>   4 <span class="pp">|</span>  $\infty$ <span class="pp">|</span></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----|-----|-----|----|</span></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $a_2$  <span class="pp">|</span> 1.366 <span class="pp">|</span> 1.522 <span class="pp">|</span> 1.530 <span class="pp">|</span></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $a_4$  <span class="pp">|</span>     <span class="pp">|</span> 0.128 <span class="pp">|</span> 0.105 <span class="pp">|</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\alpha$   <span class="pp">|</span> 2.732 <span class="pp">|</span> 2.534 <span class="pp">|</span> 2.503 <span class="pp">|</span></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $\delta$  <span class="pp">|</span> 5.73 <span class="pp">|</span> 4.843 <span class="pp">|</span> 4.669 <span class="pp">|</span></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>: The solution to the self-similarity equation, at increasingly high orders of approximation.</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lessons</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>Even in this tiny problem, we can already draw several lessons, which will appear again and again in RN:</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We assume a function is self-similar, and calculate from there.</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Self-similarity is a transform on a function (or "theory").</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We often need to use a "fudge factor" like $\alpha$ to make sure that the transformed function does not collapse to zero, for trivial reasons.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If we repeatedly apply the self-similarity transform on a function, we would obtain a scaling limit, a perfectly self-similar object -- a fractal.</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In the space of all possible theories, the self-similarity transform creates a flow-field in the theory-space. The interesting fixed-points of the flow-field are its saddle points.</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The largest eigenvalue of the saddle point describes what happens when you are close to the saddle point, but not quite there.</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Bravely calculate using the cheapest approximation you can think of. It often gets you within 50% of the right answer.</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>But if you want accuracy, you can always use a computer and calculate many orders higher.</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/banner/thumbnail_2.png)</span></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ising model and friends: RN on a grid</span></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>This section studies RN on a grid, using what is called **real space RN**, in contrast to **momentum space RN**. Real space RN is a garden of tricks, useful and intuitive, but not as powerful as momentum space RN. If you find the kind of mathematical trickery in this section fun, look at <span class="co">[</span><span class="ot">@kadanoffStatisticalPhysicsStatics1999, chapter 14; @burkhardtRealSpaceRenormalization1982</span><span class="co">]</span> for more.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This paper attracted much favorable notice since, beyond obtaining all the scaling properties, it seemed to lay out a direct route to the actual calculation of critical properties. On closer examination, however, the implied program seemed -- as I will explain briefly -- to **run rapidly into insuperable difficulties** and interest faded. In retrospect, however, Kadanoff's scaling picture embodied important features eventually seen to be basic to Wilson's conception of the full renormalization group.</span></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@fisherRenormalizationGroupTheory1998</span><span class="co">]</span></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>I recommend that you open these and play as you follow along:</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Complexity Explorables | I sing well-tempered</span><span class="co">](https://www.complexity-explorables.org/explorables/i-sing-well-tempered/)</span></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Ising model</span><span class="co">](https://www.ibiblio.org/e-notes/Perc/ising1k.htm)</span> by <span class="co">[</span><span class="ot">Evgeny Demidov</span><span class="co">](https://www.ibiblio.org/e-notes/Perc/contents.htm)</span>.</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a><span class="fu">### Percolation</span></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>Percolation is about randomly punching holes in a material until it falls apart. In the simplest setting, the material is a square lattice $\Z^2$, and each site (vertex) is either open or closed.<span class="ot">[^percolation-2d]</span> Open means there is a hole there, and closed means the site is intact. Sites are open or closed randomly and independently with probability $p$. We are interested in whether there is an infinite connected cluster of open sites.</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a><span class="ot">[^percolation-2d]: </span>Percolation on $\Z$ is trivial, even more trivial than Ising model on $\Z$. The trouble is the same: you can only go from one point to another point by one route, and if at any point on the route, you are stopped, then that's the end -- you can't get there by any other way. Thus, long-range interactions decay exponentially with distance, which means no power law, no phase transition, no critical point. See a <span class="co">[</span><span class="ot">later section</span><span class="co">](#sec-power-law-two-exponential-parents)</span>.</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>This is a model for a porous material: for example, water seeping through the ground. If we have a layer of rock, then groundwater can seep through if there is a single connected path from top to bottom. A layer of rock can be thought of as a grid, with little cracks between grid-points. According to percolation theory, at a "critical probability", suddenly we have arbitrarily large connected clusters of cracks, and so water can seep arbitrarily far in the rock -- it is all or nothing.</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>That is, there is a sharp transition: if $p$ is small, then there is no infinite cluster of open sites, and the water cannot go through; but if $p$ is large, then there is an infinite cluster of open sites, and water can go through. The critical value $p_c$ is about $0.5927...$. See <span class="co">[</span><span class="ot">@grimmettPercolation1999</span><span class="co">]</span> for more details about percolation.</span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>To use Kadanoff blocking for percolation, the first step is to coarse grain the lattice. We group the sites into blocks of $3 \times 3$ and call a block open if there is a path of open sites connecting the left and right sides of the block. Otherwise, the block is closed.</span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>The next step is to define a new percolation model on the coarse-grained lattice, but this is a little trickier than in the Ising model, because there is no obvious way to map the parameters of the original model to the parameters of the new model. We need to find a new probability $p'$ such that the new model on the coarse-grained lattice has the same behavior as the original model on the fine lattice. In particular, we want the probability of having an infinite cluster of open sites to be the same in both models.</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>It turns out that there is no exact way to do this, but we can make an approximation. One way to do this is to use Monte Carlo simulations to estimate the probability of having an infinite cluster for different values of $p'$.</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kadanoff blocking</span></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a>This section based on <span class="co">[</span><span class="ot">@simkinReinventingWillis2011, section 10</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@stinchcombeIntroductionScalingConcepts1991</span><span class="co">]</span>.</span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">When an Ising model is at the critical temperature $T_c$, coarse-graining would not end up with all-black or all-white, but would have the same amount of details no matter how much we coarse-grain. [@sethnaCourseCracklingNoise2007, figure 13].</span><span class="co">](figure/ising%20model.png)</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>The idea of Kadanoff blocking is simple: "This fractal looks the same if I take off my glasses, just bigger. If I zoom out and blur a bit, we get pretty much the same thing again. So, if I take a 3-by-3 pixel, block and replace it by one big pixel, we should get the same thing again."</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>Given a hexagonal grid, you make $p$ of them black and the rest white. What is the critical $p$ where you get a percolation (infinitely big black island)? To do this by blocking, we regard a triangle as a large spin, and "merge" the three spins on a triangle to one single spin. If two or three spins are black, then the whole spin is also black, otherwise, the whole spin is white.</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>Then, after one blocking operation, the lattice length increases from $l$ to $\sqrt 3 l$ , and the renormalized occupation probability changes from $p$ to $p^3 + 3p^2(1-p) = p^2(3-2p)$. That is, we have the RN flow equation</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>$$p' = p^3 + 3p^2(1-p) = p^2(3-2p)$$</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Kadanoff blocking on a triangular lattice. [@simkinReinventingWillis2011, figure 1]</span><span class="co">](figure/hexagonal_decimation.png)</span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>The equilibrium point is $p=1/2$. This is the percolation probability. Let the reduced probability be $\bar p = p-1/2$. We find that one iteration of the RN flow makes $\bar p_{n+1} = \frac 32 \bar p - 2 \bar p^3$ which is $\approx \frac 32 \bar p_n$ for small values of $\bar p$. </span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a><span class="al">![RN flow for Kadanoff blocking on a triangular lattice.](figure/rn_flow_hexagonal_grid_1.svg)</span></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>Suppose we start with $\bar p_0$ , and we perform $N$ repeats of RN to reach some constant $\Delta \bar p$ (for example, 0.001), then </span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>$$N = \frac{\ln \Delta \bar p - \ln \bar p_0}{\ln \frac 32}$$</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a>during which time, the lattice length has increased by</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a>$$3^{\frac 12 N} \propto \bar p_0^{-\frac{\ln 3}{2\ln \frac 32}}$$</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a>Since at constant $\Delta \bar p$, the lattice has a certain *characteristic* look-and-feel with a certain *characteristic* size for its clusters, we find that the *characteristic* length of its clusters is $\propto (p-1/2)^{-\nu}$, where<span class="ot">[^characteristic-look-and-feel]</span></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a><span class="ot">[^characteristic-look-and-feel]</span>:</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>    It amuses me to no end that the word *characteristic* is something chemists use a lot. Physicists do it too, sure, with their "characteristic length", "characteristic height", "characteristic temperature", and such, but it is abstract. You rarely need to actually check a cake's characteristic length against a standard cake. However, when you are doing chemistry, and you need to check a chemical's *characteristic smell*, then you are out of luck.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; I'm saddened to report that the chemical literature contains descriptions of dimethylcadmium's smell. Whoever provided these reports was surely exposed to far more of the vapor than common sense would allow ... its odor is variously described as "foul", "unpleasant", "metallic", "disagreeable", and (wait for it) "characteristic", which is an adjective that shows up often in the literature with regard to smells, and almost always makes a person want to punch whoever thought it was useful. ... if you're working with organocadmium derivatives and smell something nasty, but nasty in a new, exciting way that you've never quite smelled before, then you can probably assume the worst. [@loweThingsWonWork2013]</span></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>$$\nu = \frac{\ln 3}{2\ln \frac 32} \approx 1.355$$</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>The actual exponent is believed to be $\nu = 4/3$, so we are only 1.6% off. Very good for such a cheap calculation!</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Ising model</span></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>Ferromagnets are weird. A single iron atom is a tiny magnet, but if the iron atoms in a piece of magnet are not oriented roughly in the same way, then the whole piece would have all its direction washed away, thus not be a magnet at all. In the atomic world, things jiggle about and tend to destroy all order, so how is it possible for a grid of atoms to stay pointing mostly in the same direction for days and weeks -- an eternity on the atomic level?</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>In 1920s, Lenz had the idea of a simple atomic model for ferromagnetism. He gave the problem to his graduate student Ising, who solved it in the one-dimensional case. He was disappointed to see that there is no spontaneous magnetization, which is the key feature of ferromagnetism, and he conjectured that this is also true in higher dimensions. The model was thus neglected for a while, until <span class="co">[</span><span class="ot">@peierlsIsingModelFerromagnetism1936</span><span class="co">]</span> showed that the two-dimensional Ising model does exhibit a phase transition, and later, Onsager solved it completely in 1944, showing several curious features. This was a starting point of modern statistical physics. See <span class="co">[</span><span class="ot">@dombCriticalPhenomenaBrief1985</span><span class="co">]</span> for some more historical details.</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>The Ising model is a toy model of a magnet. For simplicity, we assume all the iron atoms are locked in a lattice, and each atom can only be spin up or spin down (like a coin that can be heads or tails). Neighboring atoms want to be the same, but temperature $T$ makes the atoms jiggle and be random.</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a>There are many different Ising models, but the following is the most standard. We have a grid of atoms, and each atom is nothing but a point with two possible values: $+1$ or $-1$. Nothing more complicated than that. The energy of the system is determined by the interaction between nearest neighbors, with a lower energy when neighboring spins are aligned. That is, its energy is</span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>H(s) = -J \sum_{i, j} s_i s_{j}</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a>where $J&gt;0$ is the strength of interaction between neighboring atoms, $s_i$ represents the spin at site $i$, and the summation is over neighboring pairs of $i, j$. For example, if we have Ising model on the integer line $\Z$, then $H = -J \sum_{i} s_i s_{i+1}$. Similarly, if we have an Ising model on the square grid $\Z^2$, then $H = -J \sum_{i} (s_{i} s_{i+(1, 0)} + s_{i} s_{i+(0, 1)})$.</span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>We are not going to explain why, but in statistical mechanics, we have to calculate the following, slightly mysterious quantity $Z$, called the partition function. Everything in statistical mechanics bursts forth from this one function. It is no exaggeration to say that the story of statistical mechanics is mostly about calculating $Z$ at higher and higher precisions using larger and larger computers while losing more and more hair on the head. For the Ising model, $Z$ is defined as</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>Z := \sum_{s} e^{-\beta H(s)}</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>where $\beta = 1/T$ is the inverse of the temperature $T$. The summation is over all possible configurations of the system. For example, if the Ising model is over $<span class="sc">\{</span>0, 1, 2, \dots, 99<span class="sc">\}</span>$, then the summation is over $<span class="sc">\{</span>-1, +1<span class="sc">\}</span>^{100}$. </span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>The temperature turns out to be not useful during the RN calculation, so to make the notation cleaner, we absorb it into the $J$ like this: </span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>$$Z = \sum_{s} e^{-\beta H(s)}, \quad H(s) = -J_0 \sum_{i, j} s_i s_{j}, \quad J_0 = J/T$$</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>After we finish the calculation, we should write the temperature explicitly again in order to interpret what we have found.</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kadanoff decimation, take 0</span></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>Like ancient Roman decimation, the idea of Kadanoff decimation is to pick out some spins for elimination, leaving behind the other spins gathered around the mangled nothings of their former comrades. In the case of the 1D Ising model, decimation is exactly solvable, making it a good first problem to try.</span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>Let us consider the Ising model on the integer line $\mathbb{Z}$ with periodic boundary conditions. That is, we have $2n$ spins $s_0, s_1, \dots, s_{2n-1}$ arranged in a circle. Now, we keep the even spins white and decimate the odd spins. How can we do that? Well, behold:</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>Z &amp;= \sum_{s_0, s_1, \dots, s_{2n-1}} e^{+J(s_0 s_1 + s_1s_2 + \dots + s_{2n-1}s_0)} <span class="sc">\\</span></span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a>  &amp;= \sum_{s_0, s_1, \dots, s_{2n-2}} \sum_{s_1, s_3, \dots, s_{2n-1}} e^{J(s_0 s_1 + s_1s_2 + \dots + s_{2n-1}s_0)} <span class="sc">\\</span></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>  &amp;\overset{\mathrm{hopefully}}{=} \sum_{s_0, s_2, \dots, s_{2n-2}} e^{J'(s_0 s_2 + s_2 s_4 + \dots + s_{2n-2}s_0)} <span class="sc">\\</span></span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>where we have hopefully written the $J'$. It is our ardent hope that there exists some $J'$, somewhere in the universe, that will make the equation come true.<span class="ot">[^ansatz-borges]</span></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="ot">[^ansatz-borges]</span>:</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>    Physicists call it "ansatz", but I prefer to say it like this:</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; The author of an atrocious undertaking ought to imagine that he has already accomplished it, ought to impose upon himself a future as irrevocable as the past.</span></span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; --- [Yu Tsun](https://en.wikipedia.org/wiki/The_Garden_of_Forking_Paths), professor of English at Deutsch-Chinesische Hochschule.</span></span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>Because each even spin can only reach its nearest-neighboring even spins *via* the odd spin between them, the partition function splits cleanly:</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a>Z = \sum_{s_0, s_2, \dots} \left(\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)}\right) \left(\sum_{s_1} e^{J(s_2 s_3 + s_3 s_4)}\right) \cdots </span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>Thus, our wish would be fulfilled if $\sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = e^{J's_0s_2}$ for all 4 possible choices of $s_0, s_2$. Now, by direct calculation, we see that our task is to solve the following 4 equations simultaneously:</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="pp">|</span> + <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>--<span class="pp">|</span>--<span class="pp">|</span>--<span class="pp">|</span></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> + <span class="pp">|</span> $e^{J'} = 2 \cosh(2J)$ <span class="pp">|</span> $e^{-J'} = 2$ <span class="pp">|</span> </span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> $e^{-J'} = 2$ <span class="pp">|</span> $e^{J'} = 2 \cosh(2J)$ <span class="pp">|</span></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a>Immediately we see a problem: if both $e^{J'} = 2 \cosh(2J)$ and $e^{-J'} = 2$ hold, then $1 = 4 \cosh(2J)$. This means that we have to introduce a "fudge factor" again. Does that remind you of the $\alpha$ from the logistic map calculation? It should. Add in the fudge factor $k$ with:</span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>Z = k^{n} \sum_{s_0, s_2, \dots, s_{2n-2}}e^{J'(s_0 s_2 + s_2 s_4 + \dots + s_{2n-2}s_0)}, \quad \sum_{s_1} e^{J(s_0 s_1 + s_1 s_2)} = ke^{J's_0s_2}</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>Now the solution is: $J' = \frac 12 \ln \cosh(2J), k = 2\sqrt{\cosh(2J)}$. Again, we see that the RN flow equation</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>J \mapsto \frac 12 \ln \cosh(2J)</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>But this time, the RN flow has only a single stable fixed point at $J = 0$. Not only that, since $\frac 12 \ln \cosh(2J) \approx J^2$ if $J$ is small, if we decimate for $n$ times, we would end up with $J' \sim J^{2^n}$. What does this mean?</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a><span class="al">![The RN flow for the 1D Ising model has only one fixed point at zero.](figure/rn_flow_1d_ising.svg)</span></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a>Suppose we start with a magnet with interaction strength $J$, then after we zoom out for $n$ times, where $n$ is large, we would end up with a decimated view of the magnet, where we can only see the atoms at sites $0, 2^n, 2\times 2^n, 3 \times 2^n, \dots$. Our RN flow calculation states that the strength of interaction between $s_0$ and $s_{2^n}$ are just the multiplication of all the bonds between them. This is not so strange if we think about it: on a 1D Ising model, the only way for $s_0$ to interact with $s_{2^n}$ is if they laboriously go, bond-by-bond, across all the $2^n$ bonds, and at each bond, the interaction strength must be degraded. It would be like trying to draw with a pair of chopstick holding a pair of chopsticks holding a pair of chopsticks... holding a paintbrush. You rapidly run out of all bonding strength.</span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a>In particular, it shows that no matter how strong $J$ is, the correlation between distant atoms would decay rapidly, and so there is no ferromagnetism on the 1D Ising model. This disappointment led Ising to abandon this model of magnetism back in 1925.</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a>If you are ambitious, you can try doing the same RN analysis for a "ladder" made of two $\Z$ put side-by-side. If you have "a lot of time" like Onsager, you would be <span class="co">[</span><span class="ot">on your way to solving the Ising model in 2 dimensions</span><span class="co">](#sec-note-onsager-ising-2d)</span>. I have not worked this out, because I don't have a lot of time, but it would involve a 4-by-4 matrix. See <span class="co">[</span><span class="ot">page 12</span><span class="co">](https://web.archive.org/web/20120713215413/http://micro.stanford.edu/~caiwei/me334/Chap12_Ising_Model_v04.pdf)</span> for a sketch of solution.</span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kadanoff decimation, take 1 (abortive) {#sec-kadanoff-decimation-2d-ising}</span></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>Now that we have done it for 1D Ising model, we can attempt it for the 2D model. We will not be able to solve it exactly, but we will see what is going to happen. To start our analysis, we write out the partition function and Hamiltonian energy function:</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>Z = \sum_s e^{-H}, \quad H = -J \sum_{i, j \text{ are neighbors}} s_i s_j</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>where again we have absorbed temperature into the Hamiltonian. To perform decimation, we paint the square grid like a checkerboard, and eliminate all the black-colored spins.</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">Figure source</span><span class="co">](https://phas.ubc.ca/~berciu/TEACHING/PHYS502/PROJECTS/21-Jonah.pdf)</span>](figure/Kadanoff%20decimation.png)</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a>However, we can see a problem in the picture. Whereas in the original, un-decimated grid, only nearest neighbors are connected by a bond. In the decimated grid, not just the nearest neighbors, but also the next-nearest-neighbors are connected by a double-bond. Not only can you go from $(-1, 0)$ to a nearest-neighbor $(0, +1)$ via the to-be-decimated atom $(0, 0)$, but also go to the next-nearest neighbor $(+1, 0)$. Not only that, the square of 4 spins neighboring $(0, 0)$ are connected via $(0, 0)$, so we also need to account for that interaction. Thus, the renormalized Hamiltonian would have three kinds of terms: nn term, nnn term, and square-interaction term:</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>-H' = J_{nn} \sum_{i, j \text{ are nn}} s_i s_j + J_{nnn} \sum_{i, j \text{ are nnn}} s_i s_j + J_{\square} \sum_{i, j, k, l \text{ are }\square} s_i s_j s_k s_l</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Exact solution" collapse="true"}</span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a>Let the grid have $N$ sites. We can ignore the borders, or roll up the borders like how we did it in the 1D case -- it does not matter when $N$ is large. The partition function of the system is</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>Z = \sum_{s \in <span class="sc">\{</span>-1, +1<span class="sc">\}</span>^\text{grid}} \exp\left(J \sum_{ i, j \text{ are nn}} s_i s_j\right)</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a>Now, decimate half of the grid, and leave behind the other half. As derived in <span class="co">[</span><span class="ot">@marisTeachingRenormalizationGroup1978</span><span class="co">]</span>, we have</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a>Z = f(J)^{N/2} \sum_{s \in <span class="sc">\{</span>-1, +1<span class="sc">\}</span>^\text{decimated grid}} \exp\left(J_{nn} \sum_{ i, j \text{ are nn}} s_i s_j + J_{nn} \sum_{ i, j \text{ are nnn}} s_i s_j + J_{\square} \sum_{ i, j, k, l \text{ are }\square} s_i s_j s_k s_l\right)</span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>f(J) &amp;= 2 \cosh ^{1 / 2}(2 J) \cosh ^{1 / 8}(4 J) <span class="sc">\\</span></span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a>J_{nn} &amp;= (1 / 4) \ln \cosh (4 J) <span class="sc">\\</span></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a>J_{nnn} &amp;= (1 / 8) \ln \cosh (4 J) <span class="sc">\\</span></span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a>J_{\square} &amp;= (1 / 8) \ln \cosh (4 J)-(1 / 2) \ln \cosh (2 J)</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a>Why don't we have the triangle terms like $J_\Delta s_{(-1, 0)}s_{(0, +1)}s_{(+1, 0)}$?</span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a>Since the system has the same energy if we flip all spins, the Hamiltonian is even, and so the triangle-term must be exactly zero.</span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a>Well, that's fine, you might say, what's the big deal? The big deal is that, if we were to repeat the decimation again, we will suddenly meet *infinitely many* interaction terms! How so?</span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a>Look at the square grid again. Suppose we have already included the nnn interactions, and we were to do a decimation. As we have seen, in order that the partition function before-and-after decimation are the same, we must construct one interaction term for each possible interaction mediated by a decimated spin. Well, $(-1, 0)$ and $(0, +1)$ are mediated by $(0, 0)$, so they must have an interaction term. No problem, that's the nn term, written as $J_{nn}s_{(-1, 0)}s_{(0, +1)}$. Similarly, $(-1, 0)$ and $(+1, 0)$ are mediated by $(0, 0)$, so they must have an interaction term. That's the nnn term, written as $J_{nnn}s_{(-1, 0)}s_{(+1, 0)}$. Do you see it now?</span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a>We can connect $(-1, 0)$ and $(3, 0)$ via $(0, 0), (1, -1), (2, 0)$. Not only that, we can connect them by *infinitely many* possible routes: $(0, 0), (1, -1), (2, 0), (3, -1)$, and $(0, 0), (1, -1), (2, 0), (3, 1)$, etc. And that's not even the worst of it. Just like how we have arbitrary pair-wise interactions, we also have arbitrary group-interactions within subsets with arbitrary shapes. It is all a big mess, which goes to show how difficult the Ising problem already is in 2D, let alone in 3D.</span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a>We can of course deal with the problem by cutting off the weak interaction terms, like how we did the logistic map example. For example, we can preserve all interaction terms between spins that are at most 6 distances away, then compute the RN flow on the space of all Ising models on the square grid, find a saddle point, and compute the largest eigenvalue of the RN flow near the saddle point. The more terms we account for, the better the approximation is.</span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; These calculations were much more elaborate than the model calculation described here; in my own work, for example, 217 couplings between spins were included. The critical exponents derived from the calculation agree with Onsager's values to with­ in about 0.2%.</span></span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@wilsonProblemsPhysicsMany1979</span><span class="co">]</span></span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a><span class="fu">### Migdal bond-moving trick</span></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a>Before we are finished with the Ising model, here is one more trick: the Migdal bond-moving trick. The idea is shown in the diagram.</span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The Migdal bond-moving trick. [@kadanoffStatisticalPhysicsStatics1999, figure 14.2].</span><span class="co">](figure/Migdal%20bond-moving.png)</span></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a>We perform the trick in two steps. First, we take the odd-numbered vertical bonds and move them to the even-numbered ones. Next, we decimate the crossed-out spins. Since this trick does not handle the vertical and horizontal bonds equally, we write the bond-strengths separately as $J_x, J_y$. The RN flow of this step is</span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a>(J_x, J_y) \mapsto \left(2 J_x, \frac 12 \ln \cosh(2 J_x)\right)</span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>After doing the Migdal bond-moving trick once in the horizontal direction, we can do it again in the vertical direction. The total effect is to have decimated 3/4 of the grid, preserving one spin per 2-by-2 block of spins. The RN flow of the whole process is:</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a>(J_x, J_y) \mapsto (f_1(J_x), f_2(J_y))</span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a>where $f_1(x) = \ln\cosh(2x)$ and $f_2(x) = \frac 12 \ln\cosh(4x)$. This RN flow has saddle point $(0.609, 0.305)$. If we apply the same RN, but in the other order (first horizontal, then vertical), we have instead $(J_x, J_y) \mapsto (f_2(J_x), f_1(J_y))$, with saddle point $(0.305, 0.609)$. Well, the true saddle point of the whole system should have equal $J_x, J_y$, since the true system does not discriminate the horizontal and vertical, so the cheap answer is to take their average: $(0.609 + 0.305)/2 = 0.457$.</span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>According to Onsager's exact solution, the true critical point is $J_c = \frac{\ln(1+\sqrt 2)}{2} = 0.4407$. So by this simple trick, we have already gotten within 3.7% of the true answer.</span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>A small improvement is to find the "double fixed point". That is, we consider applying the first RN flow, then the second, and calculate the fixed point of this compound RN flow. That is, we expect the real fixed point to be the simultaneous solution to </span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a>$$x = f_1(f_2(x)); \quad x = f_2(f_1(x))$$</span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a>Now, these have different solutions, so we do the obvious thing and take their midpoint. This gives $0.4417$. And with that simple idea, I got within 0.23% of the true answer.</span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a>We can also calculate the scaling exponent, like how we found $\delta$ for the logistic map. The average gradient at the fixed point is $\frac{(f_1\circ f_2)' + (f_2\circ f_1)'}{2}$ which is $2.7633$, corresponding to a length scaling exponent of $\nu = \frac{\ln 4}{\ln 2.7633} = 1.364$, which is well off the real value of 1.</span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>::: {#fig-migdal-rn-flow layout-ncol=1}</span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/Migdal_trick_ising.svg)</span>{width=70%}</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/Migdal_trick_ising_zoom.svg)</span>{width=70%}</span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a>Two RN flows on the $(J_x, J_y)$ plane found by Migdal bond-moving, and the saddle points thereof.</span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a><span class="fu">### Side note: Onsager's solution of Ising model in 2D {#sec-note-onsager-ising-2d}</span></span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a>So far, we have been doing it for the Ising model on $\Z$. But it's clear that we can also do it for two $\Z$s put side by side like a ladder. Each "rung" of the ladder can be thought of as one big atom, with 4 possible states: up-up, up-down, down-up, down-down. We can then do the same calculation as the previous case, except that instead of a $2\times 2$ matrix, we have a $4 \times 4$ matrix.</span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a>For that matter, we can do it for any finite number of those $\Z$ put together. We can then imagine doing that for such ladders with widths $2, 3, 4, 5, 6, \dots$, then discover a pattern, and take the limit. If this works, we would solve the Ising model on $\Z^2$.</span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a>Arduous as it sounds, this is exactly how Lars Onsager arrived at his solution for the Ising model on $\Z^2$. He calculated up to ladders with width 6, diagonalizing matrices of size $64\times 64$ in the process, then he guessed the general pattern and proceeded from there, emboldened by the guess. As Chen-Ning Yang reported:</span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In 1944, L. Onsager produced, quite unexpectedly, an exact evaluation of the partition function of the model in two dimensions. It was a real tour de force. I had studied his paper in Chicago in the spring of 1947, but did not understand the method, which was very, very complicated, with many algebraic somersaults....</span></span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In March, 1965... I asked him how it came about that he took all those complicated algebraic steps in his paper of 1944. He said he had a lot of time during the war, so he began to diagonalize the transfer matrix, which had already been discussed by E. Montroll and by H. A. Kramers and G. H. Wannier. He started with a $2 \times \infty$, then a $3 \times \infty$, then a $4 \times \infty$ lattice. He then went on to a $5 \times \infty$ lattice, for which the transfer matrix is $32 \times 32$ in size. Such a matrix is quite large, but the experience he had gained with the smaller matrices came in handy, and he was able, after some time, to find all 32 of the eigenvalues. He proceeded then to the $6 \times \infty$ case, and eventually diagonalized the $64 \times 64$ matrix, finding that all the eigenvalues were of the form $e^{\pm \gamma_1 \pm \gamma_2 \pm \gamma_3 \pm \gamma_4 \pm \gamma_5 \pm \gamma_6}$. That led to the concept that the algebra of the problem was a product algebra, and hence the manipulations in his paper. </span></span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@yangSelectedPapers194519802005, pages 11--13</span><span class="co">]</span></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a>I also want to quote this from the same pages, because it describes accurately what it feels like to do real-space RN:</span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a long calculation, the longest in my career. Full of local, tactical tricks, the calculation proceeded by twists and turns. There were many obstructions. But always, after a few days, a new trick was somehow found that pointed to a new path. The trouble was that I soon felt I was in a maze and was not sure whether in fact, after so many turns, I was anywhere nearer the goal than when I began. This kind of strategic overview was very depressing, and several times I almost gave up. But each time something drew me back, usually a new tactical trick that brightened the scene, even though only locally. Finally, after about six months of work off and on, all the pieces suddenly fitted together, producing miraculous cancellations, and I was staring at the amazingly simple final result </span><span class="sc">\[</span><span class="at">spontaneous magnetization of the Ising model</span><span class="sc">\]</span><span class="at"> </span></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@yangSelectedPapers194519802005, page 12</span><span class="co">]</span></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bonus: Edgeworth series</span></span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a>In a typical intermediate probability course taught to graduate students, the central limit theorem is taught in two lines: For any probability distribution with finite variance, check how its characteristic function varies under sum-then-average operations, and check that it converges to the characteristic function of the gaussian distribution.</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a>This simple argument is the seeds to functional renormalization theory. This section is based on <span class="co">[</span><span class="ot">@sethnaStatisticalMechanicsEntropy2021, exercise 12.11</span><span class="co">]</span>. </span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a>If we have a sequence of random variables $X_1, X_2, \dots$, then we can do the double-then-average trick repeatedly:</span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a>X_1, \frac{X_1 + X_2}{\sqrt 2}, \frac{\frac{X_1 + X_2}{\sqrt 2} + \frac{X_3 + X_4}{\sqrt 2}}{\sqrt 2}, \dots</span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a>Now, consider Fourier transforms to the probability density function</span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a>\tilde\rho(k) := \int_\R e^{-ikx} \rho(x) dx</span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a>If $X, X'$ are sampled from the pdf $\rho$, then $\frac{X+X'}{\sqrt 2}$ has the pdf $\rho'$ such that  </span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a>$$\tilde{\rho'}(k) = \tilde\rho(k/\sqrt 2)^2$$</span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a>Thus, we define an operator by $R<span class="co">[</span><span class="ot">\tilde f</span><span class="co">](k)</span> = \tilde f(k/\sqrt 2)^2$, and the problem of central limit theorem is finding the fixed points of $R$ and their local stability.</span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="derivation" collapse="true"}</span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>The pdf of $\mathcal N(\mu, \sigma^2)$ transforms to $e^{-ik\mu - \frac 12 \sigma^2 k^2}$, so $\mathcal N(0, \sigma^2)$ is a fixed point of $R$.</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a>Let $\rho^*$ be the pdf for $\mathcal N(0, 1)$, then the question is to find the local linear expansion of $R$ near $\tilde \rho^*$. That is, we want to find the eigenvectors of $R$:</span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>R<span class="co">[</span><span class="ot">\tilde \rho^* + \epsilon f</span><span class="co">]</span> = \tilde \rho^* + \lambda\epsilon f</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>Now, $\tilde f_n := (ik)^n \tilde \rho^*(k)$ has eigenvalue $1/2^{\frac{n-2}{2}}$. Since these $\tilde f_0, \tilde f_1, \dots$ allow for Taylor expansion, these exhaust all possible eigenvectors of $R$ near $\tilde \rho^*$.  </span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>Consider a generic function $(1+a_0) \tilde f_0 + a_1 \tilde f_1 + \cdots$. It corresponds to a pdf </span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>\rho = (1+a_0) f_0 + a_1 f_1 + \cdots</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a>f_n(x) = \frac{1}{2\pi} \int_\R e^{ikx}\tilde f_n(k) dk = \partial_x^n \rho^*(x) = \rho^*(x) He_n(-x/\sigma)/\sigma^n</span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a>He_0 = 1, He_1 = x, He_2 = x^2 - 1, He_3 = x^3 - 3x, \dots</span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a>Those are the <span class="co">[</span><span class="ot">probabilist's Hermite polynomial</span><span class="co">](https://en.wikipedia.org/wiki/Hermite_polynomials)</span>.</span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a>The only dangers to convergence are $f_n$ where $n = 0, 1, 2$, since the other terms have eigenvalue less than 1. The effect of adding a small amount of $f_0, f_1, f_2$ to $\rho^*$ is to change it from $\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x^2}{2\sigma^2}}$ to</span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a>\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x^2}{2\sigma^2} + \ln a_0 - a_1 \frac{x}{\sigma^2} + a_2 \frac{x^2 - \sigma^2 }{\sigma^4}}</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a>So we see that the effect of $a_0$ is to normalize the probability mass to 1, the effect of $a_1$ is to shift the mean, and $a_2$ is to change the variance.</span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a>Therefore, if we always subtract the mean and dividing by the variance, then the first three perturbation terms $f_0, f_1, f_2$ are zeroed out, leaving the other terms decaying to zero.</span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a>Normalize $X$ to have mean 0 and variance 1, then we find that the pdf of $2^{-n}\sum_{i=1}^{2^n}X_i$ is</span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a>\rho_{2^n} = \rho^*(1 + 2^{-n/2}a_3 He_3(-x) + 2^{-n}a_4 He_4(-x) + \cdots) </span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a>$$e^{\frac 12 k^2}\tilde \rho(k) = 1 + a_3 (ik)^3+ a_4 (ik)^4 + \cdots$$</span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="example: Bernoulli variable" collapse="true"}</span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a>The Bernoulli variable $\frac 12(\delta_{-1} + \delta_{+1})$ has</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a>$$e^{\frac 12 k^2}\tilde \rho(k) = e^{\frac 12 k^2}\cos k = 1 - \frac{k^4}{12} - \frac{k^6}{45} + \cdots$$</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>giving</span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>a_4 = -\frac{1}{12}, a_6 = \frac{1}{45}, \dots</span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a>\rho_{2^n}(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac 12 x^2} \left<span class="co">[</span><span class="ot">1 -\frac{1}{12} 2^{-n} He_4(-x) + \frac{1}{45} 2^{-2n} He_6(-x) + \cdots\right</span><span class="co">]</span></span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a>The exact result is</span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a>\rho_{2^n} = \sum_{i=0}^{2^n} \frac{1}{2^{2^n}} \binom{2^n}{i}  \delta_{\frac{-2^n + 2i}{2^{n/2}}}</span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a>which reduces to the previous result by Stirling approximation.</span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bonus: Generalized central limit theorem</span></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a>This section is based on <span class="co">[</span><span class="ot">@amirElementaryRenormalizationgroupApproach2020</span><span class="co">]</span>.</span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a>Consider three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto $\mu=0.7$. We notice two facts:</span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The random walks are self-similar. A small section has the same look-and-feel as a large section of it.</span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Different random walks have very different characters. The Gaussian walk appears smoother, while the Cauchy and Pareto walks display more dramatic jumps and bursts, reflecting the heavier tails of their respective distributions.</span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a><span class="al">![Three random walks generated from three distinct probability distributions: Gaussian, Cauchy, and Pareto $\mu=0.7$](figure/generalized_random_walks_CLT.svg)</span></span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a>When we see self-similarity, we think RN. Can we use RN to study random walks? Yes.</span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a>Let's take a fresh look at the central limit theorem. It says that if $X_1, X_2, \dots$ are IID samples from a distribution with finite mean $E<span class="co">[</span><span class="ot">X</span><span class="co">]</span>$ and variance $V<span class="co">[</span><span class="ot">X</span><span class="co">]</span>$, then $\frac{(X_1 + \dots + X_n) - n E<span class="co">[</span><span class="ot">X</span><span class="co">]</span>}{\sqrt{n V<span class="co">[</span><span class="ot">X</span><span class="co">]</span>}}$ converges to the standard normal distribution. If we think about it from the RN point of view, we can decompose each $X$ into a sum of two random variables: $X_i = A_i + Z_i$, where $Z_i$ is a normal distribution with the same mean and variance, and $A_i$ is the "noise" part of it. Each $A_i$ might be overpowering, but when we repeatedly coarse-grain by taking a bunch of $X_i$, and adding them up (a lossy operation!), we would eventually destroy all traces of what cannot survive coarse-graining, and leaving behind a fixed-point of coarse-graining.</span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a>We define the following letters:</span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X_1, X_2, \dots$ are IID random variables, with characteristic function $\phi_X(t)= E<span class="co">[</span><span class="ot">e^{itX}</span><span class="co">]</span>$.</span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$S_n = X_1 + \dots + X_n$.</span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$a_n, b_n$ are two sequences of real numbers, such that $\frac{S_n - b_n}{a_n}$ converges in distribution to a nontrivial random variable $Z$ with characteristic function $\phi(t)$.</span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Deriving the field equation by RN" collapse="true" }</span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a>Since $\frac{X_1 + \dots + X_n - b_n}{a_n}$ converges in distribution to a nontrivial random variable, the sequence $a_n$ must diverge to infinity. For, if the sequence $a_n$ is bounded, then for large enough $n$ , the sum $X_1 + \dots + X_n$ would spread wider and wider, and dividing it by $a_n$ cannot keep it together.</span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a>Let $Z$ be a random variable with characteristic function $\phi$. By assumption, $(S_n- b_n)/a_n$ is approximately distributed like $Z$ , that is, $S_n$ is approximately distributed as $a_nZ + b_n$. Thus,</span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a>$$\phi_{S_n}(t) \approx e^{ib_n t}\phi(a_nt )$$</span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a>Given $1 \ll n \ll N$ , we can compute $\phi_{S_N}$ in two ways: adding it up as $N$ copies of $X$ , or adding it up as $N/n$ copies of $S_n$. Both should give us the same result. That is:</span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a>$$\phi_{S_N} = \phi_X^N = \phi_{S_n}^{N/n}$$</span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a>However, since $n$ is very large, we have the approximations $\phi_{S_n}(t) \approx e^{ib_n t}\phi(a_nt )$. Thus, we have</span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a>\ln \phi_{S_N}(t) \approx \frac{N}{n}(ib_n t + \ln\phi(a_n t))</span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a>Note how we have an exponent of the form $Nf(n)$ , where $N$ is a very large number, and $n$ is a number that is small compared to it. This is a common pattern in RN calculation.</span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a>Since $n$ is small compared to $N$ , but large compared to $1$ , we can pretend that it's a continuous variable, and take derivative of it. Since the left side is independent of $n$ , the derivative should be zero:</span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a>\partial_n \frac{N}{n}(ib_n t + \ln\phi(a_n t)) = 0</span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a>Simplifying it, and substituting $t$ for $a_n t$ , we get the **field equation**</span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a>$$\frac{\phi'(t)}{\phi(t)}t - \ln \phi(t) \frac{a_n}{n \partial_n  a_n} + it\partial_n (b_n/n) \frac{n}{\partial_n a_n} = 0$$</span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a>Thus, we have obtained the field equation:</span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a>$$\frac{\phi'(t)}{\phi(t)}t -\frac{a_n}{n \partial_n  a_n} \ln \phi(t)  + \frac{n\partial_n (b_n/n)}{\partial_n a_n} it = 0$$</span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a>which we can solve by standard mathematical analysis without any more use of RN, so we don't do those. You can read <span class="co">[</span><span class="ot">@amirElementaryRenormalizationgroupApproach2020</span><span class="co">]</span> if you are interested.</span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a>However, there is a problem: If we have a "field" equation, what is the "field"?  Well, here is one way to think of it.</span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a>Imagine a line of atoms, at locations $1, 2, 3, \dots$. Each atom has a height $X_1, X_2, X_3, \dots$. Now, we can coarse-grain the system by a factor of $4$, by defining </span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>$$Y_1 = \frac{X_1 + \dots + X_4 - b_4}{a_4}, \quad Y_2 = \frac{X_5 + \dots + X_8 - b_4}{a_4}, \quad \dots$$</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a>from which we can perform another coarse-graining by a factor of $100$, ending up with a coarse-grain by a factor of $400$. Now, if the system has a nontrivial scaling limit, then this should give us the same result as doing a coarse-graining by $5$, then by $80$, or first $6$ then $67$. This is the RN argument we used here.</span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a>Now, since $S_n \approx a_n Z + b_n$, we see that $b_n$ can be thought of as the coarse-grained height of the height field, and $a_n$ as the  coarse-grained jaggedness of the height-field. Then, the field equation describes how the two numbers vary according to $n$.</span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Exercise: extreme value distribution" collapse="true" }</span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a>The maximum of random variables often has a nontrivial scaling limit as well. That is, there exists some sequence $a_n, b_n$ such that $\frac{\max(X_1, \dots, X_n) - b_n}{a_n}$ converges to a nontrivial distribution with cumulative distribution function (CDF) $F$.</span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a>Let $F_X$ be the CDF of $X$; then we have $F_{\max(X_1, \dots, X_N)}(t) = F_{\max(X_1)}(t)^{N}$. Now, derive the field equation by an RN argument.</span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>Answer: $\partial_n \frac 1n \ln F(\frac{t-b_n}{a_n}) = 0$.</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/banner/thumbnail_7.png)</span></span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wilson's Nobel Prize: RN in momentum space</span></span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a><span class="al">![Evolution of Firefox icon under RN flow in frequency space. The high-spatial-frequency details are summed away, leaving low-frequency broad brushstrokes.](figure/firefox-renormalization-group-theory.jpeg)</span></span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a>**RN in momentum space**, or **RN in frequency space**, is the most widely used method of RN nowadays. It is called "momentum" because it was first done by quantum field theorists to study subatomic particles, and in quantum mechanics, the spatial frequency of a particle-wave is proportional its momentum: $p = \hbar k$, for which <span class="co">[</span><span class="ot">de Broglie</span><span class="co">](https://en.wikipedia.org/wiki/Louis_de_Broglie)</span> was awarded a Nobel Prize in 1929.</span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a>In the 1970s, <span class="co">[</span><span class="ot">Kenneth Wilson</span><span class="co">](https://en.wikipedia.org/wiki/Kenneth_G._Wilson)</span> invented RN in momentum space and used it to solve many problems. He was awarded the 1982 Nobel Prize in Physics for this work.</span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a>Unfortunately, unlike RN in real space, RN in momentum space is extremely verbose, requiring pages and pages of symbols. Instead of subjecting you to the horrible experience, I will sketch out the big ideas only.</span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; There remained the possibility that there might be smaller but still infinite quantities left over. No one had the patience needed to calculate whether these theories were actually completely finite. It was reckoned it would take a good student two hundred years, and how would you know he hadn't made a mistake on the second page? Still, up to 1985, most people believed that most supersymmetric supergravity theories would be free of infinities.</span></span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@hawkingUniverseNutshell2001, page 52</span><span class="co">]</span></span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; To know for sure whether a Feynman diagram with three virtual graviton loops produces infinite quantities, we would need to evaluate $10^{20}$ terms. By five loops, a diagram spawns $10^{30}$ terms ... The unitarity method has completely changed the situation ... What would have taken the Feynman technique $10^{20}$ terms, we can now do with dozens. ... we found that the 1980s era speculations were wrong. Quantities that seemed destined to be infinite are in fact finite. Supergravity is not as nonsensical as physicists thought. In concrete terms, it means that quantum fluctuations of space and time are much more innocuous in supergravity than previously imagined. If you ply us with fine wine, you might catch us speculating that some version of it might be the long sought quantum theory of gravity. </span></span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@bernLoopsTreesSearch2012</span><span class="co">]</span></span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a><span class="fu">### Field theory: continuous Ising model</span></span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a>Arguably the first field theory was hydrodynamics. Working in the era just after Newton, Euler and Lagrange understood water as a block of infinitely many tiny mass-points. Because each point is so tiny, they do not study the velocity of individual particles of water, but study the velocity *field* of the entire block of water.</span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a>The Ising model, with its grid of spins, provides a clear example of this continuum limit. As the grid of spins grow large, the individuals blur together into a field, similar to deriving hydrodynamics from particle dynamics. Let's take a concrete example, of Ising model on the square grid $\Z^2$.</span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a>Initially, the system's energy and partition function are represented as a sum over individual spins:</span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a>Z = \sum_{s: \Z^2 \to <span class="sc">\{</span>-1, +1<span class="sc">\}</span>} e^{-H(s)}, \quad H(s) = -J \sum_{i, j \text{ are nearest neighbors}} s_i s_j</span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a>Recall how, <span class="co">[</span><span class="ot">after two Kadanoff decimations</span><span class="co">](#sec-kadanoff-decimation-2d-ising)</span>, all forms of spin-spin interactions are unlocked, and so we arrive at an energy function in the most general form:</span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a>Z = \sum_{s: \Z^2 \to <span class="sc">\{</span>-1, +1<span class="sc">\}</span>} e^{-H(s)}, \quad H(s) = -\sum_{\text{configuration }C} \sum_{i_1, i_2, \dots \text{ are configured like }C} J_C s_{i_1}s_{i_2}\cdots</span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a>We can convert the summation into an integral, and suggestively write it as $Z = \int_{s: \Z^2 \to <span class="sc">\{</span>-1, +1<span class="sc">\}</span>} e^{-H(s)} ds$. </span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a>In the continuous limit, the discrete field of spins $s: \Z^2 \to <span class="sc">\{</span>-1, +1<span class="sc">\}</span>$ blurs into a continuous field of spins $\phi: \R^2 \to \R$. The energy $H(s)$ becomes $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>$, a functional<span class="ot">[^functional-s]</span> an $\phi(x)$ and its gradients. This gives us</span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a>Z = \int_{\R^2 \to \R} D<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> \; e^{-S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>}</span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a><span class="ot">[^functional-s]: </span>A "functional" is nothing but a special kind of function. Specifically, it is a function of type $\text{function} \to \text{number}$. We use square brackets in $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>$, not round brackets like $S(\phi)$, because it is conventional for functionals to use square brackets, not round brackets. It is written as $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>$ rather than $H<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>$, and called "action", because of some old historical usage in variational calculus (as in "the principle of least action").</span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a><span class="fu">### Field theory, in general</span></span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a>Consider a mattress as an analogy. Let the state The state of the mattress is determined by the height and velocity of each point. That is, the energy function of the mattress comprises two quadratic terms: height of each point; height differences between nearby points. We assume the mattress points are massless, so that we have no kinetic energy. We can write the energy of the mattress schematically as</span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a>\text{energy} = H(\phi) = \frac 12 K_0 \sum_i \phi_i^2 + \frac 12 K_1 \sum_{i, j \text{ are neighbors}} (\phi_i - \phi_j)^2</span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a><span class="al">![A mattress in space. The energy of the mattress is determined by the height of each point, and the height differences between neighboring points.](figure/mattress_field_theory.png)</span></span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a>If the mattress is held in an atmosphere of temperature $T$, then its state becomes uncertain, following a Boltzmann distribution, just like how a pollen's position in hot water becomes uncertain, due to Brownian motion.<span class="ot">[^hot-water-theory]</span> The probability that you would find the mattress in state $\phi$ is then $e^{-H(\phi)/T}/Z$, where $Z$ is the partition function again:</span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a><span class="ot">[^hot-water-theory]: </span>When statistical field theory gets too scary, I call it "hot water theory" to make it sound nicer. </span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a>Z = \int d\phi e^{- H(\phi)/T}</span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a>In the continuum limit, this analogy leads us to statistical field theory.</span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-811"><a href="#cb1-811" aria-hidden="true" tabindex="-1"></a>Calculating useful quantities within statistical field theory often involves complex mathematical techniques. This complexity arises from the need to compute the partition function, which involves integrating over all possible states of the system. In the case of the mattress analogy, the partition function involves an integral over all possible height fields, leading to the use of path integrals, written like $\int_{\phi: \R^2 \to \R} D<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> e^{-S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>/T}$.</span>
<span id="cb1-812"><a href="#cb1-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-813"><a href="#cb1-813" aria-hidden="true" tabindex="-1"></a>Integrating over $\R^{10^{23}}$ would be bad enough, let along integrating over $\R^{\R^{2}}$, and yet the miracle is that this can be done, and can be done in a way that matches experiments.</span>
<span id="cb1-814"><a href="#cb1-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-815"><a href="#cb1-815" aria-hidden="true" tabindex="-1"></a>Statistical field theory employs various tricks to calculate the partition function or its limits. A common approach involves alternating between discrete and continuous representations of the field for calculations.</span>
<span id="cb1-816"><a href="#cb1-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-817"><a href="#cb1-817" aria-hidden="true" tabindex="-1"></a>Interestingly, statistical field theory is almost isomorphic with quantum field theory. The general idea is that if you take the dimension of time in a quantum field theory over space of dimension $n$, and do a substitution $t \mapsto it$, you somehow end up with a statistical field theory over space of dimension $n+1$. This is the <span class="co">[</span><span class="ot">Wick rotation</span><span class="co">](https://en.wikipedia.org/wiki/Wick_rotation)</span>.</span>
<span id="cb1-818"><a href="#cb1-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-819"><a href="#cb1-819" aria-hidden="true" tabindex="-1"></a>For example, the 1D Ising model is analogous to a particle in a double well. Whereas the single particle might switch from left to right after time $\Delta T$, the Ising model chain might switch from $+1$ to $-1$ after space $\Delta L$. Because of quantum tunneling, it is impossible to confine a particle in one side of the well -- it will always jump to the other side, and the jumping probability is on the order of $1 - e^{-kt}$ for some constant $k$. This corresponds to the fact that there is no way to "freeze" an Ising model in one dimension. Even at low temperatures, the system cannot be entirely frozen. Long stretches of "up" spins can suddenly flip to "down" spins, no matter how cold the chain gets.  Similarly, in the quantum analogy, the particle can tunnel between the two wells, no matter how high the barrier between them gets.</span>
<span id="cb1-820"><a href="#cb1-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-821"><a href="#cb1-821" aria-hidden="true" tabindex="-1"></a>Generally, 1D statistical fields lack phase transitions, just like how a single particle with finitely many states would always quantum-tunnel between states, no matter how cold it gets. Conversely, since the 2D Ising model can be frozen, we naturally suspect that the quantum field theory on one dimension should have some kind of phase transition.<span class="ot">[^1d-qft-phase-transition]</span></span>
<span id="cb1-822"><a href="#cb1-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-823"><a href="#cb1-823" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1d-qft-phase-transition]: </span>I don't know what it is, but perhaps Bose--Einstein condensation? I mean, I just flipped through <span class="co">[</span><span class="ot">@herbutModernApproachCritical2007</span><span class="co">]</span> and the book suggests that this is it. Don't quote me on this.</span>
<span id="cb1-824"><a href="#cb1-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-825"><a href="#cb1-825" aria-hidden="true" tabindex="-1"></a>You get another perspective by noting that magnetism requires spontaneous symmetry breaking: you have more spins pointing up than down, even though the underlying energy function does not distinguish up from down. No symmetry breaking, no phase transition. And since the quantum states of a single particle can always tunnel between each other, it cannot fall into a symmetry-breaking state. However, since magnets can exist in 2D, we see that <span class="co">[</span><span class="ot">spontaneous symmetry breaking</span><span class="co">](#sec-more-is-different)</span> can occur for a 1D line of quantum particles evolving through time.</span>
<span id="cb1-826"><a href="#cb1-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-827"><a href="#cb1-827" aria-hidden="true" tabindex="-1"></a><span class="fu">### RN flow in the space of field theories</span></span>
<span id="cb1-828"><a href="#cb1-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-829"><a href="#cb1-829" aria-hidden="true" tabindex="-1"></a>Think back to the Ising model on $\R^2$. What is its action $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>$? If we were mathematically omnipotent, then we can simply perform RN flow on the discrete Ising model, and just find its fixed point, which should hopefully tell us what $S$ is. But we can't even perform a single RN flow. What to do?</span>
<span id="cb1-830"><a href="#cb1-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-831"><a href="#cb1-831" aria-hidden="true" tabindex="-1"></a>Well, by <span class="co">[</span><span class="ot">universality</span><span class="co">](#sec-universality)</span>, we can start with some very different discrete Ising model and end up with the same continuum limit after renormalizing enough times. Why can't we start with some very different *continuous* Ising model, discretize it to a discrete Ising model, then renormalize it again until we are back to a continuous Ising model? And if we can do that, why can't we renormalize directly in the space of all continuous Ising models? We can start with whatever Ising field theory we can write down, and then just repeatedly renormalize it. By universality, we will end up in an interesting place, no matter where we started.</span>
<span id="cb1-832"><a href="#cb1-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-833"><a href="#cb1-833" aria-hidden="true" tabindex="-1"></a>But before we do that, we have to construct the space of possible Ising field theories. As Wilson would say, it is <span class="co">[</span><span class="ot">all about the symmetries</span><span class="co">](#sec-symmetries-determine-theory-space)</span>.</span>
<span id="cb1-834"><a href="#cb1-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-835"><a href="#cb1-835" aria-hidden="true" tabindex="-1"></a>There are two kinds of symmetries: the symmetry without, and the symmetry within. For the Ising field $\phi: \R^2 \to \R$, we have Euclidean symmetry for $\R^2$, and up-down symmetry for $\R$. To ensure Euclidean geometry, the action $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>$ should not explicitly depend on the position. That is, if we take some $\phi$, and translate it by $\delta$, then we must have</span>
<span id="cb1-836"><a href="#cb1-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-837"><a href="#cb1-837" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-838"><a href="#cb1-838" aria-hidden="true" tabindex="-1"></a>S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> = S<span class="co">[</span><span class="ot">x \mapsto \phi(x + \delta)</span><span class="co">]</span></span>
<span id="cb1-839"><a href="#cb1-839" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-840"><a href="#cb1-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-841"><a href="#cb1-841" aria-hidden="true" tabindex="-1"></a>Similarly for reflections and rotations of the field. This shows that $S$ must involve only terms like $\phi, \nabla \phi, \nabla^2 \phi, \nabla \phi \cdot \nabla \phi$, etc. </span>
<span id="cb1-842"><a href="#cb1-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-843"><a href="#cb1-843" aria-hidden="true" tabindex="-1"></a>To account for the symmetry within -- the up-down symmetry -- we must have $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> = S<span class="co">[</span><span class="ot">-\phi</span><span class="co">]</span>$. This shows that $S$ must have only even-ordered terms</span>
<span id="cb1-844"><a href="#cb1-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-845"><a href="#cb1-845" aria-hidden="true" tabindex="-1"></a>Under these assumptions, you can convince yourself that the most generic form of $S$ is</span>
<span id="cb1-846"><a href="#cb1-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-847"><a href="#cb1-847" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-848"><a href="#cb1-848" aria-hidden="true" tabindex="-1"></a>S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> = \int d x\left<span class="co">[</span><span class="ot">\frac{1}{2} \nabla \phi \cdot \nabla \phi+\frac{1}{2} \mu^2 \phi^2+g \phi^4+\cdots\right</span><span class="co">]</span></span>
<span id="cb1-849"><a href="#cb1-849" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-850"><a href="#cb1-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-851"><a href="#cb1-851" aria-hidden="true" tabindex="-1"></a>where we removed an irrelevant constant term independent of $\phi$, and picked the scale of length so that the coefficient for $\frac{1}{2} \nabla \phi \cdot \nabla \phi$ is one.</span>
<span id="cb1-852"><a href="#cb1-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-853"><a href="#cb1-853" aria-hidden="true" tabindex="-1"></a>This is the space of all possible Ising field theories. Each Ising field theory is completely specified if we specify the real numbers $\mu, g, \dots$. Doing RN would then consist of taking one Ising field theory specified by some $\mu, g, \dots$, then renormalize it to some other Ising field theory specified by $\mu', g', \dots$. We can then find the fixed points in the theory-space, and say, "These are the most interesting theories. Let's calculate their properties, scaling exponents, and look-and-feel."</span>
<span id="cb1-854"><a href="#cb1-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-855"><a href="#cb1-855" aria-hidden="true" tabindex="-1"></a><span class="fu">### RN in momentum space</span></span>
<span id="cb1-856"><a href="#cb1-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-857"><a href="#cb1-857" aria-hidden="true" tabindex="-1"></a>Consider an Ising field $\phi: \R^2 \to \R$. It is of course possible to directly renormalize the field in real space: we blur it a bit, then zoom out. However, this turns out to be very hard to calculate with. Instead, it is much easier to renormalize the field in frequency space.</span>
<span id="cb1-858"><a href="#cb1-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-859"><a href="#cb1-859" aria-hidden="true" tabindex="-1"></a>To "blur and zoom out" in real space -- what does it look like if we take a Fourier transform? It would look like we are removing some high-frequency vibrations, then expand the vibrations so that low-frequency vibrations become high-frequency vibrations.</span>
<span id="cb1-860"><a href="#cb1-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-861"><a href="#cb1-861" aria-hidden="true" tabindex="-1"></a>Now we are ready to meet RN in momentum space.</span>
<span id="cb1-862"><a href="#cb1-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-863"><a href="#cb1-863" aria-hidden="true" tabindex="-1"></a>Let $\tilde \phi$ denote the Fourier transform of a field. Now, solve<span class="ot">[^fourier-operator]</span> for $\tilde S$, so that $S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> = \tilde S<span class="co">[</span><span class="ot">\tilde\phi</span><span class="co">]</span>$ for all $\phi$ in theory-space.</span>
<span id="cb1-864"><a href="#cb1-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-865"><a href="#cb1-865" aria-hidden="true" tabindex="-1"></a><span class="ot">[^fourier-operator]: </span>This is typically called "Fourier-transforming the operator". For example, the Fourier transform of the gradient operator $\nabla$ is $ik$, where $k$ is the wave vector.</span>
<span id="cb1-866"><a href="#cb1-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-867"><a href="#cb1-867" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-868"><a href="#cb1-868" aria-hidden="true" tabindex="-1"></a>Z = \int D<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> e^{-S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>} = \int D<span class="co">[</span><span class="ot">\tilde\phi</span><span class="co">]</span> e^{-\tilde S<span class="co">[</span><span class="ot">\tilde\phi</span><span class="co">]</span>}</span>
<span id="cb1-869"><a href="#cb1-869" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-870"><a href="#cb1-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-871"><a href="#cb1-871" aria-hidden="true" tabindex="-1"></a>Next, we restrict the domain of integration from all frequencies, to only frequencies below an upper limit $\Lambda$. This is called "frequency cut-off"<span class="ot">[^ultraviolet-catastrophe]</span>:</span>
<span id="cb1-872"><a href="#cb1-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-873"><a href="#cb1-873" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-874"><a href="#cb1-874" aria-hidden="true" tabindex="-1"></a>Z \approx \int_{\tilde \phi(k) \text{ is nonzero only for }<span class="sc">\|</span>k <span class="sc">\|</span> \leq \Lambda} D<span class="co">[</span><span class="ot">\tilde\phi</span><span class="co">]</span> e^{-\tilde S<span class="co">[</span><span class="ot">\tilde\phi</span><span class="co">]</span>}</span>
<span id="cb1-875"><a href="#cb1-875" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-876"><a href="#cb1-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-877"><a href="#cb1-877" aria-hidden="true" tabindex="-1"></a><span class="ot">[^ultraviolet-catastrophe]</span>:</span>
<span id="cb1-878"><a href="#cb1-878" aria-hidden="true" tabindex="-1"></a>    This frequency cut-off is often called "ultraviolet cutoff", because we throw away all frequencies that are too high, and for light, ultraviolet light is high-frequency. That's it. It started as a hack meant to remove the annoying infinities that crop up everywhere in quantum field theory. You might have heard mysterious whispers of how renormalization "cuts off infinities" and makes them "normal again". In fact, this is where the horrendous name "re-normalization" came from. If I had any choice in the matter, I would have called it "re-scaling". At least that would be more descriptive.</span>
<span id="cb1-879"><a href="#cb1-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-880"><a href="#cb1-880" aria-hidden="true" tabindex="-1"></a><span class="in">    For that matter, "renormalization group theory" is also a terrible name. To an applied physicist, the name "group theory" is abstract, inspiring fear and uncertainty. To a mathematician, the name "group theory" is plain wrong, because you can always undo a group-action, but you can never undo a coarse-graining.</span></span>
<span id="cb1-881"><a href="#cb1-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-882"><a href="#cb1-882" aria-hidden="true" tabindex="-1"></a><span class="in">    In modern QFT, this compromise has been converted into a triumph. Instead of a hack to remove infinities, it is now interpreted as a necessary fact about the world. To see high-frequency features in space, we have to be able to probe it. To probe it, we need to fire some high-frequency particles at a space. High-frequency particles have high energy, and the higher the energy gets, the more complicated the interaction gets, and in this way, the quantum field theory actually changes depending on our choice of frequency-cutoff.</span></span>
<span id="cb1-883"><a href="#cb1-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-884"><a href="#cb1-884" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Wilson's analysis takes just the opposite point of view, that any quantum field theory is defined fundamentally with a cutoff A that has some physical significance. In statistical mechanical applications, this momentum scale is the inverse atomic spacing. In QED and other quantum field theories appropriate to elementary particle physics, the cutoff would have to be associated with some fundamental graininess of spacetime, perhaps a result of quantum fluctuations in gravity. ... whatever this scale is, it lies far beyond the reach of present-day experiments. The argument we have just given shows that this circumstance explains the renormalizability of QED and other quantum field theories of particle interactions. Whatever the Lagrangian of QED was at its fundamental scale, as long as its couplings are sufficiently weak, it must be described at the energies of our experiments by a renormalizable effective Lagrangian.</span></span>
<span id="cb1-885"><a href="#cb1-885" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb1-886"><a href="#cb1-886" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; [@peskinIntroductionQuantumField1995, pages 402--403]</span></span>
<span id="cb1-887"><a href="#cb1-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-888"><a href="#cb1-888" aria-hidden="true" tabindex="-1"></a>Next, we pick some small number $\epsilon$, and integrate away all frequencies on a shell:</span>
<span id="cb1-889"><a href="#cb1-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-890"><a href="#cb1-890" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-891"><a href="#cb1-891" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-892"><a href="#cb1-892" aria-hidden="true" tabindex="-1"></a>Z &amp;\approx \int_{\tilde \phi^- \text{ is nonzero only for }<span class="sc">\|</span>k <span class="sc">\|</span> \leq (1-\epsilon)\Lambda} D<span class="co">[</span><span class="ot">\tilde \phi^-</span><span class="co">]</span> \left(\int_{\tilde\phi^+ \text{ is nonzero only for }(1-\epsilon)\Lambda \leq <span class="sc">\|</span>k <span class="sc">\|</span> \leq \Lambda} D<span class="co">[</span><span class="ot">\tilde\phi^+</span><span class="co">]</span> e^{-\tilde S<span class="co">[</span><span class="ot">\tilde\phi^-, \tilde\phi^+</span><span class="co">]</span>}\right) <span class="sc">\\</span></span>
<span id="cb1-893"><a href="#cb1-893" aria-hidden="true" tabindex="-1"></a>&amp;\overset{\mathrm{hopefully}}{=} \int_{\tilde \phi^- \text{ is nonzero only for }<span class="sc">\|</span>k <span class="sc">\|</span> \leq (1-\epsilon)\Lambda} D<span class="co">[</span><span class="ot">\tilde \phi^-</span><span class="co">]</span> e^{-\tilde S'<span class="co">[</span><span class="ot">\tilde \phi^-</span><span class="co">]</span>}</span>
<span id="cb1-894"><a href="#cb1-894" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-895"><a href="#cb1-895" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-896"><a href="#cb1-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-897"><a href="#cb1-897" aria-hidden="true" tabindex="-1"></a>This gives us some renormalized action $\tilde S'$ in frequency space. Do an inverse Fourier transform to obtain $S'$, then scale space down by $(1-\epsilon)$, to obtain the fully renormalized action $S''$. The RN flow is then defined by</span>
<span id="cb1-898"><a href="#cb1-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-899"><a href="#cb1-899" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-900"><a href="#cb1-900" aria-hidden="true" tabindex="-1"></a>S \mapsto S''</span>
<span id="cb1-901"><a href="#cb1-901" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-902"><a href="#cb1-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-903"><a href="#cb1-903" aria-hidden="true" tabindex="-1"></a>Since $\epsilon$ is small, we can make it infinitesimal, to obtain the RN flow (a real flow this time!)</span>
<span id="cb1-904"><a href="#cb1-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-905"><a href="#cb1-905" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-906"><a href="#cb1-906" aria-hidden="true" tabindex="-1"></a>S \mapsto S + \epsilon F<span class="co">[</span><span class="ot">S</span><span class="co">]</span></span>
<span id="cb1-907"><a href="#cb1-907" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-908"><a href="#cb1-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-909"><a href="#cb1-909" aria-hidden="true" tabindex="-1"></a>where $F$ denotes the RN flow field in theory-space. It is a functional of $S$, since the flow field differs for each theory in theory-space.</span>
<span id="cb1-910"><a href="#cb1-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-911"><a href="#cb1-911" aria-hidden="true" tabindex="-1"></a>This is as far as we are going to discuss the RN in momentum space. Any more and I would be writing a textbook on QFT, and you are better served by a proper textbook like <span class="co">[</span><span class="ot">@zeeQuantumFieldTheory2023; @zeeQuantumFieldTheory2010</span><span class="co">]</span>, etc. </span>
<span id="cb1-912"><a href="#cb1-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-913"><a href="#cb1-913" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bonus: How to publish in quantum field theory</span></span>
<span id="cb1-914"><a href="#cb1-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-915"><a href="#cb1-915" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Work through a textbook and learn RN in momentum space.</span>
<span id="cb1-916"><a href="#cb1-916" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Learn group theory and group representation theory.</span>
<span id="cb1-917"><a href="#cb1-917" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Write down many groups with some nice geometry, like $SO(3)$.</span>
<span id="cb1-918"><a href="#cb1-918" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Construct a group out of those. For example, $G = SO(4) \rtimes SU(3) \times SU(2) \times U(1)$. The group $G$ should have around 10--20 dimensions, but if you are a string theory enthusiast, then 500 dimensions is perfectly fine.</span>
<span id="cb1-919"><a href="#cb1-919" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Construct another group $H$.</span>
<span id="cb1-920"><a href="#cb1-920" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Pick a nice space $X$. For example, $X = \R^4 \times \C^6$. It must be a space that $H$ can act upon.</span>
<span id="cb1-921"><a href="#cb1-921" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Pick another space $Y$. It must be a space that $G$ can act upon.</span>
<span id="cb1-922"><a href="#cb1-922" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Construct the most generic possible functional of type $S: (X \to Y) \to \C$ that is still compatible with the two symmetry groups $G, H$.</span>
<span id="cb1-923"><a href="#cb1-923" aria-hidden="true" tabindex="-1"></a><span class="ss">9.  </span>Spend the next month doing RN calculations about $Z := \int_{\phi: X \to Y} D<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span> \; e^{-S<span class="co">[</span><span class="ot">\phi</span><span class="co">]</span>}$, probably with Feynman diagrams scribbled everywhere.</span>
<span id="cb1-924"><a href="#cb1-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-925"><a href="#cb1-925" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Type it up in LaTeX.</span>
<span id="cb1-926"><a href="#cb1-926" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>Suffer through peer review, or just put it up on arXiv.</span>
<span id="cb1-927"><a href="#cb1-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-928"><a href="#cb1-928" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/banner/thumbnail_4.png)</span></span>
<span id="cb1-929"><a href="#cb1-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-930"><a href="#cb1-930" aria-hidden="true" tabindex="-1"></a><span class="fu">## A bag of intuitions</span></span>
<span id="cb1-931"><a href="#cb1-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-932"><a href="#cb1-932" aria-hidden="true" tabindex="-1"></a>After the long, hard slog at mathematics, we can take a break, take stock of what we have learned, and make some philosophical reflections. I guarantee that you will find at least one sentence here that you can proclaim with style at a party.</span>
<span id="cb1-933"><a href="#cb1-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-934"><a href="#cb1-934" aria-hidden="true" tabindex="-1"></a><span class="fu">### Power laws are born of two exponential parents {#sec-power-law-two-exponential-parents}</span></span>
<span id="cb1-935"><a href="#cb1-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-936"><a href="#cb1-936" aria-hidden="true" tabindex="-1"></a>Why is it that a critical point is typically surrounded by a power law? One intuition is that at a critical point, two exponentials are matched exactly -- an exponentially decaying interaction strength and an exponentially increasing number of interaction paths -- and a power law is born in their collision.</span>
<span id="cb1-937"><a href="#cb1-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-938"><a href="#cb1-938" aria-hidden="true" tabindex="-1"></a>Consider the Ising model on the plane. Fix an origin $0$ , and we ask, how strong is the correlation between the origin $0$ and a point that is at distance $(n, n)$ away from the origin, where $n$ is large?</span>
<span id="cb1-939"><a href="#cb1-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-940"><a href="#cb1-940" aria-hidden="true" tabindex="-1"></a>Well, the two points are correlated because they are connected by chains of spins. The more chains there are, the stronger the correlation, but the longer each chain is, the weaker the correlation. Since the correlation along each chain is exponentially weak, we can crudely pretend that all the correlations can be added.<span class="ot">[^weak-correlations-add]</span> As a good approximation, we consider only the shortest chains, which are of length $2n$. By Stirling approximation, there are ${2n \choose n} \sim \frac{4^{n}}{\sqrt{n\pi }}$ such chains. </span>
<span id="cb1-941"><a href="#cb1-941" aria-hidden="true" tabindex="-1"></a>We can think of spin at origin as $x_{(0,0)} + z_1 + z_2 + \cdots$, and the spin at $(n, n)$ as $x_{(n,n)} + z_1 + z_2 + \cdots$, where $z_1, z_2,...$ are random variables that are responsible for creating the correlations between the two spins along each chain.</span>
<span id="cb1-942"><a href="#cb1-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-943"><a href="#cb1-943" aria-hidden="true" tabindex="-1"></a><span class="ot">[^weak-correlations-add]</span>:</span>
<span id="cb1-944"><a href="#cb1-944" aria-hidden="true" tabindex="-1"></a>    Every biologist knows intuitively that weak correlations are additive. This is why, for instance, we can predict height accurately by a simple linear sum of the genes correlated with height, ignoring pairwise, triple-wise, and higher-order interactions. Indeed, there is a common pattern in biology: If you have a developmental process of type $\text{many genes}\to \R$, where $\R$ stands for some kind of real-valued trait, like probability of diabetes, then the developmental process is pretty much just a linear map. <span class="co">[</span><span class="ot">Epistasis</span><span class="co">](https://en.wikipedia.org/wiki/Epistasis)</span>, defanged. Heritability, vindicated.</span>
<span id="cb1-945"><a href="#cb1-945" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-946"><a href="#cb1-946" aria-hidden="true" tabindex="-1"></a><span class="in">    Why? One possibility is that "even nature does not laugh at the difficulties of integration". That is, even natural evolution will be hopelessly confused by a fully nonlinear developmental process, so such a process is unadaptive, [un-evolvable](https://en.wikipedia.org/wiki/Evolvability), a tightly woven ball of spaghetti code where a single base-pair change collapses the whole thing. No, evolution favors those that are evolvable, which usually means a linear function.</span></span>
<span id="cb1-947"><a href="#cb1-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-948"><a href="#cb1-948" aria-hidden="true" tabindex="-1"></a>Now, each chain contributes a weak correlation that decays exponentially with distance. We can assume the chains do not interact. Along each chain, we have a 1D Ising model. The covariance between two neighboring spins is </span>
<span id="cb1-949"><a href="#cb1-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-950"><a href="#cb1-950" aria-hidden="true" tabindex="-1"></a>$$Cov(s_0, s_1) = E<span class="co">[</span><span class="ot">s_0s_1</span><span class="co">]</span> - \ub{E<span class="co">[</span><span class="ot">s_0</span><span class="co">]</span>E<span class="co">[</span><span class="ot">s_1</span><span class="co">]</span>}{=0} = Pr(s_0 = s_1 ) - Pr(s_0 \neq s_1 ) = \tanh(\beta J)$$</span>
<span id="cb1-951"><a href="#cb1-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-952"><a href="#cb1-952" aria-hidden="true" tabindex="-1"></a>Now, we need a trick.<span class="ot">[^ising-transfer-matrix]</span> If you think a bit, you would see that whether $s_0 = s_1$ is independent of whether $s_1 = s_2$. Thus, </span>
<span id="cb1-953"><a href="#cb1-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-954"><a href="#cb1-954" aria-hidden="true" tabindex="-1"></a>$$Cov(s_0, s_2) = E<span class="co">[</span><span class="ot">s_0 s_2</span><span class="co">]</span> = E<span class="co">[</span><span class="ot">(s_0 s_1) (s_1 s_2)</span><span class="co">]</span> = Cov(s_0, s_1) Cov(s_1, s_2) = \tanh(\beta J)^2$$</span>
<span id="cb1-955"><a href="#cb1-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-956"><a href="#cb1-956" aria-hidden="true" tabindex="-1"></a><span class="ot">[^ising-transfer-matrix]: </span>If you don't like the trick, then you can use the transfer matrix method. We have no use for the transfer matrix, so we don't do it.</span>
<span id="cb1-957"><a href="#cb1-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-958"><a href="#cb1-958" aria-hidden="true" tabindex="-1"></a>And since the chain has length $2n$, the correlation contributed by the chain is $\tanh^{2n}(\beta J)$. The total correlation is</span>
<span id="cb1-959"><a href="#cb1-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-960"><a href="#cb1-960" aria-hidden="true" tabindex="-1"></a>$$\sim \frac{4^{n}}{\sqrt{n\pi }} \tanh^{2n}(\beta J)$$</span>
<span id="cb1-961"><a href="#cb1-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-962"><a href="#cb1-962" aria-hidden="true" tabindex="-1"></a>The two terms are exactly balanced when $\beta J = \tanh^{-1}(1/2) = 0.549\dots$. In fact, the exact result is $\beta J = 0.44\dots$ , so our crude estimate is only 25% too high.</span>
<span id="cb1-963"><a href="#cb1-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-964"><a href="#cb1-964" aria-hidden="true" tabindex="-1"></a>Now, right at the critical point, the correlation is $\sim (n\pi)^{-1/2}$ , so we see that when the exponential decay in correlation is exactly matched with the exponential growth in possible paths, the remaining polynomial decay comes to the fore. Notice that we have also estimated one of the Ising critical exponents: $\nu = 1/2$. The actual answer is $1$.</span>
<span id="cb1-965"><a href="#cb1-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-966"><a href="#cb1-966" aria-hidden="true" tabindex="-1"></a>Similarly, with </span>
<span id="cb1-967"><a href="#cb1-967" aria-hidden="true" tabindex="-1"></a>$$\binom{kn}{n, \dots n}\sim \frac{k^{kn}}{n^{\frac{k-1}2}}\frac{k^{1/2}}{(2\pi)^{\frac{k-1}2}}$$</span>
<span id="cb1-968"><a href="#cb1-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-969"><a href="#cb1-969" aria-hidden="true" tabindex="-1"></a>we can estimate that the Ising model in $\Z^k$ has a critical $\beta J \approx \tanh^{-1}(1/k)$ and critical exponent $\nu = \frac{k-1}2$. It turns out that for all dimensions $\geq 4$, we have the exact result of $\nu = 1/2$. This can be derived by "mean field theory", which we are not going to discuss.</span>
<span id="cb1-970"><a href="#cb1-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-971"><a href="#cb1-971" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stevens' power law</span></span>
<span id="cb1-972"><a href="#cb1-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-973"><a href="#cb1-973" aria-hidden="true" tabindex="-1"></a>As a side note, this "two exponents lead to a power law" is known in psychophysics as Stevens' power law <span class="co">[</span><span class="ot">@stevensNeuralEventsPsychophysical1970</span><span class="co">]</span>. </span>
<span id="cb1-974"><a href="#cb1-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-975"><a href="#cb1-975" aria-hidden="true" tabindex="-1"></a>Consider the case where the brain needs to respond to the presence of a stimulus (e.g., a sound, a smell, etc.) with intensity $I$. The response intensity (such as in the height of jumping, or a verbal report, or wincing of the face) is $R$. Stevens found that for many kinds of stimulus, $R \propto I^k$ for some exponent $k$ that depends on the type of stimulus and response.</span>
<span id="cb1-976"><a href="#cb1-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-977"><a href="#cb1-977" aria-hidden="true" tabindex="-1"></a>Stevens conjectured that the number of neurons firing $N$ is proportional to the log of intensity of stimulus $I$, and that $N$ is also proportional to the log of intensity of response $R$. Thus, we have</span>
<span id="cb1-978"><a href="#cb1-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-979"><a href="#cb1-979" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-980"><a href="#cb1-980" aria-hidden="true" tabindex="-1"></a>k_I \ln I = N = k_R \ln R</span>
<span id="cb1-981"><a href="#cb1-981" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-982"><a href="#cb1-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-983"><a href="#cb1-983" aria-hidden="true" tabindex="-1"></a>for two constants $k_I, k_R$, which implies that $R = I^{k_I/k_R}$, a power law. In this way, a small number of neurons can allow us to perceive and react to a wide range of stimuli intensities -- for example, the physical brightness between noon and a starlit moonless night is more than $10^{8}$, and yet the optical nerve, with only $10^{6}$ neurons <span class="co">[</span><span class="ot">@evangelouAnatomyRetinaOptic2016</span><span class="co">]</span>, can comfortably accommodate them both.</span>
<span id="cb1-984"><a href="#cb1-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-985"><a href="#cb1-985" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; At the Ciba Symposium in 1966, there was a general discussion on the topic "Linearity of transmission along the perceptual pathway". In that discussion, and elsewhere at the symposium, Sir John Eccles turned forceful attention to the question of whether the sense organ could adequately account for the nonlinearity in the coupling between stimulus and sensation, leaving the central nervous system with the task of performing only linear transformations. He observed that "there is no great impediment to the idea that... the transfer functions across the synaptic mechanism are approximately linear." To which Professor Mountcastle added, "The interesting point for me here is the great importance that we must now place upon the transducer process itself, at the periphery."</span></span>
<span id="cb1-986"><a href="#cb1-986" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-987"><a href="#cb1-987" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@stevensNeuralEventsPsychophysical1970</span><span class="co">]</span></span>
<span id="cb1-988"><a href="#cb1-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-989"><a href="#cb1-989" aria-hidden="true" tabindex="-1"></a><span class="fu">### RN is a journey in the space of possible theories</span></span>
<span id="cb1-990"><a href="#cb1-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-991"><a href="#cb1-991" aria-hidden="true" tabindex="-1"></a>So far, we have seen again and again the common refrain of "the space of theories" and "moving to another theory". It is time to make this clear. The space of possible theories is defined by the symmetry of the physical system, and the Renormalization Group (RG) flow defines a journey in this space.</span>
<span id="cb1-992"><a href="#cb1-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-993"><a href="#cb1-993" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@fisherRenormalizationGroupTheory1998, figure 4</span><span class="co">]</span>](figure/fisher_1998_fig_4.png)</span>
<span id="cb1-994"><a href="#cb1-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-995"><a href="#cb1-995" aria-hidden="true" tabindex="-1"></a>Consider a generic RG flow in a generic space of theories. diagram for a system with two coupling constants $K$ and $y$. Each point in the diagram represents a theory, and the arrows indicate the direction of the RG flow. The fixed points, where the arrows converge, correspond to theories that are scale-invariant.</span>
<span id="cb1-996"><a href="#cb1-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-997"><a href="#cb1-997" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@fisherRenormalizationGroupTheory1998, figure 5</span><span class="co">]</span>](figure/fisher_1998_fig_5.png)</span>
<span id="cb1-998"><a href="#cb1-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-999"><a href="#cb1-999" aria-hidden="true" tabindex="-1"></a>The above figure shows the RG flow for the Ising model, a simple model of ferromagnetism. The fixed points correspond to the paramagnetic phase ($K = 0$) and the ferromagnetic phase ($K = K_c$). The critical point, where the two phases meet, is at $K = K_c$.</span>
<span id="cb1-1000"><a href="#cb1-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1001"><a href="#cb1-1001" aria-hidden="true" tabindex="-1"></a>The RG flow provides a way to understand the behavior of a system at different length scales. As we zoom out, the system flows towards a fixed point. The fixed point describes the long-distance behavior of the system.</span>
<span id="cb1-1002"><a href="#cb1-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1003"><a href="#cb1-1003" aria-hidden="true" tabindex="-1"></a>For example, in the Ising model, as we zoom out, the system flows towards either the paramagnetic or the ferromagnetic fixed point, depending on the initial value of $K$. If $K &lt; K_c$, the system flows towards the paramagnetic fixed point, and the spins become disordered at long distances. If $K &gt; K_c$, the system flows towards the ferromagnetic fixed point, and the spins become ordered at long distances.</span>
<span id="cb1-1004"><a href="#cb1-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1005"><a href="#cb1-1005" aria-hidden="true" tabindex="-1"></a>What defines the space of possible theories? The symmetry of the physical system.</span>
<span id="cb1-1006"><a href="#cb1-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1007"><a href="#cb1-1007" aria-hidden="true" tabindex="-1"></a><span class="fu">### Symmetries determine the shape of theory-space {#sec-symmetries-determine-theory-space}</span></span>
<span id="cb1-1008"><a href="#cb1-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1009"><a href="#cb1-1009" aria-hidden="true" tabindex="-1"></a>The Ising model on $\Z^2$ is not a single theory, but an entire infinite-dimensional space of possible theories. Each Ising model can be specified by all coupling strengths for all possible spin configurations -- nearest neighbors, next-nearest neighbors, four in a square, etc. However, they must follow the symmetries. We can't have three-in-a-triangle, because switching up and down gives us the same energy. We can't have $Js_{(0, 0)}s_{(1, 0)}$ different from $J's_{(1, 0)}s_{(2, 0)}$, because translating the whole plane by $(1, 0)$ gives us the same energy.</span>
<span id="cb1-1010"><a href="#cb1-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1011"><a href="#cb1-1011" aria-hidden="true" tabindex="-1"></a>This is true in general: Symmetries determine the shape of theory-space.</span>
<span id="cb1-1012"><a href="#cb1-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1013"><a href="#cb1-1013" aria-hidden="true" tabindex="-1"></a>Conversely, if two physical systems are constrained under the same symmetries, then their behavior are the same near the critical point, because their renormalization flows are the same.</span>
<span id="cb1-1014"><a href="#cb1-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1015"><a href="#cb1-1015" aria-hidden="true" tabindex="-1"></a>The following table shows many theory-spaces with their corresponding symmetries.</span>
<span id="cb1-1016"><a href="#cb1-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1017"><a href="#cb1-1017" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> $d$, the underlying space's dimensions <span class="pp">|</span> $n$, the internal degrees of freedom <span class="pp">|</span> Theoretical Model <span class="pp">|</span> Physical System <span class="pp">|</span> Order Parameter <span class="pp">|</span></span>
<span id="cb1-1018"><a href="#cb1-1018" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|---|---|</span> </span>
<span id="cb1-1019"><a href="#cb1-1019" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 2 <span class="pp">|</span> 1 <span class="pp">|</span> Two dimensions <span class="pp">|</span> Adsorbed films <span class="pp">|</span> Surface density <span class="pp">|</span></span>
<span id="cb1-1020"><a href="#cb1-1020" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> 2 <span class="pp">|</span> XY model in two dimensions <span class="pp">|</span> Helium-4 films <span class="pp">|</span> Amplitude of superfluid phase <span class="pp">|</span></span>
<span id="cb1-1021"><a href="#cb1-1021" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> 3 <span class="pp">|</span> Heisenberg model in two dimensions <span class="pp">|</span>  <span class="pp">|</span> Magnetization <span class="pp">|</span></span>
<span id="cb1-1022"><a href="#cb1-1022" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> &gt;2 <span class="pp">|</span> ∞ <span class="pp">|</span> "Spherical" model <span class="pp">|</span> None <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb1-1023"><a href="#cb1-1023" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 3 <span class="pp">|</span> 0 <span class="pp">|</span> Self-avoiding random walk <span class="pp">|</span> Conformation of long-chain polymers <span class="pp">|</span> Density of chain ends <span class="pp">|</span></span>
<span id="cb1-1024"><a href="#cb1-1024" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> 1 <span class="pp">|</span> Ising model in three dimensions <span class="pp">|</span> Uniaxial ferromagnet <span class="pp">|</span> Magnetization <span class="pp">|</span></span>
<span id="cb1-1025"><a href="#cb1-1025" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span> Fluid near a critical point <span class="pp">|</span> Density difference between phases <span class="pp">|</span></span>
<span id="cb1-1026"><a href="#cb1-1026" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span> Mixture of liquids near consolute point <span class="pp">|</span> Concentration difference <span class="pp">|</span></span>
<span id="cb1-1027"><a href="#cb1-1027" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span> Alloy near order-disorder transition <span class="pp">|</span> Concentration difference <span class="pp">|</span></span>
<span id="cb1-1028"><a href="#cb1-1028" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> 2 <span class="pp">|</span> XY model in three dimensions <span class="pp">|</span> Planar ferromagnet <span class="pp">|</span> Magnetization <span class="pp">|</span></span>
<span id="cb1-1029"><a href="#cb1-1029" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span> Helium 4 near superfluid transition <span class="pp">|</span> Amplitude of superfluid phase <span class="pp">|</span></span>
<span id="cb1-1030"><a href="#cb1-1030" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> 3 <span class="pp">|</span> Heisenberg model in three dimensions <span class="pp">|</span> Isotropic ferromagnet <span class="pp">|</span> Magnetization <span class="pp">|</span></span>
<span id="cb1-1031"><a href="#cb1-1031" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> ≤4 <span class="pp">|</span> -2 <span class="pp">|</span>  <span class="pp">|</span> None <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb1-1032"><a href="#cb1-1032" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> 32 <span class="pp">|</span> Quantum chromodynamics <span class="pp">|</span> Quarks bound in protons, neutrons, etc. <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb1-1033"><a href="#cb1-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1034"><a href="#cb1-1034" aria-hidden="true" tabindex="-1"></a>: Table of theory-spaces with their corresponding symmetries. Reproduced from <span class="co">[</span><span class="ot">@wilsonProblemsPhysicsMany1979</span><span class="co">]</span>.</span>
<span id="cb1-1035"><a href="#cb1-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1036"><a href="#cb1-1036" aria-hidden="true" tabindex="-1"></a>As a particular example, with $d=3, n=1$, we see that water-vapor mixture has the same critical behavior as copper-zinc alloy, or Ising model on three dimensions. We can think of vapor as just up-spin, and liquid as just down-spin. Next time you find yourself meditating upon a magnet, try reimagining it as a kettle of water, about to boil over.</span>
<span id="cb1-1037"><a href="#cb1-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1038"><a href="#cb1-1038" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> physical system <span class="pp">|</span> site types <span class="pp">|</span></span>
<span id="cb1-1039"><a href="#cb1-1039" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|</span></span>
<span id="cb1-1040"><a href="#cb1-1040" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> uniaxial magnet <span class="pp">|</span> up / down <span class="pp">|</span> </span>
<span id="cb1-1041"><a href="#cb1-1041" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> fluid <span class="pp">|</span> has atom / no atom <span class="pp">|</span></span>
<span id="cb1-1042"><a href="#cb1-1042" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> brass crystal <span class="pp">|</span> zinc / copper <span class="pp">|</span></span>
<span id="cb1-1043"><a href="#cb1-1043" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> simple lattice field theory <span class="pp">|</span> has particle / no particle <span class="pp">|</span></span>
<span id="cb1-1044"><a href="#cb1-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1045"><a href="#cb1-1045" aria-hidden="true" tabindex="-1"></a>: Examples of $d=3, n=1$ systems.</span>
<span id="cb1-1046"><a href="#cb1-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1047"><a href="#cb1-1047" aria-hidden="true" tabindex="-1"></a><span class="fu">### Droplets inside droplets</span></span>
<span id="cb1-1048"><a href="#cb1-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1049"><a href="#cb1-1049" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Big whirls have little whirls that feed on their velocity,</span>  </span>
<span id="cb1-1050"><a href="#cb1-1050" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; and little whirls have lesser whirls and so on to viscosity.</span></span>
<span id="cb1-1051"><a href="#cb1-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1052"><a href="#cb1-1052" aria-hidden="true" tabindex="-1"></a>I *especially* recommend playing with the <span class="co">[</span><span class="ot">interactive Ising model</span><span class="co">](https://www.ibiblio.org/e-notes/Perc/ising1k.htm)</span> by <span class="co">[</span><span class="ot">Evgeny Demidov</span><span class="co">](https://www.ibiblio.org/e-notes/Perc/contents.htm)</span> while reading this section.</span>
<span id="cb1-1053"><a href="#cb1-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1054"><a href="#cb1-1054" aria-hidden="true" tabindex="-1"></a>We can interpret an Ising model over $\Z^3$ as a mixture of liquid and gaseous water. Each site can either be in a liquid state or in a vapor state. We start below the critical temperature, so that the bonding strength $J$ is just slightly larger than the critical $J_c$. The system must make a free choice to make between being a liquid ocean with tiny islands of liquid or a vapor ocean with tiny islands of liquid. Both choices are equally good in our toy model. Since liquid wants to be in contact with liquid and vapor with vapor, the system must decide. For the sake of intuition, we say that we have an ocean of liquid with tiny droplets of vapor inside.</span>
<span id="cb1-1055"><a href="#cb1-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1056"><a href="#cb1-1056" aria-hidden="true" tabindex="-1"></a>Now we increase the temperature towards the critical temperature. As we get hotter, thermal fluctuations become larger. In the ocean of liquid, a thermal fluctuation produces a droplet of vapor. This droplet secretes other material of the same density, and it grows larger and larger.</span>
<span id="cb1-1057"><a href="#cb1-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1058"><a href="#cb1-1058" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Vapor droplets inside an ocean of liquid. [@kadanoffOrderChaosII1999, page 298]</span><span class="co">](figure/kadanoff_1999_fig_1_3.png)</span></span>
<span id="cb1-1059"><a href="#cb1-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1060"><a href="#cb1-1060" aria-hidden="true" tabindex="-1"></a>However, as criticality is approached, the energetic cost in making a fluctuation approaches zero. The energetic penalty in creating droplets gets smaller and smaller. So, the droplets can grow large, until it becomes infinite right at the critical point.</span>
<span id="cb1-1061"><a href="#cb1-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1062"><a href="#cb1-1062" aria-hidden="true" tabindex="-1"></a>Furthermore, each droplet itself is a nearly-critical system, so fluctuations appear within the droplets, in a delicate fractal.</span>
<span id="cb1-1063"><a href="#cb1-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1064"><a href="#cb1-1064" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Droplets inside droplets inside droplets... [@kadanoffOrderChaosII1999, page 299]</span><span class="co">](figure/kadanoff_1999_fig_1_4.png)</span></span>
<span id="cb1-1065"><a href="#cb1-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1066"><a href="#cb1-1066" aria-hidden="true" tabindex="-1"></a><span class="fu">### More is different {#sec-more-is-different}</span></span>
<span id="cb1-1067"><a href="#cb1-1067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1068"><a href="#cb1-1068" aria-hidden="true" tabindex="-1"></a>Like Wilson, <span class="co">[</span><span class="ot">Philip Anderson</span><span class="co">](https://en.wikipedia.org/wiki/Philip_W._Anderson)</span> is a physicist who has won a Nobel Prize for his work on surprising things that happen when many particles interact. His Nobel Prize was awarded for his work on magnetism and "<span class="co">[</span><span class="ot">Anderson localization</span><span class="co">](https://en.wikipedia.org/wiki/Anderson_localization)</span>". Consider a pure piece of metal. An electron wave can vibrate freely across it from side to side, like a wave on a perfectly uniform infinite ocean. Now if we dope the metal with impurities, like planting wave-breakers in the ocean, then as the amount of impurity increases, an electron wave would suddenly become trapped, and the metal would become an insulator.</span>
<span id="cb1-1069"><a href="#cb1-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1070"><a href="#cb1-1070" aria-hidden="true" tabindex="-1"></a>In a famous paper <span class="co">[</span><span class="ot">@andersonMoreDifferentBroken1972</span><span class="co">]</span>, he interprets many-body physics like a philosopher (like what I've been doing in this whole section). He called the "constructionist hypothesis" the view that science divides neatly into fundamental laws and applications of those. He countered this view by proposing that scale and complexity create distinct stages in nature. Each stage necessitates new laws, concepts, and generalizations, and requires just as much ingenuity as the other stages. For example, psychology is not merely applied biology, biology is not simply applied chemistry, and condensed matter physics is not only "device engineering".</span>
<span id="cb1-1071"><a href="#cb1-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1072"><a href="#cb1-1072" aria-hidden="true" tabindex="-1"></a>Anderson proposed that new stages appear because of "broken symmetries". Consider the sugar molecule. Though the sugar molecule is governed by quantum mechanics, which does not distinguish left from right, in our world we mostly only see sugar in one chirality. Sometime in the distant past, the symmetry was broken, and we are living in the consequences of its history.</span>
<span id="cb1-1073"><a href="#cb1-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1074"><a href="#cb1-1074" aria-hidden="true" tabindex="-1"></a>More concretely, consider cars driving on a road. Why do we drive on the right instead of the left? Left or right, it's better ("lower energy") if everyone agrees. If people disagree, then it's chaos ("higher energy"), so in the past, a coin was flipped, and we are here. In a parallel universe, we are driving on the left instead of the right. This is clear in the many-worlds interpretation of quantum mechanics:</span>
<span id="cb1-1075"><a href="#cb1-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1076"><a href="#cb1-1076" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1077"><a href="#cb1-1077" aria-hidden="true" tabindex="-1"></a>\ket{\Psi_{\text{world}}} = \frac{1}{\sqrt{2}} \ket{\Psi_{\text{world where we drive on the left}}} + \frac{1}{\sqrt{2}} \ket{\Psi_{\text{world where we drive on the right}}} </span>
<span id="cb1-1078"><a href="#cb1-1078" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-1079"><a href="#cb1-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1080"><a href="#cb1-1080" aria-hidden="true" tabindex="-1"></a>The state of the entire multiverse $\ket{\Psi_{\text{world}}}$ is symmetric, but any observer has to fall into one of the sub-states, where the symmetry no longer holds.</span>
<span id="cb1-1081"><a href="#cb1-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1082"><a href="#cb1-1082" aria-hidden="true" tabindex="-1"></a>However, Anderson pointedly did not explain what allows the stages to have laws largely independent of each other. Suppose that we have explained how some symmetries of the quantum-mechanical level break on the biochemistry level; we still have many questions. Why does the biochemistry level still have simple laws? And why these biochemical laws, but not others? Why are quantum mechanical laws and the everyday physics of frogs and cats so different, without the laws "bleeding into each other"?</span>
<span id="cb1-1083"><a href="#cb1-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1084"><a href="#cb1-1084" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mesophysics: Why are things interesting between the large and the small?</span></span>
<span id="cb1-1085"><a href="#cb1-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1086"><a href="#cb1-1086" aria-hidden="true" tabindex="-1"></a>The space of all theories is big, but most of it is rather uninteresting. Consider the humble Ising model on $\Z^2$. Too much interaction and you get a block of spins all pointing in one direction. Too little interaction and you get a gas of spins pointing noisily in all directions. Only right at the critical point do we get interesting behavior in all possible scales. Not only that, the critical point is very delicate.</span>
<span id="cb1-1087"><a href="#cb1-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1088"><a href="#cb1-1088" aria-hidden="true" tabindex="-1"></a>If you take an Ising model at $J$ that is just a bit above $J_c$ and zoom out, then by the RN flow equation, the effective $J$ would keep increasing, and it becomes more and more uniform until it's a perfect shade of black/white at $J = +\infty$. Conversely, if you start with $J$ slightly below $J_c$ and keep zooming out, $J$ would approach $0$ and everything would become a uniform shade of gray, with the spins pointing up and down with no regard for any other spin. Balanced on a knife's edge is $J = J_c$, where there is interesting behavior at all levels of zooming.</span>
<span id="cb1-1089"><a href="#cb1-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1090"><a href="#cb1-1090" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb1-1091"><a href="#cb1-1091" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">video</span><span class="ot"> controls width</span><span class="op">=</span><span class="st">100%</span><span class="dt">&gt;</span></span>
<span id="cb1-1092"><a href="#cb1-1092" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">source</span><span class="ot"> src</span><span class="op">=</span><span class="st">"figure/renormalization_group_douglas_ashton.webm"</span><span class="ot"> type</span><span class="op">=</span><span class="st">"video/webm"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb1-1093"><a href="#cb1-1093" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">video</span><span class="dt">&gt;</span></span>
<span id="cb1-1094"><a href="#cb1-1094" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span>Zooming in and out of the Ising model. Video by Douglas Ashton, taken from <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"https://blog.dougmet.net/2012/04/the-renormalisation-group/"</span><span class="dt">&gt;</span>The Renormalisation Group | dougmet-dot-net<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span>.<span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb1-1095"><a href="#cb1-1095" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb1-1096"><a href="#cb1-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1097"><a href="#cb1-1097" aria-hidden="true" tabindex="-1"></a>But what keeps $J = J_c$?</span>
<span id="cb1-1098"><a href="#cb1-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1099"><a href="#cb1-1099" aria-hidden="true" tabindex="-1"></a>Why is it that we are surprised by the quantum mechanics in the microscopic world? Because daily life in the mesoscopic world does not betray its origin from the microscopic world. The details has been renormalized away. But if that's the case, how come our world is neither a homogeneous block of spins all pointing up nor a hot mess of spins unrelated to every other spin? Why is the mesoscopic world interesting?</span>
<span id="cb1-1100"><a href="#cb1-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1101"><a href="#cb1-1101" aria-hidden="true" tabindex="-1"></a>Look around you. The world is interesting, with power laws, fractal patterns, and details at all scales. You never see a pencil standing on its end without a hand keeping it there. What keeps the mesoscopic world in its critical place? </span>
<span id="cb1-1102"><a href="#cb1-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1103"><a href="#cb1-1103" aria-hidden="true" tabindex="-1"></a>One answer is that most of the interestingness did not come from criticality. However, if there exists *some* criticality in the mesoscopic world, and there does not seem to be an intelligent agent keeping the criticality there, then we have a mystery. This is the question that launched a thousand papers, including the famed <span class="co">[</span><span class="ot">self-organized criticality</span><span class="co">](https://en.wikipedia.org/wiki/Self-organized_criticality)</span> paper <span class="co">[</span><span class="ot">@bakSelforganizedCriticalityExplanation1987</span><span class="co">]</span>, itself launching a thousand papers. The idea is typically illustrated by the forest fire model.</span>
<span id="cb1-1104"><a href="#cb1-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1105"><a href="#cb1-1105" aria-hidden="true" tabindex="-1"></a>Consider the standard percolation model on a square grid. Each point might be occupied (a "tree" grows there) or unoccupied (empty plot of land). Randomly, lightning falls, and if it hits a tree, the tree catches on fire and the fire spreads to any neighboring trees. The process ends when there are no more burning trees.</span>
<span id="cb1-1106"><a href="#cb1-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1107"><a href="#cb1-1107" aria-hidden="true" tabindex="-1"></a>As the proportion $p$ of a site being occupied ("tree density") changes, we see a phase transition. For low $p &lt; p_c$, there is no percolation, and so the fire quickly dies out. For high $p &gt; p_c$, there is percolation, and so the fire spreads across the entire grid. The process automatically balances the system at the critical value $p_c$, the system is poised between these two regimes, and the burnt-out patches can be of any size, following a power-law distribution. In this way, the delicate critical point in the standard percolation model has been transformed to a robust critical point.</span>
<span id="cb1-1108"><a href="#cb1-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1109"><a href="#cb1-1109" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Nature shows an amazing variety of length scales: There is the Hubble radius of the universe, $10^{10}$ light years or so and the radius of our own solar system, $10^{11}$ meters roughly, and us-two meters perhaps, and an atom $-10^{-10}$ meters in radius, and a proton $10^{-16}$ meters, and the characteristic length of quantum gravity-which involves another factor of about $10^{20}$.</span></span>
<span id="cb1-1110"><a href="#cb1-1110" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-1111"><a href="#cb1-1111" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; How these vastly different lengths arise is a very interesting and fundamental question.... However, we can think about how one describes the region between these lengths. If one is looking at scales between two fundamental lengths, there are no nearby characteristic lengths. Similarly in critical phenomena, whenever one looks </span><span class="sc">\[</span><span class="at">at</span><span class="sc">\]</span><span class="at"> any event which takes place between the scale of the lattice constant </span><span class="sc">\[</span><span class="at">the spacing between molecules or spins</span><span class="sc">\]</span><span class="at"> and the much larger scale of the coherence length, one is in a situation in which there are no nearby characteristic lengths. </span><span class="co">[</span><span class="ot">@kadanoffStatisticalPhysicsStatics1999, page 251</span><span class="co">]</span></span>
<span id="cb1-1112"><a href="#cb1-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1113"><a href="#cb1-1113" aria-hidden="true" tabindex="-1"></a>In a footnote, Kadanoff suggested that self-organized criticality might explain forms of stable criticality we see around us. Kadanoff wrote the words in 1999, near the end of the 1980s--90s chaos theory boom. Since then, the self-organized criticality theory has fallen by the wayside, like fractal compression, mirrorshades, and large-folio printed pages of fractal art. The modern evaluation is that while it was oversold by Per Bak, and certainly could not explain *all* critical phenomena <span class="co">[</span><span class="ot">@bakHowNatureWorks1996</span><span class="co">]</span>, it can explain some of them <span class="co">[</span><span class="ot">@watkins25YearsSelforganized2016, section 8</span><span class="co">]</span>.</span>
<span id="cb1-1114"><a href="#cb1-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1115"><a href="#cb1-1115" aria-hidden="true" tabindex="-1"></a><span class="fu">### Universality: The details don't matter {#sec-universality}</span></span>
<span id="cb1-1116"><a href="#cb1-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1117"><a href="#cb1-1117" aria-hidden="true" tabindex="-1"></a>Or: Why elephants don't know quantum mechanics, but don't need to know it either.</span>
<span id="cb1-1118"><a href="#cb1-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1119"><a href="#cb1-1119" aria-hidden="true" tabindex="-1"></a>In the early 20th century, material scientists noticed the remarkable phenomenon of "corresponding states". As first reported by <span class="co">[</span><span class="ot">@guggenheimPrincipleCorrespondingStates1945</span><span class="co">]</span>, scientists measured the density $\rho$ of many substances near their liquid-vapor critical point. They fixed pressure and increased temperature $T$ around the critical temperature $T_c$. As they plotted the relation between $T$ and $\rho$, rescaled by critical temperature $T_c$ and density at critical point $\rho_c$, remarkably, all the substances fell onto a single curve.</span>
<span id="cb1-1120"><a href="#cb1-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1121"><a href="#cb1-1121" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">High-resolution reprint of [@guggenheimPrincipleCorrespondingStates1945, figure 2] in [@herbutModernApproachCritical2007, page 13].</span><span class="co">](figure/herbut_2007_corresponding_states.png)</span></span>
<span id="cb1-1122"><a href="#cb1-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1123"><a href="#cb1-1123" aria-hidden="true" tabindex="-1"></a>Despite the diversity of intermolecular forces, the phase transition behavior of a wide variety of gasses follows a universal pattern.</span>
<span id="cb1-1124"><a href="#cb1-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1125"><a href="#cb1-1125" aria-hidden="true" tabindex="-1"></a>We see this pattern over and over again in physics. To give another example, imagine water flowing through a porous rock, oil through sand, or electricity through a random network of resistors. These systems, seemingly completely unrelated, share the same underlying mathematical structure and exhibit universal behavior near the percolation threshold. This universality arises because the details of the microscopic interactions become irrelevant at larger scales, and the system's behavior is governed by the collective properties of its components.</span>
<span id="cb1-1126"><a href="#cb1-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1127"><a href="#cb1-1127" aria-hidden="true" tabindex="-1"></a>It is often noted that quantum mechanics is unintuitive, because the mesoscopic physics is so different from the microscopic physics, so we had evolved to intuit the mesoscopic world, and not the microscopic world. But why is it possible to ignore quantum mechanics? Elephants don't need to know, or don't care about, the Standard Model of particle physics.<span class="ot">[^elephants-dont-play-chess]</span> They don't know and don't need to know, because there is no natural selective pressure favoring elephants that have intuitions about quantum mechanics. When they walk, they push dirt around. When water flows, it pushes water around.</span>
<span id="cb1-1128"><a href="#cb1-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1129"><a href="#cb1-1129" aria-hidden="true" tabindex="-1"></a><span class="ot">[^elephants-dont-play-chess]: </span>Nor do they play chess. <span class="co">[</span><span class="ot">@brooksElephantsDonPlay1990</span><span class="co">]</span></span>
<span id="cb1-1130"><a href="#cb1-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1131"><a href="#cb1-1131" aria-hidden="true" tabindex="-1"></a>For example, a cup of water is a composite of H2O molecules, so it should be studied by quantum mechanics. However, zooming out and out, we would find that the system falls into one of several possible fixed points (liquid, gas, solid) or critical points (boiling point, freezing point, triple point). In each case, the quantum-mechanical messiness has been renormalized away.</span>
<span id="cb1-1132"><a href="#cb1-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1133"><a href="#cb1-1133" aria-hidden="true" tabindex="-1"></a>In general, the lesson is that the long-distance behavior of a system does not depend on the details of the short-distance interactions. The benefit of RN is that it saves us from the effort of understanding the microscopic details. On the flip-side, the details do matter if we are far from the critical or fixed point.</span>
<span id="cb1-1134"><a href="#cb1-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1135"><a href="#cb1-1135" aria-hidden="true" tabindex="-1"></a>This is an overarching theme in renormalization theory, which might be called the **universality hypothesis**:</span>
<span id="cb1-1136"><a href="#cb1-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1137"><a href="#cb1-1137" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; All phase transition problems can be divided into a small number of different classes depending upon the dimensionality of the system and the symmetries of the order state. Within each class, all phase transitions have identical behaviour in the critical region, only the names of the variables are changed. </span><span class="co">[</span><span class="ot">@kadanoffOrderChaosII1999, page 273</span><span class="co">]</span></span>
<span id="cb1-1138"><a href="#cb1-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1139"><a href="#cb1-1139" aria-hidden="true" tabindex="-1"></a>Furthermore, universality justifies our toy models as "serious play". When I was first learning statistical mechanics, I was terribly confused by it. "They can't be serious -- do they think I'm stupid? How could the Ising model possibly be relevant to real magnets?" But  the universality hypothesis justifies Ising models as serious toy models. Even if they are completely different from real magnets when far from the critical point, as we approach the critical point, their behavior becomes *exactly equal* at the limit.</span>
<span id="cb1-1140"><a href="#cb1-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1141"><a href="#cb1-1141" aria-hidden="true" tabindex="-1"></a>RN theory explains why certain features matter when we zoom out and others are washed away. It explains why we have universal classes according to symmetry and dimension and not, say, the precise shape of the atomic force laws. It justifies the use of toy models instead of microscopically accurate models. Specifically, one starts with some microscopic accurate model, then apply renormalization and show that the details fall away and we end up at the same destination as the toy model. <span class="co">[</span><span class="ot">@battermanUniversalityRGExplanations2019</span><span class="co">]</span></span>
<span id="cb1-1142"><a href="#cb1-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1143"><a href="#cb1-1143" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We may well try to simplify the nature of a model to the point where it represents a ‘mere caricature’ of reality. But notice that when one looks at a good political cartoon one can recognize the various characters even though the artist has portrayed them with but a few strokes. ... </span><span class="sc">\[</span><span class="at">A</span><span class="sc">\]</span><span class="at"> good theoretical model of a complex system should be like a good caricature: it should emphasize those features which are most important and should downplay the inessential details. </span><span class="co">[</span><span class="ot">@fisherScalingUniversalityRenormalization1983, page 47</span><span class="co">]</span></span>
<span id="cb1-1144"><a href="#cb1-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1145"><a href="#cb1-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Koan: Sociophysics</span></span>
<span id="cb1-1146"><a href="#cb1-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1147"><a href="#cb1-1147" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "The details don't matter." said them triumphantly as they declared their independence from biophysics.</span></span>
<span id="cb1-1148"><a href="#cb1-1148" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-1149"><a href="#cb1-1149" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "'The details don't matter.'" said them mockingly as they declared their insurrection against sociophysics.</span></span>
<span id="cb1-1150"><a href="#cb1-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1151"><a href="#cb1-1151" aria-hidden="true" tabindex="-1"></a>**Yuxi's Comment:**<span class="ot">[^tolstoy-sociophysics]</span> The traditional approach of historians, going back to the days of "kings and battles", is to run to personality theory and the individual acts, when confronted by a problem in sociology or economics! One establishes the individual actors, makes some (hopefully) sensible approximations of their personality makeups, and then attempts to explain events, actions, and so on. However, for truly complicated systems that these days are studied under the name of "sociophysics", this is a hopeless task; furthermore, the questions it answers are not even the right ones. The modern theorist would rather explain how the stable features of the problem are *invariant* under different assumptions of what individual people do, and arise from features of their interactions. Indeed, if one had a perfect archive of exactly what every person thought and said during the start of WWI, one would still have no understanding of why it started!</span>
<span id="cb1-1152"><a href="#cb1-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1153"><a href="#cb1-1153" aria-hidden="true" tabindex="-1"></a><span class="ot">[^tolstoy-sociophysics]</span>:</span>
<span id="cb1-1154"><a href="#cb1-1154" aria-hidden="true" tabindex="-1"></a>    Inspired by Tolstoy's *War and Peace*.</span>
<span id="cb1-1155"><a href="#cb1-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1156"><a href="#cb1-1156" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; The movement of humanity, arising as it does from innumerable arbitrary human wills, is continuous. To understand the laws of this continuous movement is the aim of history. But to arrive at these laws, resulting from the sum of all those human wills, man's mind postulates arbitrary and disconnected units. The first method of history is to take an arbitrarily selected series of continuous events and examine it apart from others, though there is and can be no beginning to any event, for one event always flows uninterruptedly from another.</span></span>
<span id="cb1-1157"><a href="#cb1-1157" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb1-1158"><a href="#cb1-1158" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; The second method is to consider the actions of some one man -- a king or a commander -- as equivalent to the sum of many individual wills; whereas the sum of individual wills is never expressed by the activity of a single historic personage.</span></span>
<span id="cb1-1159"><a href="#cb1-1159" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb1-1160"><a href="#cb1-1160" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Historical science in its endeavor to draw nearer to truth continually takes smaller and smaller units for examination. But however small the units it takes, we feel that to take any unit disconnected from others, or to assume a beginning of any phenomenon, or to say that the will of many men is expressed by the actions of any one historic personage, is in itself false.</span></span>
<span id="cb1-1161"><a href="#cb1-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1162"><a href="#cb1-1162" aria-hidden="true" tabindex="-1"></a>**[Mumon's Comment](https://en.wikipedia.org/wiki/The_Gateless_Barrier):** Historians search for the king's motive and the general's ambition. They build castles of personality, moats of actions, yet understand nothing of the war. The great black spider traps its preys with a net so dense that nothing is lost after it has digested all of them.</span>
<span id="cb1-1163"><a href="#cb1-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1164"><a href="#cb1-1164" aria-hidden="true" tabindex="-1"></a><span class="al">![](figure/banner/thumbnail_3.png)</span></span>
<span id="cb1-1165"><a href="#cb1-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1166"><a href="#cb1-1166" aria-hidden="true" tabindex="-1"></a><span class="fu">## Appendix {.appendix}</span></span>
<span id="cb1-1167"><a href="#cb1-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-1168"><a href="#cb1-1168" aria-hidden="true" tabindex="-1"></a>If only I understood what this is saying, then I would have written it in. <span class="co">[</span><span class="ot">@mehtaExactMappingVariational2014</span><span class="co">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>