<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-11-01">
<meta name="description" content="Neural networks scale they way they do, purely because of data.">

<title>Neural Scaling Laws by Data Manifold Dimensions – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Neural Scaling Laws by Data Manifold Dimensions – Yuxi on the Wired">
<meta property="og:description" content="Neural networks scale they way they do, purely because of data.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/scaling-law-by-data-manifold/figure/cvae_latent_space.jpg">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta name="twitter:title" content="Neural Scaling Laws by Data Manifold Dimensions – Yuxi on the Wired">
<meta name="twitter:description" content="Neural networks scale they way they do, purely because of data.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/scaling-law-by-data-manifold/figure/cvae_latent_space.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Neural Scaling Laws by Data Manifold Dimensions</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Neural networks scale they way they do, purely because of data.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">scaling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 1, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">January 20, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-manifold-hypothesis" id="toc-the-manifold-hypothesis" class="nav-link active" data-scroll-target="#the-manifold-hypothesis">The manifold hypothesis</a></li>
  <li><a href="#deriving-the-scaling-law" id="toc-deriving-the-scaling-law" class="nav-link" data-scroll-target="#deriving-the-scaling-law">Deriving the scaling law</a>
  <ul class="collapse">
  <li><a href="#prototype-case" id="toc-prototype-case" class="nav-link" data-scroll-target="#prototype-case">Prototype case</a></li>
  <li><a href="#generalization" id="toc-generalization" class="nav-link" data-scroll-target="#generalization">Generalization</a></li>
  <li><a href="#scaling-of-nearest-neighbor-rule" id="toc-scaling-of-nearest-neighbor-rule" class="nav-link" data-scroll-target="#scaling-of-nearest-neighbor-rule">Scaling of nearest neighbor rule</a></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">Experiments</a>
  <ul class="collapse">
  <li><a href="#synthetic-data-manifolds" id="toc-synthetic-data-manifolds" class="nav-link" data-scroll-target="#synthetic-data-manifolds">Synthetic data manifolds</a></li>
  <li><a href="#cifar-10" id="toc-cifar-10" class="nav-link" data-scroll-target="#cifar-10">CIFAR-10</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<p>This is a theory of neural scaling law, proposed by <span class="citation" data-cites="bahriExplainingNeuralScaling2021 sharmaScalingLawsData2022">(<a href="#ref-bahriExplainingNeuralScaling2021" role="doc-biblioref">Bahri et al. 2021</a>; <a href="#ref-sharmaScalingLawsData2022" role="doc-biblioref">Sharma and Kaplan 2022</a>)</span></p>
<p>According to this theory, a neural network, when trained to convergence, allocates its <span class="math inline">\(N\)</span> parameters in two parts: * A fixed number of parameters that map the data to an intrinsic data manifold of dim <span class="math inline">\(d\)</span>. * All other parameters that handle pieces of this manifold. Loss <span class="math inline">\(\propto\)</span> the volume of each manifold piece.</p>
<p>They argued that the loss function should scale as <span class="math inline">\(L \propto N^{-4/d}\)</span> for cross-entropy and mean-square losses.</p>
<section id="the-manifold-hypothesis" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-manifold-hypothesis">The manifold hypothesis</h2>
<p>Consider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is <span class="math inline">\(\mathbb R^{28\times 28} = \mathbb R^{784}\)</span>.</p>
<p>However, as you may have seen in experiments with the VAE, most of the MNIST dataset “collapses” onto a much smaller subset of <span class="math inline">\(\mathbb R^{784}\)</span>. This is the “(intrinsic) data manifold”, with a dimension much smaller than <span class="math inline">\(784\)</span>. Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful “2D slices” of the dataset:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/cvae_latent_space.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption">Figure from <a href="https://web.archive.org/web/20231123105455/https://www.tensorflow.org/tutorials/generative/cvae"><em>Convolutional Variational Autoencoder</em> - Tensorflow Core Tutorials</a>. More images can be found in <a href="https://savadikarc.github.io/post/2020/01/30/vae_vis/"><em>Visualizing the Variational Autoencoder</em></a>, and can be regenerated in about 10 minutes with a Google Colab GPU.</figcaption>
</figure>
</div>
<p>Real data can be expensive, though, which is why we often use “toy” datasets with known dimensions, generated by a known random process. For example, the following is the “Swiss roll” dataset. It is generated by first populating a 2D square <span class="math inline">\([0, 1]^2\)</span>, then use a function <span class="math inline">\(f: \mathbb R^2 \to \mathbb R^3\)</span> to “roll up” the square into 3D space.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Isomap_on_Swiss_roll.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The Isomap algorithm <span class="citation" data-cites="tenenbaumGlobalGeometricFramework2000">(<a href="#ref-tenenbaumGlobalGeometricFramework2000" role="doc-biblioref">Tenenbaum, Silva, and Langford 2000</a>)</span>, popular for constructing data manifolds. <a href="https://en.wikipedia.org/wiki/File:Isomap_on_Swiss_roll.png">Figure from Wikipedia</a>.</figcaption>
</figure>
</div>
</section>
<section id="deriving-the-scaling-law" class="level2">
<h2 class="anchored" data-anchor-id="deriving-the-scaling-law">Deriving the scaling law</h2>
<p>We can get a feel for where the number <span class="math inline">\(4/d\)</span> came from by studying a simpler model.</p>
<section id="prototype-case" class="level3">
<h3 class="anchored" data-anchor-id="prototype-case">Prototype case</h3>
<p>Consider a problem of regression. We have to learn the true function on the <span class="math inline">\(n\)</span>-dimensional cube: <span class="math inline">\(f: [0, 1]^d \to \mathbb{R}\)</span>. Assume it is <a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuous</a>, that is, its first derivative is upper bounded by <span class="math inline">\(\lambda\)</span>. In particular, this means <span class="math inline">\(|f(x) - f(y)| \leq \lambda |x-y|\)</span> for all <span class="math inline">\(x, y\)</span> in the domain.</p>
<p>We approximate the true function with a piecewise-constant function <span class="math inline">\(\hat f: [0, 1]^d \to \mathbb{R}\)</span>, meaning that its graph looks like a staircase. We divide the cube into <span class="math inline">\(N\)</span> equal smaller cubic pieces, and define <span class="math inline">\(\hat f\)</span> to be equal to the value of <span class="math inline">\(f\)</span> at the center of each cubic piece.</p>
<div id="thm-mse-scaling" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> When the loss is mean square error, it scales like <span class="math inline">\(L = \Theta(N^{-2/d})\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>With <span class="math inline">\(N\)</span> parameters, we can divide the <span class="math inline">\([0, 1]^d\)</span> cube into <span class="math inline">\(N\)</span> equal parts, therefore, each cube has side length <span class="math inline">\(N^{-1/d}\)</span>. Therefore, the distance between the center of each cube and the point farthest from the center is also <span class="math inline">\(\Theta(N^{-1/d})\)</span>.</p>
<p>Now, since the function has bounded first derivative, we know that the difference between the function value at the center of the cube and the function value at the farthest point is bounded by <span class="math inline">\(\lambda \cdot \Theta(N^{-1/d}) = \Theta(N^{-1/d})\)</span>. Therefore, the mean square loss on each individual little cube is bounded by <span class="math inline">\(\Theta(N^{-2/d})\)</span>.</p>
<p>And since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by <span class="math inline">\(\Theta(N^{-2/d})\)</span>.</p>
</div>
</section>
<section id="generalization" class="level3">
<h3 class="anchored" data-anchor-id="generalization">Generalization</h3>
<p>More generally, if <span class="math inline">\(f\)</span> has bounded second-derivative, and we use a piecewise-linear <span class="math inline">\(\hat f\)</span> function approximator, then the mean square loss scales like <span class="math inline">\(\Theta(N^{-4/d})\)</span>. By piece-wise linear, we mean that the domain of <span class="math inline">\(\hat f\)</span> is divided into little cubes, and it is linear on each little cube.</p>
<p>Indeed, this generalizes in the obvious way:</p>
<div id="thm-general-case" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> If the loss is mean <span class="math inline">\(p\)</span>-th power loss, <span class="math inline">\(f\)</span> has bounded <span class="math inline">\(k+1\)</span>-th order derivatives, and <span class="math inline">\(\hat f\)</span> is composed of piece-wise <span class="math inline">\(k\)</span>-degree polynomials, then the loss scales like <span class="math inline">\(\Theta(N^{-p(k+1)/d})\)</span>.</p>
<p>Since the KL-divergence is approximately MSE loss when the predictor is close to correct, the loss scales like <span class="math inline">\(\Theta(N^{-2(k+1)/d})\)</span> in this case.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We prove another case where the loss is still mean square error, but <span class="math inline">\(f\)</span> has bounded <span class="math inline">\(2\)</span>-th order derivatives.</p>
<p>By Taylor expansion, if we use the first-order Taylor expansion to approximate <span class="math inline">\(f\)</span> at the center of each cube, then the error is bounded by <span class="math inline">\(\Theta(N^{-2/d})\)</span>. And since the mean square loss is the average of the square of the error, the total mean squared loss is bounded by <span class="math inline">\(\Theta(N^{-4/d})\)</span> on each little cube.</p>
<p>And since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by <span class="math inline">\(\Theta(N^{-4/d})\)</span>.</p>
<p>For the general case, take the Taylor expansion to the <span class="math inline">\(k\)</span>-th order at the center of each little cube.</p>
</div>
</section>
<section id="scaling-of-nearest-neighbor-rule" class="level3">
<h3 class="anchored" data-anchor-id="scaling-of-nearest-neighbor-rule">Scaling of nearest neighbor rule</h3>
<p>What is the worst possible scaling? It would be when <span class="math inline">\(k=0\)</span> and <span class="math inline">\(p=1\)</span>, giving us <span class="math inline">\(L = \Theta(N^{-1/d})\)</span>. What does this mean? To have <span class="math inline">\(k=0\)</span> means that we use piecewise-constant fitting function <span class="math inline">\(\hat f\)</span>. To have <span class="math inline">\(p=1\)</span> means that we are using the L1-loss. This is essentially piecewise constant, median regression.</p>
<p>Under mild assumptions, the nearest neighbor rule for classification has the same form of scaling, where the loss is not L1-loss, but 0-1 loss.</p>
<p>People have found sharper scaling laws under assuming nicer datasets, using complicated functional analysis tools scaling laws. A dataset can be “nice” in several ways. One way is to have few outliers: it should have a thin tail, looking more like a box, rather than a gently rising hill. Another way is to have smoothly varying boundaries: its boundary should not look “bumpy”. See <span class="citation" data-cites="yangTwoPhasesScaling2023">(<a href="#ref-yangTwoPhasesScaling2023" role="doc-biblioref">Yang and Zhang 2023</a>)</span> for a brief review and further citations to the literature.</p>
</section>
</section>
<section id="experiments" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p>According to the theory, if the data manifold has dimension <span class="math inline">\(d\)</span>, then as we scale up a neural network with <span class="math inline">\(N\)</span> parameters, the MSE loss of a fully-trained network would scale like <span class="math inline">\(L \sim N^{-\alpha}\)</span>, where <span class="math inline">\(\alpha \approx 4/d\)</span>. We test this in two ways, once with synthetic datasets, where we know that the data manifold has the desired number of dimensions, and once with the <a href="https://en.wikipedia.org/wiki/CIFAR-10">CIFAR-10</a> dataset, where we do not have the dimension of the data manifold, and must estimate it.</p>
<p>All code for generating the dataset, and for analyzing the dataset, are in a GitHub repo: <a href="https://github.com/yuxi-liu-wired/scaling-law-by-data-manifold"><code>yuxi-liu-wired/scaling-law-by-data-manifold</code></a>.</p>
<section id="synthetic-data-manifolds" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="synthetic-data-manifolds">Synthetic data manifolds</h3>
<p>Since Consider the simplest data manifold: <span class="math inline">\(\mathbb R^d\)</span>, affine-transformed, then embedded in <span class="math inline">\(\mathbb R^n\)</span>, with <span class="math inline">\(n &gt; d\)</span>.</p>
<p>To synthesize such a data manifold, we randomly initialize a <strong>teacher network</strong>, so-called because it implements the function that a <strong>student network</strong> will fit to by supervised training. Each teacher network is constructed thus:</p>
<ul>
<li>The number of neurons in each layer are: <span class="math inline">\([d, 9, 600, 600, 1]\)</span></li>
<li>It has 0 bias.</li>
<li>The weights between layers are sampled from <span class="math inline">\(\mathcal N(0, m^{-1/2})\)</span> , where <span class="math inline">\(m\)</span> is the input size of the layer (a form of He initialization).</li>
<li>The activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.</li>
</ul>
<p>Once we have constructed a teacher network, we use it to generate a dataset <span class="math inline">\(\{(x_i, y_i)\}_i\)</span> in this way:</p>
<ul>
<li>Generate random gaussian vectors <span class="math inline">\(\{t_i\}_i\)</span> in <span class="math inline">\(\mathbb R^d\)</span>, with mean <span class="math inline">\(0\)</span> and std <span class="math inline">\(I_{d\times d}\)</span>.</li>
<li>For each <span class="math inline">\(t \in \{t_i\}_i\)</span>, push <span class="math inline">\(t\)</span> through the teacher network.</li>
<li>Let <span class="math inline">\(x \in \mathbb R^9\)</span> be the teacher network activation at the second layer, with 9 neurons.</li>
<li>Let <span class="math inline">\(y \in \mathbb R\)</span> be the teacher network output.</li>
</ul>
<p>First, we define the “student” neural network architecture:</p>
<ul>
<li>The number of neurons in each layer are: <span class="math inline">\([9, n, n, 1]\)</span>.</li>
<li>The biases are initialized to 0.</li>
<li>The weights between layers are sampled from <span class="math inline">\(\mathcal N(0, m^{-1/2})\)</span>, where <span class="math inline">\(m\)</span> is the input size of the layer (a form of He initialization).</li>
<li>All activation functions are ReLU.</li>
</ul>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/teacher_neural_network.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">An example teacher network architecture with <span class="math inline">\([9, 5, 5, 1]\)</span> neurons.</figcaption>
</figure>
</div>
<p>The parameter count is</p>
<p><span class="math display">\[
N = \underbrace{(n+n+1)}_{\text{first layer}} + \underbrace{(9n + n^2 + n)}_{\text{second layer}}
\]</span></p>
<p>With these settings, I ran the experiment many times, for <span class="math inline">\(N\)</span> ranging from <span class="math inline">\(500\)</span> to <span class="math inline">\(10000\)</span>, and <span class="math inline">\(d\)</span> from <span class="math inline">\(2\)</span> to <span class="math inline">\(18\)</span>. The results do not look as clean as given in the paper, despite that I have tried my best to match the experimental design as specified in the paper.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/training_runs_and_linear_fits.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Experimental data for various synthetic dataset dimensions and student network sizes.</figcaption>
</figure>
</div>
</section>
<section id="cifar-10" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cifar-10">CIFAR-10</h3>
<p>The CIFAR-10 dataset is a popular benchmark, consisting of 32-by-32 RGB images in 10 different image classes, with 6,000 images per class. While the images live in a space of dimension <span class="math inline">\(32^2 \times 3 = 3072\)</span>, <span class="citation" data-cites="sharmaScalingLawsData2022">(<a href="#ref-sharmaScalingLawsData2022" role="doc-biblioref">Sharma and Kaplan 2022</a>)</span> reports that the CIFAR-10 images lies in a data manifold with dimension of only around 16–18.</p>
<p>To fit the dataset, I trained a family of convolutional networks with 3 convolution layers and 2 fully connected layers on CIFAR-10. In order to run a controlled experiment, I varied as few parameters as possible, with the following designs:</p>
<ul>
<li>The network architecture is fixed, and the network parameter count is changed by changing a single number: the number of channels in the convolutional layers.</li>
<li>The experiment is run with 20 different network sizes, from 5408 to 115114.</li>
<li>Each training run lasts 50 epochs, with batch size 128.</li>
<li>The optimizer is <code>AdamW</code> with <code>lr=5e-4</code>.</li>
</ul>
<p>With these settings, I generated all the data and logged them into TensorBoard log files, then cleaned them up for quantile regression. Plotting in log-log scale, with the x-axis being the model parameter count, and the y-axis being the cross-entropy loss, we would get a downward sloping line. Our hope is that the line should have a slope of close to <span class="math inline">\(-4/d\)</span>, where <span class="math inline">\(d \approx 17\)</span>.</p>
<p>This is exactly what I have found. Not only is it true for cross-entropy loss, it is also true for classification accuracy (0-1 loss), except the slope is <span class="math inline">\(+4/d\)</span>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/violin_plot_with_linear_fits.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Experimental data for the train/validation splits of CIFAR-10, and with two different criteria: cross entropy loss and accuracy. We see that in all 4 cases, the scaling exponent is close to the theoretical prediction.</figcaption>
</figure>
</div>


<!-- -->


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bahriExplainingNeuralScaling2021" class="csl-entry" role="listitem">
Bahri, Yasaman, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. 2021. <span>“Explaining Neural Scaling Laws.”</span> <em>arXiv Preprint arXiv:2102.06701</em>. <a href="https://arxiv.org/abs/2102.06701">https://arxiv.org/abs/2102.06701</a>.
</div>
<div id="ref-sharmaScalingLawsData2022" class="csl-entry" role="listitem">
Sharma, Utkarsh, and Jared Kaplan. 2022. <span>“Scaling Laws from the Data Manifold Dimension.”</span> <em>The Journal of Machine Learning Research</em> 23 (1): 343–76.
</div>
<div id="ref-tenenbaumGlobalGeometricFramework2000" class="csl-entry" role="listitem">
Tenenbaum, Joshua B., Vin de Silva, and John C. Langford. 2000. <span>“A <span>Global Geometric Framework</span> for <span>Nonlinear Dimensionality Reduction</span>.”</span> <em>Science</em> 290 (5500): 2319–23. <a href="https://doi.org/10.1126/science.290.5500.2319">https://doi.org/10.1126/science.290.5500.2319</a>.
</div>
<div id="ref-yangTwoPhasesScaling2023" class="csl-entry" role="listitem">
Yang, Pengkun, and Jingzhao Zhang. 2023. <span>“Two <span>Phases</span> of <span>Scaling Laws</span> for <span>Nearest Neighbor Classifiers</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2308.08247">https://doi.org/10.48550/arXiv.2308.08247</a>.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Neural Scaling Laws by Data Manifold Dimensions"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2024-01-20"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [AI, scaling]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Neural networks scale they way they do, purely because of data."</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># image: "figure/banner.png"</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "draft"</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "certain"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 5</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>This is a theory of neural scaling law, proposed by <span class="co">[</span><span class="ot">@bahriExplainingNeuralScaling2021; @sharmaScalingLawsData2022</span><span class="co">]</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>According to this theory, a neural network, when trained to convergence, allocates its $N$ parameters in two parts:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A fixed number of parameters that map the data to an intrinsic data manifold of dim $d$.</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>All other parameters that handle pieces of this manifold. Loss $\propto$ the volume of each manifold piece.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>They argued that the loss function should scale as $L \propto N^{-4/d}$ for cross-entropy and mean-square losses.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## The manifold hypothesis</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>Consider the space of all MNIST images. Each MNIST image is a 28x28 grayscale, so the total space is $\mathbb R^{28\times 28} = \mathbb R^{784}$.</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>However, as you may have seen in experiments with the VAE, most of the MNIST dataset "collapses" onto a much smaller subset of $\mathbb R^{784}$. This is the "(intrinsic) data manifold", with a dimension much smaller than $784$. Indeed, the very fact that the intrinsic dimension is small allows us to have meaningful "2D slices" of the dataset:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Figure from [*Convolutional Variational Autoencoder* - Tensorflow Core Tutorials](https://web.archive.org/web/20231123105455/https://www.tensorflow.org/tutorials/generative/cvae). More images can be found in [*Visualizing the Variational Autoencoder*](https://savadikarc.github.io/post/2020/01/30/vae_vis/), and can be regenerated in about 10 minutes with a Google Colab GPU.</span><span class="co">](figure/cvae_latent_space.jpg)</span>{width=50%}</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>Real data can be expensive, though, which is why we often use "toy" datasets with known dimensions, generated by a known random process. For example, the following is the "Swiss roll" dataset. It is generated by first populating a 2D square $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^2$, then use a function $f: \mathbb R^2 \to \mathbb R^3$ to "roll up" the square into 3D space.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The Isomap algorithm [@tenenbaumGlobalGeometricFramework2000], popular for constructing data manifolds. [Figure from Wikipedia](https://en.wikipedia.org/wiki/File:Isomap_on_Swiss_roll.png).</span><span class="co">](figure/Isomap_on_Swiss_roll.png)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deriving the scaling law</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>We can get a feel for where the number $4/d$ came from by studying a simpler model.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prototype case</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>Consider a problem of regression. We have to learn the true function on the $n$-dimensional cube: $f: <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^d \to \R$. Assume it is <span class="co">[</span><span class="ot">Lipschitz continuous</span><span class="co">](https://en.wikipedia.org/wiki/Lipschitz_continuity)</span>, that is, its first derivative is upper bounded by $\lambda$. In particular, this means $|f(x) - f(y)| \leq \lambda |x-y|$ for all $x, y$ in the domain. </span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>We approximate the true function with a piecewise-constant function $\hat f: <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^d \to \R$, meaning that its graph looks like a staircase. We divide the cube into $N$ equal smaller cubic pieces, and define $\hat f$ to be equal to the value of $f$ at the center of each cubic piece.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>::: {#thm-mse-scaling}</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>When the loss is mean square error, it scales like $L = \Theta(N^{-2/d})$.</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>With $N$ parameters, we can divide the $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^d$ cube into $N$ equal parts, therefore, each cube has side length $N^{-1/d}$. Therefore, the distance between the center of each cube and the point farthest from the center is also $\Theta(N^{-1/d})$.</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>Now, since the function has bounded first derivative, we know that the difference between the function value at the center of the cube and the function value at the farthest point is bounded by $\lambda \cdot \Theta(N^{-1/d}) = \Theta(N^{-1/d})$. Therefore, the mean square loss on each individual little cube is bounded by $\Theta(N^{-2/d})$.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>And since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by $\Theta(N^{-2/d})$.</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generalization</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>More generally, if $f$ has bounded second-derivative, and we use a piecewise-linear $\hat f$ function approximator, then the mean square loss scales like $\Theta(N^{-4/d})$. By piece-wise linear, we mean that the domain of $\hat f$ is divided into little cubes, and it is linear on each little cube. </span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>Indeed, this generalizes in the obvious way:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>::: {#thm-general-case}</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>If the loss is mean $p$-th power loss, $f$ has bounded $k+1$-th order derivatives, and $\hat f$ is composed of piece-wise $k$-degree polynomials, then the loss scales like $\Theta(N^{-p(k+1)/d})$.</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>Since the KL-divergence is approximately MSE loss when the predictor is close to correct, the loss scales like $\Theta(N^{-2(k+1)/d})$ in this case.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>We prove another case where the loss is still mean square error, but $f$ has bounded $2$-th order derivatives.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>By Taylor expansion, if we use the first-order Taylor expansion to approximate $f$ at the center of each cube, then the error is bounded by $\Theta(N^{-2/d})$. And since the mean square loss is the average of the square of the error, the total mean squared loss is bounded by $\Theta(N^{-4/d})$ on each little cube.</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>And since the overall mean square loss is the average of the loss on each individual cube, the total loss is also bounded by $\Theta(N^{-4/d})$.</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>For the general case, take the Taylor expansion to the $k$-th order at the center of each little cube.</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scaling of nearest neighbor rule</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>What is the worst possible scaling? It would be when $k=0$ and $p=1$, giving us $L = \Theta(N^{-1/d})$. What does this mean? To have $k=0$ means that we use piecewise-constant fitting function $\hat f$. To have $p=1$ means that we are using the L1-loss. This is essentially piecewise constant, median regression.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>Under mild assumptions, the nearest neighbor rule for classification has the same form of scaling, where the loss is not L1-loss, but 0-1 loss.</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>People have found sharper scaling laws under assuming nicer datasets, using complicated functional analysis tools scaling laws. A dataset can be "nice" in several ways. One way is to have few outliers: it should have a thin tail, looking more like a box, rather than a gently rising hill. Another way is to have smoothly varying boundaries: its boundary should not look "bumpy". See <span class="co">[</span><span class="ot">@yangTwoPhasesScaling2023</span><span class="co">]</span> for a brief review and further citations to the literature.</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="fu">## Experiments</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>According to the theory, if the data manifold has dimension $d$, then as we scale up a neural network with $N$ parameters, the MSE loss of a fully-trained network would scale like $L \sim N^{-\alpha}$, where $\alpha \approx 4/d$. We test this in two ways, once with synthetic datasets, where we know that the data manifold has the desired number of dimensions, and once with the <span class="co">[</span><span class="ot">CIFAR-10</span><span class="co">](https://en.wikipedia.org/wiki/CIFAR-10)</span> dataset, where we do not have the dimension of the data manifold, and must estimate it.</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>All code for generating the dataset, and for analyzing the dataset, are in a GitHub repo: <span class="co">[</span><span class="ot">`yuxi-liu-wired/scaling-law-by-data-manifold`</span><span class="co">](https://github.com/yuxi-liu-wired/scaling-law-by-data-manifold)</span>.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="fu">### Synthetic data manifolds</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>Since Consider the simplest data manifold: $\mathbb R^d$, affine-transformed, then embedded in $\mathbb R^n$, with $n &gt; d$.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>To synthesize such a data manifold, we randomly initialize a **teacher network**, so-called because it implements the function that a **student network** will fit to by supervised training. Each teacher network is constructed thus:</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The number of neurons in each layer are: $<span class="co">[</span><span class="ot">d, 9, 600, 600, 1</span><span class="co">]</span>$</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>It has 0 bias.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The weights between layers are sampled from $\mathcal N(0, m^{-1/2})$ , where $m$ is the input size of the layer (a form of He initialization).</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The activation function at the second (with 9 neurons) and last layers are identity. All other activation functions are ReLU.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>Once we have constructed a teacher network, we use it to generate a dataset $<span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>_i$ in this way:</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Generate random gaussian vectors $<span class="sc">\{</span>t_i\}_i$ in $\mathbb R^d$, with mean $0$ and std $I_{d\times d}$.</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For each $t \in <span class="sc">\{</span>t_i<span class="sc">\}</span>_i$, push $t$ through the teacher network.</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Let $x \in \mathbb R^9$ be the teacher network activation at the second layer, with 9 neurons.</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Let $y \in \mathbb R$ be the teacher network output.</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>First, we define the "student" neural network architecture:</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The number of neurons in each layer are: $<span class="co">[</span><span class="ot">9, n, n, 1</span><span class="co">]</span>$.</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The biases are initialized to 0.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The weights between layers are sampled from $\mathcal N(0, m^{-1/2})$, where $m$ is the input size of the layer (a form of He initialization).</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>All activation functions are ReLU.</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">An example teacher network architecture with $[9, 5, 5, 1]$ neurons.</span><span class="co">](figure/teacher_neural_network.svg)</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>The parameter count is</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>N = \ub{(n+n+1)}{first layer} + \ub{(9n + n^2 + n)}{second layer}</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>With these settings, I ran the experiment many times, for $N$ ranging from $500$ to $10000$, and $d$ from $2$ to $18$. The results do not look as clean as given in the paper, despite that I have tried my best to match the experimental design as specified in the paper. </span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="al">![Experimental data for various synthetic dataset dimensions and student network sizes.](figure/training_runs_and_linear_fits.png)</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="fu">### CIFAR-10</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>The CIFAR-10 dataset is a popular benchmark, consisting of 32-by-32 RGB images in 10 different image classes, with 6,000 images per class. While the images live in a space of dimension $32^2 \times 3 = 3072$, <span class="co">[</span><span class="ot">@sharmaScalingLawsData2022</span><span class="co">]</span>  reports that the CIFAR-10 images lies in a data manifold with dimension of only around 16--18.</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>To fit the dataset, I trained a family of convolutional networks with 3 convolution layers and 2 fully connected layers on CIFAR-10. In order to run a controlled experiment, I varied as few parameters as possible, with the following designs:</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The network architecture is fixed, and the network parameter count is changed by changing a single number: the number of channels in the convolutional layers.</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The experiment is run with 20 different network sizes, from 5408 to 115114.</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each training run lasts 50 epochs, with batch size 128.</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The optimizer is <span class="in">`AdamW`</span> with <span class="in">`lr=5e-4`</span>.</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>With these settings, I generated all the data and logged them into TensorBoard log files, then cleaned them up for quantile regression. Plotting in log-log scale, with the x-axis being the model parameter count, and the y-axis being the cross-entropy loss, we would get a downward sloping line. Our hope is that the line should have a slope of close to $-4/d$, where $d \approx 17$.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>This is exactly what I have found. Not only is it true for cross-entropy loss, it is also true for classification accuracy (0-1 loss), except the slope is $+4/d$.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="al">![Experimental data for the train/validation splits of CIFAR-10, and with two different criteria: cross entropy loss and accuracy. We see that in all 4 cases, the scaling exponent is close to the theoretical prediction.](figure/violin_plot_with_linear_fits.png)</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>