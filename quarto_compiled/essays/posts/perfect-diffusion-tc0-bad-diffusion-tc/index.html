<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2025-04-17">
<meta name="description" content="An application of computational complexity theory to diffusion language models. Unlikely to be useful for anything, but it is cute!">

<title>Perfect diffusion is TC0 – Bad diffusion is Turing-complete – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Perfect diffusion is TC0 – Bad diffusion is Turing-complete – Yuxi on the Wired">
<meta property="og:description" content="An application of computational complexity theory to diffusion language models. Unlikely to be useful for anything, but it is cute!">
<meta property="og:image" content="https://yuxi.ml/essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/figure/banner.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="512">
<meta property="og:image:width" content="768">
<meta name="twitter:title" content="Perfect diffusion is TC0 – Bad diffusion is Turing-complete – Yuxi on the Wired">
<meta name="twitter:description" content="An application of computational complexity theory to diffusion language models. Unlikely to be useful for anything, but it is cute!">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/perfect-diffusion-tc0-bad-diffusion-tc/figure/banner.png">
<meta name="twitter:image-height" content="512">
<meta name="twitter:image-width" content="768">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Perfect diffusion is TC<sup>0</sup> – Bad diffusion is Turing-complete</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          An application of computational complexity theory to diffusion language models. Unlikely to be useful for anything, but it is cute!
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">math</div>
                <div class="quarto-category">fun</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 17, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">April 17, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#idea" id="toc-idea" class="nav-link active" data-scroll-target="#idea">Idea</a>
  <ul class="collapse">
  <li><a href="#width-and-depth" id="toc-width-and-depth" class="nav-link" data-scroll-target="#width-and-depth">Width and depth</a></li>
  <li><a href="#a-brief-history-of-scaling" id="toc-a-brief-history-of-scaling" class="nav-link" data-scroll-target="#a-brief-history-of-scaling">A brief history of scaling</a></li>
  <li><a href="#actually-existing-complexity" id="toc-actually-existing-complexity" class="nav-link" data-scroll-target="#actually-existing-complexity">Actually existing complexity</a></li>
  <li><a href="#diffusion-models" id="toc-diffusion-models" class="nav-link" data-scroll-target="#diffusion-models">Diffusion models</a></li>
  </ul></li>
  <li><a href="#setting-up-the-framework" id="toc-setting-up-the-framework" class="nav-link" data-scroll-target="#setting-up-the-framework">Setting up the framework</a>
  <ul class="collapse">
  <li><a href="#diffusion-modeling" id="toc-diffusion-modeling" class="nav-link" data-scroll-target="#diffusion-modeling">Diffusion modeling</a></li>
  <li><a href="#circuit-complexity-theory" id="toc-circuit-complexity-theory" class="nav-link" data-scroll-target="#circuit-complexity-theory">Circuit complexity theory</a></li>
  <li><a href="#language-modeling" id="toc-language-modeling" class="nav-link" data-scroll-target="#language-modeling">Language modeling</a></li>
  <li><a href="#counter-machines" id="toc-counter-machines" class="nav-link" data-scroll-target="#counter-machines">Counter machines</a></li>
  </ul></li>
  <li><a href="#the-main-part" id="toc-the-main-part" class="nav-link" data-scroll-target="#the-main-part">The main part</a>
  <ul class="collapse">
  <li><a href="#perfect-diffusion-is-in-tc0" id="toc-perfect-diffusion-is-in-tc0" class="nav-link" data-scroll-target="#perfect-diffusion-is-in-tc0">Perfect diffusion is in TC<sup>0</sup></a></li>
  <li><a href="#inexact-score-matching" id="toc-inexact-score-matching" class="nav-link" data-scroll-target="#inexact-score-matching">Inexact score-matching</a></li>
  <li><a href="#bad-diffusion-is-turing-complete" id="toc-bad-diffusion-is-turing-complete" class="nav-link" data-scroll-target="#bad-diffusion-is-turing-complete">Bad diffusion is Turing-complete</a></li>
  </ul></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work">Future work</a>
  <ul class="collapse">
  <li><a href="#theoretical" id="toc-theoretical" class="nav-link" data-scroll-target="#theoretical">Theoretical</a></li>
  <li><a href="#empirical" id="toc-empirical" class="nav-link" data-scroll-target="#empirical">Empirical</a></li>
  <li><a href="#architectural" id="toc-architectural" class="nav-link" data-scroll-target="#architectural">Architectural</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<p><small>This essay has a <a href="figure/paper.pdf">companion paper</a> on arXiv.</small></p>
<section id="idea" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="idea">Idea</h2>
<section id="width-and-depth" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="width-and-depth">Width and depth</h3>
<p>Some problems take a lot of work to solve. It is a lot of work to solve Riemann’s Hypothesis. It is also a lot of work to solve a trillion grade-school arithmetic problems. However, they are not the same kind of hard work, even if the required amount of “work” may be the same. One is “deep” while another one is “wide”. Recall the old joke about how 1000 people cannot dig a hole in 1 second.</p>
<p>We say that a computation is “wide” if it has many steps, but the steps can take place in any order. In particular, they can be done concurrently. It can be finished faster either by using a processor with a faster clock, or more processors. We say that a computation is “deep” (or perhaps “inherently sequential”) if it has many steps that must take place one after another. Using more processors would not help. Only using a processor with a faster clock would help.</p>
<p>Language modeling research has shown that autoregressive models need “chain of thought” when solving certain reasoning tasks. This benefit isn’t just observed in practice, but has theoretical foundations. <span class="citation" data-cites="fengRevealingMysteryChain2023">(<a href="#ref-fengRevealingMysteryChain2023" role="doc-biblioref">Feng et al. 2023</a>)</span> In the big picture, both theory and practice has shown that chain of thought approaches benefit precisely in allowing a variable amount of sequential processing, which is necessary for solving problems that need deeper computation than one forward pass allows. Trying to do it with less leads to errors, guesswork, or inefficient memorization in a desperate attempt to solve the problem “out of the model’s depth”.</p>
<p>As an example, if your program solve both easy and difficult Sudoku puzzles in exactly the same number of steps, you might reasonably question whether your method actually works for the hard puzzles. At one end of the limit, an easy Sudoku puzzle could be solved in a few steps by filling in each blank in parallel, since each blank could be solved by checking its row, column, and square. There is no sequential dependence between the blanks. At the other end of the limit, a hard Sudoku puzzle would involve a large amount of dependence between many blanks, which would require deep tree searches and significant backtracking to solve. It would be unreasonable to expect a parallel algorithm to solve a hard Sudoku puzzle, no matter the “width” of the parallelism, if it does not have the requisite “depth”.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/sudoku_easy.png" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption class="margin-caption">An easy Sudoku puzzle can be solved in parallel. Here, the two circled blanks have no interdependence. Each can be filled simply by checking their own rows, columns, and squares.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/sudoku_hard.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption class="margin-caption">Advanced Sudoku puzzles require advanced techniques to solve, which usually involve drawing interdependencies across the board. Illustrated here are “equivalent sets”. In each image, the red-colored blanks must contain the same numbers as the blue-colored blanks. Expert Sudoku puzzlers memorize a large number of such patterns to cut down the possibilities.</figcaption>
</figure>
</div>
</section>
<section id="a-brief-history-of-scaling" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="a-brief-history-of-scaling">A brief history of scaling</h3>
<p>The recent history of deep learning has been drunk-high on parallel computing. Pushed to the very limits of parallelism: 256, 2048, 8192. How many cores can you pack on a single chip? This came at the price of precision. FP64, FP32, INT8… How low can you go? There are even proposals of a E4M0 floating point number – 4-bit float, 0-bit mantissa!</p>
<p>But it hasn’t always like this.</p>
<p>Fundamentally, this was not an unqualified success, but also a cope with lagging hardware. Since 2005, clock rate has been stuck at 4 GHz, placing a hard limit on how many serial steps you can make per wallclock time. GPUs are stuck at 1–2 GHz. This failure at the hardware level eventually propagated up the entire tech stack.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/CPU_clock_rate_plateau.jpeg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The clock rate has plateaued since 2005. Subsequent progress in computer processing speed relied mostly on parallelism, which here is shown in the form of multi-core performance. <span class="citation" data-cites="leisersonTheresPlentyRoom2020">(<a href="#ref-leisersonTheresPlentyRoom2020" role="doc-biblioref">Leiserson et al. 2020, fig. 2</a>)</span></figcaption>
</figure>
</div>
<p>At the architecture level, we went from sequential models like RNNs to the highly parallelizable Transformer architecture. Some architectures are inherently more sequential than others. Recurrent neural networks (RNN) must process <span class="math inline">\(n\)</span> tokens in <span class="math inline">\(n\)</span> steps. So if a single step takes 1 ms, then processing 1000 tokens take 1 second. In contrast, a Transformer can take any number of tokens in 1 step, limited only by the size of your VRAM and processor count. As long as memory and processor count scaled, the stagnation in clock rate was not a binding constraint… Is it?</p>
<p>At the algorithm level, we went from reinforcement learning (RL) to imitation learning and pretraining. To do true RL, the network had to actually play out games one step after another. Imitation learning however is simply pretraining by another name, and can be scaled in parallel.</p>
<p>The success of pretraining over RL, 1M-batch over minibatch, Transformers over RNN, Nvidia over Intel, are all triumphs of the parallel over the serial, but perhaps we need to take account of it again? Might the pendulum swing back?</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>sequential</th>
<th>parallel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>algorithm</td>
<td>RL</td>
<td>imitation learning, (self-)supervised (pre)training</td>
</tr>
<tr class="even">
<td>architecture</td>
<td>RNN</td>
<td>CNN, Transformer, SSM</td>
</tr>
<tr class="odd">
<td>hardware</td>
<td>CPU</td>
<td>GPU</td>
</tr>
</tbody>
</table>
<p>For language generation, the story is quite clear. Before 2016, the SOTA was LSTM, which must be trained sequentially. Starting around 2018, pretrained Transformers took the lead, for which training is as parallel as the hardware can bear. Yet around 2021, sequential compute returned to inference in the form of “chain of thought”. With the DeepSeek-R1 of 2025, it is now quite probable that training will became a lot more sequential.</p>
<p>A similar story could be told for image generation. Before 2019, the SOTA was the U-net, which generates the whole image in parallel. In 2021, OpenAI shocked the world with DALL-E, a standard Transformer architecture that generated images patch-by-patch. But soon, diffusion models took over with their fully parallel generation across all pixels. And most recently, the image generator in OpenAI’s ChatGPT seemed to have reverted back to generating images patch-by-patch.</p>
<p>As for RL itself, the 2013–2017 period was its highlight, with RL agents becoming superhuman at Atari, Dota, and Go. Indeed, during the early days of OpenAI, it focused on RL, with the Dota bot being its first publicity success. However, things changed starting with GPT-1 of 2018, the generatively pretrained Transformer. The so-called “RL from human feedback” (RLHF) is only <em>barely</em> RL, since each “game” in an RLHF training run consists of a <em>single</em> reply! It is as if playing a game that ends after making exactly one move.</p>
<p>If the best kind of training shifts back to be more serial than parallel, then there will be a reckoning. If a few months down the line, we see a new scaling law emerge from the big labs pushing the R1-Zero training method to even longer chains at even smaller batches, then deep learning can suddenly acquire a very serial RL phase. Pretraining would still need giant GPU clusters, but the RL step would suddenly look more like Cray supercomputers – a few giant CPUs, immersed in Fluorinert, running at 100 GHz. Compared to Nvidia GPUs, those would be much more expensive in terms of FLOP/USD, but in terms of serial-FLOP/sec/USD, cheap!</p>
<blockquote class="blockquote">
<p>Seymour Cray was famous for packing, powering, and cooling circuits incredibly densely. Classic Crays were made obsolete by microprocessors, but we may yet do similar things at a larger scale. Hyperscale data centers and even national supercomputers are loosely coupled things today, but if challenges demanded it, there is a world with a zetta scale, tightly integrated, low latency matrix dissipating a gigawatt in a swimming pool of circulating fluorinert.</p>
<p>— <a href="https://x.com/ID_AA_Carmack/status/1300280139071270914">John Carmack (2020-08-30)</a></p>
</blockquote>
<p>In this scenario, an RNN-Transformer hybrid might become the common architecture for cutting-edge training, in two phases. In the first phase, it would be pretrained in parallel over ~1 million GPUs with ~1 million batch size. In the second phase, it would be reinforcement-learning with a minibatch of rollouts per update, on special hardware that maximizes the update frequency. This future hardware could be a small cluster of ~10 massive RL-specific chips with diameter ~1 meter packaged closely to maximize memory bandwidth, immersed in liquid nitrogen cooling to run at ~100 GHz without melting. Such hardware does not exist today, since it is highly uneconomical compared to existing GPUs, as measured in FLOP/sec.&nbsp;However, in a scenario where a long sequence of serial training is necessary, such hardware may be economical as measured in serial FLOP/sec.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Why these numbers?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why these numbers?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An RL-specific chip may be ~1 meter in diameter, because Cerebras already produces chips that are ~20 cm on each side, giving a lower bound. On the other direction, a silicon wafer is ~50 cm in diameter, giving an upper bound.</p>
<p>The GPU and batch size are both in the millions, because those numbers are used for the current (2025) largest pretraining runs.</p>
<p>AS for the ~100 GHz clock rate, we have the following data:</p>
<ul>
<li>As of 2025, <a href="https://skatterbencher.com/cpu-overclocking-world-record-history/">hobbyist overclockers have already reached 9.1 GHz</a> using just a commercially available Intel CPU, cooled by liquid nitrogen, so I believe another factor of 10 is within the technical possibilities.</li>
<li>In <a href="https://www.anandtech.com/show/680/6">2000, Intel planned for a CPU at 10 GHz for 2005</a>. The plan fell through, and in 2004, Intel hastily cancelled the originally planned-for next-generation chips (<a href="https://en.wikipedia.org/wiki/Tejas_and_Jayhawk">Tejas and Jayhawk</a>) due to intractable overheating. However, it shows that 10 GHz is well within the limits of known silicon technology. The heat dissipation issues are certainly not a hard technological limit, but merely too uneconomical in a consumer product.</li>
</ul>
<p>Though currently uneconomical, it is entirely possible for such technology to be resurrected for cutting-edge AI training. Compared to the currently planned million-GPU datacenters, such a special-purpose CPU cluster would be comparatively affordable.</p>
</div>
</div>
</div>
</section>
<section id="actually-existing-complexity" class="level3">
<h3 class="anchored" data-anchor-id="actually-existing-complexity">Actually existing complexity</h3>
<p>Like axiomatic logicians, computational complexity theorists can occasionally be defensive, since they are often haunted by the fear of irrelevance. This is not an idle fear, especially in the context of AI research. It is healthy to try to not become like Minsky and Papert who, having exhaustively demonstrated that 1-layered neural networks are not powerful, declared deeper neural networks to be <a href="https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/index.html#chapter-13">a “sterile extension” of 1-layered neural networks, based on their intuition</a>.</p>
<p>In the best case, computational complexity theory can discover algorithms that are fast in practice, or prove that certain algorithms are impossible <em>in a way that matters</em>. In the less happy case, algorithms can be invented that are fast in theory, but not in practice, such as the “<a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic algorithms</a>”. In the worst case, an impossibility proof is correct, yet wrong <em>in a way that matters</em>. How could this be the case? Every mathematical abstraction makes certain inaccuracies, and a lot can slip through the cracks between the real world and the mathematical realm.</p>
<p>As a concrete example, consider protein folding. In theory, a protein is merely a floppy string of amino acids, and to fold it, there seems to be no way but to try out every one of its configurations in search of its minimal-energy form. However, even nature does not laugh at the difficulties of computation. If a protein is actually hard to fold, then it would be dangerous for the organism to produce such a thing, and thus it would be removed by evolution. In this way, the problem of “folding proteins that a biologist might wish to fold in practice” turns out to be an easier problem than “folding proteins”. This perhaps explains the success of AlphaFold, which can fold a protein in a few forward passes. Surely it is not solving a hard problem with an impossibly small amount of compute. Rather, what appears to be a hard problem is not really hard, and what went wrong was the original act of abstraction that turned protein folding in practice into an unrealistically hard problem in theory.</p>
<p>So where is this paper placed? We place it into the category of “half-irrelevant for practice”. On the one side, the “perfect” diffusion is a vanishing point that can never be reached in practice. On the other side, the Turing-complete bad diffusion is a construction that has given up any pretense of being about diffusion models. It is a deliberate <a href="https://en.wikipedia.org/wiki/Arbitrary_code_execution">arbitrary code execution</a> exploit of the standard diffusion framework. Indeed, we believe that any attempt to train a score network in practice would result in a score network that is quite tame, and definitely incapable of arbitrary code execution.</p>
<p>So why this paper? One, because it is fun. Two, because some of the intuition might be helpful for understanding actually existing diffusion models. It is in this sense that we say this paper is “half-irrelevant”. Real diffusion models are not perfect, but also not malicious, and they seem to behave closer to perfection than malice. Because of this, we conjecture that real diffusion models <em>do</em> converge fast, in <span class="math inline">\(O(1)\)</span> steps. Assuming this is the case, then using them for solving tasks that require more than <span class="math inline">\(O(1)\)</span> serial steps is impossible.</p>
</section>
<section id="diffusion-models" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="diffusion-models">Diffusion models</h3>
<p>A diffusion model can be used to sample from a distribution with a variable amount of computing steps. This is usually understood as an advantage, in the sense of providing a compute-precision tradeoff: With a few steps, one can sample from the distribution approximately, and with increasing number of steps, the distribution can be sampled from with increasing precision. However, this intuitive picture also suggest that this advantage may be a curse. Specifically, it suggests that after a few sampling steps, further computation is “wasted” in the sense that they refine the result in a way that does not matter, because the result has converged.</p>
<p>Indeed, empirically, diffusion models typically converge rapidly with a fixed number of denoising steps regardless of input. For example, <span class="citation" data-cites="ravishankarScalingPropertiesDiffusion2024">(<a href="#ref-ravishankarScalingPropertiesDiffusion2024" role="doc-biblioref">Ravishankar et al. 2024</a>)</span> studied using diffusion models for depth-perception, and showed that there is no difference between 5 and 100 sampling steps, Similarly, <span class="citation" data-cites="austinStructuredDenoisingDiffusion2021">(<a href="#ref-austinStructuredDenoisingDiffusion2021" role="doc-biblioref">Austin et al. 2021</a>)</span> showed that for a diffusion language model they trained, the perplexity of language modeling was essentially the same for 10 and 1024 diffusion steps.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/ravishankar_2024_fig_8.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="ravishankarScalingPropertiesDiffusion2024">(<a href="#ref-ravishankarScalingPropertiesDiffusion2024" role="doc-biblioref">Ravishankar et al. 2024, fig. 8</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/austin_2021_fig_2.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="austinStructuredDenoisingDiffusion2021">(<a href="#ref-austinStructuredDenoisingDiffusion2021" role="doc-biblioref">Austin et al. 2021, fig. 2</a>)</span></figcaption>
</figure>
</div>
<p>While such rapid convergence may be regarded as an advantage, this would not be when the problem is deep. Converging on a solution faster than the depth of the problem requires would likely lead to failures. Indeed, certain kinds of empirical failures of diffusion language models suggest that they struggle precisely on tasks that require sequential processing. For instance, when using diffusion models to solve Sudoku puzzles, <span class="citation" data-cites="wewerSpatialReasoningDenoising2025">(<a href="#ref-wewerSpatialReasoningDenoising2025" role="doc-biblioref">Wewer et al. 2025</a>)</span> found that denoising all digits simultaneously worked for easy puzzles but failed for difficult ones. Performance improved only when denoising fewer digits at a time, with optimal results achieved by denoising just one digit at a time, essentially reverting to a purely sequential process.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/diffusion_sudoku.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Solving Sudoku with a diffusion model. If the entire grid is denoised all at once, the model would nearly always fail on the hard puzzles. If the grid is denoised one number at a time, the model would sometimes succeed on the hard puzzles. <span class="citation" data-cites="wewerSpatialReasoningDenoising2025">(<a href="#ref-wewerSpatialReasoningDenoising2025" role="doc-biblioref">Wewer et al. 2025</a>)</span></figcaption>
</figure>
</div>
<p><span class="citation" data-cites="arriolaBlockDiffusionInterpolating2025">(<a href="#ref-arriolaBlockDiffusionInterpolating2025" role="doc-biblioref">Arriola et al. 2025</a>)</span> noted that discrete diffusion models underperform compared to autoregressive approaches. They proposed to solve this by… reintroducing autoregressive generation, applying diffusion to generate a few tokens at a time.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/block_diffusion.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The “block diffusion” language modeling method, which despite the hopeful captioning, is only 4 times as “parallelizable” as autoregressive language modeling. <span class="citation" data-cites="arriolaBlockDiffusionInterpolating2025">(<a href="#ref-arriolaBlockDiffusionInterpolating2025" role="doc-biblioref">Arriola et al. 2025, fig. 1</a>)</span></figcaption>
</figure>
</div>
</section>
</section>
<section id="setting-up-the-framework" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-framework">Setting up the framework</h2>
<p>The theory part of the paper has three parts:</p>
<ul>
<li>A mathematical framework in which these empirical observations kind of exists in kind of the same way. We make no promises that this framework actually has anything to do with practice, but it looks fun.</li>
<li>A theorem showing that a “perfect” diffusion models are constrained to the <span class="math inline">\(\mathsf{TC}^0\)</span> complexity class due to their rapid convergence.</li>
<li>An explicit construction of a “bad” diffusion models to perform any Turing-computable operation. The construction works precisely because they do not converge quickly. They do not converge quickly precisely because they fail at reversing any forward diffusion process.</li>
</ul>
<p>We begin by setting up the framework.</p>
<section id="diffusion-modeling" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-modeling">Diffusion modeling</h3>
<p>Diffusion models work by gradually adding noise to data and then learning to reverse this process. Think of it as watching a drop of ink spread through water, and then learning to recover the original drop from the diluted state. This intuitive physical analogy connects to their mathematical foundation, which borrows concepts from thermodynamics and statistical physics.</p>
<p>In machine learning literature, two main formulations have emerged to describe this process. The first, <strong>Denoising Diffusion Probabilistic Models (DDPM)</strong>, approaches the problem through discrete time steps. The second, <strong>score-matching with Langevin dynamics (SMLD)</strong>, uses continuous differential equations. Despite their different origins, these two approaches are fundamentally equivalent <span class="citation" data-cites="kingmaVariationalDiffusionModels2021 luoUnderstandingDiffusionModels2022">(<a href="#ref-kingmaVariationalDiffusionModels2021" role="doc-biblioref">Kingma et al. 2021</a>; <a href="#ref-luoUnderstandingDiffusionModels2022" role="doc-biblioref">Luo 2022</a>)</span>.</p>
<p>The connection works in both directions. DDPM can be seen as a discretized version of SMLD, where each DDPM update step corresponds to using the Euler–Maruyama method to solve SMLD’s stochastic differential equation (SDE). Conversely, if we take the limit of infinitely many DDPM steps with infinitesimally small noise additions, we recover the continuous SDE formulation of SMLD. This equivalence means that models trained using either framework can be used interchangeably for sampling purposes.</p>
<p>For clarity and mathematical convenience, we primarily use the SMLD formulation throughout this paper, though our results apply equally to both frameworks, since they are equivalent.</p>
<p>Consider a <strong>data distribution</strong> <span class="math inline">\(\rho_{data}\)</span> over the real space <span class="math inline">\(\mathbb{R}^d\)</span>. The task of SMLD is to learn a <strong>score-matching function</strong> <span class="math inline">\(f_\theta\)</span> that allows us to sample from <span class="math inline">\(\rho_{data}\)</span>.</p>
<p>A <strong>noise schedule</strong> is a continuous function <span class="math inline">\(\beta\)</span> of type <span class="math inline">\([0, \infty) \to [0, \infty)\)</span>, such that <span class="math inline">\(\beta(t)\)</span> can be interpreted as the noising rate in the forward diffusion process at time <span class="math inline">\(t\)</span>. We require <span class="math inline">\(\int_0^\infty \beta(t) dt = \infty\)</span>, which can be interpreted as saying that eventually all signal is destroyed, leaving only noise.</p>
<p>Define the distribution at <span class="math inline">\(t=0\)</span> by <span class="math inline">\(\rho_0 := \rho_{data}\)</span>. Suppose we sample a data point <span class="math inline">\(x_0 \sim \rho_0\)</span>, and let it evolve according to the SDE</p>
<p><span class="math display">\[
dx_t = -\frac{1}{2} \beta(t) x_t dt + \sqrt{\beta(t)} dW_t
\]</span></p>
<p>then this implies a time-evolution of the data distribution over time, which can be directly solved by the Fokker-Planck equation:</p>
<p><span class="math display">\[
\partial_t \rho_t = \frac{1}{2} \beta(t) (\nabla \cdot (x \rho_t) + \Delta \rho_t)
\]</span></p>
<p>At the <span class="math inline">\(t \to \infty\)</span> limit, the distribution converges to the standard normal distribution <span class="math inline">\(\mathcal{N}(0, I_d)\)</span>.</p>
<p>For any time <span class="math inline">\(T &gt; 0\)</span>, the time-evolution can be exactly reversed as follows. Let <span class="math inline">\(\hat{x}_T\)</span> be sampled according to <span class="math inline">\(\rho_{\hat{x}, 0} := \rho_T\)</span>, then the following SDE equation would lead to an exact reversal: <span class="math display">\[
d\hat{x}_t = \frac{1}{2} \beta(T-t) \hat{x}_{t} dt + \beta(T-t) \underbrace{\nabla_{\hat{x}_{t}} \ln \rho_{T-t}(\hat{x}_{t})}_{\text{score function}} dt + \sqrt{\beta(T-t)} dW_t
\]</span> where by “reversal” we mean that <span class="math inline">\(\rho_{\hat{x}, t} = \rho_{T-t}\)</span> for any <span class="math inline">\(t \in [0, T]\)</span> <span class="citation" data-cites="andersonReversetimeDiffusionEquation1982">(<a href="#ref-andersonReversetimeDiffusionEquation1982" role="doc-biblioref">Anderson 1982</a>)</span>.</p>
<p>Assuming that a score-matching function <span class="math inline">\(f_\theta\)</span> has been trained, such that</p>
<p><span class="math display">\[
f_\theta(x, t) \approx \nabla_x \ln \rho_t(x)
\]</span></p>
<p>for all <span class="math inline">\(t, x\)</span>, then <span class="math inline">\(\rho_{data}\)</span> can be approximately sampled by initializing a pure-noise sample <span class="math inline">\(\hat{x}_T \sim \mathcal{N}(0, I_d)\)</span>, then solving the backward SDE</p>
<p><span class="math display">\[
\hat{x}_{t-dt} = \frac{1}{2} \beta(t) \hat{x}_t dt + \beta(t) f_\theta(\hat{x}_t, t) dt + \sqrt{\beta(t)} dW_t
\]</span></p>
<p>by any SDE integration method, such as Euler–Maruyama method. By varying the sizes of the <span class="math inline">\(dt\)</span> steps in the Euler–Maruyama method, we can recover different noise-schedules for DDPM.</p>
<p>If <span class="math inline">\(f_\theta(x, t) = \nabla_x \ln \rho_t(x)\)</span> is exact, then at the limit of <span class="math inline">\(T \to \infty\)</span> and infinitely many steps in the Euler–Maruyama method, we can exactly sample from <span class="math inline">\(\rho_{data}\)</span>.</p>
</section>
<section id="circuit-complexity-theory" class="level3">
<h3 class="anchored" data-anchor-id="circuit-complexity-theory">Circuit complexity theory</h3>
<p>A <a href="https://en.wikipedia.org/wiki/Circuit_complexity">circuit complexity</a> class is a style computational complexity classes. In our paper, we focus on the <span class="math inline">\(\mathsf{TC}^0\)</span> class, which is particularly suited to studying the computational complexity of neural networks, because a family of feedforward neural networks with a constant number of layers is essentially a <span class="math inline">\(\mathsf{TC}^0\)</span> circuit family. Indeed, the class of <span class="math inline">\(\mathsf{TC}^0\)</span> were first proposed specifically in the 1980s to model the computational complexity of neural networks. <span class="citation" data-cites="parberryParallelComputationThreshold1988">(<a href="#ref-parberryParallelComputationThreshold1988" role="doc-biblioref">Parberry and Schnitger 1988</a>)</span></p>
<p>Formally, <span class="math inline">\(\mathsf{TC}^0\)</span> is defined as the class of problems that can be decided by a family of boolean circuits with the following properties:</p>
<ul>
<li><strong>Boolean circuits:</strong> A boolean circuit is a directed acyclic graph where each node (or gate) computes a boolean function of its inputs. The inputs to the circuit are boolean variables, and the output is a single boolean value.</li>
<li><strong>Unbounded fan-in:</strong> Each gate in the circuit can receive inputs from an arbitrary number of other gates. This contrasts with bounded fan-in circuits where gates have a limited number of inputs. Convolutional neurons have bounded fan-in, but fully-connected neurons have unbounded fan-in.</li>
<li><strong>Polynomial width:</strong> The number of gates at each level of the circuit is bounded by a polynomial in the input size <span class="math inline">\(n\)</span>.</li>
<li><strong>Constant depth:</strong> The longest path from any input to the output is bounded by a constant that does not depend on the input size. This may be interpreted as stating the circuit family is “highly parallelizable”.</li>
<li><strong>Threshold gates:</strong> A threshold gate is a binary neuron. It can be written as a function <span class="math inline">\(\theta(\sum_i w_i x_i + t)\)</span>, where <span class="math inline">\(w_i, t\)</span> are real numbers, and <span class="math inline">\(\theta\)</span> is the binary step-function</li>
</ul>
<p>For those unfamiliar, here is a short exercise:</p>
<ul>
<li>With 1 layer of threshold gates, construct “gadgets” such as the AND gate, the NOT gate, and all other common boolean gates.</li>
<li>With 2 layers, construct the <span class="math inline">\(k\)</span>-EQUALS gate for each <span class="math inline">\(k\)</span>, which outputs 1 if exactly <span class="math inline">\(k\)</span> inputs are 1, and 0 otherwise.</li>
<li>With 3 layers, construct the IS-IN gate for any finite subset of <span class="math inline">\(\mathbb{N}\)</span>.</li>
</ul>
<p>From the definition, it is clear that each member <span class="math inline">\(\mathsf{TC}^0\)</span> circuit family is essentially a feedforward neural network. However, this only consists of a single member. Here is where the “family” part of the definition becomes important.</p>
<p>Since a neural network has a fixed number of inputs, it would be unable to process more inputs than the number of neurons in its input. This brings the idea of a circuit <em>family</em>. A circuit family is a set of circuits <span class="math inline">\(C_1, C_2, \dots\)</span> such that each <span class="math inline">\(C_n\)</span> is capable of processing exactly inputs of length <span class="math inline">\(n\)</span>. Computational complexity theory studies not the complexity of problems solvable by a single circuit, but a circuit family, because any single circuit is merely equivalent to a lookup table, and the complexity of the problem it solves is always trivial. If this seems odd to you, remember that to a computational complexity theorist, 1 and 1 trillion are the same – both are <span class="math inline">\(O(1)\)</span>.</p>
<p>Consequently, a <span class="math inline">\(\mathsf{TC}^0\)</span> family of feedforward neural networks is defined as a set of neural networks <span class="math inline">\(C_n\)</span>, such that there exists a constant <span class="math inline">\(D\)</span> (the upper bound on depth), and a polynomial <span class="math inline">\(p\)</span> (the polynomial bound on width), such that each <span class="math inline">\(C_n\)</span> has depth <span class="math inline">\(\leq D\)</span> and number of neurons <span class="math inline">\(\leq p(n)\)</span>.</p>
<p>While the <span class="math inline">\(\mathsf{TC}^0\)</span> class is most similar to feedforward fully-connected neural networks, this is not necessarily the case. Indeed, a family of bounded-depth polynomial-width Transformers is still in the <span class="math inline">\(\mathsf{TC}^0\)</span> class. This means the theorem in the paper applies to them as well.</p>
</section>
<section id="language-modeling" class="level3">
<h3 class="anchored" data-anchor-id="language-modeling">Language modeling</h3>
<p>At the most abstract level, a language is simply a set of words made of letters. Formally:</p>
<ul>
<li>An <strong>alphabet</strong> <span class="math inline">\(\Sigma\)</span> is a finite nonempty set. Each element in the alphabet may be called a <strong>letter</strong> or a <strong>token</strong>.</li>
<li>A <strong>word</strong> in an alphabet <span class="math inline">\(\Sigma\)</span> is a finite sequence of elements of <span class="math inline">\(\Sigma\)</span>.</li>
<li>A <strong>language</strong> <span class="math inline">\(L\)</span> in an alphabet <span class="math inline">\(\Sigma\)</span> is a set of words in the alphabet <span class="math inline">\(\Sigma\)</span>.</li>
</ul>
<p>A <strong>prefix language modeling problem</strong> is, given a sequence of tokens <span class="math inline">\(x_1, \dots, x_n\)</span>, to compute the next token <span class="math inline">\(x_{n+1}\)</span>. An example would be the word problem for finite groups: Given a finite group <span class="math inline">\(G\)</span>, and a sequence of elements in the group <span class="math inline">\(g_1, \dots, g_n\)</span>, compute <span class="math inline">\(\prod_{i=1}^n g_i\)</span>. In particular, if <span class="math inline">\(G\)</span> is the permutation group on 5 elements, then the corresponding group multiplication problem is strongly suspected to lie outside <span class="math inline">\(\mathsf{TC}^0\)</span> class.</p>
<p>An example would be the word problem for finite groups: Given a finite group <span class="math inline">\(G\)</span>, and a sequence of elements in the group <span class="math inline">\(g_1, \dots, g_n\)</span>, compute <span class="math inline">\(\prod_{i=1}^n g_i\)</span>. Intuitively, there is a method that computes this in <span class="math inline">\(\log_2(n)\)</span> parallel steps by binary multiplication: the first parallel step computes <span class="math inline">\(g_1g_2, g_3g_4, \dots\)</span>, and so on. Since <span class="math inline">\(\log_2(n)\)</span> is not constant, this would not lie within the <span class="math inline">\(\mathsf{TC}^0\)</span> class. For certain groups, there are shortcuts to this process. For example, for any prime number <span class="math inline">\(p\)</span>, the word problem in the mod-<span class="math inline">\(p\)</span> multiplicative group is computable in constant number of parallel steps via Fermat’s little theorem. However, shortcuts probably do not exist in general. Indeed, if <span class="math inline">\(G\)</span> is the permutation group on 5 elements, then the corresponding word problem is not in the <span class="math inline">\(\mathsf{TC}^0\)</span> class, assuming widely believed conjectures in computational complexity theory. <span class="citation" data-cites="liuTransformersLearnShortcuts2023">(<a href="#ref-liuTransformersLearnShortcuts2023" role="doc-biblioref">Liu et al. 2023</a>)</span></p>
<p>While usually, a diffusion model is used for generating from a continuous state space such as <span class="math inline">\(\mathbb{R}^d\)</span>, it can be used to model discrete distributions as well. This is necessary for language modeling. We consider the case closest to continuous state space modeling – quantization: One divides the continuous state space <span class="math inline">\(\mathbb{R}^d\)</span> into regions, and assigns a token to each region. This then allows sampling a discrete distribution from a diffusion model with continuous state space. Formally, if <span class="math inline">\(\Sigma = \{a_1, a_2, \dots, a_M\}\)</span> is the alphabet, then we divide <span class="math inline">\(\mathbb{R}^d\)</span> into <span class="math inline">\(M\)</span> regions <span class="math inline">\(V_1, \dots, V_M\)</span>, such that each region <span class="math inline">\(V_i\)</span> maps to a token <span class="math inline">\(a_i\)</span>.</p>
<p>Also, as usual in circuit complexity theory, we need more than a single score-network <span class="math inline">\(f_\theta\)</span>, but rather, a full sequence of them, so we define a **<span class="math inline">\(\mathsf{TC**^0\)</span> family of score-networks} to be a family of feed-forward neural networks <span class="math inline">\(f_{\theta, 0}, f_{\theta, 1}, \dots\)</span> , such that:</p>
<ul>
<li>Each <span class="math inline">\(f_{\theta, n}\)</span> takes as input <span class="math inline">\(n+2\)</span> elements <span class="math inline">\(x_1, \dots, x_n, x, t\)</span>, and produces an output <span class="math inline">\(f_{\theta, n}(x, t | x_1, \dots, x_n)\)</span>.</li>
<li>The family <span class="math inline">\(f_{\theta, n}\)</span> has <span class="math inline">\(O(1)\)</span> depth and <span class="math inline">\(\mathsf{poly}(n)\)</span> width.</li>
</ul>
<p>Note that for the theorem to hold, it is not necessary to assume the family of neural networks are feed-forward. The theorem holds for any family of score-networks for which a single forward pass is in <span class="math inline">\(\mathsf{TC}^0\)</span>. This includes, for example, Transformers and state-space models <span class="citation" data-cites="merrillIllusionStateStateSpace2025 merrillIllusionStateStateSpace2025">(<a href="#ref-merrillIllusionStateStateSpace2025" role="doc-biblioref">Merrill, Petty, and Sabharwal 2025</a>, <a href="#ref-merrillIllusionStateStateSpace2025" role="doc-biblioref">2025</a>)</span>. We stay with feedforward networks because it is visually obvious how they are in the <span class="math inline">\(\mathsf{TC}^0\)</span> class.</p>
<p>Finally, since a diffusion model may solve a problem only with high enough probability, instead of solving it deterministically, we make the following definition: A prefix language modeling problem is <strong>solved with constant probability bound</strong> if there exists some <span class="math inline">\(\epsilon &gt; 0\)</span>, such that for each input token sequence <span class="math inline">\(x_1, \dots, x_n\)</span>, let <span class="math inline">\(x_{correct}\)</span> be the correct response, then <span class="math display">\[
p(x_{correct}|x_1, \dots, x_n) &gt; p(x'|x_1, \dots, x_n) + \epsilon, \quad \forall x' \neq x_{correct}.
\]</span></p>
</section>
<section id="counter-machines" class="level3">
<h3 class="anchored" data-anchor-id="counter-machines">Counter machines</h3>
<p>To show that a deliberately bad diffusion model may be Turing-complete, we show how they could simulate a particular kind of Turing-complete abstract machines: the counter machines. This is not necessary for understanding the theorem on “perfect” diffusion models.</p>
<p>A <strong>counter machine</strong> can be thought of as <strong>finite-state automata</strong> augmented with memories, each of which can hold a single unbounded integer. In our paper, we use the following form of counter machine, lightly modified from <span class="citation" data-cites="fischerCounterMachinesCounter1968">(<a href="#ref-fischerCounterMachinesCounter1968" role="doc-biblioref">Fischer, Meyer, and Rosenberg 1968</a>)</span>:</p>
<ul>
<li>The machine has access to a finite number <span class="math inline">\(k\)</span> of <strong>registers</strong>, notated as <span class="math inline">\(r_1, \dots, r_k\)</span>. Each register stores a single integer.</li>
<li>The machine also has access to a read-only <strong>input tape</strong>, on which the machine has a read-head that can be moved in either direction. At machine start-up, the input tape has contents ^<span class="math inline">\(a_1a_2\dots a_n\)</span>$, where ^ and $ denote the beginning and the end of the word, and <span class="math inline">\(n\)</span> is the length of the input word. The read-head is placed at the character just after ^, which may be $ if the input word is empty.</li>
<li>A <strong>program</strong> for the machine is a numbered list of instructions.</li>
<li>Each <strong>instruction</strong> is of the following format: conditional on the state of the read-head on the input tape and on whether each register is zero or not, modify every register by an amount in <span class="math inline">\(\{-1, 0, +1\}\)</span>, move the read-head by up to one position in either direction, then jump to another instruction.</li>
<li>There is a special instruction named “HALT”. If the machine arrives at such an instruction, it halts. Each HALT instruction may be marked as either an <strong>accepting</strong> HALT, or a <strong>rejecting</strong> HALT.</li>
<li>To <strong>accept</strong> an input word means the machine reaches an accepting HALT state. Similarly for <strong>rejection</strong>.</li>
<li>A <strong>decider</strong> for a language is a machine that accepts words in the language, and rejects words out of the language. It must halt on all inputs.</li>
</ul>
<p>It is known that counter machines are Turing-complete, in the sense that a universal Turing machine can be simulated by a counter machine with 2 registers. <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967</a>)</span> This implies in particular that any language that is decidable by a Turing machine is decidable by a counter machine.</p>
</section>
</section>
<section id="the-main-part" class="level2">
<h2 class="anchored" data-anchor-id="the-main-part">The main part</h2>
<section id="perfect-diffusion-is-in-tc0" class="level3">
<h3 class="anchored" data-anchor-id="perfect-diffusion-is-in-tc0">Perfect diffusion is in TC<sup>0</sup></h3>
<p>Suppose there exists a <span class="math inline">\(\mathsf{TC}^0\)</span> family of score-networks <span class="math inline">\(f_{\theta, 0}, f_{\theta, 1}, \dots\)</span>, such that for each <span class="math inline">\(n\)</span> and each <span class="math inline">\(x_1, \dots, x_n\)</span>, the function <span class="math inline">\(f_{\theta, n}(x, t | x_1, \dots, x_n)\)</span> exactly computes the score function of <em>some</em> initial distribution <span class="math inline">\(\rho_{0, n}\)</span> with bounded first moment: <span class="math inline">\(\mathbb{E}_{x_0 \sim \rho_{0, n}}[\|x_0\|] \leq 1\)</span>.</p>
<p>If this family solves a prefix language modeling problem at the limit of infinite time SMLD with constant probability bound, then the problem is in the <span class="math inline">\(\mathsf{TC}^0\)</span> class.</p>
<p>The idea of the proof is simple. We first quote an inequality from the literature, which provides a universal upper bound on how many steps are sufficient for sampling the SMLD within a constant probability bound, then we derandomize it while still remaining within the <span class="math inline">\(\mathsf{TC}^0\)</span> class.</p>
<p>The big idea of the derandomization is as follows: Given an algorithm that generates the correct token with probability that is of a bounded amount higher than generating any incorrect token, we can run the algorithm many times, and take the majority vote. By <a href="https://en.wikipedia.org/wiki/Hoeffding's_inequality">Hoeffding’s inequality</a>, there exists some random seed, for which the majority vote is correct on every single length-<span class="math inline">\(n\)</span> input. Now we hardcode that random seed.</p>
<p>The details of the proof (which is short) are in the paper. <span class="math inline">\(\blacksquare\)</span></p>
</section>
<section id="inexact-score-matching" class="level3">
<h3 class="anchored" data-anchor-id="inexact-score-matching">Inexact score-matching</h3>
<p>The requirement for <em>exact</em> score-matching is necessary for the following two reasons:</p>
<p>First, the technical reason is that the full form of the inequality we quoted is</p>
<p><span class="math display">\[
TV(\rho_{DDPM, T}, \rho_{\hat{x}, 0}) \leq c \frac{d (\log T)^3}{T} + c \epsilon_{\text{score}} \sqrt{\log T}.
\]</span></p>
<p>where the term <span class="math inline">\(\epsilon_{\text{score}}\)</span> denotes the score-matching error between the true score function of <span class="math inline">\(\rho_{\hat{x}, 0}\)</span> and the approximation <span class="math inline">\(f_\theta\)</span>. As this extra term <em>increases</em> with <span class="math inline">\(T\)</span>, the proof above does not apply. Perfect score-matching sets that term to zero, thus allowing the theorem to work.</p>
<p>Second, the intuitive reason is that if we have <em>no</em> requirement on score-matching, then there is essentially no constraint on the computational power of SMLD, by the construction in the next section.</p>
<p>Practically relevant score-networks are intermediate between two extreme cases. We believe that if <span class="math inline">\(f_\theta\)</span> is a good enough, but not perfect, score-matching network, then a generalized version of the above theorem still applies. However, finding the right way to <em>quantify</em> the goodness, as well as proving such a generalization, is left as future work.</p>
</section>
<section id="bad-diffusion-is-turing-complete" class="level3">
<h3 class="anchored" data-anchor-id="bad-diffusion-is-turing-complete">Bad diffusion is Turing-complete</h3>
<p>Since counter machines are Turing-complete, it suffices to show how to simulate any counter machine with diffusion. Suppose we are given a counter machine with <span class="math inline">\(k\)</span> registers, we simulate it by constructing a “pinball” machine that operates according to the SDE</p>
<p><span class="math display">\[
d\hat{x}_t = \frac{1}{2} \hat{x}_t dt + f_\theta(\hat{x}_t, t) dt + dW_t
\]</span></p>
<p>under a smooth force field <span class="math inline">\(f_\theta\)</span>. The pinball machine has a single ball, whose location is <span class="math inline">\(\hat{x}_t\)</span>. The ball rolls around a state space <span class="math inline">\(\mathbb{R}^d\)</span> guided by the force field <span class="math inline">\(f_\theta(\hat{x}_t, t)\)</span>. Indeed, the force field can be time-independent, so we write it as <span class="math inline">\(f_\theta(\hat{x}_t)\)</span> instead.</p>
<p>The state space is divided into three parts as <span class="math inline">\(\mathbb{R}^d = \mathbb{R}^k \times \mathbb{R} \times \mathbb{R}\)</span>. The first part <span class="math inline">\(\mathbb{R}^k\)</span> represents the <span class="math inline">\(k\)</span> registers. The second part <span class="math inline">\(\mathbb{R}\)</span> represents the program counter, which tracks the line-number of the program. The third part <span class="math inline">\(\mathbb{R}\)</span> is used for jumping between instructions, providing enough room for the ball to roll without “crossing the wires”.</p>
<p>The space is divided into cubic cells of side lengths <span class="math inline">\(L\)</span>. We denote each cell by <span class="math inline">\(k + 2\)</span> integers.</p>
<p>Like the state space, the force field has three parts too. One part simply cancels out the <span class="math inline">\(\frac{1}{2} x_t\)</span> term. Another part forms “grooves” along which the ball rolls, thus implementing the counter machine. The third part points towards the center-lines of the grooves, so that the ball is not knocked off the grooves by the noise term <span class="math inline">\(dW_t\)</span>. Of course, eventually the noise will knock the ball off the grooves, but if the force is strong enough, and the cubic cells have a large enough side length, then the machine will reach completion without being knocked off the grooves, with arbitrarily large probability.</p>
<p>Instead of formally specifying the grooves, it is simpler to give an example. Suppose at line number 32, the instruction reads “If the current state of register 1 is zero, then increment register 2 and jump to line 23, else jump to line 33”, then this is implemented by drawing the following paths:</p>
<ul>
<li><span class="math inline">\((0, r_2, \dots, r_d, 32, 0) \to (0, r_2 + 1, \dots, r_d, 32, 32) \to (0, r_2 + 1, \dots, r_d, 23, 32) \to (0, r_2 + 1, \dots, r_d, 23, 0)\)</span>.</li>
<li><span class="math inline">\((r_1, r_2, \dots, r_d, 32, 0) \to (0, r_2, \dots, r_d, 32, 32) \to (0, r_2, \dots, r_d, 33, 32) \to (0, r_2, \dots, r_d, 33, 0)\)</span> for nonzero <span class="math inline">\(r_1\)</span>.</li>
</ul>
<p>By smoothing the corners of the paths, we obtain a smooth force field.</p>
<div class="callout callout-style-default callout-note callout-titled" title="How to keep the ball on grooves">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How to keep the ball on grooves
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>How strong must the force be to keep the ball rolling on the grooves? The noise term <span class="math inline">\(dW_t\)</span> would, over a long enough time, eventually knock the ball off the grooves. This can be suppressed by either using a strong confinement force field, or by using a weak confinement force field but a large cubic cell side length <span class="math inline">\(L\)</span>. It turns out that we don’t need a very strong force or a very large cell.</p>
<p>Let the counter machine have <span class="math inline">\(N\)</span> instructions. Suppose it halts within <span class="math inline">\(S\)</span> steps, then the total distance travelled by the pinball would be <span class="math inline">\(O(NSL)\)</span>, where we need to account for the time necessary to jump between instructions. Then, since the rate of leakage is on the order of <span class="math inline">\(e^{-L^2}\)</span>, we need only require <span class="math inline">\(L \geq O(\sqrt{\ln (NSL)})\)</span> to suppress the probability of leakage during the entire computation to a small constant. In particular, for any fixed <span class="math inline">\(N, S\)</span>, because <span class="math inline">\(L\)</span> grows faster than <span class="math inline">\(\sqrt{\ln (NSL)}\)</span>, there exists a big enough <span class="math inline">\(L\)</span> for which the machine will halt without leakage, for probability as close to <span class="math inline">\(1\)</span> as one desires. This machine operates under a force field that is smooth, and has Lipschitz-continuity bounded by universal constant.</p>
<p>Suppose that we have a language that is decidable by a Turing machine when it is restricted to a working tape with length <span class="math inline">\(O(f(n))\)</span>, where <span class="math inline">\(n\)</span> is the input length, and <span class="math inline">\(f\)</span> is some monotonically increasing function, then by <span class="citation" data-cites="fischerCounterMachinesCounter1968">(<a href="#ref-fischerCounterMachinesCounter1968" role="doc-biblioref">Fischer, Meyer, and Rosenberg 1968</a>, Theorems 3.1 and 3.2)</span>, it is decidable by a counter machine that takes <span class="math inline">\(e^{O(f(n))}\)</span> steps to halt. Thus, it suffices when <span class="math inline">\(L \geq O(\sqrt{f(n)})\)</span>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="future-work" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="future-work">Future work</h2>
<section id="theoretical" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="theoretical">Theoretical</h3>
<p>We have shown that perfect diffusion models with exact score matching are constrained to <span class="math inline">\(\mathsf{TC}^0\)</span>, while deliberately “bad” diffusion models can be Turing-complete. The more realistic intermediate case remains open, where the score network approximately computes the score function.</p>
<p>We conjecture that similar computational limitations apply when the approximation quality is sufficiently high, but formalizing this notion of “sufficiently good approximation” and proving the corresponding result requires further work. We make this conjecture based on two reasons. One, the aforementioned empirical observation that diffusion models converge rapidly. Two, because the forward diffusion converges exponentially rapidly to the standard normal distribution <span class="math inline">\(\mathcal N(0, I)\)</span>, we believe that the backward diffusion process, as long as it is sufficiently close to the score function of a forward diffusion process, would be forced to converge in <span class="math inline">\(O(1)\)</span> time, since exponential decay is fast decay.</p>
<p>Our analysis focuses on diffusion models operating on <span class="math inline">\(\mathbb{R}^d\)</span> with subsequent discretization. However, other formulations of discrete diffusion exist, such as the directly discrete approach in <span class="citation" data-cites="austinStructuredDenoisingDiffusion2021">(<a href="#ref-austinStructuredDenoisingDiffusion2021" role="doc-biblioref">Austin et al. 2021</a>)</span>. For these models, we conjecture that <span class="math inline">\(\mathsf{TC}^0\)</span> limitations apply regardless of score network quality, as the finite state space inherently constrains the “computational capacity” of the diffusion process. Intuitively, a finite state space allows encoding only a finite number of bits per state before the signal-to-noise ratio<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> is exhausted, and the reverse diffusion reaches <span class="math inline">\(t=0\)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;See <span class="citation" data-cites="kingmaVariationalDiffusionModels2021">(<a href="#ref-kingmaVariationalDiffusionModels2021" role="doc-biblioref">Kingma et al. 2021</a>)</span> for a formalization of signal-to-noise in diffusion modeling.</p></div></div><p>Between finite state spaces and <span class="math inline">\(\mathbb{R}^d\)</span> lies the intermediate case of continuous but compact state spaces, such as the unit ball in <span class="math inline">\(\mathbb{R}^d\)</span>. While our “pinball machine” construction would still work in such spaces, it would require dividing the compact space into an increasing number of cells. This means the force field, while smooth, cannot maintain bounded Lipschitz-continuity coefficients. Because of this, we hypothesize that under the additional requirement of <span class="math inline">\(O(1)\)</span> Lipschitz-continuity, diffusion models on compact spaces would be constrained to <span class="math inline">\(\mathsf{TC}^0\)</span> regardless of score network quality, effectively making them computationally equivalent to finite state models.</p>
</section>
<section id="empirical" class="level3">
<h3 class="anchored" data-anchor-id="empirical">Empirical</h3>
<p>The animating big-picture idea behind this paper is that certain tasks are inherently sequential, such that any parallel computation that takes too little sequential steps must necessarily err. Sequential processing and consequences of its lack has been systematically studied for Transformers under the name of “chain of thought”, but not for diffusion models. We have collected a few suggestive examples gleaned from the literature, but it would be a valuable contribution to the literature to test this hypothesis systematically on diffusion models. We conjecture:</p>
<ul>
<li>Tasks requiring deep sequential reasoning should exhibit a sharp performance cliff when addressed by diffusion models with a fixed number of denoising steps.</li>
<li>Adding more denoising steps beyond a certain threshold should yield minimal improvements for <span class="math inline">\(\mathsf{TC}^0\)</span> tasks but continued improvements for tasks outside this complexity class.</li>
<li>Performance on complex sequential tasks should improve significantly when introducing autoregressive components, as seen in <span class="citation" data-cites="arriolaBlockDiffusionInterpolating2025">(<a href="#ref-arriolaBlockDiffusionInterpolating2025" role="doc-biblioref">Arriola et al. 2025</a>)</span>.</li>
</ul>
<p>Controlled experiments testing these predictions would provide valuable empirical validation of our theoretical framework and guidance for the further development of diffusion models.</p>
</section>
<section id="architectural" class="level3">
<h3 class="anchored" data-anchor-id="architectural">Architectural</h3>
<p>The most promising direction may be architectures that interpolates sequential and parallel computation dynamically, shifting to the sequential mode for tasks that demand them. We point out several particularly worthy directions for interpolation:</p>
<ul>
<li>In architecture, interpolation between massively parallel models (Transformers, state-space models) and sequential ones (recurrent neural networks).</li>
<li>For language generation, interpolation between full-sequence generation (typical of diffusion language models) and autoregressive generation (common in Transformer-based models). While both approaches have been studied extensively in isolation, their combination remains relatively unexplored.</li>
<li>Interpolation between SMLD and <a href="https://en.wikipedia.org/wiki/Neural_differential_equation">neural ODE</a> frameworks. SMLD offers rapid convergence through massive parallelism, while neural ODEs provide slower convergence with more sequential computation.</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I don’t read conclusions in papers, and I don’t write them either. They seem like a stupid convention, when having an abstract and an introduction is not enough to pad your page numbers, so you had to rephrase them, differently but the same… In fact, the only reason there is a conclusion in the arXiv paper is because academic style requires there to be one, which is why I got Claude 3.7 to write it… without any modification!</p>


<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-andersonReversetimeDiffusionEquation1982" class="csl-entry" role="listitem">
Anderson, Brian DO. 1982. <span>“Reverse-Time Diffusion Equation Models.”</span> <em>Stochastic Processes and Their Applications</em> 12 (3): 313–26. <a href="https://www.sciencedirect.com/science/article/pii/0304414982900515">https://www.sciencedirect.com/science/article/pii/0304414982900515</a>.
</div>
<div id="ref-arriolaBlockDiffusionInterpolating2025" class="csl-entry" role="listitem">
Arriola, Marianne, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. 2025. <span>“Block <span>Diffusion</span>: <span>Interpolating Between Autoregressive</span> and <span>Diffusion Language Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2503.09573">https://doi.org/10.48550/arXiv.2503.09573</a>.
</div>
<div id="ref-austinStructuredDenoisingDiffusion2021" class="csl-entry" role="listitem">
Austin, Jacob, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021. <span>“Structured <span>Denoising Diffusion Models</span> in <span>Discrete State-Spaces</span>.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>, 34:17981–93. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html</a>.
</div>
<div id="ref-fengRevealingMysteryChain2023" class="csl-entry" role="listitem">
Feng, Guhao, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2023. <span>“Towards Revealing the Mystery Behind Chain of Thought: A Theoretical Perspective.”</span> <em>Advances in Neural Information Processing Systems</em> 36: 70757–98. <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html">https://proceedings.neurips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html</a>.
</div>
<div id="ref-fischerCounterMachinesCounter1968" class="csl-entry" role="listitem">
Fischer, Patrick C., Albert R. Meyer, and Arnold L. Rosenberg. 1968. <span>“Counter Machines and Counter Languages.”</span> <em>Mathematical Systems Theory</em> 2 (3): 265–83. <a href="https://doi.org/10.1007/BF01694011">https://doi.org/10.1007/BF01694011</a>.
</div>
<div id="ref-kingmaVariationalDiffusionModels2021" class="csl-entry" role="listitem">
Kingma, Diederik, Tim Salimans, Ben Poole, and Jonathan Ho. 2021. <span>“Variational Diffusion Models.”</span> <em>Advances in Neural Information Processing Systems</em> 34: 21696–707. <a href="https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html</a>.
</div>
<div id="ref-leisersonTheresPlentyRoom2020" class="csl-entry" role="listitem">
Leiserson, Charles E., Neil C. Thompson, Joel S. Emer, Bradley C. Kuszmaul, Butler W. Lampson, Daniel Sanchez, and Tao B. Schardl. 2020. <span>“There’s Plenty of Room at the <span>Top</span>: <span>What</span> Will Drive Computer Performance After <span>Moore</span>’s Law?”</span> <em>Science</em> 368 (6495): eaam9744. <a href="https://doi.org/10.1126/science.aam9744">https://doi.org/10.1126/science.aam9744</a>.
</div>
<div id="ref-liuTransformersLearnShortcuts2023" class="csl-entry" role="listitem">
Liu, Bingbin, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. <span>“Transformers <span>Learn Shortcuts</span> to <span>Automata</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2210.10749">https://doi.org/10.48550/arXiv.2210.10749</a>.
</div>
<div id="ref-luoUnderstandingDiffusionModels2022" class="csl-entry" role="listitem">
Luo, Calvin. 2022. <span>“Understanding Diffusion Models: <span>A</span> Unified Perspective.”</span> <em>arXiv Preprint arXiv:2208.11970</em>. <a href="https://papers.baulab.info/papers/also/Luo-2022.pdf">https://papers.baulab.info/papers/also/Luo-2022.pdf</a>.
</div>
<div id="ref-merrillIllusionStateStateSpace2025" class="csl-entry" role="listitem">
Merrill, William, Jackson Petty, and Ashish Sabharwal. 2025. <span>“The <span>Illusion</span> of <span>State</span> in <span>State-Space Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2404.08819">https://doi.org/10.48550/arXiv.2404.08819</a>.
</div>
<div id="ref-minskyComputationFiniteInfinite1967" class="csl-entry" role="listitem">
Minsky, Marvin. 1967. <em>Computation: Finite and Infinite Machines</em>. Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-parberryParallelComputationThreshold1988" class="csl-entry" role="listitem">
Parberry, Ian, and Georg Schnitger. 1988. <span>“Parallel Computation with Threshold Functions.”</span> <em>Journal of Computer and System Sciences</em> 36 (3): 278–302. <a href="https://www.sciencedirect.com/science/article/pii/002200008890030X">https://www.sciencedirect.com/science/article/pii/002200008890030X</a>.
</div>
<div id="ref-ravishankarScalingPropertiesDiffusion2024" class="csl-entry" role="listitem">
Ravishankar, Rahul, Zeeshan Patel, Jathushan Rajasegaran, and Jitendra Malik. 2024. <span>“Scaling <span>Properties</span> of <span>Diffusion Models</span> for <span>Perceptual Tasks</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2411.08034">https://doi.org/10.48550/arXiv.2411.08034</a>.
</div>
<div id="ref-wewerSpatialReasoningDenoising2025" class="csl-entry" role="listitem">
Wewer, Christopher, Bart Pogodzinski, Bernt Schiele, and Jan Eric Lenssen. 2025. <span>“Spatial <span>Reasoning</span> with <span>Denoising Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.21075">https://doi.org/10.48550/arXiv.2502.21075</a>.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Perfect diffusion is TC&lt;sup&gt;0&lt;/sup&gt; -- Bad diffusion is Turing-complete"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-04-17"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2025-04-17"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [math, fun]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    resources:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        "figure/**"</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "An application of computational complexity theory to diffusion language models. Unlikely to be useful for anything, but it is cute!"</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner.png"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "likely"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 1</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">small</span><span class="dt">&gt;</span>This essay has a <span class="co">[</span><span class="ot">companion paper</span><span class="co">](figure/paper.pdf)</span> on arXiv.<span class="dt">&lt;/</span><span class="kw">small</span><span class="dt">&gt;</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">## Idea</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">### Width and depth</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>Some problems take a lot of work to solve. It is a lot of work to solve Riemann's Hypothesis. It is also a lot of work to solve a trillion grade-school arithmetic problems. However, they are not the same kind of hard work, even if the required amount of "work" may be the same. One is "deep" while another one is "wide". Recall the old joke about how 1000 people cannot dig a hole in 1 second.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>We say that a computation is "wide" if it has many steps, but the steps can take place in any order. In particular, they can be done concurrently. It can be finished faster either by using a processor with a faster clock, or more processors. We say that a computation is "deep" (or perhaps "inherently sequential") if it has many steps that must take place one after another. Using more processors would not help. Only using a processor with a faster clock would help.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>Language modeling research has shown that autoregressive models need "chain of thought" when solving certain reasoning tasks. This benefit isn't just observed in practice, but has theoretical foundations. <span class="co">[</span><span class="ot">@fengRevealingMysteryChain2023</span><span class="co">]</span> In the big picture, both theory and practice has shown that chain of thought approaches benefit precisely in allowing a variable amount of sequential processing, which is necessary for solving problems that need deeper computation than one forward pass allows. Trying to do it with less leads to errors, guesswork, or inefficient memorization in a desperate attempt to solve the problem "out of the model's depth".</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>As an example, if your program solve both easy and difficult Sudoku puzzles in exactly the same number of steps, you might reasonably question whether your method actually works for the hard puzzles. At one end of the limit, an easy Sudoku puzzle could be solved in a few steps by filling in each blank in parallel, since each blank could be solved by checking its row, column, and square. There is no sequential dependence between the blanks. At the other end of the limit, a hard Sudoku puzzle would involve a large amount of dependence between many blanks, which would require deep tree searches and significant backtracking to solve. It would be unreasonable to expect a parallel algorithm to solve a hard Sudoku puzzle, no matter the "width" of the parallelism, if it does not have the requisite "depth".</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="al">![An easy Sudoku puzzle can be solved in parallel. Here, the two circled blanks have no interdependence. Each can be filled simply by checking their own rows, columns, and squares.](figure/sudoku_easy.png)</span>{width=30%}</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="al">![Advanced Sudoku puzzles require advanced techniques to solve, which usually involve drawing interdependencies across the board. Illustrated here are "equivalent sets". In each image, the red-colored blanks must contain the same numbers as the blue-colored blanks. Expert Sudoku puzzlers memorize a large number of such patterns to cut down the possibilities.](figure/sudoku_hard.png)</span>{width=40%}</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### A brief history of scaling</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>The recent history of deep learning has been drunk-high on parallel computing. Pushed to the very limits of parallelism: 256, 2048, 8192. How many cores can you pack on a single chip? This came at the price of precision. FP64, FP32, INT8... How low can you go? There are even proposals of a E4M0 floating point number -- 4-bit float, 0-bit mantissa!</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>But it hasn't always like this.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>Fundamentally, this was not an unqualified success, but also a cope with lagging hardware. Since 2005, clock rate has been stuck at 4 GHz, placing a hard limit on how many serial steps you can make per wallclock time. GPUs are stuck at 1–2 GHz. This failure at the hardware level eventually propagated up the entire tech stack.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The clock rate has plateaued since 2005. Subsequent progress in computer processing speed relied mostly on parallelism, which here is shown in the form of multi-core performance. [@leisersonTheresPlentyRoom2020, figure 2]</span><span class="co">](figure/CPU_clock_rate_plateau.jpeg)</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>At the architecture level, we went from sequential models like RNNs to the highly parallelizable Transformer architecture. </span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>Some architectures are inherently more sequential than others. Recurrent neural networks (RNN) must process $n$ tokens in $n$ steps. So if a single step takes 1 ms, then processing 1000 tokens take 1 second. In contrast, a Transformer can take any number of tokens in 1 step, limited only by the size of your VRAM and processor count. As long as memory and processor count scaled, the stagnation in clock rate was not a binding constraint... Is it?</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>At the algorithm level, we went from reinforcement learning (RL) to imitation learning and pretraining. To do true RL, the network had to actually play out games one step after another. Imitation learning however is simply pretraining by another name, and can be scaled in parallel.</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>The success of pretraining over RL, 1M-batch over minibatch, Transformers over RNN, Nvidia over Intel, are all triumphs of the parallel over the serial, but perhaps we need to take account of it again? Might the pendulum swing back?</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> sequential <span class="pp">|</span> parallel <span class="pp">|</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> algorithm <span class="pp">|</span> RL <span class="pp">|</span> imitation learning, (self-)supervised (pre)training <span class="pp">|</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> architecture <span class="pp">|</span> RNN <span class="pp">|</span> CNN, Transformer, SSM <span class="pp">|</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> hardware <span class="pp">|</span> CPU <span class="pp">|</span> GPU <span class="pp">|</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>For language generation, the story is quite clear. Before 2016, the SOTA was LSTM, which must be trained sequentially. Starting around 2018, pretrained Transformers took the lead, for which training is as parallel as the hardware can bear. Yet around 2021, sequential compute returned to inference in the form of "chain of thought". With the DeepSeek-R1 of 2025, it is now quite probable that training will became a lot more sequential.</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>A similar story could be told for image generation. Before 2019, the SOTA was the U-net, which generates the whole image in parallel. In 2021, OpenAI shocked the world with DALL-E, a standard Transformer architecture that generated images patch-by-patch. But soon, diffusion models took over with their fully parallel generation across all pixels. And most recently, the image generator in OpenAI's ChatGPT seemed to have reverted back to generating images patch-by-patch. </span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>As for RL itself, the 2013--2017 period was its highlight, with RL agents becoming superhuman at Atari, Dota, and Go. Indeed, during the early days of OpenAI, it focused on RL, with the Dota bot being its first publicity success. However, things changed starting with GPT-1 of 2018, the generatively pretrained Transformer. The so-called "RL from human feedback" (RLHF) is only *barely* RL, since each "game" in an RLHF training run consists of a *single* reply! It is as if playing a game that ends after making exactly one move.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>If the best kind of training shifts back to be more serial than parallel, then there will be a reckoning. If a few months down the line, we see a new scaling law emerge from the big labs pushing the R1-Zero training method to even longer chains at even smaller batches, then deep learning can suddenly acquire a very serial RL phase. Pretraining would still need giant GPU clusters, but the RL step would suddenly look more like Cray supercomputers -- a few giant CPUs, immersed in Fluorinert, running at 100 GHz. Compared to Nvidia GPUs, those would be much more expensive in terms of FLOP/USD, but in terms of serial-FLOP/sec/USD, cheap!</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Seymour Cray was famous for packing, powering, and cooling circuits incredibly densely. Classic Crays were made obsolete by microprocessors, but we may yet do similar things at a larger scale. Hyperscale data centers and even national supercomputers are loosely coupled things today, but if challenges demanded it, there is a world with a zetta scale, tightly integrated, low latency matrix dissipating a gigawatt in a swimming pool of circulating fluorinert.</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- </span><span class="co">[</span><span class="ot">John Carmack (2020-08-30)</span><span class="co">](https://x.com/ID_AA_Carmack/status/1300280139071270914)</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>In this scenario, an RNN-Transformer hybrid might become the common architecture for cutting-edge training, in two phases. In the first phase, it would be pretrained in parallel over ~1 million GPUs with ~1 million batch size. In the second phase, it would be reinforcement-learning with a minibatch of rollouts per update, on special hardware that maximizes the update frequency. This future hardware could be a small cluster of ~10 massive RL-specific chips with diameter ~1 meter packaged closely to maximize memory bandwidth, immersed in liquid nitrogen cooling to run at ~100 GHz without melting. Such hardware does not exist today, since it is highly uneconomical compared to existing GPUs, as measured in FLOP/sec. However, in a scenario where a long sequence of serial training is necessary, such hardware may be economical as measured in serial FLOP/sec.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Why these numbers?" collapse="true" }</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>An RL-specific chip may be ~1 meter in diameter, because Cerebras already produces chips that are ~20 cm on each side, giving a lower bound. On the other direction, a silicon wafer is ~50 cm in diameter, giving an upper bound.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>The GPU and batch size are both in the millions, because those numbers are used for the current (2025) largest pretraining runs.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>AS for the ~100 GHz clock rate, we have the following data:</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>As of 2025, <span class="co">[</span><span class="ot">hobbyist overclockers have already reached 9.1 GHz</span><span class="co">](https://skatterbencher.com/cpu-overclocking-world-record-history/)</span> using just a commercially available Intel CPU, cooled by liquid nitrogen, so I believe another factor of 10 is within the technical possibilities.</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In <span class="co">[</span><span class="ot">2000, Intel planned for a CPU at 10 GHz for 2005</span><span class="co">](https://www.anandtech.com/show/680/6)</span>. The plan fell through, and in 2004, Intel hastily cancelled the originally planned-for next-generation chips (<span class="co">[</span><span class="ot">Tejas and Jayhawk</span><span class="co">](https://en.wikipedia.org/wiki/Tejas_and_Jayhawk)</span>) due to intractable overheating. However, it shows that 10 GHz is well within the limits of known silicon technology. The heat dissipation issues are certainly not a hard technological limit, but merely too uneconomical in a consumer product.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>Though currently uneconomical, it is entirely possible for such technology to be resurrected for cutting-edge AI training. Compared to the currently planned million-GPU datacenters, such a special-purpose CPU cluster would be comparatively affordable.</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="fu">### Actually existing complexity</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>Like axiomatic logicians, computational complexity theorists can occasionally be defensive, since they are often haunted by the fear of irrelevance. This is not an idle fear, especially in the context of AI research. It is healthy to try to not become like Minsky and Papert who, having exhaustively demonstrated that 1-layered neural networks are not powerful, declared deeper neural networks to be <span class="co">[</span><span class="ot">a "sterile extension" of 1-layered neural networks, based on their intuition</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/reading-perceptron-book/index.html#chapter-13)</span>.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>In the best case, computational complexity theory can discover algorithms that are fast in practice, or prove that certain algorithms are impossible *in a way that matters*. In the less happy case, algorithms can be invented that are fast in theory, but not in practice, such as the "[galactic algorithms](https://en.wikipedia.org/wiki/Galactic_algorithm)". In the worst case, an impossibility proof is correct, yet wrong *in a way that matters*. How could this be the case? Every mathematical abstraction makes certain inaccuracies, and a lot can slip through the cracks between the real world and the mathematical realm.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>As a concrete example, consider protein folding. In theory, a protein is merely a floppy string of amino acids, and to fold it, there seems to be no way but to try out every one of its configurations in search of its minimal-energy form. However, even nature does not laugh at the difficulties of computation. If a protein is actually hard to fold, then it would be dangerous for the organism to produce such a thing, and thus it would be removed by evolution. In this way, the problem of "folding proteins that a biologist might wish to fold in practice" turns out to be an easier problem than "folding proteins". This perhaps explains the success of AlphaFold, which can fold a protein in a few forward passes. Surely it is not solving a hard problem with an impossibly small amount of compute. Rather, what appears to be a hard problem is not really hard, and what went wrong was the original act of abstraction that turned protein folding in practice into an unrealistically hard problem in theory.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>So where is this paper placed? We place it into the category of "half-irrelevant for practice". On the one side, the "perfect" diffusion is a vanishing point that can never be reached in practice. On the other side, the Turing-complete bad diffusion is a construction that has given up any pretense of being about diffusion models. It is a deliberate <span class="co">[</span><span class="ot">arbitrary code execution</span><span class="co">](https://en.wikipedia.org/wiki/Arbitrary_code_execution)</span> exploit of the standard diffusion framework. Indeed, we believe that any attempt to train a score network in practice would result in a score network that is quite tame, and definitely incapable of arbitrary code execution.</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>So why this paper? One, because it is fun. Two, because some of the intuition might be helpful for understanding actually existing diffusion models. It is in this sense that we say this paper is "half-irrelevant". Real diffusion models are not perfect, but also not malicious, and they seem to behave closer to perfection than malice. Because of this, we conjecture that real diffusion models *do* converge fast, in $O(1)$ steps. Assuming this is the case, then using them for solving tasks that require more than $O(1)$ serial steps is impossible.</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusion models</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>A diffusion model can be used to sample from a distribution with a variable amount of computing steps. This is usually understood as an advantage, in the sense of providing a compute-precision tradeoff: With a few steps, one can sample from the distribution approximately, and with increasing number of steps, the distribution can be sampled from with increasing precision. However, this intuitive picture also suggest that this advantage may be a curse. Specifically, it suggests that after a few sampling steps, further computation is "wasted" in the sense that they refine the result in a way that does not matter, because the result has converged.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>Indeed, empirically, diffusion models typically converge rapidly with a fixed number of denoising steps regardless of input. For example, <span class="co">[</span><span class="ot">@ravishankarScalingPropertiesDiffusion2024</span><span class="co">]</span> studied using diffusion models for depth-perception, and showed that there is no difference between 5 and 100 sampling steps, Similarly, <span class="co">[</span><span class="ot">@austinStructuredDenoisingDiffusion2021</span><span class="co">]</span> showed that for a diffusion language model they trained, the perplexity of language modeling was essentially the same for 10 and 1024 diffusion steps.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@ravishankarScalingPropertiesDiffusion2024, figure 8</span><span class="co">]</span>](figure/ravishankar_2024_fig_8.png)</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">@austinStructuredDenoisingDiffusion2021, figure 2</span><span class="co">]</span>](figure/austin_2021_fig_2.png){width=50%}</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>While such rapid convergence may be regarded as an advantage, this would not be when the problem is deep. Converging on a solution faster than the depth of the problem requires would likely lead to failures. Indeed, certain kinds of empirical failures of diffusion language models suggest that they struggle precisely on tasks that require sequential processing. For instance, when using diffusion models to solve Sudoku puzzles, <span class="co">[</span><span class="ot">@wewerSpatialReasoningDenoising2025</span><span class="co">]</span> found that denoising all digits simultaneously worked for easy puzzles but failed for difficult ones. Performance improved only when denoising fewer digits at a time, with optimal results achieved by denoising just one digit at a time, essentially reverting to a purely sequential process.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Solving Sudoku with a diffusion model. If the entire grid is denoised all at once, the model would nearly always fail on the hard puzzles. If the grid is denoised one number at a time, the model would sometimes succeed on the hard puzzles. [@wewerSpatialReasoningDenoising2025]</span><span class="co">](figure/diffusion_sudoku.png)</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">@arriolaBlockDiffusionInterpolating2025</span><span class="co">]</span> noted that discrete diffusion models underperform compared to autoregressive approaches. They proposed to solve this by... reintroducing autoregressive generation, applying diffusion to generate a few tokens at a time.</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The "block diffusion" language modeling method, which despite the hopeful captioning, is only 4 times as "parallelizable" as autoregressive language modeling. [@arriolaBlockDiffusionInterpolating2025, figure 1]</span><span class="co">](figure/block_diffusion.png)</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setting up the framework</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>The theory part of the paper has three parts:</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A mathematical framework in which these empirical observations kind of exists in kind of the same way. We make no promises that this framework actually has anything to do with practice, but it looks fun.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A theorem showing that a "perfect" diffusion models are constrained to the $\mathsf{TC}^0$ complexity class due to their rapid convergence.</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>An explicit construction of a "bad" diffusion models to perform any Turing-computable operation. The construction works precisely because they do not converge quickly. They do not converge quickly precisely because they fail at reversing any forward diffusion process.</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>We begin by setting up the framework.</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusion modeling</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>Diffusion models work by gradually adding noise to data and then learning to reverse this process. Think of it as watching a drop of ink spread through water, and then learning to recover the original drop from the diluted state. This intuitive physical analogy connects to their mathematical foundation, which borrows concepts from thermodynamics and statistical physics.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>In machine learning literature, two main formulations have emerged to describe this process. The first, **Denoising Diffusion Probabilistic Models (DDPM)**, approaches the problem through discrete time steps. The second, **score-matching with Langevin dynamics (SMLD)**, uses continuous differential equations. Despite their different origins, these two approaches are fundamentally equivalent <span class="co">[</span><span class="ot">@kingmaVariationalDiffusionModels2021; @luoUnderstandingDiffusionModels2022</span><span class="co">]</span>.</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>The connection works in both directions. DDPM can be seen as a discretized version of SMLD, where each DDPM update step corresponds to using the Euler–Maruyama method to solve SMLD's stochastic differential equation (SDE). Conversely, if we take the limit of infinitely many DDPM steps with infinitesimally small noise additions, we recover the continuous SDE formulation of SMLD. This equivalence means that models trained using either framework can be used interchangeably for sampling purposes.</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>For clarity and mathematical convenience, we primarily use the SMLD formulation throughout this paper, though our results apply equally to both frameworks, since they are equivalent.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>Consider a **data distribution** $\rho_{data}$ over the real space $\mathbb{R}^d$. The task of SMLD is to learn a **score-matching function** $f_\theta$ that allows us to sample from $\rho_{data}$.</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>A **noise schedule** is a continuous function $\beta$ of type $[0, \infty) \to [0, \infty)$, such that $\beta(t)$ can be interpreted as the noising rate in the forward diffusion process at time $t$. We require $\int_0^\infty \beta(t) dt = \infty$, which can be interpreted as saying that eventually all signal is destroyed, leaving only noise. </span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>Define the distribution at $t=0$ by $\rho_0 := \rho_{data}$. Suppose we sample a data point $x_0 \sim \rho_0$, and let it evolve according to the SDE</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>dx_t = -\frac{1}{2} \beta(t) x_t dt + \sqrt{\beta(t)} dW_t</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>then this implies a time-evolution of the data distribution over time, which can be directly solved by the Fokker-Planck equation:</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>\partial_t \rho_t = \frac{1}{2} \beta(t) (\nabla \cdot (x \rho_t) + \Delta \rho_t)</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>At the $t \to \infty$ limit, the distribution converges to the standard normal distribution $\mathcal{N}(0, I_d)$.</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>For any time $T &gt; 0$, the time-evolution can be exactly reversed as follows. Let $\hat{x}_T$ be sampled according to $\rho_{\hat{x}, 0} := \rho_T$, then the following SDE equation would lead to an exact reversal:</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>d\hat{x}_t = \frac{1}{2} \beta(T-t) \hat{x}_{t} dt + \beta(T-t) \underbrace{\nabla_{\hat{x}_{t}} \ln \rho_{T-t}(\hat{x}_{t})}_{\text{score function}} dt + \sqrt{\beta(T-t)} dW_t</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>where by "reversal" we mean that $\rho_{\hat{x}, t} = \rho_{T-t}$ for any $t \in <span class="co">[</span><span class="ot">0, T</span><span class="co">]</span>$ <span class="co">[</span><span class="ot">@andersonReversetimeDiffusionEquation1982</span><span class="co">]</span>.</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>Assuming that a score-matching function $f_\theta$ has been trained, such that</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>f_\theta(x, t) \approx \nabla_x \ln \rho_t(x)</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>for all $t, x$, then $\rho_{data}$ can be approximately sampled by initializing a pure-noise sample $\hat{x}_T \sim \mathcal{N}(0, I_d)$, then solving the backward SDE</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>\hat{x}_{t-dt} = \frac{1}{2} \beta(t) \hat{x}_t dt + \beta(t) f_\theta(\hat{x}_t, t) dt + \sqrt{\beta(t)} dW_t</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>by any SDE integration method, such as Euler–Maruyama method. By varying the sizes of the $dt$ steps in the Euler–Maruyama method, we can recover different noise-schedules for DDPM.</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>If $f_\theta(x, t) = \nabla_x \ln \rho_t(x)$ is exact, then at the limit of $T \to \infty$ and infinitely many steps in the Euler–Maruyama method, we can exactly sample from $\rho_{data}$.</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="fu">### Circuit complexity theory</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>A <span class="co">[</span><span class="ot">circuit complexity</span><span class="co">](https://en.wikipedia.org/wiki/Circuit_complexity)</span> class is a style computational complexity classes. In our paper, we focus on the $\mathsf{TC}^0$ class, which is particularly suited to studying the computational complexity of neural networks, because a family of feedforward neural networks with a constant number of layers is essentially a $\mathsf{TC}^0$ circuit family. Indeed, the class of $\mathsf{TC}^0$ were first proposed specifically in the 1980s to model the computational complexity of neural networks. <span class="co">[</span><span class="ot">@parberryParallelComputationThreshold1988</span><span class="co">]</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>Formally, $\mathsf{TC}^0$ is defined as the class of problems that can be decided by a family of boolean circuits with the following properties:</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Boolean circuits:** A boolean circuit is a directed acyclic graph where each node (or gate) computes a boolean function of its inputs. The inputs to the circuit are boolean variables, and the output is a single boolean value.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Unbounded fan-in:** Each gate in the circuit can receive inputs from an arbitrary number of other gates. This contrasts with bounded fan-in circuits where gates have a limited number of inputs. Convolutional neurons have bounded fan-in, but fully-connected neurons have unbounded fan-in.</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Polynomial width:** The number of gates at each level of the circuit is bounded by a polynomial in the input size $n$.</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Constant depth:** The longest path from any input to the output is bounded by a constant that does not depend on the input size. This may be interpreted as stating the circuit family is "highly parallelizable".</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Threshold gates:** A threshold gate is a binary neuron. It can be written as a function $\theta(\sum_i w_i x_i + t)$, where $w_i, t$ are real numbers, and $\theta$ is the binary step-function</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>For those unfamiliar, here is a short exercise: </span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>With 1 layer of threshold gates, construct "gadgets" such as the AND gate, the NOT gate, and all other common boolean gates.</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>With 2 layers, construct the $k$-EQUALS gate for each $k$, which outputs 1 if exactly $k$ inputs are 1, and 0 otherwise.</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>With 3 layers, construct the IS-IN gate for any finite subset of $\N$.</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>From the definition, it is clear that each member $\mathsf{TC}^0$ circuit family is essentially a feedforward neural network. However, this only consists of a single member. Here is where the "family" part of the definition becomes important.</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>Since a neural network has a fixed number of inputs, it would be unable to process more inputs than the number of neurons in its input. This brings the idea of a circuit *family*. A circuit family is a set of circuits $C_1, C_2, \dots$ such that each $C_n$ is capable of processing exactly inputs of length $n$. Computational complexity theory studies not the complexity of problems solvable by a single circuit, but a circuit family, because any single circuit is merely equivalent to a lookup table, and the complexity of the problem it solves is always trivial. If this seems odd to you, remember that to a computational complexity theorist, 1 and 1 trillion are the same -- both are $O(1)$.</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>Consequently, a $\mathsf{TC}^0$ family of feedforward neural networks is defined as a set of neural networks $C_n$, such that there exists a constant $D$ (the upper bound on depth), and a polynomial $p$ (the polynomial bound on width), such that each $C_n$ has depth $\leq D$ and number of neurons $\leq p(n)$.</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>While the $\mathsf{TC}^0$ class is most similar to feedforward fully-connected neural networks, this is not necessarily the case. Indeed, a family of bounded-depth polynomial-width Transformers is still in the $\mathsf{TC}^0$ class. This means the theorem in the paper applies to them as well.</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="fu">### Language modeling</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>At the most abstract level, a language is simply a set of words made of letters. Formally:</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>An **alphabet** $\Sigma$ is a finite nonempty set. Each element in the alphabet may be called a **letter** or a **token**.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A **word** in an alphabet $\Sigma$ is a finite sequence of elements of $\Sigma$.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A **language** $L$ in an alphabet $\Sigma$ is a set of words in the alphabet $\Sigma$.</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>A **prefix language modeling problem** is, given a sequence of tokens $x_1, \dots, x_n$, to compute the next token $x_{n+1}$. An example would be the word problem for finite groups: Given a finite group $G$, and a sequence of elements in the group $g_1, \dots, g_n$, compute $\prod_{i=1}^n g_i$. In particular, if $G$ is the permutation group on 5 elements, then the corresponding group multiplication problem is strongly suspected to lie outside $\mathsf{TC}^0$ class.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>An example would be the word problem for finite groups: Given a finite group $G$, and a sequence of elements in the group $g_1, \dots, g_n$, compute $\prod_{i=1}^n g_i$. Intuitively, there is a method that computes this in $\log_2(n)$ parallel steps by binary multiplication: the first parallel step computes $g_1g_2, g_3g_4, \dots$, and so on. Since $\log_2(n)$ is not constant, this would not lie within the $\mathsf{TC}^0$ class. For certain groups, there are shortcuts to this process. For example, for any prime number $p$, the word problem in the mod-$p$ multiplicative group is computable in constant number of parallel steps via Fermat's little theorem. However, shortcuts probably do not exist in general. Indeed, if $G$ is the permutation group on 5 elements, then the corresponding word problem is not in the $\mathsf{TC}^0$ class, assuming widely believed conjectures in computational complexity theory. <span class="co">[</span><span class="ot">@liuTransformersLearnShortcuts2023</span><span class="co">]</span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>While usually, a diffusion model is used for generating from a continuous state space such as $\mathbb{R}^d$, it can be used to model discrete distributions as well. This is necessary for language modeling. We consider the case closest to continuous state space modeling -- quantization: One divides the continuous state space $\mathbb{R}^d$ into regions, and assigns a token to each region. This then allows sampling a discrete distribution from a diffusion model with continuous state space. Formally, if $\Sigma = <span class="sc">\{</span>a_1, a_2, \dots, a_M<span class="sc">\}</span>$ is the alphabet, then we divide $\mathbb{R}^d$ into $M$ regions $V_1, \dots, V_M$, such that each region $V_i$ maps to a token $a_i$.</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>Also, as usual in circuit complexity theory, we need more than a single score-network $f_\theta$, but rather, a full sequence of them, so we define a **$\mathsf{TC**^0$ family of score-networks} to be a family of feed-forward neural networks $f_{\theta, 0}, f_{\theta, 1}, \dots$ , such that:</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each $f_{\theta, n}$ takes as input $n+2$ elements $x_1, \dots, x_n, x, t$, and produces an output $f_{\theta, n}(x, t | x_1, \dots, x_n)$.</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The family $f_{\theta, n}$ has $O(1)$ depth and $\mathsf{poly}(n)$ width.</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>Note that for the theorem to hold, it is not necessary to assume the family of neural networks are feed-forward. The theorem holds for any family of score-networks for which a single forward pass is in $\mathsf{TC}^0$. This includes, for example, Transformers and state-space models <span class="co">[</span><span class="ot">@merrillIllusionStateStateSpace2025; @merrillIllusionStateStateSpace2025</span><span class="co">]</span>. We stay with feedforward networks because it is visually obvious how they are in the $\mathsf{TC}^0$ class.</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>Finally, since a diffusion model may solve a problem only with high enough probability, instead of solving it deterministically, we make the following definition: A prefix language modeling problem is **solved with constant probability bound** if there exists some $\epsilon &gt; 0$, such that for each input token sequence $x_1, \dots, x_n$, let $x_{correct}$ be the correct response, then </span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>p(x_{correct}|x_1, \dots, x_n) &gt; p(x'|x_1, \dots, x_n) + \epsilon, \quad \forall x' \neq x_{correct}.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a><span class="fu">### Counter machines</span></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>To show that a deliberately bad diffusion model may be Turing-complete, we show how they could simulate a particular kind of Turing-complete abstract machines: the counter machines. This is not necessary for understanding the theorem on "perfect" diffusion models.</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>A **counter machine** can be thought of as **finite-state automata** augmented with memories, each of which can hold a single unbounded integer. In our paper, we use the following form of counter machine, lightly modified from <span class="co">[</span><span class="ot">@fischerCounterMachinesCounter1968</span><span class="co">]</span>:</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The machine has access to a finite number $k$ of **registers**, notated as $r_1, \dots, r_k$. Each register stores a single integer.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The machine also has access to a read-only **input tape**, on which the machine has a read-head that can be moved in either direction. At machine start-up, the input tape has contents ^$a_1a_2\dots a_n$\$, where ^ and \$ denote the beginning and the end of the word, and $n$ is the length of the input word. The read-head is placed at the character just after ^, which may be \$ if the input word is empty.</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A **program** for the machine is a numbered list of instructions.</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each **instruction** is of the following format: conditional on the state of the read-head on the input tape and on whether each register is zero or not, modify every register by an amount in $<span class="sc">\{</span>-1, 0, +1<span class="sc">\}</span>$, move the read-head by up to one position in either direction, then jump to another instruction.</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>There is a special instruction named "HALT". If the machine arrives at such an instruction, it halts. Each HALT instruction may be marked as either an **accepting** HALT, or a **rejecting** HALT.</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To **accept** an input word means the machine reaches an accepting HALT state. Similarly for **rejection**.</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A **decider** for a language is a machine that accepts words in the language, and rejects words out of the language. It must halt on all inputs.</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>It is known that counter machines are Turing-complete, in the sense that a universal Turing machine can be simulated by a counter machine with 2 registers. <span class="co">[</span><span class="ot">@minskyComputationFiniteInfinite1967</span><span class="co">]</span> This implies in particular that any language that is decidable by a Turing machine is decidable by a counter machine.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a><span class="fu">## The main part</span></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Perfect diffusion is in TC&lt;sup&gt;0&lt;/sup&gt;</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>Suppose there exists a $\mathsf{TC}^0$ family of score-networks $f_{\theta, 0}, f_{\theta, 1}, \dots$, such that for each $n$ and each $x_1, \dots, x_n$, the function $f_{\theta, n}(x, t | x_1, \dots, x_n)$ exactly computes the score function of *some* initial distribution $\rho_{0, n}$ with bounded first moment: $\mathbb{E}_{x_0 \sim \rho_{0, n}}<span class="co">[</span><span class="ot">\|x_0\|</span><span class="co">]</span> \leq 1$.</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>If this family solves a prefix language modeling problem at the limit of infinite time SMLD with constant probability bound, then the problem is in the $\mathsf{TC}^0$ class.</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>The idea of the proof is simple. We first quote an inequality from the literature, which provides a universal upper bound on how many steps are sufficient for sampling the SMLD within a constant probability bound, then we derandomize it while still remaining within the $\mathsf{TC}^0$ class. </span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>The big idea of the derandomization is as follows: Given an algorithm that generates the correct token with probability that is of a bounded amount higher than generating any incorrect token, we can run the algorithm many times, and take the majority vote. By <span class="co">[</span><span class="ot">Hoeffding's inequality</span><span class="co">](https://en.wikipedia.org/wiki/Hoeffding's_inequality)</span>, there exists some random seed, for which the majority vote is correct on every single length-$n$ input. Now we hardcode that random seed.</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>The details of the proof (which is short) are in the paper. $\blacksquare$</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inexact score-matching</span></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>The requirement for *exact* score-matching is necessary for the following two reasons:</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>First, the technical reason is that the full form of the inequality we quoted is</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>TV(\rho_{DDPM, T}, \rho_{\hat{x}, 0}) \leq c \frac{d (\log T)^3}{T} + c \epsilon_{\text{score}} \sqrt{\log T}.</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>where the term $\epsilon_{\text{score}}$ denotes the score-matching error between the true score function of $\rho_{\hat{x}, 0}$ and the approximation $f_\theta$. As this extra term *increases* with $T$, the proof above does not apply. Perfect score-matching sets that term to zero, thus allowing the theorem to work.</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>Second, the intuitive reason is that if we have *no* requirement on score-matching, then there is essentially no constraint on the computational power of SMLD, by the construction in the next section.</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>Practically relevant score-networks are intermediate between two extreme cases. We believe that if $f_\theta$ is a good enough, but not perfect, score-matching network, then a generalized version of the above theorem still applies. However, finding the right way to *quantify* the goodness, as well as proving such a generalization, is left as future work.</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bad diffusion is Turing-complete</span></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>Since counter machines are Turing-complete, it suffices to show how to simulate any counter machine with diffusion. Suppose we are given a counter machine with $k$ registers, we simulate it by constructing a "pinball" machine that operates according to the SDE</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>d\hat{x}_t = \frac{1}{2} \hat{x}_t dt + f_\theta(\hat{x}_t, t) dt + dW_t</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>under a smooth force field $f_\theta$. The pinball machine has a single ball, whose location is $\hat{x}_t$. The ball rolls around a state space $\mathbb{R}^d$ guided by the force field $f_\theta(\hat{x}_t, t)$. Indeed, the force field can be time-independent, so we write it as $f_\theta(\hat{x}_t)$ instead.</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>The state space is divided into three parts as $\mathbb{R}^d = \mathbb{R}^k \times \mathbb{R} \times \mathbb{R}$. The first part $\mathbb{R}^k$ represents the $k$ registers. The second part $\mathbb{R}$ represents the program counter, which tracks the line-number of the program. The third part $\mathbb{R}$ is used for jumping between instructions, providing enough room for the ball to roll without "crossing the wires".</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>The space is divided into cubic cells of side lengths $L$. We denote each cell by $k + 2$ integers. </span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>Like the state space, the force field has three parts too. One part simply cancels out the $\frac{1}{2} x_t$ term. Another part forms "grooves" along which the ball rolls, thus implementing the counter machine. The third part points towards the center-lines of the grooves, so that the ball is not knocked off the grooves by the noise term $dW_t$. Of course, eventually the noise will knock the ball off the grooves, but if the force is strong enough, and the cubic cells have a large enough side length, then the machine will reach completion without being knocked off the grooves, with arbitrarily large probability.</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>Instead of formally specifying the grooves, it is simpler to give an example. Suppose at line number 32, the instruction reads "If the current state of register 1 is zero, then increment register 2 and jump to line 23, else jump to line 33", then this is implemented by drawing the following paths:</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$(0, r_2, \dots, r_d, 32, 0) \to (0, r_2 + 1, \dots, r_d, 32, 32) \to (0, r_2 + 1, \dots, r_d, 23, 32) \to (0, r_2 + 1, \dots, r_d, 23, 0)$.</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$(r_1, r_2, \dots, r_d, 32, 0) \to (0, r_2, \dots, r_d, 32, 32) \to (0, r_2, \dots, r_d, 33, 32) \to (0, r_2, \dots, r_d, 33, 0)$ for nonzero $r_1$.</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>By smoothing the corners of the paths, we obtain a smooth force field.</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="How to keep the ball on grooves" collapse="true" }</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>How strong must the force be to keep the ball rolling on the grooves? The noise term $dW_t$ would, over a long enough time, eventually knock the ball off the grooves. This can be suppressed by either using a strong confinement force field, or by using a weak confinement force field but a large cubic cell side length $L$. It turns out that we don't need a very strong force or a very large cell.</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>Let the counter machine have $N$ instructions. Suppose it halts within $S$ steps, then the total distance travelled by the pinball would be $O(NSL)$, where we need to account for the time necessary to jump between instructions. Then, since the rate of leakage is on the order of $e^{-L^2}$, we need only require $L \geq O(\sqrt{\ln (NSL)})$ to suppress the probability of leakage during the entire computation to a small constant. In particular, for any fixed $N, S$, because $L$ grows faster than $\sqrt{\ln (NSL)}$, there exists a big enough $L$ for which the machine will halt without leakage, for probability as close to $1$ as one desires. This machine operates under a force field that is smooth, and has Lipschitz-continuity bounded by universal constant.</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>Suppose that we have a language that is decidable by a Turing machine when it is restricted to a working tape with length $O(f(n))$, where $n$ is the input length, and $f$ is some monotonically increasing function, then by <span class="co">[</span><span class="ot">@fischerCounterMachinesCounter1968, Theorems 3.1 and 3.2</span><span class="co">]</span>, it is decidable by a counter machine that takes $e^{O(f(n))}$ steps to halt. Thus, it suffices when $L \geq O(\sqrt{f(n)})$.</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a><span class="fu">## Future work</span></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="fu">### Theoretical</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>We have shown that perfect diffusion models with exact score matching are constrained to $\mathsf{TC}^0$, while deliberately "bad" diffusion models can be Turing-complete. The more realistic intermediate case remains open, where the score network approximately computes the score function.</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>We conjecture that similar computational limitations apply when the approximation quality is sufficiently high, but formalizing this notion of "sufficiently good approximation" and proving the corresponding result requires further work. We make this conjecture based on two reasons. One, the aforementioned empirical observation that diffusion models converge rapidly. Two, because the forward diffusion converges exponentially rapidly to the standard normal distribution $\mathcal N(0, I)$, we believe that the backward diffusion process, as long as it is sufficiently close to the score function of a forward diffusion process, would be forced to converge in $O(1)$ time, since exponential decay is fast decay.</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>Our analysis focuses on diffusion models operating on $\mathbb{R}^d$ with subsequent discretization. However, other formulations of discrete diffusion exist, such as the directly discrete approach in <span class="co">[</span><span class="ot">@austinStructuredDenoisingDiffusion2021</span><span class="co">]</span>. For these models, we conjecture that $\mathsf{TC}^0$ limitations apply regardless of score network quality, as the finite state space inherently constrains the "computational capacity" of the diffusion process. Intuitively, a finite state space allows encoding only a finite number of bits per state before the signal-to-noise ratio<span class="ot">[^snr]</span> is exhausted, and the reverse diffusion reaches $t=0$.</span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a><span class="ot">[^snr]: </span>See <span class="co">[</span><span class="ot">@kingmaVariationalDiffusionModels2021</span><span class="co">]</span> for a formalization of signal-to-noise in diffusion modeling.</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a>Between finite state spaces and $\mathbb{R}^d$ lies the intermediate case of continuous but compact state spaces, such as the unit ball in $\mathbb{R}^d$. While our "pinball machine" construction would still work in such spaces, it would require dividing the compact space into an increasing number of cells. This means the force field, while smooth, cannot maintain bounded Lipschitz-continuity coefficients. Because of this, we hypothesize that under the additional requirement of $O(1)$ Lipschitz-continuity, diffusion models on compact spaces would be constrained to $\mathsf{TC}^0$ regardless of score network quality, effectively making them computationally equivalent to finite state models.</span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a><span class="fu">### Empirical</span></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>The animating big-picture idea behind this paper is that certain tasks are inherently sequential, such that any parallel computation that takes too little sequential steps must necessarily err. Sequential processing and consequences of its lack has been systematically studied for Transformers under the name of "chain of thought", but not for diffusion models. We have collected a few suggestive examples gleaned from the literature, but it would be a valuable contribution to the literature to test this hypothesis systematically on diffusion models. We conjecture:</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Tasks requiring deep sequential reasoning should exhibit a sharp performance cliff when addressed by diffusion models with a fixed number of denoising steps.</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Adding more denoising steps beyond a certain threshold should yield minimal improvements for $\mathsf{TC}^0$ tasks but continued improvements for tasks outside this complexity class.</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Performance on complex sequential tasks should improve significantly when introducing autoregressive components, as seen in <span class="co">[</span><span class="ot">@arriolaBlockDiffusionInterpolating2025</span><span class="co">]</span>.</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a>Controlled experiments testing these predictions would provide valuable empirical validation of our theoretical framework and guidance for the further development of diffusion models.</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a><span class="fu">### Architectural</span></span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>The most promising direction may be architectures that interpolates sequential and parallel computation dynamically, shifting to the sequential mode for tasks that demand them. We point out several particularly worthy directions for interpolation:</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>In architecture, interpolation between massively parallel models (Transformers, state-space models) and sequential ones (recurrent neural networks).</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For language generation, interpolation between full-sequence generation (typical of diffusion language models) and autoregressive generation (common in Transformer-based models). While both approaches have been studied extensively in isolation, their combination remains relatively unexplored.</span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Interpolation between SMLD and <span class="co">[</span><span class="ot">neural ODE</span><span class="co">](https://en.wikipedia.org/wiki/Neural_differential_equation)</span> frameworks. SMLD offers rapid convergence through massive parallelism, while neural ODEs provide slower convergence with more sequential computation.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>I don't read conclusions in papers, and I don't write them either. They seem like a stupid convention, when having an abstract and an introduction is not enough to pad your page numbers, so you had to rephrase them, differently but the same... In fact, the only reason there is a conclusion in the arXiv paper is because academic style requires there to be one, which is why I got Claude 3.7 to write it... without any modification!</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>