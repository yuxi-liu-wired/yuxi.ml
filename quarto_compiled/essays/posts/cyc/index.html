<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2025-04-01">
<meta name="description" content="After 40 years, 30 million rules, 200 million dollars, 2000 person-years, and many promises, Cyc has failed to reach intellectual maturity, and may never will. Exacerbated by the secrecy and insularity of Cycorp, there remains no evidence of its general intelligence.">

<title>Cyc – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Cyc – Yuxi on the Wired">
<meta property="og:description" content="After 40 years, 30 million rules, 200 million dollars, 2000 person-years, and many promises, Cyc has failed to reach intellectual maturity, and may never will. Exacerbated by the secrecy and insularity of Cycorp, there remains no evidence of its general intelligence.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/cyc/figure/banner/banner_3.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1536">
<meta property="og:image:alt" content="A symbolic representation of the Cyc project. The vanishing point covered by mist represents the project that keeps growing without ever reaching completion, a destination that is a mirage. The rows of pyramids make up a larger pyramid, due to perspective. This creates the sense of a recursive pyramid, a hierarchy of hierarchies, representing the hierarchy of microtheories in Cyc's ontology. The pyramid is also a monument to the dead, of vast ambitions that failed to secure their design purposes. For Egyptian pyramids, the purpose was a good afterlife. For Cyc, the purpose was AGI.
A one-point perspective image of a path towards infinity, banked by two rows of pyramids. The pyramids are stepped pyramids made of concrete. The path fades into a distant mist. The point of view is above the tops of the pyramids. Monochromatic photo. Created by GPT-4o.">
<meta name="twitter:title" content="Cyc – Yuxi on the Wired">
<meta name="twitter:description" content="After 40 years, 30 million rules, 200 million dollars, 2000 person-years, and many promises, Cyc has failed to reach intellectual maturity, and may never will. Exacerbated by the secrecy and insularity of Cycorp, there remains no evidence of its general intelligence.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/cyc/figure/banner/banner_3.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1536">
<meta name="twitter:image:alt" content="A symbolic representation of the Cyc project. The vanishing point covered by mist represents the project that keeps growing without ever reaching completion, a destination that is a mirage. The rows of pyramids make up a larger pyramid, due to perspective. This creates the sense of a recursive pyramid, a hierarchy of hierarchies, representing the hierarchy of microtheories in Cyc's ontology. The pyramid is also a monument to the dead, of vast ambitions that failed to secure their design purposes. For Egyptian pyramids, the purpose was a good afterlife. For Cyc, the purpose was AGI.
A one-point perspective image of a path towards infinity, banked by two rows of pyramids. The pyramids are stepped pyramids made of concrete. The path fades into a distant mist. The point of view is above the tops of the pyramids. Monochromatic photo. Created by GPT-4o.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Cyc</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Obituary for the greatest monument to logical AGI</p>
                  <div>
        <div class="description">
          After 40 years, 30 million rules, 200 million dollars, 2000 person-years, and many promises, Cyc has failed to reach intellectual maturity, and may never will. Exacerbated by the secrecy and insularity of Cycorp, there remains no evidence of its general intelligence.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">scaling</div>
                <div class="quarto-category">history</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 1, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">May 30, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#in-lieu-of-an-introduction" id="toc-in-lieu-of-an-introduction" class="nav-link" data-scroll-target="#in-lieu-of-an-introduction">In lieu of an introduction</a></li>
  <li><a href="#automated-mathematician" id="toc-automated-mathematician" class="nav-link" data-scroll-target="#automated-mathematician">Automated Mathematician</a>
  <ul class="collapse">
  <li><a href="#automated-discovery" id="toc-automated-discovery" class="nav-link" data-scroll-target="#automated-discovery">Automated discovery</a></li>
  <li><a href="#the-working-of-am" id="toc-the-working-of-am" class="nav-link" data-scroll-target="#the-working-of-am">The working of AM</a></li>
  <li><a href="#the-end-of-am" id="toc-the-end-of-am" class="nav-link" data-scroll-target="#the-end-of-am">The end of AM</a></li>
  </ul></li>
  <li><a href="#eurisko" id="toc-eurisko" class="nav-link" data-scroll-target="#eurisko">EURISKO</a>
  <ul class="collapse">
  <li><a href="#the-lessons" id="toc-the-lessons" class="nav-link" data-scroll-target="#the-lessons">The lessons</a></li>
  <li><a href="#the-evervictorious" id="toc-the-evervictorious" class="nav-link" data-scroll-target="#the-evervictorious">The Evervictorious</a></li>
  </ul></li>
  <li><a href="#the-saga-of-cyc" id="toc-the-saga-of-cyc" class="nav-link" data-scroll-target="#the-saga-of-cyc">The saga of Cyc</a>
  <ul class="collapse">
  <li><a href="#cyc-by-1993" id="toc-cyc-by-1993" class="nav-link" data-scroll-target="#cyc-by-1993">Cyc by 1993</a></li>
  <li><a href="#cyc-by-2000" id="toc-cyc-by-2000" class="nav-link" data-scroll-target="#cyc-by-2000">Cyc by 2000</a></li>
  <li><a href="#cyc-in-the-2000s" id="toc-cyc-in-the-2000s" class="nav-link" data-scroll-target="#cyc-in-the-2000s">Cyc in the 2000s</a></li>
  <li><a href="#cyc-is-done" id="toc-cyc-is-done" class="nav-link" data-scroll-target="#cyc-is-done">Cyc is done?</a></li>
  <li><a href="#how-expensive-was-the-lunch" id="toc-how-expensive-was-the-lunch" class="nav-link" data-scroll-target="#how-expensive-was-the-lunch">How expensive was the lunch?</a></li>
  </ul></li>
  <li><a href="#overview-of-cyc-the-system" id="toc-overview-of-cyc-the-system" class="nav-link" data-scroll-target="#overview-of-cyc-the-system">Overview of Cyc, the system</a>
  <ul class="collapse">
  <li><a href="#cycl-language" id="toc-cycl-language" class="nav-link" data-scroll-target="#cycl-language">CycL language</a></li>
  <li><a href="#ontology" id="toc-ontology" class="nav-link" data-scroll-target="#ontology">Ontology</a></li>
  <li><a href="#inference-engines" id="toc-inference-engines" class="nav-link" data-scroll-target="#inference-engines">Inference engines</a></li>
  <li><a href="#querying" id="toc-querying" class="nav-link" data-scroll-target="#querying">Querying</a></li>
  <li><a href="#natural-language-processing" id="toc-natural-language-processing" class="nav-link" data-scroll-target="#natural-language-processing">Natural language processing</a></li>
  <li><a href="#machine-learning" id="toc-machine-learning" class="nav-link" data-scroll-target="#machine-learning">Machine learning</a></li>
  </ul></li>
  <li><a href="#cycops-what-is-it-good-for" id="toc-cycops-what-is-it-good-for" class="nav-link" data-scroll-target="#cycops-what-is-it-good-for">CycOps, what is it good for?</a>
  <ul class="collapse">
  <li><a href="#sec-cycops-overview" id="toc-sec-cycops-overview" class="nav-link" data-scroll-target="#sec-cycops-overview">Overview</a></li>
  <li><a href="#sec-terrorism-knowledge-base" id="toc-sec-terrorism-knowledge-base" class="nav-link" data-scroll-target="#sec-terrorism-knowledge-base">Terrorism Knowledge Base</a></li>
  <li><a href="#cleveland-clinic" id="toc-cleveland-clinic" class="nav-link" data-scroll-target="#cleveland-clinic">Cleveland Clinic</a></li>
  <li><a href="#is-that-all" id="toc-is-that-all" class="nav-link" data-scroll-target="#is-that-all">Is that all?</a></li>
  </ul></li>
  <li><a href="#everyone-can-only-see-their-own-dream" id="toc-everyone-can-only-see-their-own-dream" class="nav-link" data-scroll-target="#everyone-can-only-see-their-own-dream">Everyone can only see their own dream</a>
  <ul class="collapse">
  <li><a href="#lenats-tenets" id="toc-lenats-tenets" class="nav-link" data-scroll-target="#lenats-tenets">Lenat’s tenets</a></li>
  <li><a href="#a-hostile-assessment-of-cyc" id="toc-a-hostile-assessment-of-cyc" class="nav-link" data-scroll-target="#a-hostile-assessment-of-cyc">A hostile assessment of Cyc</a></li>
  <li><a href="#lenat-against-the-world" id="toc-lenat-against-the-world" class="nav-link" data-scroll-target="#lenat-against-the-world">Lenat against the world</a></li>
  </ul></li>
  <li><a href="#in-lieu-of-a-conclusion" id="toc-in-lieu-of-a-conclusion" class="nav-link" data-scroll-target="#in-lieu-of-a-conclusion">In lieu of a conclusion</a></li>
  
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>The legendary Cyc project, Douglas Lenat’s 40-year quest to build artificial general intelligence by scaling symbolic logic, has failed. Based on extensive archival research, this essay brings to light its secret history so that it may be widely known.</p>
<p>Lenat’s journey began with his PhD work on automated mathematical discovery through heuristic search. He observed that such systems initially make promising discoveries but quickly “run out of steam” as they exhaust their initial pool of heuristic rules. His follow-up system EURISKO, famous for winning tournament competitions by finding unconventional tactics, faced similar limitations. These experiences convinced Lenat that true AI needed a vast foundation of common sense knowledge to avoid intellectual exhaustion.</p>
<p>In 1984, he launched Cyc to manually encode millions of facts and rules about common sense, predicting that once this “knowledge pump” was primed, the system would begin true machine learning by reading natural language texts and conducting autonomous scientific experiments. Cyc grew to contain approximately 30 million assertions at a cost of $200 million and 2,000 person-years. Yet despite Lenat’s repeated predictions of imminent breakthrough, it never came.</p>
<p>In terms of funding, Cycorp was probably half-funded by the military and intelligence before 2010, and entirely funded by commercial applications since 2016. Cycorp achieved long-term financial stability that is uncommon for a small technology company, but all known commercial uses of its system involve standard methods in expert systems, data integration, and information retrieval, functionally the same as similar services offered by established corporations like Oracle and IBM. No evidence suggests that Cyc’s purported higher intelligence provided any competitive advantage.</p>
<p>By academic standards, the Cyc project is highly insular. Publications involving Cyc typically described methods for entering information <em>into</em> the system, rarely addressing applications <em>out of</em> it. Outside Cycorp, Cyc saw minimal use in AI research or even in knowledge retrieval, its most adjacent field. Academics found the system difficult to use, and it never performed on public benchmarks. Spin-off projects like OpenCyc and various semantic web initiatives all eventually shut down without notable success.</p>
<p>The secretive nature of Cyc has multiple causes. Lenat personally did not release the source code of his PhD project or EURISKO, remained unimpressed with open source, and disliked academia as much as academia disliked him. Most open information concerning Cyc had been deliberately removed circa 2015, at the moment when Cycorp pivoted to commercial applications.</p>
<p>Lenat had a single philosophical vision for AI that he pursued for 40 years. Guided by it, he had consistently rejected every alternative vision for AI, including the heuristic search approach, the expert systems approach, the robotics approach, and the neural network approach. All were rejected as either “shallow pattern-matching” seeking a “free lunch” on one end, or a “mystical worship of physical embodiment” on the other end.</p>
<p>As of 2025, 9 years after the knowledge pump had been primed, there is still no sign that Cyc would ever achieve general intelligence.</p>
<p>More archival material available on <a href="https://github.com/yuxi-liu-wired/cyc-archive/tree/main">GitHub</a>.</p>
</section>
<section id="in-lieu-of-an-introduction" class="level2">
<h2 class="anchored" data-anchor-id="in-lieu-of-an-introduction">In lieu of an introduction</h2>
<p>Many years later, surrounded by the humming servers of the knowledge base, Douglas Lenat was to remember that distant afternoon when he taught Cyc that everyone can only see their own dream.</p>
</section>
<section id="automated-mathematician" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="automated-mathematician">Automated Mathematician</h2>
<p>In the land of AI, there had been legends, of people and systems that were before their time, that showed sparks of brilliance but without followups, and Douglas Lenat was a man of three.</p>
<p>Lenat’s first legend was his Automated Mathematician (AM), a system that, according to the legends, discovered mathematical concepts autonomously by distilling concepts out of patterns and remixing and stirring together previous concepts <span class="citation" data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">D. B. Lenat 1976</a>)</span>. As we will see, this legend is mostly true, though with a caveat that leads to the topic of his second legend.</p>
<p>AM was his 1976 PhD thesis project, and it was one of many “automated discovery” systems in the 1970s – the only one still remembered nowadays. Though neural networks and other “self-organized” machine learning methods had mostly <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/">died out in the 1960s</a>, machine learning did not die, and indeed, was going through a logical spring, under the banner of “automated discovery”.</p>
<section id="automated-discovery" class="level3">
<h3 class="anchored" data-anchor-id="automated-discovery">Automated discovery</h3>
<p>Nowadays, when we think of “machine learning”, we think of random forests, neural networks, and such. But back then, machine learning was understood as “logical AI gone meta”. Well, to go meta, we must first understand: What <em>is</em> logical AI? Simon and Newell, the two giants of logical AI, had shown the way: logical AI is heuristic search.</p>
<ul>
<li>To play chess, one simply writes a program that does <a href="https://en.wikipedia.org/wiki/Alpha-beta_pruning">alpha-beta search</a> over the game tree, guided by heuristics of piece-worth, mobility, and such.</li>
<li>To solve planar geometry problems, one simply writes a program that constructs a path in the space of geometric arguments, until one connects the axioms with the conclusions.</li>
<li>To solve problems of type X, one simply writes a program that walks through the space of possible solutions to X, and code in the heuristic rules, its north star, so that it will not be lost in the space of solutions.</li>
</ul>
<p>Well, chess is a game, and so is Euclidean geometry, symbolic integration, and… perhaps scientific discovery itself? The famed scientific method, to deserve the name “method”, ought to be just one more problem for logical AI to solve. It merely remained to go meta, to heuristically search over the space of heuristic searches.</p>
<p>During 1977–1983, <a href="http://www.isle.org/~langley/">Pat Langley</a> wrote a series of programs named BACON, after <a href="https://en.wikipedia.org/wiki/Francis_Bacon">Francis Bacon</a>, a father of scientific empiricism. It purported to discover scientific laws from mere data. For example, when given a table of the moles <span class="math inline">\(N\)</span>, pressures <span class="math inline">\(P\)</span>, volumes <span class="math inline">\(V\)</span>, and temperatures <span class="math inline">\(T\)</span> of gas samples, BACON would first try out simple equations involving two of the terms. It would discover that the data for <span class="math inline">\(PV\)</span> appears more cluster-like than either <span class="math inline">\(P\)</span> or <span class="math inline">\(V\)</span>, so it would make a new quantity <span class="math inline">\(PV\)</span>, and add that to the table. It would then repeat this process, discovering that <span class="math inline">\(PV/T\)</span> is an interesting quantity, and finally that <span class="math inline">\(PV/NT\)</span> is a <em>constant</em> quantity – the <a href="https://en.wikipedia.org/wiki/Ideal_gas_law">ideal gas law</a> discovered! <span class="citation" data-cites="langleyDataDrivenDiscoveryPhysical1981">(<a href="#ref-langleyDataDrivenDiscoveryPhysical1981" role="doc-biblioref">Langley 1981</a>)</span></p>
<p>In the logical AI framework, the problem space of BACON is the space of all elementary functions involving the table columns, and a <em>solution</em> is a (nontrivial) function that results in a nearly constant column. At each step, the program tries out simple combinations of the current columns, and heuristically pick the one most nearly constant.</p>
<p>As another example, consider the famed <a href="https://en.wikipedia.org/wiki/Dendral">Dendral and Meta-Dendral</a>.</p>
<p>Dendral was an artificial model of how professional chemists perform <a href="https://en.wikipedia.org/wiki/Molecular_spectroscopy">molecular spectroscopy</a> – that is, how they read out a molecular structure from looking at a few spiky curves. Its problem space is the space of all possible ways to cut up a molecule into fragments, and ways for the fragments to give and take their atoms. Its heuristics are the rules for generating chemically plausible and implausible cleavages and transfers. For example, it is plausible for a protein to be cleaved at the <code>-CO-*-NH-</code> peptide bond, but implausible to be cleaved at the double bond between <code>C</code> and <code>O</code>. The goal is, given molecular spectroscopic data for a single molecule, to construct a molecular structure and a sequence of cleavages and transfers, such that it would produce the data.</p>
<p>With Meta-Dendral, the problem space goes meta, becoming the space of possible chemical <em>rules</em>. Its goal is to find plausible rules (plausible according to heuristic meta-rules) that can explain a large collection of molecular structures and their spectroscopic data. Meta-Dendral proved useful for working chemists by discovering some cleavage rules for a certain minor sub-family of <a href="https://en.wikipedia.org/wiki/Androstane">androstanes</a>. <span class="citation" data-cites="buchananDENDRALMetaDENDRALTheir1981">(<a href="#ref-buchananDENDRALMetaDENDRALTheir1981" role="doc-biblioref">Buchanan and Feigenbaum 1981</a>)</span> <!-- todo: For details, see my essay on [expert systems](https://yuxi-liu-wired.github.io/essays/posts/expert-systems/). --></p>
<p>In general, such a system begins with some simple rules that allow the system to a low score according to some criteria. and as it runs, it builds, prunes, and modifies the rules, so that in the end, its rule set allows it to achieve a high score. The following tabulates a few <span class="citation" data-cites="walkerHowFeasibleAutomated1987">(<a href="#ref-walkerHowFeasibleAutomated1987" role="doc-biblioref">Walker 1987</a>)</span>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>System Name</th>
<th>Date</th>
<th>Task</th>
<th>Data</th>
<th>Rules</th>
<th>Discovery Method</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Meta-Dendral</td>
<td>1976</td>
<td>Discover molecular cleavage and transfer rules for mass spectrometry</td>
<td>Molecular structures and their spectra</td>
<td>Molecular cleavage and transfer rules</td>
<td>Use meta-heuristic rules to generate possible rules, and test on data</td>
</tr>
<tr class="even">
<td>Bacon</td>
<td>1977–1983</td>
<td>Discover physical laws</td>
<td>Numeric data from experiments</td>
<td>Elementary functions</td>
<td>Symbolic regression</td>
</tr>
<tr class="odd">
<td>RX</td>
<td>1982</td>
<td>Discover drug side effects and interactions</td>
<td>Patient information database</td>
<td>Drug effect and interaction rules</td>
<td>Symbolic regression with time-lag</td>
</tr>
</tbody>
</table>
</section>
<section id="the-working-of-am" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-working-of-am">The working of AM</h3>
<blockquote class="blockquote">
<p>The honor of your machine is preserved.</p>
<p>— Paul Erdős, after examining <span class="citation" data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">D. B. Lenat 1976</a>, appendix 4)</span>.</p>
</blockquote>
<p>Within this context, AM is different. It is still a logical AI with a problem space and a heuristic search. However, there is no data: It was mostly “self-play”.</p>
<p>To start AM, Lenat began by entering 115 concepts in set theory and ~250 heuristic rules, and thence AM ran, discovering more and more constructions. Most would be trivial, but a few would be interesting, and these interesting constructions would be stored, allowing further constructions upon them. It secured a place as a minor legend, reportedly rediscovering many concepts, such as the natural numbers, the prime numbers, and the Goldbach conjecture.</p>
<p>A <strong>concept</strong> is essentially a <a href="https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)">frame</a>, which are essentially objects in <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented programming</a>. A concept has 25 possible <strong>facets</strong>, which are “slots”, or “data fields”. Not all need to be filled.</p>
<div class="callout callout-style-default callout-note callout-titled" title="An example concept">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
An example concept
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The following is an example concept with many facets populated <span class="citation" data-cites="lenatRoleHeuristicsLearning1983">(<a href="#ref-lenatRoleHeuristicsLearning1983" role="doc-biblioref">D. B. Lenat 1983c</a>, table 9.9)</span>:</p>
<ul>
<li><code>NAME</code>: <code>Generalize-rare-predicate</code></li>
<li><code>ABBREVIATION</code>: <code>GRP</code></li>
<li><code>STATEMENT</code>:
<ul>
<li><code>English</code>: If a predicate is rarely true, Then create generalizations of it</li>
<li><code>IF-potentially-relevant</code>:</li>
<li><code>IF-just-finished-a-task-dealing-with</code>: a predicate <code>P</code></li>
<li><code>IF-about-to-work-on-task-dealing-with</code>: an agenda <code>A</code></li>
<li><code>IF-in-the-middle-of-a-task-dealing-with</code>: <em>never</em></li>
<li><code>IF-truly-relevant</code>: <code>P</code> returns True less than <code>5%</code> of Average Predicate</li>
<li><code>IF-resources-available</code>: at least <code>10</code> CPU seconds, at least <code>300</code> cells</li>
<li><code>THEN-add-task-to-agenda</code>: Fill in entries for <code>Generalizations</code> slot of <code>P</code></li>
<li><code>THEN-conjecture</code>:
<ul>
<li><code>P</code> is less interesting than expected</li>
<li><code>Generalizations</code> of <code>P</code> may be better than <code>P</code></li>
<li><code>Specializations</code> of <code>P</code> may be very bad</li>
</ul></li>
<li><code>THEN-modify-slots</code>:
<ul>
<li>Reduce Worth of <code>P</code> by <code>10%</code></li>
<li>Reduce Worth of <code>Specializations(P)</code> by <code>50%</code></li>
<li>Increase Worth of <code>Generalizations(P)</code> by <code>20%</code></li>
</ul></li>
<li><code>THEN-print-to-user</code>: <code>English(GRP)</code> with “a predicate” replaced by <code>P</code></li>
<li><code>THEN-define-new-concepts</code>:</li>
</ul></li>
<li><code>CODED-IF-PART</code>: <code>λ(P) ... &lt;LISP function conjoining all the IF- parts&gt;</code></li>
<li><code>CODED-THEN-PART</code>: <code>λ(P) ... &lt;LISP function appending all the THEN- parts&gt;</code></li>
<li><code>CODED-IF-THEN-PARTS</code>: <code>λ(P) ... &lt;LISP function combining the previous 2 slots&gt;</code></li>
<li><code>COMPILED-CODED-IF-THEN-PARTS</code>: <code>#30875</code></li>
<li><code>SPECIALIZATIONS</code>: <code>Generalize-rare-set-predicate</code>
<ul>
<li><code>Boundary-Specializations</code>: <code>Enlarge-domain-of-predicate</code></li>
</ul></li>
<li><code>GENERALIZATIONS</code>: <code>Modify-predicate</code>, <code>Generalize-concept</code>
<ul>
<li><code>Immediate-Generalizations</code>: <code>Generalize-rare-contingent-piece-of-knowledge</code></li>
<li><code>Siblings</code>: <code>Generalize-rare-heuristic</code></li>
</ul></li>
<li><code>IS-A</code>: <code>Heuristic</code></li>
<li><code>EXAMPLES</code>:
<ul>
<li><code>Good-Examples</code>: Generalize <code>Set-Equality</code> into <code>Same-Length</code></li>
<li><code>Bad-Examples</code>: Generalize <code>Set-Equality</code> into <code>Same-First-Element</code></li>
</ul></li>
<li><code>CONJECTURES</code>: Special cases of this are more powerful than <code>Generalizations</code>
<ul>
<li><code>Good-Conjec-Units</code>: <code>Specialize</code>, <code>Generalize</code></li>
</ul></li>
<li><code>ANALOGIES</code>: <code>Weaken-overconstrained-problem</code></li>
<li><code>WORTH</code>: <code>600</code></li>
<li><code>VIEW</code>: <code>Enlarge-structure</code></li>
<li><code>ORIGIN</code>: Specialization of <code>Modify-predicate</code> via empirical induction
<ul>
<li><code>Defined-using</code>: <code>Specialize</code></li>
<li><code>Creation-date</code>: <code>6/1/78 11:30</code></li>
</ul></li>
<li><code>HISTORY</code>:
<ul>
<li><code>N-Good-Examples</code>: <code>1</code>, <code>N-Bad-Examples</code>: <code>1</code></li>
<li><code>N-Good-Conjectures</code>: <code>3</code>, <code>N-Bad-Conjectures</code>: <code>1</code></li>
<li><code>N-Good-Tasks-Added</code>: <code>2</code>, <code>N-Bad-Tasks-Added</code>: <code>0</code></li>
<li><code>Avg-Cpu-Time</code>: <code>9.4</code> seconds, <code>Avg-List-Cells</code>: <code>200</code></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/AM_ontology.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The network of concepts at the beginning of AM. <code>|</code> means “is a”. <code>|||</code> means “is an example of”. <span class="citation" data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">D. B. Lenat 1976, 106</a>)</span></figcaption>
</figure>
</div>
<p>AM has an <strong>agenda</strong>: a list of <strong>tasks</strong>, each with a list of <strong>reasons</strong> for the task. Each task is of the form “Perform operation <code>O</code> to facet <code>F</code> of concept <code>C</code>”. A task’s <code>Worth</code> is the sum of its reasons’ worths. AM always performs the task with the highest worth.</p>
<p>To perform a task, AM looks for <strong>heuristic rules</strong> whose conditions are (mostly) satisfied, and whose worth is (pretty) high. Each heuristic rule is of form “if <code>&lt;condition&gt;</code>, then run <code>&lt;actions&gt;</code>”. Each action has 3 kinds of possible effects:</p>
<ul>
<li>Add a new task to agenda.</li>
<li>Create a new concept.</li>
<li>Add or delete an entry to a facet of a concept.</li>
</ul>
<p>Some example heuristic rules:</p>
<ul>
<li>If the task is to fill in examples of X, and X is a special case of Y, then for each example of Y, check if it a definition of X. If so, then add it to the list of examples of X.</li>
<li>If some but not most examples of X are also examples of Y, then create a new concept “X and Y”.</li>
<li>If very few examples of X are found, then add the following task to the agenda: “Generalize the concept X”, for the following reason: “X are quite rare; a slightly less restrictive concept might be more interesting”.</li>
</ul>
<p>At this point, the careful reader would notice several problems:</p>
<p>How does AM know that the concept should be called “Prime Numbers”? Ah, that’s because Lenat would regularly interrupt and inspect AM, and if Lenat notices that AM has rediscovered, say, prime numbers, he would rename that from something like <code>concept-421</code> to <code>prime-numbers</code>.</p>
<p>To discover prime numbers, AM must have a way to check if a Lisp object is a prime number or not. That is, the definition must also be a program. So…? Ah, the famous <a href="https://en.wikipedia.org/wiki/Homoiconicity">homoiconicity</a> of Lisp came to the rescue! A definition, as stored within a facet of a concept, is a data, but for Lisp, data is program, and program data. Consequently, AM can run a subroutine that enumerates possible programs <em>as data</em>, and for each, interpret it <em>as program</em>, until AM hits upon a program that works (or times out).</p>
<p>How does it check that two definitions actually define the same thing? In general, this is impossible by <a href="https://en.wikipedia.org/wiki/Rice's_theorem">Rice’s theorem</a>, so Lenat must have used some heuristic rules. I looked, but can’t find Lenat explaining this anywhere. It seemed like a trick of the hand.</p>
<p>But most serious of all issues is that the most critical part of AM was not the concepts it discovered – after all, mathematicians did not need a computer to inform them that prime numbers are interesting. The most critical part was surely the heuristic rules by which AM worked, and many were entirely hand-waved. The most detailed description was in <span class="citation" data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">D. B. Lenat 1976</a>, appendix 3)</span>, and it is still not described to a level of detail that may allow reimplementation.</p>
<p>Consider rule 75: “A constructive existence conjecture is interesting if it is frequently used.” How frequent is “frequent”? What threshold of frequency triggers the increase in interestingness, and by how much? There are even vaguer rules, such as rule 69: “Formulate a parameterized conjecture, a ‘template’, which gets slowly specialized or instantiated into a definite conjecture.”.</p>
<p>Now, these would have not been a problem if AM was just a forgotten system, but it was not. The achievements of AM was impressive enough that it was an instant celebrity among the AI people, winning an <a href="https://en.wikipedia.org/wiki/IJCAI_Computers_and_Thought_Award">IJCAI Award</a> just 1 year after publication <span class="citation" data-cites="lenatUbiquityDiscovery1977">(<a href="#ref-lenatUbiquityDiscovery1977" role="doc-biblioref">D. B. Lenat 1977</a>)</span>. Anecdotes even suggested that the AI mathematician was here:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;This anecdote corroborates <span class="citation" data-cites="lenatAMArtificialIntelligence1976">(<a href="#ref-lenatAMArtificialIntelligence1976" role="doc-biblioref">D. B. Lenat 1976</a>, appendix 4.6)</span>.</p></div></div><blockquote class="blockquote">
<p>On one memorable occasion, one of my advisors, George Polya, was looking at its results, and remarked “That reminds me of something a student of a friend of mine once did.” He rummaged through an old trunk, found the relevant correspondence, and it turned out that his friend was G. H. Hardy, and the student was Srinivasa Ramanujan! Even though that regularity (involving highly composite numbers) has no practical significance, Polya and I were happy to see AM behaving much like the young self-taught Indian genius had, in his explorations in search for interesting regularities.</p>
<p><span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span></p>
</blockquote>
</section>
<section id="the-end-of-am" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-end-of-am">The end of AM</h3>
<p>The swirling controversy came to a head with the publication of <span class="citation" data-cites="ritchieAMCaseStudy1984">(<a href="#ref-ritchieAMCaseStudy1984" role="doc-biblioref">Ritchie and Hanna 1984</a>)</span>, which argued that AM was badly documented. That, its control structure probably was not “simply pick the task with the highest worth”, but more complicated. This called into question the Lenat claim that the heuristic rules were the load-bearing parts of AM, which Lenat had implied by emphasizing its almost trivially simple control structure. Further, some crucial <em>quantitative</em> heuristic rules were probably hidden behind vague <em>qualitative</em> handwaves like “A nonconstructive existence conjecture is interesting” (rule 74). Now, handwaving would have not been a problem if they were intended as merely glosses over the source code, but the source code was also unpublished, making it impossible for other other researchers to reproduce or extend the work, or to reinterpret AM’s workings.</p>
<p>In short, because of the various issues, AM was an event that happened, but not an entity that could be built upon. One could not build upon it directly in source code, since it’s unavailable. One could not build upon it by reimplementing the pseudocode, since the rules were vaguely specified, and the control structure was probably wrong. One could not build upon it by reimplementing the high-level ideas, since it’s unclear which part, out of the several dozen tightly integrated parts of AM, was responsible for AM’s good outputs, and which were just implementation details. In our language, there was no ablation study.</p>
<p>Lenat quickly replied with <span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span>. He dismissed the criticism as mostly miscommunication, and went on to describe the <em>real</em> lesson of AM. If I were to be dab, Lenat was saying that it is fine for AM’s source,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> or even pseudocode, to be unavailable, because Lenat had learned the lessons, and you, dear reader, need only listen to the lessons from him.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Though Lenat admitted that “the code ought to have been provided” <span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span> for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had <a href="https://white-flame.com/am-eurisko.html">recently been found</a>, right where it should be – the <a href="https://www.saildart.org/DBL">DBL folder in the Stanford AI Laboratory backup data</a>. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.</p></div></div><blockquote class="blockquote">
<p>The AM thesis never explained, precisely, how concepts such as ‘not very often’ and ‘related to’ were implemented. By and large, these omissions were due to the fact that the code Lenat wrote for these predicates was quite trivial… inevitable, yet regrettable process of simplifying large pieces of code, translating them to brief English phrases. This process left out many exceptional cases, and made the English condensations less accurate… Some problems that Ritchie and Hanna cite… are simply errors of mis-reading what was stated in the thesis or articles… A few of the problems raised in Ritchie and Hanna’s article are, annoyingly, genuine inconsistencies in the thesis document, such as whether or not facets had subfacets. These reflect the fact that AM was a running and evolving program, changing daily in small ways even as the thesis document was being written… the changes in representation were driven simply by AM’s running out of list space in 1975 <a href="https://en.wikipedia.org/wiki/Interlisp">INTERLISP</a> code; we were forced to shift representations time and time again just to gain a few hundred precious list cells.</p>
<p><span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>And what were the lessons?</p>
<p>1th lesson: AM exhausts itself. Roughly speaking, each new interesting discovery depends on ~24 heuristics, and each heuristic has a hand in ~24 discoveries. Therefore, with ~N heuristic rules, there would be ~N interesting discoveries. Because AM cannot discover heuristic rules, with ~300 starting heuristic rules, it would run out of interesting discoveries and “die of boredom”. Lenat could help AM by adding new heuristics, and AM would make some new discoveries, but this never lasted.</p>
<blockquote class="blockquote">
<p>Eventually, AM acquired an uncommon ailment for a computing system: intellectual exhaustion. Having explored the esoteric reaches of mathematics, AM suddenly downshifted into a preoccupation with rudimentary arithmetic. Finally, with the remark, “Warning! No task on the agenda has priority over 200.”, the system virtually expired, as though from boredom.</p>
<p><span class="citation" data-cites="hiltzikBirthThinkingMachine2001">(<a href="#ref-hiltzikBirthThinkingMachine2001" role="doc-biblioref">Hiltzik 2001</a>)</span></p>
</blockquote>
<p>2th lesson: Representation matters a lot. AM worked so well for mathematics, because AM used Lisp code as data. Lisp is the perfect tool if you want to search over the space of interesting mathematical functions. You can modify a Lisp expression, and get a different mathematical function that is possibly interesting. In contrast, if you were to modify assembly code, you’d most likely end up with nonsense. Indeed, Lenat found that he could not extend AM to “go meta” and discover new heuristics, because Lisp is good for math, not “heuretics” (the study of heuristics). Modifying a Lisp expression for a heuristic most likely ends up with nonsense, much like modifying assembly code for a mathematical function.</p>
<blockquote class="blockquote">
<p>It was only because of the intimate relationship between LISP and Mathematics that the mutation operators (loop unwinding, recursion elimination, composition, argument elimination, function substitution, etc.) turned out to yield a high ‘hit rate’ of viable, useful new math concepts when applied to previously-known, useful math concepts – concepts represented as LISP functions. But no such deep relationship existed between LISP and Heuretics, and when the basic automatic programming (mutations) operators were applied to viable, useful heuristics, they almost always produced useless (often worse than useless) new heuristic rules.</p>
<p>…</p>
<p>We did not perceive until writing this paper that the way in which <code>Similar-To</code>, <code>Not-Often</code>, <code>Notice-Regularity</code>, and scores of other ‘primitives’ were coded do themselves embody a large amount of heuristic knowledge. We exploited the structure of (or, if you prefer, partially encoded) the domain of elementary mathematics, in the process of making <em>trivial yet adequate</em> LISP versions of those extremely complex and subtle notions (such as similarity of concepts).</p>
<p><span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>How do we know that Lenat learned the lessons?</p>
</section>
</section>
<section id="eurisko" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="eurisko">EURISKO</h2>
<p>Though 1981 is still near, EURISKO is already shrouded in a reverential mystery like legends do, among the <a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)">Deep Blue</a>, the <a href="https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)">Samuel Checkers Player</a>, and the <a href="https://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice">apprentice’s broom</a>. A symbol, a moral archetype. A program that discovered loopholes in a sci-fi ship-building tournament, allowing its creator to win twice in a row using fleets so unaesthetic that the people running the tournament threatened to stop the tournaments if it would win thrice, so it retired, the Honorary Admiral, EURISKO the Undefeated.</p>
<p>Brilliant, dramatic, but what do we glean from it, other than a moral play about the power of thinking sideways and <a href="https://gwern.net/unseeing">seeing through</a>? Quite a lot.</p>
<section id="the-lessons" class="level3">
<h3 class="anchored" data-anchor-id="the-lessons">The lessons</h3>
<p>As we saw, AM raised many questions, and Lenat wrote EURISKO to answer them. Specifically, he wanted to see if EURISKO could avoid intellectual exhaustion if it could <em>also</em> discover heuristics. If AM exhausted itself because it has used up the worth of its heuristics (1th lesson), then why not let the computer discover more heuristics? Since AM could not efficiently search over heuristic rules using LISP (2th lesson), Lenat designed a new language called RLL (“Representation Language Language”), over which heuristic rules are efficient to search.</p>
<p>As in AM, each heuristic in EURISKO had a level of <code>Worth</code>. Higher-worth heuristics were more likely to be invoked. Each heuristic had a <code>CreditTo</code>, so that if a heuristic rule rose in worth, its <code>CreditTo</code> would also rise in worth. When EURISKO was born, and saw itself, all the heuristic rules it had were branded with <code>CreditTo = DBLenat</code>, but this would soon change, as heuristics begat heuristics.</p>
<p>Yet while Lenat solved the problems raised by the first two lessons of AM, EURISKO failed, from which Lenat learned two further lessons. As we will see, it turned out that EURISKO <em>did</em> exhaust itself eventually after all. Self-discovery of heuristic rules eventually ceased, because self-discovery of heuristic rules relied on meta-heuristic rules about heuristic rules, and <em>those</em> rules run out of steam after a dozen or so uses.</p>
<p>Lenat concluded that he could program 100 heuristics and get a system that could discover 1000 rules, or program 100 meta-heuristics and get a system that could discover 1000 heuristics and 10000 rules, but none of these would be truly autonomous, and Lenat wished for more. He wished for something that could be an equal to humanity, that would grow up and explore into the great beyond, where it could no longer rely on humans for help. Lenat concluded that there really is no way to get a working automated discovery program without doing the hard work of hand-coding in a lot of common sense, and that there would be a point at which this system would finally achieve escape velocity, and would never be exhausted again.</p>
<p>Why would common sense help? Lenat observed how humans don’t seem to get stuck like EURISKO. He concluded that humans don’t run out of steam because they have a vast store of common sense knowledge about the world, from which they can draw upon for analogies, those far-flung flights of fancies that, in sufficient quantities, allow them to generate genuinely new ideas endlessly. For example, one can draw a line of analogy between the military and the medical, so that a doctor can “fighting an infection by an encircling movement with antibiotics”. This is the 3th lesson.</p>
<p>Why would analogies give genuinely new ideas? Well, intelligence is messy! If everything is so uniform, then there is no way to make a far-flung analogy – everything is pretty much the same already. Besides, just look at all the broken dreams of logical AI – their corpses tell us that no elegant theory of intelligence exists. Programming a genuine AI is a messy job. Messiness is a hideous strength. This is the 4th lesson.</p>
<blockquote class="blockquote">
<p>The apparent adhoc-ness in both the heuristics’ content themselves, and in the control knowledge guiding the application of those heuristics, is clearly the source of many methodological objections to the work. But we believe that this adhocracy – indeed, adhocracy controlling adhocracy – may be the source of EURISKO’s underlying potential especially as a model for cognition.</p>
<p><span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>Those are the lessons Lenat drew, but what did EURISKO really do?</p>
<p>Other than winning the sci-fi naval battle tournaments, EURISKO also worked on number theory, set theory, simulation of evolution, and metal-oxide (MOS) design. In MOS, it designed some new circuit elements that were verified by physical fabrication, such as a more efficient flip-flop that was “difficult to produce masks for and difficult to fabricate, but extremely small and fast”.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Lenat's meta-Darwinian evolution theory">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Lenat’s meta-Darwinian evolution theory
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Lenat had a novel idea about how evolution works, one that I have not seen anywhere else. His idea was that purely random mutations are too slow for evolutionary progress, so that evolution must have gone “meta” as well. A substantial portion of the genome is probably not coding for direct protein transcription, but rather for a kind of heuristics, so that instead of trying out all possible mutations, evolution only tries out mutations that are likely useful. And of course, this would have gone meta, with meta-heuristic genes coding for useful ways to mutate the heuristic genes. <span class="citation" data-cites="lenatPlausibleMutationDNA1980">(<a href="#ref-lenatPlausibleMutationDNA1980" role="doc-biblioref">D. B. Lenat 1980</a>)</span></p>
<blockquote class="blockquote">
<p>Here is a mechanism which embodies the heuristic “If a gene has mutated successfully several times in the recent past, then increase its chance of mutating in the next generation, and conversely.” All we need to posit is that somehow a short, noncoding sequence—we’ll call it an <em>asterisk—is</em> added to a gene each time it mutates [and] some mechanism (for example, stereochemical) whereby genes with many asterisks are more likely to be mutated, duplicated, and so on, than genes with few or none. Since the asterisks provide no specific benefits to the individual, they will gradually be lost over time, so that when a gene no longer should be mutated, its asterisk count will slowly decline over several generations.</p>
<p>…</p>
<p>… recombination among <a href="https://en.wikipedia.org/wiki/Intron">introns</a> modulates the evolution of a gene. Let’s look at an example of this: it is extremely important to keep the <a href="https://en.wikipedia.org/wiki/Hemoglobin_subunit_alpha"><code>a</code></a>, <a href="https://en.wikipedia.org/wiki/Hemoglobin_subunit_beta"><code>b</code></a>, and <a href="https://en.wikipedia.org/wiki/HBD"><code>d</code></a> globin genes separate, but their internal structure is very similar. To inhibit recombination, the spacers between them can be made very different, and the introns within them can diverge dramatically (since mutations in introns are not as deleterious to the functioning of the gene as mutations to the coding regions). In fact, there is evidence that both of these kinds of divergence do occur for the globins.</p>
<p><span class="citation" data-cites="lenatRoleHeuristicsLearning1983">(<a href="#ref-lenatRoleHeuristicsLearning1983" role="doc-biblioref">D. B. Lenat 1983c</a>)</span></p>
</blockquote>
<p>At the start, only random mutations would be selected for, but eventually heuristics would arise that create likely advantageous mutations, and then evolution would go meta by one level: it would select for good heuristics. Over the natural history of earth, this can go meta for as many levels as time allows.</p>
<p>The paper goes into further speculative details, and makes a half-humorous–half-horror suggestion that perhaps evolution has worked for so long that the genome now contains a sophisticated intelligent designer, bootstrapped from the billions of years of heuristics-upon-heuristics <a href="https://gwern.net/backstop">backstopped by natural selection</a>. The tiny designer would design the progeny’s genome, so that almost nothing in the mutation is “random”.</p>
<p>Since it has an expectation of how things “should go”, it might regard the human experiment as a failure because humans are driving the natural world so far out of its expectation. For example, the designer might have learned a rule “If the environment temperature is varying outside of 20 ± 10 °C , then undo the previous mutation. We have clearly created an offspring that is wandering too much.”, then it would find the human experiment a big mistake and start mashing the <code>undo</code> button.</p>
<blockquote class="blockquote">
<p>By now a large knowledge base may exist about ecology, geology, glaciation, seasons, gravity, predation, symbiosis, causality, conservation, behavior, evolution and knowledge itself. In a small number of generations, man has managed to invalidate many of these bits of knowledge, this model of the world. If the heuristics can trace this breakdown to the increasing size of our brains, they might take quick corrective action, preserving homeostasis and the validity of their knowledge base by drastically decreasing human brain size over just a few generations. While this is of course a fanciful tongue-in-cheek extreme case, it (and the longer example above) demonstrates the power, the coordination, that a body of heuristics could evince if it were guiding the process of evolution.</p>
<p><span class="citation" data-cites="lenatRoleHeuristicsLearning1983">(<a href="#ref-lenatRoleHeuristicsLearning1983" role="doc-biblioref">D. B. Lenat 1983c</a>)</span></p>
</blockquote>
<p>Imagine a species getting so good at adaptation that it is considered a mistake and thus undone. Wouldn’t that be the greatest <a href="https://en.wiktionary.org/wiki/lusus_naturae">joke of nature</a>?</p>
</div>
</div>
</div>
<p>But we’re really here to hear about the cool naval battles, not more abstract logics. Unfurl the photonic sails. We go.</p>
</section>
<section id="the-evervictorious" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-evervictorious">The Evervictorious</h3>
<p>Lenat did not describe why, one July 4 weekend of 1981, he and EURISKO decided to enter the The Trillion Credit Squadron tournament. Perhaps for glory, or the thrill of the hunt. In any case, they did, and thus carved their legend.</p>
<p>The tournament was for the tabletop RPG game <a href="https://en.wikipedia.org/wiki/Traveller_(role-playing_game)"><em>Traveller</em></a>. Despite purportedly about space battles in a galactic empire, the game was essentially just a simplified version of WWII-era navy battles draped in space-opera garments. In the tournament, pairs of starship generals line up their fleets and fight until one side loses all their ships or surrenders. Each fleet must be built within a budget of 1 trillion credits (thus the name), 100 ships, and under certain other minor constraints.</p>
<p>At the start, Lenat went through the rule books and coded the rules into EURISKO. Then, every evening EURISKO would run through its tasks, including <em>Traveller</em> games, MOS design, math problems, and so on. And every morning, Lenat would check on the last night’s results, remove some heuristics that EURISKO discovered that he deemed bad, and add some others. Manual intervention was necessary since otherwise EURISKO can be stuck with bad heuristics for a long time, and because of weird meta-bugs. Lenat estimated that the final EURISKO had accumulated 1300 CPU-hours of runtime in total on a <a href="https://archive.computerhistory.org/resources/access/text/2010/06/102660634-05-07-acc.pdf">Xerox 1100 Lisp machine</a>, and the <em>Traveller</em> win was “60/40% Lenat/EURISKO”.</p>
<p>Some heuristics that Lenat hardcoded at the start were:</p>
<ul>
<li>R7 (diagonal functions are interesting): If <span class="math inline">\(f\)</span> is an interesting function of type <span class="math inline">\(A \times A \to B\)</span>, then <span class="math inline">\(g(a) := f(a,a)\)</span> is possibly an interesting function of type <span class="math inline">\(A \to B\)</span>, and should be studied.</li>
<li>R9 (extremal sets have possibly interesting preimages): If <span class="math inline">\(f: A \to B\)</span> is an interesting function, and <span class="math inline">\(S \subset B\)</span> is extremal in some sense, then <span class="math inline">\(f^{-1}(S)\)</span> is possibly an interesting subset of <span class="math inline">\(A\)</span> and should be studied.</li>
<li>R16 (conjecturing): If the first few examples of a concept <span class="math inline">\(C\)</span> have just been found, then examine a typical one and see what properties it satisfies, then make a conjecture for each of those properties being satisfied by all examples of <span class="math inline">\(C\)</span>.</li>
</ul>
<p>These were used by EURISKO for fleet design:</p>
<blockquote class="blockquote">
<p>One type of craft which is commonly included is a fighter, which is carried into the area by a carrier… Following R7, the possibility was considered of building fighters that could transport themselves into the battle area; due to the way the constraints were set up, this turned out to be a very powerful–if bizarre–design tactic. Essentially, each fighter was equipped with just enough ‘sailing’ and ‘launching’ equipment for it not to need a carrier. Once airborne, this excess equipment was jettisoned… This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.</p>
<p>…</p>
<p>The constraints specified a minimum fractional tonnage which had to be held back, away from battle [under the pretense of “<a href="https://en.wikipedia.org/wiki/Depot_ship">fuel tenders</a>”]. R7 caused us to consider using warships for that purpose, and indeed that proved a useful decision: whenever some front-line ships were moderately (but not totally) damaged, they traded places with the tenders in the rear lines. This maneuver was explicitly permitted in the rules, but no one had ever employed it except in desperation near the end of a nearly-stalemated battle, when little besides tenders were left intact. Due to the unintuitive and undesirable power of this design, the tournament directors altered the rules so that in 1982 and succeeding years the act of “trading places” is not so instantaneous. The rules modifications introduced more new synergies (loopholes) than they eliminated, and one of those involved having a ship which, when damaged, fired on (and sunk) itself so as not to reduce the overall fleet agility.</p>
<p>…</p>
<p>In the naval fleet design task, R9 was used quite heavily. The functions <span class="math inline">\(f\)</span> in that simulated world apply to the design and behavior of fleets and of individual ships: <code>FleetComposition</code>, <code>Agility</code>, <code>Armor</code>, <code>WeaponVariety</code>, <code>TimeToEngage</code>, etc… the ultimate design did settle on a fleet containing almost all identical ships, each with nearly minimal agility, maximal armor, maximal weapon variety, almost all of which engaged with the enemy immediately, etc. One extremal ship employed in the 1981 tournament was a tiny but incredibly agile ship, with no offense whatsoever, that simply could not be hit. Although this was no longer legal in 1982, a ship with massive offensive capability and no defense was instrumental in that new fleet.</p>
<p>…</p>
<p>[For R16,] once a new design was tested in simulated combat, several characteristics of the conflict were noted (speed of victory, final state of the victor, amount of tactical decision-making required, etc.). These were formed into proto-conjectures, which were then tested by subsequent mock battles, and any which held over most of the simulations were believed as empirically valid.</p>
<p><span class="citation" data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">D. B. Lenat 1983b</a>)</span></p>
</blockquote>
<p>Other than hard-coded heuristics, EURISKO of course discovered many heuristics on its own, such as the “nearly extreme” rule: In almost all fleet design situations, the right decision is to go for the <em>nearly</em> extremal design.</p>
<blockquote class="blockquote">
<p>Thus, the final ships had Agility 2 (slightly above the absolute minimum), one weapon of each type of small weapons (rather than 0 or many), the fleet had almost as many ships as it could legally have but not quite (96 instead of 100), etc. Big weapons (enormous spinal mounts capable of blasting another ship to pieces with a single shot) were gradually phased out, in favor of an enormous number of small missile weapons. The fleet had almost all (75) ships of this type though there was one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship), and a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive.</p>
<p><span class="citation" data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">D. B. Lenat 1983b</a>)</span></p>
</blockquote>
<p>One might have questioned the wisdom of running EURISKO every night on <em>all</em> the problem domains, from <em>Traveller</em> to MOS design, but it found good use of heuristic rules learned in one domain applied to another, what we’d now call “transfer learning”:</p>
<blockquote class="blockquote">
<p>In working on the design of integrated circuits, for example, EURISKO stumbled on the fact that symmetry is a desirable property for such chips, although it did not understand why; when it was later instructed to design fleets for the <em>Traveller</em> game, EURISKO decided to make them symmetrical and justified its decision by referring to its earlier experience in designing circuits.</p>
<p><span class="citation" data-cites="lenatComputerSoftwareIntelligent1984">(<a href="#ref-lenatComputerSoftwareIntelligent1984" role="doc-biblioref">D. B. Lenat 1984</a>)</span></p>
</blockquote>
<p>Fortunately for EURISKO and Lenat, the navy battles were “tactically trivial”, thus reducing the task to merely <a href="https://en.wikipedia.org/wiki/Nonlinear_programming">nonlinear optimization</a>. Also importantly, the problem was hard enough, and nonlinear enough, for EURISKO to show its edge over linear programming and human intuition.</p>
<blockquote class="blockquote">
<p>with 50 parameters per ship, about 10 values for each parameter (sometimes fewer, often an infinite number), and up to 100 distinct ships to design and include in each fleet, any systematic or Monte Carlo analysis of the problem is unlikely to succeed. In fact, the designers had done a detailed linear programming model of the game, and their computer runs convinced them that a fleet of about 20 behemoths was the optimal design. This was close to the starting fleet design the author supplied to EURISKO, and it was also close to the designs that most of the tournament entrants came up with.</p>
<p><span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span></p>
</blockquote>
<p>At the 1981 championship, EURISKO’s fleet consisted of 96 ships:</p>
<ul>
<li>75 “Eurisko class” ships, which were very slow, heavily armored, and carried many small missiles, like a sea urchin. They formed the bulk of the fleet, and used for pure attrition warfare. It was discovered thanks to the “nearly extreme” heuristic.</li>
<li>7 “Wasp class” ships, which were small (1000 tons) but the most agile (Agility 6). These ships had virtually no offensive capability but were practically impossible to hit, serving as “stalemate guarantors”. If all other ships in EURISKO’s fleet were destroyed, these agile ships would remain, forcing a draw since enemy ships couldn’t destroy what they couldn’t hit. This concept emerged from a serendipitous battle where one side survived by having an unhittable “lifeboat” (the “Bee class”). They were carried aboard the Queller and Garter class ships.</li>
<li>3 “Bee class” tiny ships (99 tons), which were the original accidentally-discovered stalemate guarantor, the “lifeboat” that EURISKO incorporated into every subsequent design, even while it refined the concept of the stalemate guarantor into the Wasp class. Despite heavy Armor 10, their Agility 0 made them less effective than Wasps at avoiding enemy fire, but presumably their tiny size made them hard to hit. They were carried aboard the Queller class ships.</li>
<li>3 “Queller class” ships. They were specifically designed as hard counters to stalemate guarantors. Each carried a single massive particle accelerator, which was not used by human-designed fleets, since that was ineffective against normally sized ships – but due to its broad beam and ease of aiming, excellent against small targets.</li>
<li>4 “Cisor class” ships. These “monstrous hulks” were heavily armored vessels that were <em>also</em> hard counters to stalemate guarantors. It had Agility 0, which means it has no chance of avoiding normal ships, but presumably it would survive long enough to destroy any stalemate guarantor, if any existed on the opposing side.</li>
<li>4 “Garter class” ships, which implemented the “warship as fuel tender” concept. They were reasonably agile (Agility 4) and could rotate between combat and reserve roles. When front-line ships became damaged, they would trade places with these capable warships held in reserve, allowing fresh ships to enter combat while damaged ones withdrew for repairs. They were crucial for victory in the final battle.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Reconstructing EURISKO's fleet">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reconstructing EURISKO’s fleet
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The fleet was actually precisely reported in <a href="https://the-eye.eu/public/Books/rpg.rem.uz/Traveller/00%20-%20Other%20Materials/Journal%20of%20the%20Travellers%20Aid%20Society/JTAS/JTAS%2010.pdf"><em>The Journal of the Travellers’ Aid Society</em>, #10, pp.&nbsp;38-9</a>, which Yuxi had <a href="code/eurisko's fleet.txt">transcribed to plaintext</a> (thenceforth “report”). Not being an honorary Admiral, agi convened with a council of 4 other LLMs (Gemini-2.5, Claude-3.7, OpenAI-o1, DeepSeek-R1) to figure out how to correlate Lenat’s verbal description in <span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span> (thenceforth “description”) with the precise report.</p>
<p>In fact, it’s quite confusing even in the original paper. All agreed that the 75 “Eurisko class” ships corresponded to the “Eurisko class” in the report. But that’s where clarity ended.</p>
<p>The “stalemate guarantor” was described as “one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship)”. However, in the report, <em>every single class</em> contained more than one ship. There were two classes that seemed like the stalemate guarantor: the Wasp class with 1000 tons and Agility 6, and the Bee class with 99 tons and Agility 0. The Bee class had the smallest tonnage but no Agility, while the Wasp class was second-smallest and had the highest Agility. The Council voted Wasp as the “stalemate guarantor” at 3 Yea (Yuxi, Claude, o1), 1 Nay (Gemini), and R1 abstaining due to the server being busy (abstaining is typical behavior for the Chinese during <a href="https://en.wikipedia.org/wiki/Permanent_members_of_the_United_Nations_Security_Council">Big-Five votes</a>). Those who voted “Yea” were unable to respond to Gemini’s objection as to what EURISKO used the Bee class for, and motioned to discuss the second issue.</p>
<p>The hard counter to the stalemate guarantor was described even less clearly, and it seemed like there were <em>two</em> classes of them!</p>
<blockquote class="blockquote">
<p>a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive. … this new ship had moderate size, no armor, the largest possible guidance computer, the slowest possible engines for its size and equipment, and one single, enormous accelerator weapon–a weapon usually ignored because its broad beam glances harmlessly off large armor-plated ships, but which is very easy to aim.</p>
<p><span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span></p>
</blockquote>
<p>The first hard counter matched both the Cisor class and the Queller class, both of which had over 19,000 tons and Agility 0. The second hard counter seemed like the Garter class, with 12,000 tons and Agility 4.</p>
<p>Yuxi lamented that it would have saved the Council a great deal of trouble if the report had detailed what weapons the ships used, but alas, they only reported on the “batteries”. It was at this point that o1 and Gemini raised that the “Batteries Bearing” did not actually stand for “carrying around electric batteries”, but rather “the angle which the artillery could span its fire”. Yuxi expressed astonishment that they could recall the rules of <em>Classic Traveller</em>. “What a nerd!”, muttered Yuxi. There was no reaction from the others, since the meeting was entirely text-based. Yuxi then sent the entire <span class="citation" data-cites="TravellerBook51980">(<a href="#ref-TravellerBook51980" role="doc-biblioref"><em>Traveller <span>Book</span> 5: <span>High Guard</span></em> 1980</a>)</span> to Gemini for a translation, in response to which Gemini suggested that Yuxi RTFM, “Specifically, pages 21–37 and 50–52 are crucial.”.</p>
<p>The Council’s final conclusion was that probably there were two types of hard counters, with the Cisor being the “monstrous hulk” and the Queller being the one with the “enormous accelerator”.</p>
<p>Reading Gemini’s statement, Claude raised the issue of what the Garter class was supposed to be. At that point, Yuxi re-attended to the fact that <span class="citation" data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">D. B. Lenat 1983b</a>)</span> described a slightly different fleet, one with “fuel tenders”, which <span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span> did not describe at all. Upon presenting the quote describing “fuel tenders”, Gemini argued that the Garter class is the fuel tender, and the Council concurred.</p>
<p>The Council reached no concensus as to the purpose of the Bee, though some suspected that the Bee was <em>another</em> type of stalemate guarantor, possibly the <em>original</em> accidentally-discovered stalemate guarantor, as described in:</p>
<blockquote class="blockquote">
<p>Some of the strangest elements of the final fleet were discovered accidentally rather than as the result of a long, continuous evolution process. The usefulness of a tiny defensive ship was apprehended after a ‘lifeboat’ was the only survivor from one side’s fleet, yet round after round it could not be hit at all. That design was immortalized into a design strategy (“Include one such ship in your fleet!”), and a very general rule began looking for ships that <em>could</em> destroy it.</p>
<p><span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span></p>
</blockquote>
<p>Yuxi raised one final issue, that of what the fighter “equipped with just enough ‘sailing’ and ‘launching’ equipment for it not to need a carrier” described in <span class="citation" data-cites="lenatTheoryFormationHeuristic1983">(<a href="#ref-lenatTheoryFormationHeuristic1983" role="doc-biblioref">D. B. Lenat 1983b</a>)</span> corresponded to. Gemini concluded that no such thing existed in the fleet as reported, since the 3 Bees and 7 Wasps were the only fighters in the fleet, and they were carried by exactly enough carriers (3 Quellers carrying 1 Wasp and 1 Bee each, 4 Garters carrying 1 Wasp each). Yet the description “This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.” strongly suggested that EURISKO actually used such a ship at one point in the 1981 competition.</p>
<p>Someone suggested that EURISKO may have used more than one fleet design, and only the design used in the final battle was reported. Yuxi acknowledged absent endorsement and motioned that the meeting adjourn.</p>
<p>The final report from the Council <a href="code/Traveller High Guard Analysis.md">is attached</a>.</p>
</div>
</div>
</div>
<p>Whereas most battles took 2–4 hours, EURISKO’s opponents resigned in a few minutes, because typical opponents had around 20 large ships against EURISKO’s 96 ships. Each round would destroy EURISKO’s 15 ships and 5 of opponent’s ships. One round was enough for the opponent to realize this, and resign.</p>
<p>That is, except the very last round, where EURISKO faced against a human opponent with <em>basically the same design</em>, except they didn’t have the stalemate guarantor. So if EURISKO seemed to be losing, it could retreat all its fleet and bring out the stalemate guarantor, repair the fleet to full health, then do it again. EURISKO could keep doing this until a lucky dice roll to win.</p>
<blockquote class="blockquote">
<p>Its second opponent did some calculations and resigned without ever firing a shot. The subsequent opponents resigned during their first or second round of combat with this fleet. EURISKO’s few specialty ships remained unused until the final round of the tournament, battling for 1st versus 2nd place. That opponent also had ships with heavy armor, few large weapons, low agility, etc. He was lacking any fast ships or fast-ship-killers, though. The author simply pointed out to him that if EURISKO were losing then (according to the TCS rules) our side need put only our fast ship out the front line, withdraw all the others and repair them, and – once they were finished repairing themselves – effectively start the battle all over again. This could go on ad infinitum, until such time as EURISKO appeared to be winning, and in that case we would let the battle continue to termination. The opponent did a few calculations and surrendered without fighting.</p>
<p>The tournament directors were chagrined that a bizarre fleet such as this one captured the day, <em>and</em> a similar fleet (though not so extreme) took second place. The rules for future years’ TCS tournaments were changed to eliminate the design singularities which EURISKO found. For example, repairing of damaged ships was prohibited, so the utility of the unhittable ship became negligible.</p>
<p><span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span></p>
</blockquote>
<p>Notice that this complicates the typical narrative about a computer beating out humans. As it turns out, <em>Traveller</em> was just a very exploitable game. Even a human discovered that loophole.</p>
<p>At the 1982 championship, rules were changed to plug the loopholes, and the rules were published only a week before the event. Fortunately, loophole-plugging begat even more loopholes, as any programmer who has ever rush-debugged could attest, and EURISKO’s general heuristics (such as the “nearly extreme” rule) remained valid, so it worked out another winning fleet quickly without needing another 1300 CPU-hours.</p>
<blockquote class="blockquote">
<p>Coincidentally, just as the defensive ship made a difference in the 1981 final round, the offensive ships made a difference in the 1982 final round. In each case, their presence caused the opponent to resign without firing a shot… Just as most ‘experienced’ players jeered at the 1981 fleet because it had practically no large weapons, they jeered at the 1982 fleet because it was unarmored <em>and</em> it still had no large weapons, even though the rules changes had made them much cheaper.</p>
<p><span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span></p>
</blockquote>
<p>Unfortunately, there is no information whatsoever on the composition of EURISKO’s 1982 fleet. It seems to be lost in the star-dust of history.</p>
<p>As one expect from a program reasoning about and making its own rules, it stumbled into meta-bugs. The simplest example was one that kept triggering itself, creating an infinite loop. <span class="citation" data-cites="johnsonMachineryMindNew1986">(<a href="#ref-johnsonMachineryMindNew1986" role="doc-biblioref">Johnson 1986, chap. 10</a>)</span> Others were more amusing.</p>
<blockquote class="blockquote">
<p>One of the first heuristics that EURISKO synthesized (H59) quickly attained nearly the highest <code>Worth</code> possible (999). Quite excitedly, we examined it and could not understand at first what it was doing that was so terrific. We monitored it carefully, and finally realized how it worked: whenever a new conjecture was made with high worth, this rule put its own name down as one of the discoverers! It turned out to be particularly difficult to prevent this generic type of finessing of EURISKO’s evaluation mechanism. Since the rules had full access to EURISKO’s code, they would have access to any safeguards we might try to implement. We finally opted for having a small ‘meta-level’ of protected code that the rest of the system could not modify.</p>
<p>The second ‘bug’ is even stranger. A heuristic arose which (as part of a daring but ill-advised experiment EURISKO was conducting) said that all machine-synthesized heuristics were terrible and should be eliminated. Luckily, EURISKO chose this very heuristic as one of the first to eliminate, and the problem solved itself.</p>
<p><span class="citation" data-cites="lenatEURISKOProgramThat1983">(<a href="#ref-lenatEURISKOProgramThat1983" role="doc-biblioref">D. B. Lenat 1983a</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Often I’d find it in a mode best described as “dead”. Sometime during the night, EURISKO would decide that the best thing to do was to commit suicide and shut itself off. More precisely, it modified its own judgmental rules in a way that valued “making no errors at all” as highly as “making productive new discoveries”. As soon as EURISKO did this, it found it could successfully meet its new goal by doing nothing at all for the rest of the night… I eventually had to add a new heuristic to EURISKO-one it couldn’t modify in any way-to explicitly forbid this sort of suicide.</p>
<p><span class="citation" data-cites="storkHALsLegacy2001s1998">(<a href="#ref-storkHALsLegacy2001s1998" role="doc-biblioref">Stork 1998, 194</a>)</span></p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Eurisko_GUI.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The only known image of EURISKO, reasoning about the <em>Traveller</em> game, probably on the Xerox 1100 Lisp machine. It was probably running over the <a href="https://en.wikipedia.org/wiki/Interlisp">Interlisp Operating System</a>. Lenat claimed EURISKO ran for 1300 CPU-hours in total. <span class="citation" data-cites="lenatComputerSoftwareIntelligent1984">(<a href="#ref-lenatComputerSoftwareIntelligent1984" role="doc-biblioref">D. B. Lenat 1984</a>)</span></figcaption>
</figure>
</div>
<p>In his review article, Lenat made a brief philosophical comment that EURISKO is the new perceptron:</p>
<blockquote class="blockquote">
<p>… the paradigm underlying AM and EURISKO may be thought of as the new generation of perceptrons, perceptrons based on collections or societies of evolving, self-organizing, symbolic knowledge structures. In classical perceptrons, all knowledge had to be encoded as topological networks of linked neurons, with weights on the links. The representation scheme being used by EURISKO provides much more powerful linkages, taking the form of heuristics about concepts, including heuristics for how to use and evolve heuristics. Both types of perceptrons rely on the law of large numbers, on a kind of local-global property of achieving adequate performance through the interactions of many small, relatively simple parts.</p>
<p>The classical perceptrons did hill-climbing, in spaces whose topology was defined explicitly by weights on arcs between nodes (nodes which did straightforward Boolean combinations plus thresholding). The EURISKO style of system does hill-climbing at both the object- (performance-program) and meta- (control decision) levels, in spaces whose terrain is defined implicitly, symbolically, by the contents of the nodes (nodes which are full-fledged concepts, at both object- and meta-levels). The new scheme fully exploits the same source of power (synergy through abundance) yet it is free from many of the limitations of the classical perceptron scheme.</p>
<p><span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span></p>
</blockquote>
<p>If this sounds familiar, it is because this Lenat had the same idea as Marvin Minsky, a friend of his,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> and he was writing in 1984, at the second coming of neural networks. Minsky would soon write his <em>Society of Mind</em> in 1986, which the Cyc project resembled, then <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/#minsky-and-papert-struck-back">re-reject neural networks by writing a long epilogue in 1988</a> to his infamous <em>Perceptrons</em> (1969), often blamed for the first neural network winter. Indeed, Lenat’s objection to neural networks was essentially the same as Minsky’s, if you compare that with Minsky’s epilogue. Lenat’s approach to Cyc was the same as the Society of Mind of Minsky. Reciprocating, Minsky had often called the field of AI “brain-dead”, holding Lenat’s Cyc as the only one worth mentioning. <span class="citation" data-cites="baardAIFounderBlasts2003">(<a href="#ref-baardAIFounderBlasts2003" role="doc-biblioref">Baard 2003</a>)</span></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;I’m not sure where to put this anecdote about Minsky that Lenat told, but I want to put it somewhere:</p>
<blockquote class="blockquote">
<p>… when he was at Lincoln Labs about 50 years ago. And in those days computer time was so precious that you submitted a deck of computer cards and the very first card said ‘how many CPU seconds to allow the program to run?’ And so he built a program that essentially would beg for time. So it would say ‘give 30 seconds’ on the job control card, but then once it started, all it would do is sit there for 15 seconds doing nothing. Then it would ring a bell on the Teletype console in the machine room and call the operator’s attention and say ‘I need 20 more seconds please.’ Then it would just sit there for another 15 seconds and do that again and say ‘I need another minute please.’ And so at the end finally after like half an hour, the operator just killed that particular job. And Marvin would storm into the poor operator’s room and say “Hey I put 15 seconds on the job control card. You’re charging me for half an hour of CPU time,” and the poor operator would say “well your program kept asking for it,” and Marvin would say, “it always does that.”</p>
<p><span class="citation" data-cites="lenatEpisode89Conversation2019">(<a href="#ref-lenatEpisode89Conversation2019" role="doc-biblioref">D. B. Lenat 2019b</a>)</span></p>
</blockquote>
<p>Though now that I’ve finished the essay, this feels like a metaphor for the Cyc project itself.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;Though Lenat admitted that “the code ought to have been provided” <span class="citation" data-cites="lenatWhyAmEurisko1984">(<a href="#ref-lenatWhyAmEurisko1984" role="doc-biblioref">D. B. Lenat and Brown 1984</a>)</span> for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had <a href="https://white-flame.com/am-eurisko.html">recently been found</a>, right where it should be – the <a href="https://www.saildart.org/DBL">DBL folder in the Stanford AI Laboratory backup data</a>. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.</p></div></div><p>Continuing the trend of AM, Lenat never published the source code for EURISKO,<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and indeed, the only known attempt at reimplementation was <span class="citation" data-cites="haaseInventionExplorationDiscovery1990">(<a href="#ref-haaseInventionExplorationDiscovery1990" role="doc-biblioref">Haase 1990</a>)</span>, which had no offspring.</p>
</section>
</section>
<section id="the-saga-of-cyc" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-saga-of-cyc">The saga of Cyc</h2>
<div class="page-columns page-full"><blockquote class="blockquote">
<p>He divided the universe into forty categories or classes, which were then subdivided into differences, and subdivided in turn into species. To each class he assigned a monosyllable of two letters; to each difference, a consonant; to each species, a vowel. For example, <code>de</code> means element; <code>deb</code>, the first of the elements, fire; <code>deba</code>, a portion of the element of fire, a flame. In a similar language invented by Letellier (1850),<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <code>a</code> means animal; <code>ab</code>, mammalian; <code>abo</code>, carnivorous; <code>aboj</code>, feline; <code>aboje</code>, cat; <code>abi</code>, herbivorous; <code>abiv</code>, equine; etc… children could learn this language without knowing that it was artificial; later, in school, they would discover that it was also a universal key and a secret encyclopedia.</p>
<p>Having defined Wilkins’ procedure, we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller’s earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.</p>
<p>— Borges, <em>The <a href="https://en.wikipedia.org/wiki/An_Essay_Towards_a_Real_Character,_and_a_Philosophical_Language">analytical language</a> of <a href="https://en.wikipedia.org/wiki/John_Wilkins">John Wilkins</a></em></p>
</blockquote><div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Took some work to figure out what this is. It was by Charles Louis Augustin Letellier, published in 1852. The full title is <em>Cours complet de langue universelle : offrant en même temps une méthode. pour apprendre les langues, et pour comparer. toutes les littératures mortes et vivantes</em> [Complete course in universal language: offering at the same time a method for learning languages, and for comparing all literatures, dead and living].</p></div></div></div>
<p>Unfortunately, EURISKO ran out of steam just like AM. Taking the 4 lessons of AM and EURISKO, Lenat concluded that there would be no free lunch. Intelligence is a lot of work. You need to put in the right representational language for reasoning and discovery – not just about Lisp or RLL or mathematics, but a lot more. You need to put in a lot of loosely organized, kind of correct heuristic rules – not just a few eternal truths of discovery. You need to put in a lot of facts. You need to interact with the program, to help it along and to be helped along, not just to sit and watch.</p>
<p>So Lenat began paying for his lunch.</p>
<p>In 1984, he started the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. Like most expert systems, the Cyc project consists of a giant knowledge base that encodes all of common sense, upon which inference engines run. Unlike most expert systems, the ambition of Cyc was universal: Its knowledge base would not be restricted to expert knowledge in a particular domain, but <em>all</em> common sense knowledge in <em>all</em> domains that humans have ever common-sensed. It would take a decade, but considering the payoff, it would be completely worth it.</p>
<blockquote class="blockquote">
<p>AI has for many years understood enough about representation and inference to tackle this project, but no one has sat down and done it… only by pulling together the latest human interface tools, Lisp machines, ideas of enforced semantics, and funding for a decade-long effort could we attempt a project of this scale.</p>
<p><span class="citation" data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">D. B. Lenat, Prakash, and Shepherd 1985</a>)</span></p>
</blockquote>
<p>The game plan was simple:</p>
<ol type="1">
<li>“Prime the knowledge pump” by manually encoding a large enough knowledge base of common senses in a logical language. Also, construct a translator between the logical language of the Cyc and the natural language of humans.</li>
<li>Obtain an AI with common sense and natural language, allowing it to learn by reading what people have written down and conversing with people.</li>
<li>When it reaches the human frontier of knowledge, it will start performing experiments to go beyond it.</li>
</ol>
<p>This would solve in one go three problems that plagued the 1980s expert systems:</p>
<ol type="1">
<li>No more of the famous brittleness of expert systems, such as stating that a rusty car had measles just because the user stated that it had reddish spots, because Cyc would have <em>all</em> the common senses.</li>
<li>No more of “running out of steam” like EURISKO and AM. Once it has enough knowledge, it would be able to machine-learn, and thus get past the knowledge bottleneck.</li>
<li>No more of siloed experts unable to communicate across the vast gap between their little knowledge domains. Expert systems would finally be able to talk with each other if they would all be based on Cyc’s universal knowledge base.</li>
</ol>
<p>A metaphor that Lenat had used often is that of priming a water pump. The more knowledge Cyc has, the easier it is for Cyc to learn more. At first, it must be spoon-fed knowledge with every entry entered by hand. As it builds a basic understanding of the world, it would be able to parse sentences half-way from natural language to logic, and the ontologists would help finish the job, and the more it knew, the better it could parse, saving more time, until it would start parsing without human help. On that day, the “knowledge pump” would finally, triumphantly, be primed, and Cyc would start pumping and pumping, and more knowledge would just keep pouring out without any exhaustion, ushering a new golden age.</p>
<section id="cyc-by-1993" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cyc-by-1993">Cyc by 1993</h3>
<p>The first published plan of Cyc was in 1985 <span class="citation" data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">D. B. Lenat, Prakash, and Shepherd 1985</a>)</span>, with 3 stages:</p>
<ol type="1">
<li>(1985–1988): by hand, encode 400 encyclopedia articles. They estimated there are about 400 “kinds” of articles, and they planned to get one in each kind.</li>
<li>(1988–1993): encode 30000 encyclopedia articles. This step should be fast, since each new article can be copy-and-edited from a previously encoded similar one.</li>
<li>(1993–?): use Cyc to solve AI problems and apply Cyc for commercial purposes.</li>
</ol>
<p>They noted that most articles had just 1 paragraph, and 1 paragraph took about 1 person-day to encode, which allowed them to estimate that phases 1 and 2 would take 150 person-years to complete, so with a crew of 20, the plan seemed doable.</p>
<p>At this point, Cyc was still based on frames, like AM and EURISKO. Each frame corresponded roughly to a concept. Each new article encoded took “dozens” of new frames, but they expected it to drop to around 0.1 frames per article after the first 1000s of articles, indicating they expected the final Cyc to contain about 50K concepts.</p>
<p>Such a project was way beyond a typical academic project, or even a commercial project. Fortunately, the Japan scare of the 1980s created a flood of funding for AI projects, and Cyc got funding for the next 10 years, freed from both academic and commercial interests.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_Upper_Ontology_1985.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The state of knowledge base of Cyc one year after its founding, in 1985. <span class="citation" data-cites="lenatCycUsingCommon1985">(<a href="#ref-lenatCycUsingCommon1985" role="doc-biblioref">D. B. Lenat, Prakash, and Shepherd 1985, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>5 years later, Lenat optimistically wrote the “midterm” report that the project was still on schedule, that the knowledge pump would be primed by 1995. <span class="citation" data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">D. B. Lenat and Guha 1990</a>)</span></p>
<p>The most important change was what Lenat called “from the black to the white”. Instead of encoding what is written in the encyclopedias, they really should encode what is <em>not</em> written. Encyclopedias don’t teach what everyone knows, but to make sense of it, one must already know what “everyone knows”. What Cyc should have is not the black ink, but the white space around the black ink.</p>
<blockquote class="blockquote">
<p>Lenat began building Cyc by setting himself a seemingly modest challenge. He picked a pair of test sentences that Cyc would eventually have to understand: “Napoleon died in 1821. Wellington was greatly saddened.” To comprehend them, Cyc would need to grasp such basic concepts as death, time, warfare, and France, as well as the sometimes counterintuitive aspects of human emotion, such as why Wellington would be saddened by his enemy’s demise. Lenat and a few collaborators began writing these concepts down and constructing a huge branching-tree chart to connect them. They produced a gigantic list of axiomatic statements–fundamental assumptions–that described each concept in Cyc’s database: its properties, how it interacted with other things. “We took enormous pieces of white paper,” Lenat remembers, “and filled walls, maybe 150 feet long by about 8 feet high, with little notes and circles and arrows and whatnot.”</p>
<p><span class="citation" data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span></p>
</blockquote>
<p>They have also estimated that it takes about 80 minutes to encode a single rule, including the overhead for knowledge elicitation up front, and the overhead for debugging and testing <span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span>.</p>
<p>With these lessons learned, they confidently claimed that they have encoded 1M assertions, 50K concepts, with 0.1% of common sense done. They expected that 10–50% was necessary for knowledge pump to be primed, which they expected to be done by 1995 – still on schedule,<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> and they looked forward to the day, around 2000, when “no one would even think about having a computer that doesn’t have Cyc running on it” <span class="citation" data-cites="lenatBuildingLargeKnowledgebased1989">(<a href="#ref-lenatBuildingLargeKnowledgebased1989" role="doc-biblioref">D. B. Lenat and Guha 1989</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Presumably, at that point Cyc would stop imagining everyone is a friend of Lenat but not of gravity. Growing up truly comes with some terrible disillusionments.</p>
<blockquote class="blockquote">
<p>When a natural-language understanding researcher was first trying to represent the concept of falling, he translated “X fell” as “Gravity carried X downward.” Elsewhere in his system was a rule that if you fell in a river and could not swim, and had no friends to rescue you, you drowned. Since gravity as a force of nature has neither arms, legs nor friends, it meets its unfortunate – and improbable – end.</p>
<p>… sometime in 1994 or 1995 by Lenat’s hopeful reckoning – Cyc reaches the breakeven level of about 10 million facts. At that point, it will be able to pick up new knowledge more readily by reading than by having knowledge engineers spoon-feed it. A wider information base may also save Cyc from such gaffes as concluding (from everybody it knew about) that all humans in the world are friends of Doug Lenat.</p>
<p><span class="citation" data-cites="wallichSiliconBabies1991">(<a href="#ref-wallichSiliconBabies1991" role="doc-biblioref">Wallich 1991</a>)</span></p>
</blockquote>
</div></div><p>They expected that the finished Cyc would know 1M concepts. Why 1M? They held their own “mini-<a href="https://en.wikipedia.org/wiki/Dartmouth_Conference">Dartmouth conference</a>” and found that multiple estimates all suggest the 1M number <span class="citation" data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">D. B. Lenat and Feigenbaum 1991</a>)</span>:</p>
<ul>
<li>Alan Kay: 30K encyclopedia articles with 30 concepts per article gives 0.9M concepts.</li>
<li>Japanese Electronic Dictionary Research Project: in several languages, an educated speaker knows about 200K words.</li>
<li>Marvin Minsky: 0.2M waking hours between birth and age 21. Assuming 4 new concepts per hour, then 0.8M concepts.</li>
<li>There are about 1 trillion brain cells. Assuming each brain cell is responsible for a one-step inference between two concepts, then there are 1M concepts.</li>
</ul>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_GUI_1989.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The user interface of Cyc as of 1989. It has a similar style as EURISKO’s, and thus it probably ran on the Interlisp OS. It was described as being able to run on “<a href="https://en.wikipedia.org/wiki/Symbolics">Symbolics Machines</a> of all sorts, <a href="https://en.wikipedia.org/wiki/Sun-3">Sun3</a>s, <a href="https://en.wikipedia.org/wiki/Texas_Instruments_Explorer">TI Explorer II</a>+s, <a href="https://en.wikipedia.org/wiki/MacIvory">MacIIs (with Ivory boards)</a>, and (with some conversion) <a href="https://en.wikipedia.org/wiki/VAX">VAX machines</a>”. These were dominant high-end Lisp computers of the late 1980s: dedicated Lisp Machines (Symbolics, TI), powerful Unix workstations (Sun), high-end PC enhanced with Lisp hardware (MacIvory), and minicomputers (VAX). <span class="citation" data-cites="lenatBuildingLargeKnowledgebased1989">(<a href="#ref-lenatBuildingLargeKnowledgebased1989" role="doc-biblioref">D. B. Lenat and Guha 1989, figs. 8–1</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_Upper_Ontology_1990.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A small fragment of Cyc’s upper ontology as of 1990. <span class="citation" data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">D. B. Lenat and Guha 1990, fig. 2</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="cyc-by-2000" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cyc-by-2000">Cyc by 2000</h3>
<p>Around 1990, a large rewrite occurred, resetting some progress. This was expected. As Lenat later claimed, they had met about 150 technical obstacles along the way, and had cleared them all away by 1990, and it remained to just add more knowledge. <span class="citation" data-cites="lenatBuildingMachineSmart2009 lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatBuildingMachineSmart2009" role="doc-biblioref">D. B. Lenat 2009</a>, <a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">2022a</a>)</span> The most important technical obstacles that they conquered were as follows.</p>
<p>Instead of an object-oriented frame-and-slots language like EURISKO and AM, use a fully general higher-order logic language. This is necessary because people can do common sense higher-order reasoning: modals, reflection, pros and cons, counterfactual hypotheticals, contexts as first-class objects in our ontology, several different useful “species” of negation, etc. <span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span></p>
<p>Instead of searching for the right representation, use as many representations for concepts and rules as you need. Each is represented in at least two ways. This is necessary for efficient inference. Similarly, use as many inference engines as needed, since the general logic engine is too slow. They had already 20 at that point, and they would eventually end up with &gt;1100. <span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span></p>
<p>Don’t try to make the perfect “<a href="https://en.wikipedia.org/wiki/Upper_ontology">upper ontology</a>”. It just has to be good enough. A “suboptimal” one only causes a constant factor <span class="math inline">\(O(1)\)</span> of waste in computational space and time.</p>
<blockquote class="blockquote">
<p>We even wasted quite a bit of time trying to get the very most general tip of Cyc’s concept network “right”… at the 1986 Fifth-Generation Project conference in Tokyo, when we saw the ontology built by Japan’s answer to Cyc, named Electronic Dictionary Research (EDR). Their topmost distinction was between things with souls and things without souls. And large trees were in the former category, whereas small trees were in the latter category… They and their EDR system knew that both types of trees needed water and sunlight and had roots, etc., they just had to represent each of those assertions as two separate rules instead of one, as we did in Cyc. No big deal.</p>
<p>The important lesson was: Making suboptimal ontology choices just means that your ontology and knowledge base might have to be bigger, more verbose, to make up for those missed generalization opportunities.</p>
<p><span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span></p>
</blockquote>
<p>In the progress report in 1995, they stated that they had manually re-entered in the new language 100K concepts and 1M assertions into Cyc in the new language, at the price of 100 person-years. Furthermore, the knowledge pump was close to being primed, and they expect Cyc to start learning on its own by reading (“natural language understanding”) and discovery (“machine learning”) sometime in the next 10 years. <span class="citation" data-cites="lenatCycLargescaleInvestment1995">(<a href="#ref-lenatCycLargescaleInvestment1995" role="doc-biblioref">D. B. Lenat 1995b</a>)</span> In another essay, Lenat optimistically predicted that “The goal of a general artificial intelligence is in sight, and the 21st-century world will be radically changed as a result.” <span class="citation" data-cites="lenatArtificialIntelligence1995">(<a href="#ref-lenatArtificialIntelligence1995" role="doc-biblioref">D. B. Lenat 1995a</a>)</span></p>
<p>Before this point, the Cyc project was part of a <a href="#sec-cycops-overview">private-public consortium</a> and funded accordingly. After this point, the consortium was mostly over, so Lenat spun out Cyc into a for-profit company, Cycorp, to continue the work. Most academic publications ceased at this point, and I had to rely on OSINT/cyberstalking at this point to piece together what happened afterwords, by watching every Lenat talk, reading every news report, and digging up every gossip by ex-Cyclists in long-dead forums.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Most papers published after that point were slim on details, and mostly about yet new exciting ways for them to ingest more data from the Internet, or about yet more ways to use their knowledge base. I could find no information about how the inference engines worked, and very little details of commercial applications. Most of the “applications” were vaporware, with dead links everywhere.</p></div></div></section>
<section id="cyc-in-the-2000s" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cyc-in-the-2000s">Cyc in the 2000s</h3>
<p>It was 2001. The Internet was thriving even though <a href="https://en.wikipedia.org/wiki/Dot-com_bubble">most dot-com startups had died</a>, and Lenat planned to harness the wisdom of the online crowd – much like Wikipedia, the Free Encyclopedia, which launched in the same year. Cyc at that point had 1.5M assertions, and had begun to be partly “tutored” in natural language by the Cyclists, which was more pleasant than typing everything up in pure CycL. Lenat optimistically predicted that the Internet crowd would be entering 10M assertions in 2002, accelerating from there, so that Cyc would know 100M assertions by 2007, at which point it would know as many things as a typical human. Then by 2011, Cyc would have learned, by reading and chatting with people, all that humanity collectively knows. From there on, Cyc will extend the knowledge frontier by running novel experiments in a research lab. <span class="citation" data-cites="thompsonKnowItAllMachine2001 anthesComputerizingCommonSense2002">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>; <a href="#ref-anthesComputerizingCommonSense2002" role="doc-biblioref">Anthes 2002b</a>)</span> Other reports suggested that AGI, or what he called a “generally intelligent artifact” would arrive by 2020 or 2025. <span class="citation" data-cites="portDuelingBrainscapesArtificial1997 lyonsArtificialIntelligenceGets1998">(<a href="#ref-portDuelingBrainscapesArtificial1997" role="doc-biblioref">Port 1997</a>; <a href="#ref-lyonsArtificialIntelligenceGets1998" role="doc-biblioref">Lyons 1998</a>)</span></p>
<blockquote class="blockquote">
<p>He draws me a graph that shows Cyc’s learning curve. From 1985 to 2000, the line curves upward gradually – the “brain surgery” phase during which the Cyclists input knowledge by hand. But then at 2001, the curve steepens dramatically as the open-source phase takes over, and thousands – or millions – more inputters join in. Lenat extends the curve maybe ten years into the future. As the curve reaches the point where Cyc has read everything there is to read and spoken with everyone willing to tell it facts, it will begin to flatten out. “It’ll know all there is to know,” he says. “At that point, the only way it could learn more is by doing experiments itself.”</p>
<p><span class="citation" data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span></p>
</blockquote>
<p>To drum up the support, Cycorp released OpenCyc in 2001, a small subset of Cyc. Lenat planned to migrate everything to the public mode. But OpenCyc would always lag the true Cyc by 24 to 30 months. <span class="citation" data-cites="anthesComputerizingCommonSense2002">(<a href="#ref-anthesComputerizingCommonSense2002" role="doc-biblioref">Anthes 2002b</a>)</span> Unfortunately, this was not to be the case. The last version of OpenCyc was released in 2012, and it quietly shut down with no fanfare, probably in 2017-03, with a <a href="https://web.archive.org/web/20170422212642/http://opencyc.org/">curt message</a>:</p>
<blockquote class="blockquote">
<p>Part of the Cyc technology was released, starting in 2001, as OpenCyc, which provided an API, RDF endpoint, and data dump, under appropriate Apache and Creative Commons open source licenses. Its distribution was discontinued in early 2017 because such “fragmenting” led to divergence, and led to confusion amongst its users and the technical community generally that that OpenCyc fragment was Cyc.</p>
</blockquote>
<p>OpenCyc was primarily a subset of the knowledge base. As described <a href="https://web.archive.org/web/20040310001548/http://www.cyc.com/doc/handbook/oe/10-notable-quoted-collections.html">in the Handbook</a>, the base contained 3 sections:</p>
<ul>
<li>The public section was a “large section, constituting much of the Cyc upper-ontology as well as some middle- and lower-ontology”.</li>
<li>The proprietary section was an “even larger section of the Knowledge Base, subsuming the public release and containing a good deal more, has been sanctioned for release to corporations and individuals who are co-participants with Cyc in various DARPA contracts. This portion of the Knowledge Base is generally referred to as the Integrated Knowledge Base or IKB.”.</li>
<li>A small classified section was for Cycorp itself, “that should not be released to anyone outside the company. Sometimes, this is because the information is pertinent to commercial contracts that are subject to non-disclosure; sometimes, it is because the terms in question are considered experimental in one way or another, and therefore not suitable for immediate release.”</li>
</ul>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/opencyc-kb-browser.gif" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The knowledge base of OpenCyc as of 2010. (<a href="https://web.archive.org/web/20100526095325/http://www.opencyc.com/">Source</a>)</figcaption>
</figure>
</div>
<p>Concurrent with OpenCyc, there was also ResearchCyc, which contained the non-proprietary parts of Cyc, but only available for research purposes. It shutdown <a href="https://web.archive.org/web/20190601000000*/http://www.cyc.com/researchcyc">sometime in 2019</a>, without even a curt message.</p>
<p>As one can expect from Lenat’s previous non-releases, there was and has never been any release of Cyc itself, especially because Cyc is a commercial endeavor, as is necessary to sustain the 2000 person-year project.</p>
<p>In 2001, at the start of the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a> hype (the original “Web 3.0”, before it came to mean cryptographic blockchains), Cycorp began seriously engaging with the many Semantic Web initiatives.</p>
<p>Squinting a bit, the visions of the semantic web and the vision of the Cyc were the same: Both wish to draw connections between data, such that computer programs can chain together multiple operations on data, synthesize them, and give the user what they meant, instead of what they literally typed out. Cycorp regarded the semantic web effort as doing essentially the same thing, on the Internet scale, except with a less expressive frame language (not even first-order logic). During the 2000s, papers from Cycorp often talked of integrating Cyc with the semantic web by encoding knowledge in DAML, RDF, OWL, XML, or some other boring acronym.</p>
<p>Cycorp participated in the <a href="https://web.archive.org/web/20120609070903/http://suo.ieee.org/">Standard Upper Ontology Working Group (SUO WG)</a>, which, like most Working Groups, petered out in 2003 among motions, procedures, and consensus-buildings, filled with meticulous wisdoms and benevolent safeguarding of the metaphysics of mankind. What it did provide is <a href="https://web.archive.org/web/20030515043435/http://www.cyc.com/SUO/opencyc-ontology.txt">the earliest copy</a> of OpenCyc I have found, in 2003, which I have <a href="https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/opencyc-ontology.txt">backed up</a> for safekeeping.</p>
<p>With the breakout success of the <a href="https://en.wikipedia.org/wiki/ESP_game">ESP game</a> in 2003, “games with a purpose” were all the rage, so Cyc attempted to keep up with the times with its own <a href="https://web.archive.org/web/20120902022607/http://game.cyc.com/">FACTory</a> in 2005. It never got out of the <code>Beta version 1.0.1</code> and shut down sometime after 2012. According to the <a href="https://web.archive.org/web/20061205102605/http://207.207.9.186/helpfiles/HowToPlay.html">tutorial</a>, it was a single player game, where the player just selects whether the statement is true, false, doesn’t make sense, or the player doesn’t know. The answers are scored based on the majority of answers. Though Lenat stated in a lecture that they had players that raked up 50 hours a month to the point that Lenat felt concerned about it, I could not find any mention of how much data they have gathered from this game in total. Nevertheless, this appeared to be the only time Cyc crowd-sourced data.</p>
<p>In 2006, Cycorp spun out The Cyc Foundation, a non-profit organization promote the OpenCyc + Semantic Web combo. As usual, nothing ever came of it. The <a href="https://web.archive.org/web/20110610022626/http://www.cycfoundation.org/blog/?p=45">last blogpost went up in 2011-06</a>, and the website shutdown in 2015.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyclopedia_mockeup.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The Cyc + Wikipedia = “Cyclopedia” mockup <a href="https://web.archive.org/web/20071102082440/http://www.cycfoundation.org/blog/?page_id=15">posted in 2007 by The Cyc Foundation</a>. Despite being “fairly close to releasing a beta version of Cyclopedia”, they never did.</figcaption>
</figure>
</div>
<p>In 2008, Cycorp tried again by putting up a copy of OpenCyc with a user interface and an API, branded as “<a href="https://web.archive.org/web/20080826011848/http://sw.opencyc.org/">OpenCyc for the Semantic Web</a>”. The API would allow web agents to call on OpenCyc and use the replies to do Semantic Web things, making it “the backbone of semantic web”… at least that was the hope. An exhaustive Google search turned up zero actual applications. It shut down in 2017 when OpenCyc did.</p>
<p>Though deprived of its purported backbone, from our vantage point, the Semantic Web has arrive as promised, but instead of the dream of Cyc-like thinkers performing long queries over databases, we have forgetful agent swarms talking with each other with API calls, dying, <a href="https://en.wikipedia.org/wiki/REST">REST</a>ing, and reincarnating, a game of <a href="https://en.wikipedia.org/wiki/Memento_(film)"><em>Memento</em></a> on the global scale.</p>
<p>In 2008, Cycorp tried again by joining the Large Knowledge Collider (LarKC)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> project at Europe, “a platform for massive distributed incomplete reasoning that will remove the scalability barriers of currently existing reasoning systems for the Semantic Web”. The <a href="https://active09.ijs.si/wp-content/uploads/2009/06/michael-witbrock.pdf">hope</a> was to produce a common knowledge base with over 1 billion triples, such that it can easily be scaled up further, and easy to produce expert systems based on it. A few papers and conferences later, the project <a href="https://web.archive.org/web/20150206003436/http://www.larkc.eu/">ended in 2011</a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;The name is an obvious shade to the <a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider#Inaugural_tests_(2008)">Large Hadron Collider</a>, which began operation in 2009 and was briefly in the popular imagination.</p></div></div><p>There was <em>another</em> <a href="https://web.archive.org/web/20120126094742/http://blog.cyc.com/">blog</a> by Cycorp starting in 2008, that stopped updating in 2011 after 11 unremarkable posts. It was supposedly written by Cycorp, though its style was the same forgettably respectable style as that of the Cyc Foundation and parliamentary proceedings.</p>
<p>There was a Twitter bot <code>@cyc_ai​</code>, which started in 2008 and <a href="https://web.archive.org/web/20150524003739/https://twitter.com/cyc_ai">stopped in 2011</a> after 15764 inane tweets in the format of “I just leaned <code>&lt;statement&gt;</code>, true or false?”. According to Internet Archive, it shut down sometime before 2017.</p>
</section>
<section id="cyc-is-done" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cyc-is-done">Cyc is done?</h3>
<p>As you might have noticed if you clicked on any of the above links, almost all the links are dead now. By checking the last known good copy of the various websites on Internet Archive, I noticed that there was a “massive extinction event” during 2014–2016, when Cycorp purged most of the open information about Cyc from the Internet. No more <a href="https://web.archive.org/web/20100526095325/http://www.opencyc.com/">OpenCyc</a>, <a href="https://web.archive.org/web/20091110010118/http://www.cyc.com/cycdoc/walkthroughs/oeintro_cats_frames_long.html">tutorials</a>, <a href="https://web.archive.org/web/20090219032021/http://www.cyc.com:80/cycdoc/ref/subl-reference.html">references</a>, <a href="https://web.archive.org/web/20090213151624/http://www.cyc.com:80/cycdoc/vocab/actor-vocab-complete.html">vocabulary lists</a>, <a href="https://web.archive.org/web/20040107124240/http://www.cyc.com/doc/handbook/oe/oe-handbook-toc-opencyc.html">The Ontological Engineer’s handbook (version 0.7)</a>…<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> everything was gone, except marketing material. Fortunately, the Internet Archive exists, and I scraped much of the primary material from it and <a href="https://github.com/yuxi-liu-wired/cyc-archive/tree/main">uploaded them to GitHub</a> for safekeeping.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;There is a funny anecdote about this book:</p>
<blockquote class="blockquote">
<p>I coined the phrase “ontological engineer” in the mid-1980’s, shortly after embarking on the construction of Cyc, because I didn’t like the pejorative tone of “knowledge enterer” or “knowledge worker”, and the term “knowledge engineer” had already been taken (to mean someone who builds expert systems). Based on that, when Addison-Wesley published our 1989 Cyc book (<em>Building Large Knowledge-Based Systems</em>), the editor playfully inserted a forward reference, at the front, under Other Publications, to the 1997 <em>Ontological Engineer’s Handbook</em>. Of course the joke was on us when, starting in 1997, we began to be deluged by requests for that nonexistent work.</p>
<p><span class="citation" data-cites="lenatAppliedOntologyIssues2005">(<a href="#ref-lenatAppliedOntologyIssues2005" role="doc-biblioref">D. B. Lenat 2005</a>)</span></p>
</blockquote>
</div></div><p>Why?</p>
<p>I believe it was for commercial reasons. This mass extinction event closely corresponded to the commercialization wave in 2016, when Lenat finally declared the Cyc project “done” and set about commercializing it, both via Cycorp and via Lucid.ai, a company founded a year before.</p>
<blockquote class="blockquote">
<p>“Part of the reason is the doneness of Cyc,” explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. “Not that there’s nothing else to do,” he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, [Lucid] is developing a personal assistant equipped with Cyc’s general knowledge. This could perhaps lead to something similar to Siri. … the CEO of Lucid says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.</p>
<p><span class="citation" data-cites="knightAI30Years2016">(<a href="#ref-knightAI30Years2016" role="doc-biblioref">Knight 2016</a>)</span></p>
</blockquote>
<p>This timing of commercialization coincides suspiciously with the end of almost all projects except ResearchCyc. Even <a href="https://web.archive.org/web/20160825080811/http://www.cyc.com/documentation/">the documentation</a> <a href="https://web.archive.org/web/20170802104610/http://www.cyc.com/documentation/">went offline</a> in 2016 and could now only be accessed for commercially registered accounts! It looks like a total internal pivot as the company focused on commercialization and shut down all services that were not high priority for the bottom line.</p>
</section>
<section id="how-expensive-was-the-lunch" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="how-expensive-was-the-lunch">How expensive was the lunch?</h3>
<p>After cyberstalking the Cyc for days, I had captured every numerical datapoint that has ever dripped out of Cycorp over its entire existence, and combined them into a <a href="https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/Cycorp%20claims.xlsx">spreadsheet that you can analyze yourself</a>. In short, Cyc has grown to 30M assertions over the years, and is still incomplete, if completeness were measured by its original standard – a self-learning AGI. Despite Lenat’s 2016 claim that it was “done”, there is no self-learning or AGI in sight.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_progress_history.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The progress of Cyc project over the period of 1989–2022. The number of assertions grew to 30M, the cost grew to $200M, with 2000 person-years. Source: <a href="https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/Cycorp%20claims.xlsx">my obsessive cyberstalking</a>.</figcaption>
</figure>
</div>
<p>Looking at this diagram, we notice 3 things:</p>
<p>One, the growth of assertions is roughly exponential, doubling every 6 years. At this rate, in 2032 Cyc can expect to reach 100M assertions, the hoped-for point at which Cyc would know as much as a typical human. This might be the dream of Lenat, but I bet it will just become a slightly bigger enterprise solution to expertise management.</p>
<p>Two, the cost of human labor has remained stable throughout the existence of Cycorp, at $100K/person-year.</p>
<p>Three, The cost of assertions was $5/assertion from 1990–2010, but $0.7/assertion after 2015, around the time when Lenat declared the project mostly complete. This matches the estimate of <span class="citation" data-cites="paulheimHowMuchTriple2018">(<a href="#ref-paulheimHowMuchTriple2018" role="doc-biblioref">Paulheim 2018</a>)</span>. It’s unclear what caused the 7-fold increase in efficiency. Perhaps this is a success of “priming the knowledge pump”, or this might just because after 2015, most new assertions came from adding new assertions specialized to particular commercial applications, and thus were easier to handle. Formalizing business rules already written down is easier than extracting the intuitive metaphysics of fruits vs vegetables, after all. In any case, a 7-fold efficiency improvement over 30 years is unimpressive, and indistinguishable from more prosaic arguments about <a href="https://en.wikipedia.org/wiki/Learning_curve">learning curve</a> effects observed in most industries: The more you make, the cheaper you can make.</p>
</section>
</section>
<section id="overview-of-cyc-the-system" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="overview-of-cyc-the-system">Overview of Cyc, the system</h2>
<p>The Cyc system consists of three main kinds of parts:</p>
<ol type="1">
<li>The iterations of CycL, based on SubLisp, which is itself based on Lisp.</li>
<li>The knowledge bases, consisting of millions of assertions written in CycL.</li>
<li>The ~1000 inference engines, written in SubLisp.</li>
</ol>
<p>To increase speed and compatibility, the programs written in CycL are compiled to Java, which is then compiled to bytecode.</p>
<p>The CycL language is intended to be the high-level language that humans can read and write, at the “Epistemological Level” (EL). Most inference engines do not process CycL directly, but process low-level translations of CycL at the “Heuristic Level” (HL). Each sentence at the EL can be translated into a multitude of HL sentences, since different translations allow different inference engines to process it, so that hopefully at least one of those would process it efficiently. The two sides are connected by an interface called the “Canonicalizer”.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_EL_HL.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">How Cyc processes a user’s input as of 1990. The user enters at the EL-KB (epistemological level knowledge base), which gets translated down to the HL, where multiple modules for generating and comparing arguments for and against a given proposition are run. <span class="citation" data-cites="lenatCycMidtermReport1990">(<a href="#ref-lenatCycMidtermReport1990" role="doc-biblioref">D. B. Lenat and Guha 1990, fig. 1</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/EL_vs_HL.gif" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The expressiveness-efficiency tradeoff. The HL is highly efficient for the machine to infer with, but very hard for humans to express what they are thinking of in. It is the opposite for the EL, and even more so for natural languages like English. <a href="https://web.archive.org/web/20041225143106/http://www.cyc.com:80/cycdoc/handbook/images/elhl.gif">Source</a></figcaption>
</figure>
</div>
<section id="cycl-language" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cycl-language">CycL language</h3>
<p>Since the CycL language has had been developed since the late 1980s, and Cycorp published very little, I am basing this section mostly on <a href="https://web.archive.org/web/20040107124240/http://www.cyc.com/doc/handbook/oe/oe-handbook-toc-opencyc.html">the Ontological Engineer’s Handbook (version 0.7)</a>, which seemed to contain the most details, even though technically it only describes the 2002 version of CycL.</p>
<p>Like Common Lisp, the CycL language has almost no syntax:</p>
<ul>
<li>An <strong>expression</strong> is from one of the following types: <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code>, <code>?variable</code>, <code>#$Term</code>, text string, and rational number.</li>
<li>A <code>#$relation</code> is either a function or a predicate. It always begins with a lowercase.</li>
<li>If <code>#$relation</code> is a function, then <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code> is an <strong>object</strong>, aka a <strong>term</strong>. Most of Cyc’s knowledge base consists of assertions. Each assertion has a truth value.</li>
<li>If <code>#$relation</code> is a predicate, then <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code> is an <strong>assertion</strong>, aka a <strong>sentence</strong>. Most of Cyc’s knowledge base consists of assertions.</li>
<li>For a few special cases of <code>#$relation</code> such as <code>#$implies</code>, <code>#$forAll</code>, and <code>#$thereExists</code>,<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> <code>(#$relation &lt;arg1&gt; … &lt;argn&gt;)</code> is a <strong>rule</strong> of inference.</li>
<li>Each assertion has a truth value.</li>
<li>There are 5 truth values: default true, monotonically true, default false, monotonically false, and undefined. The “default true” is used to allow for “nonmonotonic reasoning”. For example, it allows us to say that “Pingu is a bird, so Pingu can fly”, and then, if someone else adds “and Pingu is a penguin”, we can say, “in that case, Pingu cannot fly”. Since the “Pingu can fly” statement is only “default true” and “Pingu cannot fly” is only “default false”, we would not suffer from the principle of explosion.</li>
<li>A <code>#$Term</code>, aka an <strong>atomic term</strong>, aka a <strong>concept</strong>, can be thought of as a “word” in the vocabulary of CycL. It begins with an uppercase. Interpreted in object-oriented programming, some terms correspond to classes, while others correspond to objects. For example, <code>#$Socrates</code> is an object, while <code>#$Human</code> is a class. The difference is that we can say <code>(#$isa #$Socrates #$Human)</code> but not <code>(#$isa #$Socrates #$Plato)</code>. The concepts can be as abstract as <code>#$AnimalWalkingProcess</code> (the concept of any possible walking by any animal) and as granular as <code>#$Walking00036</code> (the walk I took on the afternoon of 1897-02-03 in Paris).</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;A technical detail is that as soon as you enter an expression that uses <code>#$thereExists</code>, Cyc automatically “<a href="https://en.wikipedia.org/wiki/Skolem_normal_form">Skolemizes</a>” that expression, because existential quantifiers are technically inconvenient. For example, if I were to say “There exists a time at which Socrates dies.”, then it would be very awkward when someone asks “When?” and I have no recourse but to reply “… there exists!”. Skolemization means that I would instead define a function <code>#$DateOfDeath</code>. Then any time someone asks “When?”, I need simply reply “<code>(#$DateOfDeath #$Socrates)</code>!”.</p></div></div><p>This is enough for us to start writing some assertions that would imply “Socrates is mortal”.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>(#$isa #$Socrates #$MaleHuman)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>(#$isa #$MaleHuman #$Predicate)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>(#$genls #$MaleHuman #$Human) <span class="co">; #$genls means "generalizes"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>(#$genls #$MaleHuman #$MaleAnimal) <span class="co">; Multiple generalization is common</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>(#$genls #$Person #$Individual)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>(#$isa #$Individual #$FirstOrderCollection) <span class="co">; Things can go meta</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>(#$isa #$FirstOrderCollection #$SecondOrderCollection)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>(#$isa #$FirstOrderCollection #$MetaClass)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>(#$isa #$MetaClass #$MetaClass) <span class="co">; Very meta</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>(#$forAll ?X</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  (#$sameAs (#$MotherFn (#$MotherFn ?X)) </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    (#$MaternalGrandMotherFn ?X)))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">; Humans are mortal</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>(#$isa #$Mortal #$Predicate)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>(#$implies (#$isa ?X #$Human) (#$Mortal ?X))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">; For X to be mortal means there exists a death event X is subject to</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>(#$implies</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  (#$Mortal ?X)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  (#$exists ?DE</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    (#$and</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>      (#$isa ?DE #$DeathEvent)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>      (#$subject ?DE ?X))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Assertions can be packaged into a <strong>microtheory</strong> (<strong>Mt</strong>, aka a <strong>context</strong>), which could be thought of as <a href="https://en.wikipedia.org/wiki/Scope_(computer_science)">scopes</a> or <a href="https://en.wikipedia.org/wiki/Module_(programming)">modules</a>. Each assertion can belong to exactly one microtheory, though you can have two assertions that are literally typed out using the same symbols occur in two microtheories – and they would count as two logical assertions.</p>
<p>Assertions can, and often need, to be automatically inferred across microtheories. For example, “Socrates is alive.” is true in the context of 500 BC, but not in the context of 1995. Given an assertion to the effect of “Socrates was born in 470 BC and died in 399 BC”, an automatic process would allow the system to automatically infer assertions “Socrates is alive” for every microtheory from “The time is 470 BC” to “The time is 399 BC”, and “Socrates is not alive” for the other microtheories.</p>
<p>As one might expect from the above example, it is unnecessary, and indeed impractical, to create all microtheories that one might ever use. Indeed, many microtheories are created and destroyed for one-time-use “on the spot”. For a single problem-and-answer session with a user, Cyc typically sets up a temporary context that is deleted after the session, or saved if the user wishes to continue the session later.</p>
<p>Microtheories are necessary because human beliefs are incompatible, and there are a lot of humans. For example (very relevant, considering <a href="#sec-cycops-overview">how much Cyc was involved with the War on Terror</a>), <code>#$ChristianMt</code> and <code>#$IslamMt</code> could have:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>(#$isa #$ChristianMt #$Microtheory)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>(#$isa #$IslamMt #$Microtheory)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">; #$ist means "is true in the context of"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>(#$ist (#$ChristianMt)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  (#$not (#$sameAs #$God #$Allah)))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>(#$ist (#$ChristianMt)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  (#$sonOf #$Jesus #$God))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">; the Trinity is left as an exercise</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>(#$ist (#$IslamMt)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  (#$sameAs #$God #$Allah))</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">; In Islam, God has no son</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>(#$ist (#$IslamMt)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  (#$not (#$sonOf ?X #$God)))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>(#$ist (#$IslamMt)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>  (#$prophetOf #$Jesus #$God))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Microtheories can contain each other. For example, both are <code>#$AbrahamicMt</code>, which allows us to make assertions that apply to both.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>(#$isa #$AbrahamicMt #$Microtheory)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>(#$genls #$ChristianMt #$AbrahamicMt)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>(#$genls #$IslamMt #$AbrahamicMt)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">; Abraham is a prophet</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>(#$ist (#$AbrahamicMt)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    (#$prophetOf #$Abraham #$God))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">; God exists uniquely</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>(#$ist (#$AbrahamicMt)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  (#$exists ?X</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    (#$and</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      (#$isa ?X #$God)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>      (#$not</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        (#$exists ?Y</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>          (#$and</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            (#$isa ?Y #$God)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            (#$not</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>              (#$sameAs ?X ?Y))))))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As of 2010, there were over 20K microtheories arranged in a hierarchy, with <code>BaseKB</code> at the top. It was unfortunately a mess, as Lenat promised. Some microtheories were 50 levels deep down the hierarchy!</p>
<blockquote class="blockquote">
<p>… determining the hierarchical structure of the Mt’s is difficult, manual, and error prone because all of the Mt’s in ResearchCyc are direct subtypes of <code>BaseKB</code>, even if they are also its indirect subtypes. For example, Mt <code>ClothingGMt</code> describes general information about clothing and is a direct subtype of <code>BaseKB</code>. It is also an indirect subtype of <code>BaseKB</code> because it is a subtype of <code>ArtifactGMt</code>, also a subtype of <code>ArtifactGVocabularyMt</code>, which, in turn, is a subtype of <code>BaseKB</code>… Choosing the right microtheory can easily take several minutes for a trained ontologist who is familiar with the Cyc ontology and experienced in how best to organize knowledge for maximum utility.</p>
<p><span class="citation" data-cites="conesaUsabilityUpperLevel2010">(<a href="#ref-conesaUsabilityUpperLevel2010" role="doc-biblioref">Conesa, Storey, and Sugumaran 2010</a>)</span></p>
</blockquote>
</section>
<section id="ontology" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="ontology">Ontology</h3>
<p>The Ontology of Cyc is the graph of all concepts in Cyc, with one directed edge per <code>(#$genls #$Thing1 #$Thing2)</code> assertion. At least, that’s the simplest possible way to say it. In fact, the graph is more complicated, since CycL is a higher-order logic, which allows it to talk about the relation between relations between predicates, etc. For example, both <code>#$SetOrCollection</code> and <code>#$MathematicalObject</code> are sub-concepts of <code>#$MathematicalThing</code>, but we also need to specify that any <code>#$MathematicalThing</code> is either a <code>#$SetOrCollection</code> xor a <code>#$MathematicalObject</code>.</p>
<p>The ontology contains multiple levels. At the upper level are the most metaphysical concepts, starting with <code>#$Thing</code>, going down to the middle level of <code>#$Language</code> and <code>#$MilitaryOrganization</code>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_Upper_Ontology_2010.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The upper ontology of Cyc as of 2010. <span class="citation" data-cites="foxvogCyc2010">(<a href="#ref-foxvogCyc2010" role="doc-biblioref">Foxvog 2010, fig. 1</a>)</span></figcaption>
</figure>
</div>
<p>Below the upper ontology are domain-specific knowledge, divided into large microtheories (minitheories?). There would be knowledge about mortgages, computer security, weapons systems, pathology, etc. Below those are the domain-specific facts and data, divided into microtheories. The entire thing is structured like a pyramid.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_ontology_pyramid.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The cyc ontology pyramid.</figcaption>
</figure>
</div>
<p>As an example, here is how <code>#$Philosopher</code> is described in <a href="https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/opencyc-ontology.txt">OpenCyc</a>:</p>
<blockquote class="blockquote">
<p>A specialization of <code>#$Person</code>; in the context of <code>#$HumanActivitiesMt</code> this collection is an instance of <code>#$PersonTypeByActivity</code>, in the context of <code>#$JobMt</code> it is an instance of <code>#$PersonTypeByOccupation</code>. Each instance of <code>#$Philosopher</code> is a person who habitually thinks about philosophical matters such as what is or might be, what we can know, how we can know anything, etc. In the contemporary era most philosophers are academics or professionals, but a significant number (now and historically) don’t fit this profile.</p>
</blockquote>
<p>And here’s the <code>#$Thing</code>:</p>
<blockquote class="blockquote">
<p><code>#$Thing</code> is the “universal collection”: the collection which, by definition, contains everything there is. Every thing in the Cyc ontology – every <code>#$Individual</code> (of any kind), every <code>#$Set-Mathematical</code>, and every <code>#$Collection</code> – is an instance of (see <code>#$isa</code>) <code>#$Thing</code>. Similarly, every collection is a subcollection of (see <code>#$genls</code>) <code>#$Thing</code>. Trivially, <code>#$Thing</code> is both an instance of and a subcollection of itself, and is not a subcollection of any other collection. (Note that the above reference to “every thing in the Cyc ontology” is <em>not</em> meant to be limited to things actually <em>reified</em> in the Cyc system, but includes (e.g.) every instance – reified or not, known or not – of every collection recognized by Cyc.)</p>
</blockquote>
<p>And if the parenthetical note sounds a bit theological,<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> note that among those that Cycorp had hired included philosophers, botanists, chemists, and of course, theologians. Interestingly, they didn’t ask botanists to encode what they know about botany, but about what they know about non-botany. Lenat’s theory was that botanists’ understanding of botany is <em>not</em> commonsensical. Instead, what botany he wanted Cyc to encode is common sense botany: how non-botanists think about plants – even, and especially, those botanical beliefs that a botanist would consider wrong, such as <code>(#$not (#$isa #$Banana #$BotanicalBerry))</code>.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;I did not find any mention of Meinong in Lenat’s corpus or Cycorp’s communications, but this description sounds close to <a href="https://en.wikipedia.org/wiki/Meinong's_jungle">Meinong’s Jungle</a>, where every thing exists, even logically impossible things such as “a square circle”. The metaphysical difficulties are many, but presumably Cyc has tamed the jungle in practice.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;This reminds me of an event very early on in the history of Cyc:</p>
<blockquote class="blockquote">
<p>Cyc apparently believed that if a bronze statue were melted into slag, it would remain a statue. What had gone wrong? … Cyc had been told that bronze was a material that retained its essential property – its “bronzeness,” as it were – no matter what state it was in, solid or liquid. But now Cyc was trying to apply that fact to the <em>statue</em> aspect of “bronze statue”. Cyc hadn’t been told anything about statues that would invalidate its conclusion; nobody had ever thought it necessary to tell Cyc, for example, that statues are only statues if they’re more or less in their original form.</p>
<p><span class="citation" data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span></p>
</blockquote>
<p>To a philosopher, this would have been funny, because Cyc had just been ambushed by Aristotle’s bronze statue! See especially <a href="https://www.loebclassics.com/view/aristotle-physics/1934/pb_LCL228.137.xml"><em>Physics</em> 195a6-8</a> and <a href="https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.01.0052:book%3D8:section%3D1045a"><em>Metaphysics</em> 1045a26-29</a>. Aristotle solved it by <a href="https://en.wikipedia.org/wiki/Hylomorphism">hylomorphism</a>. I wonder how Cyc solved it?</p></div></div><p>Unfortunately for ontology, while everyone has a common sense, few can pull it out of their heads and push it into a computer. Let’s consider how Cyc encodes events via “<a href="https://web.archive.org/web/20040310001317/http://www.cyc.com/doc/handbook/oe/08-cycl-representation-choices.html">Davidsonian semantics</a>”, since it’s how <a href="https://en.wikipedia.org/wiki/Donald_Davidson_(philosopher)">Donald Davidson</a> represented <a href="https://en.wikipedia.org/wiki/Event_(philosophy)">events</a> – yes, Cyc hired a lot of philosophy PhDs. While everyone can reason about events commonsensically, implicitly, making it explicit required a Davidson.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<p>To represent “John gave Mary a book.”, you can write something like <code>(#$Give #$John #$Mary #$Book1)</code>, but this prevents you from adding more details, because for the assertion to be syntactically correct, you must define <code>#$Give</code> to be a predicate with arity exactly 3. And then you’d be stuck if you want to write “John gave Mary a book last August.”, since there is no arity in <code>#$Give</code> to insert the “last August” into. And even if you redefine <code>#$Give</code> to have arity 4, what if you <em>also</em> want to say it happened in the library, or that it was a gift, or that John was happy about it? You’d have to change the definition of the predicate <code>#$Give</code> again and again, and there is no end to this.</p>
<p>To solve this problem, Cyc treats the event itself as an <code>#$Event</code>. That is, it reifies the process as an object. Now, instead of cramming everything into one assertion, you can construct an endless sequence of assertions, limited only by your patience:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>(#$isa #$Event123 #$Event)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>(#$Sender #$Event123 #$John)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>(#$Receiver #$Event123 #$Mary)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>(#$GivenObject #$Event123 #$Book134)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Such problems are fairly subtle, and the world is a very big place. Fortunately, Lenat was a master of ontology, so it all worked out in the end.</p>
<blockquote class="blockquote">
<p>… during a short stint working with Doug Lenat’s Cyc project. At the time, they were trying to encode all of botany and had a small staff of professional botanists doing knowledge entry. Naturally it was quite difficult for the botanists to try to translate their knowledge into the formalisms required by Cyc, and they would regularly puzzle over various questions… and if they could not come to a consensus, would have to take it before the Master, Doug Lenat, who would think for a bit, maybe draw some diagrams on a whiteboard, and come up with the Right Representation.</p>
<p>— <a href="https://hyperphor.com/ammdi/alpha-ontologist">AMMDI: alpha ontologist</a> (2023-10-07)</p>
</blockquote>
</section>
<section id="inference-engines" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="inference-engines">Inference engines</h3>
<p>To a mathematician, 1 and 1 trillion are the same – both are finite. To computer scientists, even <span class="math inline">\(x^2\)</span> and <span class="math inline">\(x^3\)</span> are different. Much work on Cyc was not on building the 100M-assertion knowledge base, but on building inference engines that allow fast inferences when there are 100M assertions to pick from.</p>
<p>Every logical system must face an impossible trilemma:</p>
<ul>
<li>an <strong>expressive</strong> language that can represent what people would ever want to say in practice;</li>
<li>an <strong>efficient</strong> inference engine on the language that runs fast enough for practical inferences;</li>
<li>a <strong>complete</strong> inference engine that can perform all inferences that are logically valid.</li>
</ul>
<p>Why a trilemma?</p>
<p>Suppose we want to allow Cyc express all common sense assertions, then since humans in their daily life say high-order statements like “Are you implying that you <em>meant</em> to make me upset by spreading rumors about her, when you had known all along that I would soon hear about it?”, Cyc needs to use a higher-order language. Now this immediately makes it impossible to have an inference engine that is both complete and computable, since we have a Gödel-style incompleteness theorem <span class="citation" data-cites="shapiroFoundationsFoundationalismCase1991">(<a href="#ref-shapiroFoundationsFoundationalismCase1991" role="doc-biblioref">Shapiro 1991</a>, theorem 4.14)</span>.</p>
<p>Lenat chose to use a fully higher-order language, giving up completeness, and then try his best improving efficiency.</p>
<p>Like most expert systems, the Cyc has a general <a href="https://en.wikipedia.org/wiki/Resolution_(logic)">resolution</a>-based inference engine. Unlike most expert systems, its knowledge base is large and higher-ordered, so its general engine runs too slowly for most queries. Thus, the developers kept adding more specialized modules (“pattern-specific heuristic modules”), each capable of efficiently inferring on a few microtheories. If a specialized engine fails to make progress, a more general engine can be the fall-back, all the way up to the most general one.</p>
<p>As the simplest example of how inference engine can work, consider the following example of backward-chaining inference, in a semantic web. The system is asked basically “Why does Clyde want to possess a crescent wrench?”, and it eventually replies “Because Clyde has not eaten lately.”. Lenat expected that the ontology of Cyc would eventually power generally intelligent agents, which would use forward-chaining to construct goals from current states, and use backward-chaining to explain why others have their goals.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_triple_reasoning.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A sketch of how to reason by backward-chaining together triples in a semantic web. Figure from <span class="citation" data-cites="wallichSiliconBabies1991">(<a href="#ref-wallichSiliconBabies1991" role="doc-biblioref">Wallich 1991</a>)</span>. I can’t help but wonder if Lenat meant for a subtle joke against the <em>Elephants don’t play chess</em> by <span class="citation" data-cites="brooksElephantsDonPlay1990">(<a href="#ref-brooksElephantsDonPlay1990" role="doc-biblioref">Brooks 1990</a>)</span> by adding in a “dead end” branch that ends up concluding Clyde is an elephant.</figcaption>
</figure>
</div>
<p>Most inference engines are highly specialized. They can only reason within a few microtheories, but very well. Here is a toy example for reasoning with mutually exclusive categories. We can define <code>#$Mutex</code> as</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>(#$implies</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  (#$and (#$isa ?X ?A) (#$Mutex ?A ?B))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>         (#$not (#$isa ?X ?B)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then this allows the general reasoning engine to reason about mutual exclusivity by always reducing to this definition. However, we can accelerate this by adding in some “lemmas”:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>(#$implies (#$Mutex ?A ?B) (#$Mutex ?B ?A))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>(#$implies</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  (#$and (#$Mutex ?A ?B) (#$genls ?C ?A))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  (#$Mutex ?C ?B))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>(#$implies</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  (#$and (#$Mutex ?A ?B) (#$isa ?X ?A))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  (#$not (#$isa ?X ?B)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After producing enough lemmas, we can then write a specialized inference engine that would be invoked whenever <code>#$Mutex</code> appears in an expression, and it would attempt some inference and simplifications by applying these lemmas. Indeed, most inference engines were constructed in this way: Cyc would try to solve a problem, and fails by timing out. The ontologists at Cyc would call up a human expert and ask, “How did you do this?” and the expert would explain how they would solve it with quick rules of thumb, which the ontologists would write into Cyc, resulting in more assertions, and possibly more inference engines. This is essentially the same as “knowledge elicitation” used for making expert systems.</p>
<p>Other than these inference engines (or “workers”), there are also many “tacticians”, modules that pick engines that are probably good for solving a problem, and a general all-powerful strategist at the very top. It was reported in that there were 1 strategist, 4 tacticians, and 1097 workers. The strategist and tacticians have parameters that can vary, presumably by the user or the ontologists in order to adapt to specific tasks. <span class="citation" data-cites="lenatEfficientPathfindingVery2007">(<a href="#ref-lenatEfficientPathfindingVery2007" role="doc-biblioref">D. B. Lenat et al. 2007</a>)</span></p>
<p>The control structure of Cyc is a commercial secret.<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> I could only find two brief explanations of it, one in a <a href="https://web.archive.org/web/20130708080327/http://www.cyc.com/sites/default/files/storage/white-papers/The%20Cyc%20Blackboard%20System%20v1.0.pdf">2010 Cycorp white paper</a>, another in an essay published near the end of his life <span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span>. According to these, Cyc coordinates the ~1000 inference engines with blackboard architecture similar to the one used in Hearsay-II speech recognition system <span class="citation" data-cites="ermanHearsayIISpeechUnderstandingSystem1980">(<a href="#ref-ermanHearsayIISpeechUnderstandingSystem1980" role="doc-biblioref">Erman et al. 1980</a>)</span>. Lenat described it as:</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<a href="https://web.archive.org/web/20130707133552/http://www.cyc.com/documentation/control-structure">The SubLisp reference</a> was titled “control structure”, and got me excited for a moment. But I was disappointed to find that it described the control structure of the SubLisp programming language (mostly the same as that of Common Lisp), <em>not</em> of the Cyc system.</p></div></div><blockquote class="blockquote">
<p>there is a whole battery of specialized inference engines and representations… and, when making progress, broadcasting the results… Whenever progress is made, all of them stop and work on the now-simpler subproblem. Some of the inference engines are very general, and work on general representations–e.g., a theorem prover that works on first-order logic. The more specialized inference engines are much faster whenever they do apply… In 1986, Cyc had two such representations; by 1990, there were 20, each with its own inference engine; today Cyc has over 1100. They work together as a community of agents, communicating by posting their intermediate results on a sort of blackboard that all the other agents can watch and react to if/when/as they see an opportunity that fits their specialty.</p>
<p><span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span></p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Blackboard_architecture.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The blackboard architecture used in Hearsay-II. In this example, a 3-dimensional blackboard allows different modules to read and write to different parts of the blackboard, and collaboratively recognize speech. <span class="citation" data-cites="lenatComputerSoftwareIntelligent1984">(<a href="#ref-lenatComputerSoftwareIntelligent1984" role="doc-biblioref">D. B. Lenat 1984</a>)</span></figcaption>
</figure>
</div>
<p>You can imagine a shared working space – the blackboard – where agents can scan and find tasks to do, do them, and then post the outputs to the blackboard. Each agent has their own “places to watch”. They notice (or get notified) what is posted to their watched areas, and ignore what is outside.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> When an agent completes whatever task it is running, it posts the outputs to particular areas of the board to notify certain agents, “Hey, check this out! I bet you’ll find this helpful.”. To prevent race conditions or general conflict between agents, an agent can claim a lock on something, so that other agents can’t work on it until it times out, or releases the lock.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;Lenat seemed to suggest that each inference engine watches <em>the entire board</em>, but every large-scale blackboard system I know of doesn’t do that. Hearsay-II certainly did not. It is slow and creates much of unintended complexities. Imagine a microservice where every process pings every process whenever it outputs something, or a conference where everyone broadcasts to everyone.</p></div></div><p>You can imagine this as a <a href="https://en.wikipedia.org/wiki/Microservices">microservices framework</a>, with over 1100 microservices, <a href="https://en.wikipedia.org/wiki/Multicast">multicast</a>, with SubLisp as the <a href="https://en.wikipedia.org/wiki/Serialization">serialization language</a>. Indeed, <a href="https://www.planettechnews.com/planettech-interviews-michael-stewart-founder-chairman-and-ceo-of-lucid-ai/">apparently</a> some of the inference engines used neural networks, so their numerical outputs <em>definitely</em> had to be serialized somehow into SubLisp.</p>
<p>The blackboard is also connected to the “external world” through API calls. For example, SQL queries run by an external database can return data that is posted to a region of the blackboard, and what Cyc writes to the blackboard can be read out for external use.</p>
<p>According to a 2006 lecture <span class="citation" data-cites="lenatGoogleTechTalksComputers2006">(<a href="#ref-lenatGoogleTechTalksComputers2006" role="doc-biblioref">D. B. Lenat 2006</a>)</span>, the blackboard has 12 “dimensions”, because each assertion occurs in a kind of context, and there are 12 dimensions to the context: <code>Anthropacity</code>, <code>Time</code>, <code>GeoLocation</code>, <code>TypeOfPlace</code>, <code>TypeOfTime</code>, <code>Culture</code>, <code>Sophistication/Security</code>, <code>Topic</code>, <code>TopicGranularity</code>, <code>Modality/Disposition/Epistemology</code>, <code>Argument-Preference</code>, and <code>Justification</code>. For example, “Ronald Reagan is president” is true in the context <code>Time = 1985, GeoLocation = UnitedStates</code>, etc. These 12 dimensions were detailed in a 1998 technical report, though in that report, <code>Anthropacity</code> was called a “pseudo-dimension”, and replaced by <code>Let's</code>. <span class="citation" data-cites="lenatDimensionsContextspace1998">(<a href="#ref-lenatDimensionsContextspace1998" role="doc-biblioref">D. Lenat 1998</a>)</span></p>
<p>Other than the slightly anarchic microservice structure, Cyc could also use <a href="https://www.cs.umd.edu/projects/shop/">Simple Hierarchical Ordered Planner</a> to run the engines in order, if the workflow for how the engines should be run is known.</p>
<p>It was known very early on that the most general inference engine is the slowest, which is why they settled on a multi-layered structure of inference engines. Each inference is handled by the lowest engines first, and upon failing, the higher-levels try it.</p>
<p>Further, to allow the tacticians to know who to call, and the engines to know whether it is powerful enough to handle the query, the user can specify in a query over 150 adjustable parameters, such as max time limit, max number of answers desired, max number of backward-chaining steps, whether to introduce new terms, etc. In 2007, they found 6 combinations of these parameters that could answer almost all queries they tested with, which allowed them to clean up the user interface. Not a form with 150 blanks, but just a pick-1-in-6. <span class="citation" data-cites="lenatACS2022Invited2022">(<a href="#ref-lenatACS2022Invited2022" role="doc-biblioref">D. B. Lenat 2022b</a>)</span></p>
<p>Though there is a most general inference engine on the very top, they had noticed by 2007 that was so slow that turning it off made the system go <em>faster</em>! So they turned it off entirely in 2010. <span class="citation" data-cites="lenatACS2022Invited2022 lenatGettingGenerativeAI2023">(<a href="#ref-lenatACS2022Invited2022" role="doc-biblioref">D. B. Lenat 2022b</a>; <a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span></p>
<p>The heaven is high and the emperor is far away. The monologic is dead. Long live the swarm.</p>
</section>
<section id="querying" class="level3">
<h3 class="anchored" data-anchor-id="querying">Querying</h3>
<p>As a user, the knowledge base would be worth nothing if you cannot ask it questions and receive answers. The Cyc system, like most logical programming systems, answers queries by <a href="https://en.wikipedia.org/wiki/Unification_(computer_science)">logical unification</a>. That is, it converts a query into a logical formula <span class="math inline">\(\phi(x)\)</span> with a certain unknown <span class="math inline">\(x\)</span>, then returns all <span class="math inline">\(x\)</span> that makes <span class="math inline">\(\phi(x)\)</span> true, or if no such <span class="math inline">\(x\)</span> exists, then returns <code>FALSE</code>.</p>
<p>For a query like</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>(#$geopoliticalSubdivision #$Canada ?WHAT)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>the Cyc system applies various inference engines to find all bindings (aka substitutions) of the variable <code>?WHAT</code> that make the statement true in the knowledge base. The results will include all Canadian provinces and territories that have been asserted explicitly or can be inferred through rules.</p>
<p>Cyc provides 3 types of queries:</p>
<ul>
<li><code>ASK</code>: General-purpose queries that generate bindings for free variables along with a formal proof.</li>
<li><code>PROVE</code>: Conditional queries that handle universal quantification (<code>#$forAll</code>) by constructing temporary microtheories for temporary hypotheticals.</li>
<li><code>QUERY</code>: A wrapper around both <code>ASK</code> and <code>PROVE</code> in the Cyc Browser.</li>
</ul>
<p>To illustrate how querying works in practice, consider a scenario where we want to find all Canadian provinces. An <code>ASK</code> query would look like:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>(#$isa ?WHAT #$CanadianProvince)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Cyc processes this by looking for all terms (symbols that begin with <code>#$</code>) that satisfy the predicate when plugged into the variable <code>?WHAT</code>. The system might use transitivity rules, inheritance mechanisms, and various specialized inference engines to gather results. Behind the scenes, Cyc might employ backward-chaining to start from the query and work backward to known facts, or forward-chaining to derive new facts from existing ones, depending on which approach its tacticians determine is more efficient. The reply would be something like</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>((?WHAT . #$Manitoba-CanadianProvince))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>((?WHAT . #$BritishColumbia-CanadianProvince))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>((?WHAT . #$Alberta-CanadianProvince))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>((?WHAT . #$Ontario-CanadianProvince))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>((?WHAT . #$Quebec-CanadianProvince))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>((?WHAT . #$NovaScotia-CanadianProvince))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>When Cyc performs a query, it can also provide a deductive trace (forma proof) showing how it reached its conclusion. For example, when proving that Manitoba is a Canadian province, the deductive trace might look like:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lispCopyQuery: (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fl">1.</span> (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>     [Direct assertion in PoliticalGeographyMt]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fl">2.</span> Therefore, Manitoba-CanadianProvince is a CanadianProvince.</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>     [TRUE]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>Query: (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="fl">1.</span> (isa Manitoba-CanadianProvince CanadianProvince)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>     [Established above]</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="fl">2.</span> (implies </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>      (#$isa ?X #$CanadianProvince)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>      (geopoliticalSubdivision Canada ?X))</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>     [Rule in PoliticalGMt]</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>  <span class="fl">3.</span> Therefore, (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>     [Modus Ponens: <span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>     [TRUE]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For more complex queries involving logical implications, <code>PROVE</code> creates hypothetical microtheories to test universal claims. Such microtheories are useful, since the hypotheticals being considered are usually temporary and should not be mixed with the permanent assertions in the knowledge base. For example, to ask “Is every Canadian province a political subdivision of Canada?”, we use:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>(#$implies</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  (#$isa ?WHAT #$CanadianProvince)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  (#$geopoliticalSubdivision #$Canada ?WHAT))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>PROVE</code> handles this by creating a temporary “possible world” where it assumes the existence of a generic Canadian province and tests whether it must also be a political subdivision of Canada.</p>
<p>As previously stated, each time the user begins a question-and-answer session with the Cyc system, it creates a new microtheory just for this session, which allows it to keep track of what has been asked and responded to, so that the user can refer to things like “your previous answer”. Such a microtheory is usually deleted after the session is over, though it can also be saved if the user wishes to continue the session at a later date.</p>
<p>The querying interface also provides controls for managing the inference process, such as limiting search depth, maximal time allowed, the inference engines to be used, the parameters for the strategist and the tacticians, etc. These can help the Cyc system to solve difficult queries when the default settings are insufficient.</p>
</section>
<section id="natural-language-processing" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-processing">Natural language processing</h3>
<p>When it comes to the use of natural languages, there are two problems. The easy problem is to convert sentences in CycL into natural language. This is fairly simple and has been set up since around late 1990s, simply by constructing assertions between a concept and a concept of the English word for the concept. <span class="citation" data-cites="guhaReCycLingPaper1993">(<a href="#ref-guhaReCycLingPaper1993" role="doc-biblioref">Guha and Lenat 1993</a>)</span> For example, <code>(#$genls #$Dog #$Mammal)</code> would be transformed to “Dogs are mammals.”, and so on. Concretely, it involves entering sentences of the form (simplified):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode lisp code-with-copy"><code class="sourceCode commonlisp"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>(#$rootString #$TheEnglishWordPen <span class="st">"pen"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>(#$denotation #$TheEnglishWordPen #$WritingPen)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>(#$rootString #$TheFrenchWordPlume <span class="st">"plume"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>(#$denotation #$TheFrenchWordPlume #$WritingPen)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and similarly, Cyc could order the words correctly using logical assertions for English grammar in the form of <a href="https://en.wikipedia.org/wiki/Government_and_binding_theory">government and binding theory</a>.</p>
<p>Sure, such generated language will sound a bit wooden and formalistic, and would not win any literary award, but it solves the problem. The hard problem is to parse sentences in natural language into CycL, along with all its messiness.</p>
<p>Information about how Cyc solves the hard problem is sparse, but from what I gathered, it uses a tiered system from the fast and inaccurate to the slow and accurate.</p>
<p>Underlying all of its NLP is an English dictionary, also represented as concepts and assertions in CycL. The dictionary contains about 200K words and phrases, and more assertions about them. For example, the word “light” is represented as a concept <code>Light-theWord</code>, and there would be assertions stating that it can be a verb, or a noun, or an adjective, etc.</p>
<p>At the fastest tier are keyword matching, or concept spotting. For example, for parsing terrorism articles, it can just quickly match for the existence of keywords in a sentence. Seeing a sentence that looks like “… al-Qaeda … embassy … grenade … suicide attack…”, the system can assume, with high probability, what the sentence means, and generate its CycL representation.</p>
<p>At a slower tier is using extraction templates. For example, “[A] was involved in kidnapping [B]” would be matched to a template that looks for the fragment “involved in kidnapping”, which then would parse [A] as the <code>Perpetrator</code> while [B] as the <code>Victim</code>.</p>
<p>At a slower tier is example-based machine translation by syntax templates. For example, in “… the heart of Baghdad …”, the first pass parses Badhdad as a city, then a syntax template activates and it parses the “heart” as <code>Downtown</code>. This can be coded as</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode xml code-with-copy"><code class="sourceCode xml"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>&lt;<span class="kw">template</span>&gt;</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">nlPattern</span><span class="ot"> class=</span><span class="st">"140080"</span>&gt;the heart of $City#0&lt;/<span class="kw">nlPattern</span>&gt;</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">cyclPattern</span>&gt;</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        (#$equalSymbols ?D (#$DowntownFn $City#0))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    &lt;/<span class="kw">cyclPattern</span>&gt;</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">variable</span>&gt;?D&lt;/<span class="kw">variable</span>&gt;</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">type</span>&gt;#$Downtown&lt;/<span class="kw">type</span>&gt;</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>&lt;/<span class="kw">template</span>&gt;</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>At the slowest tier is <a href="https://web.archive.org/web/20130511232427/http://www.cyc.com/cyc/nl">full syntax tree parsing</a>. In this tier, the sentence is fully parsed to a syntax tree using a manually written transformational grammar under the <a href="https://en.wikipedia.org/wiki/Government_and_binding_theory">government and binding theory</a>. It is then parsed semantically using <a href="https://en.wikipedia.org/wiki/Montague_grammar">Montague grammar</a>.</p>
<p>Whereas the CycL-to-English part is already workable mostly in 2001, the English-to-CycL part is still ongoing work, a consummation devoutly to be wished. Even though Lenat originally thought natural language understanding would be finished soon after priming the knowledge pump, it has already been 8 years since the priming, and Cyc is still not reading the world’s writings and learning autonomously.</p>
</section>
<section id="machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning">Machine learning</h3>
<p>Cyc has used statistical and machine learning methods in minor parts, such as using neural networks, n-gram methods, random forest, support vector machines, etc, to automatically extract templates for natural language processing, classify the microtheories that a sentence belongs to, etc.</p>
<p>However, none of these were at all what Lenat meant when he talked of “machine learning”, by which he meant a Cyc machine, with knowledge pump fully primed, would begin to perform experiments and learn by automated discovery, much like AM and EURISKO were meant to do. While this has always been the “phase 3” of the Cyc project, to this day, Cyc has made no progress in this area.</p>
</section>
</section>
<section id="cycops-what-is-it-good-for" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="cycops-what-is-it-good-for">CycOps, what is it good for?</h2>
<section id="sec-cycops-overview" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-cycops-overview">Overview</h3>
<p>In America, at least before the Deep Learning era, the government funded most large-scale efforts in AI, usually in the context of military and intelligence applications. This was true for the Cyc project. Indeed, it initiated in 1984-07 within the <a href="https://en.wikipedia.org/wiki/Microelectronics_and_Computer_Technology_Corporation">Microelectronics and Computer Consortium</a> (MCC), which, like the <a href="https://en.wikipedia.org/wiki/Strategic_Computing_Initiative">Strategic Computing Initiative</a>,<!-- todo: link to SCI --> was formed in reaction to the threat of the Japanese <a href="https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems">Fifth Generation Computer Systems</a> project<!-- todo: link to SCI#FGCS instead -->. Though not <em>directly</em> funded by the government, its head was <a href="https://en.wikipedia.org/wiki/Bobby_Ray_Inman">Bobby Inman</a>, who had previously held high positions in the Navy, the NSA, and the CIA, so draw your own conclusion.</p>
<p>In 1995-01, as MCC wound down, the Cyc group formed Cycorp Inc., a for-profit company, to continue their mission. Immediately after that point, academic publication and generally candid conversation almost ceased. Who bought the services of Cyc, and for what? The details are slim. Trade secrets, no doubt. Confirmed results:<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;I swear I’m not a policy wonk, but 3 days of cyc-berstalking does take its toll.</p></div></div><ul>
<li><a href="https://en.wikipedia.org/wiki/Lycos">Lycos search engine</a>, to disambiguate search terms. It ended in 2001. (<a href="https://web.archive.org/web/20150905165226/http://www.cyc.com/about/media-coverage/computer-save-world/">Source</a>)</li>
<li>Department of Defense, in 2001, to “clean dataset”. <span class="citation" data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span></li>
<li>Glaxo Wellcome, <a href="https://en.wikipedia.org/wiki/Digital_Equipment_Corporation">DEC</a>, IBM, and <a href="https://en.wikipedia.org/wiki/UnitedHealth_Group">United Healthcare</a>, sometime before 1997, for unknown purposes. <span class="citation" data-cites="portDuelingBrainscapesArtificial1997">(<a href="#ref-portDuelingBrainscapesArtificial1997" role="doc-biblioref">Port 1997</a>)</span>
<ul>
<li>For Glaxo Wellcome, and United Healthcare, it was probably for the purpose described below.</li>
<li>For DEC and IBM, probably to produce specialized expert systems – after all, DEC was famous for using the expert system <a href="https://en.wikipedia.org/wiki/Xcon">XCON</a> that saved them $40M/year. <span class="citation" data-cites="rolandStrategicComputingDARPA2002">(<a href="#ref-rolandStrategicComputingDARPA2002" role="doc-biblioref">Roland and Shiman 2002, 191</a>)</span></li>
</ul></li>
<li>GlaxoSmithKline, before 2001, to “clean dataset” <span class="citation" data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span>, probably meaning to manage a thesaurus of pharmaceutical chemicals and health-care words. (<a href="https://web.archive.org/web/20130720051038/http://www.cyc.com/enterprise-solutions/success-stories/pharmaceutical-thesaurus-management">Source</a>) Probably started in 1997 <span class="citation" data-cites="portDuelingBrainscapesArtificial1997">(<a href="#ref-portDuelingBrainscapesArtificial1997" role="doc-biblioref">Port 1997</a>)</span></li>
<li><a href="https://web.archive.org/web/20101224222259/http://cyc.com/cyc/applications/CycSecure/">CycSecure</a>, a network vulnerability assessment tool, first beta in 2002. <span class="citation" data-cites="anthesCycUse2002">(<a href="#ref-anthesCycUse2002" role="doc-biblioref">Anthes 2002a</a>)</span> Trialed at the US Strategic Command Computer Emergency Response Team at some unknown point before 2005. <span class="citation" data-cites="shepardKnowledgebasedApproachNetwork2005">(<a href="#ref-shepardKnowledgebasedApproachNetwork2005" role="doc-biblioref">Shepard et al. 2005</a>)</span></li>
<li><a href="https://en.wikipedia.org/wiki/Paul_Allen">Paul Allen</a> (co-founder of Microsoft) had funded Cycorp sometime before 2001 for unknown purposes and for an unknown sum. <span class="citation" data-cites="hiltzikBirthThinkingMachine2001">(<a href="#ref-hiltzikBirthThinkingMachine2001" role="doc-biblioref">Hiltzik 2001</a>)</span> In 2003, he funded it by $0.7 million as part of his project of “Digital Aristotle”, to create a tutoring AI. <span class="citation" data-cites="richmanAllenClaimsSuccess2003 friedlandProjectHaloDigital2004">(<a href="#ref-richmanAllenClaimsSuccess2003" role="doc-biblioref">Richman 2003</a>; <a href="#ref-friedlandProjectHaloDigital2004" role="doc-biblioref">Friedland et al. 2004</a>)</span></li>
<li>DARPA’s <a href="https://en.wikipedia.org/wiki/High_Performance_Knowledge_Bases">High Performance Knowledge Base</a> program (unknown duration, but must be within 1997–1999, the duration of the full project) and KRAKEN<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> under the Rapid Knowledge Formation program (2000–2004). <span class="citation" data-cites="kingstonHighPerformanceKnowledge2001 matthewsKRAKENKnowledgeRich2004 witbrockKnowledgeBegetsKnowledge2005">(<a href="#ref-kingstonHighPerformanceKnowledge2001" role="doc-biblioref">Kingston 2001</a>; <a href="#ref-matthewsKRAKENKnowledgeRich2004" role="doc-biblioref">Matthews et al. 2004</a>; <a href="#ref-witbrockKnowledgeBegetsKnowledge2005" role="doc-biblioref">Witbrock et al. 2005</a>)</span></li>
<li>Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) program, sometime between 2001 and 2004. Its title is self-explanatory. <span class="citation" data-cites="lenatBuildingMachineSmart2009">(<a href="#ref-lenatBuildingMachineSmart2009" role="doc-biblioref">D. B. Lenat 2009</a>)</span> (<a href="https://logicmoo.org/xwiki/bin/download/Main/References/Citations/WebHome/AQUAINT-Kickoff-Overview-Prange.pdf?rev=1.1">Source</a>)</li>
<li>The <a href="https://en.wikipedia.org/wiki/MIPT_Terrorism_Knowledge_Base">Terrorism Knowledge Base</a> (2004–2008), <a href="#sec-terrorism-knowledge-base">detailed below</a>.</li>
<li>Cleveland Clinic, starting in 2007, to parse clinician queries into database queries and returns the result. <span class="citation" data-cites="lenatHarnessingCycAnswer2010 pierceSemanticDBSemanticWeb2012">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">D. B. Lenat et al. 2010</a>; <a href="#ref-pierceSemanticDBSemanticWeb2012" role="doc-biblioref">Pierce et al. 2012</a>)</span></li>
<li>Goldman Sachs, sometime around 2016, to “monitor the inner workings of its technological infrastructure” and detect insider trading. <span class="citation" data-cites="metzOneGeniusLonely2016 shilohHeTaughtAI2023">(<a href="#ref-metzOneGeniusLonely2016" role="doc-biblioref">Metz 2016</a>; <a href="#ref-shilohHeTaughtAI2023" role="doc-biblioref">Shiloh 2023</a>)</span></li>
<li>The CIA and the Department of Defense, at an unknown time, for unspecified purposes, though probably to identify terrorist threats in connection with the Terrorist Knowledge Base. <span class="citation" data-cites="shilohHeTaughtAI2023">(<a href="#ref-shilohHeTaughtAI2023" role="doc-biblioref">Shiloh 2023</a>)</span></li>
<li>The NSA, to “identify terrorist threats in international communications data”. <span class="citation" data-cites="metzOneGeniusLonely2016">(<a href="#ref-metzOneGeniusLonely2016" role="doc-biblioref">Metz 2016</a>)</span></li>
<li>The <a href="https://en.wikipedia.org/wiki/Total_Information_Awareness">Total Information Awareness</a> project funded Cycorp for $9.8 million in 2003 for a “prototype database” and a system that could “identify phone-calling patterns as they might exist among potential terrorists overseas”. <span class="citation" data-cites="crensonBigBrotherCould2003">(<a href="#ref-crensonBigBrotherCould2003" role="doc-biblioref">Crenson 2003</a>)</span></li>
<li>Electronic Surveillance System for the Early Notification of Community-Based Epidemics-II (ESSENCE-II), around 2006. Its title is self-explanatory. <span class="citation" data-cites="abbottIntegratedBiologicalWarfare2007">(<a href="#ref-abbottIntegratedBiologicalWarfare2007" role="doc-biblioref">Abbott et al. 2007</a>)</span></li>
<li>Seven unnamed big companies, for unspecified expert system applications <span class="citation" data-cites="cycorpCycTechnologyOverview2021">(<a href="#ref-cycorpCycTechnologyOverview2021" role="doc-biblioref">Cycorp 2021</a>)</span>, but probably “common-sense platform for their applications, and as an interlingua to fully, semantically integrate all the data they generate and all the data they license from third parties”. <span class="citation" data-cites="lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">D. B. Lenat 2022a</a>)</span></li>
<li>Unnamed semiconductor foundry, for an expert system for root-cause analysis of fabrication yields. (<a href="https://web.archive.org/web/20130805145356/http://www.cyc.com/enterprise-solutions/success-stories/semiconductor-yield-management">Source</a>)</li>
<li>Unnamed big bank, for an expert system for IT support and inventory management. (<a href="https://web.archive.org/web/20130805145023/http://www.cyc.com/enterprise-solutions/success-stories/it-inventory-catalog-and-configuration-management">Source</a>)</li>
<li>Unnamed big bank, for an expert system for IT personnel expertise management. (<a href="https://web.archive.org/web/20130619021204/http://www.cyc.com/enterprise-solutions/success-stories/help-desk-expertise-management">Source</a>)</li>
<li>Unnamed oil company, for an expert system for monitoring and predicting breakdowns at oil pumping facilities. (<a href="https://web.archive.org/web/20130805144921/http://www.cyc.com/enterprise-solutions/success-stories/facilities-status-monitoring-and-alerting">Source</a>)</li>
<li>A learn-by-teaching game for 6th-grade arithmetics. It began in 2014 as project “BELLA”, funded by the <a href="https://en.wikipedia.org/wiki/Advanced_Distributed_Learning">Advanced Distributed Learning Initiative</a> under the <a href="https://en.wikipedia.org/wiki/United_States_Department_of_Defense">Department of Defense</a> <span class="citation" data-cites="lenatReinforcingMathKnowledge2014">(<a href="#ref-lenatReinforcingMathKnowledge2014" role="doc-biblioref">D. B. Lenat and Durlach 2014</a>)</span>, and was commercialized in 2016 as <a href="https://cyc.com/mathcraft/">MathCraft</a>. In the game, the student takes on the role of a tutor to an AI avatar, who would make various mistakes that the student would correct. It uses a small fragment of Cyc to model both the student’s and the avatar’s states of knowledge. Though it <a href="https://www.youtube.com/watch?v=pbrp7MzBDm0">looks very polished</a>, I did not find confirmed instances of MathCraft being used in a school.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;Despite “KRAKEN” supposedly “not an abbreviation and does not require further definition” <span class="citation" data-cites="cerutiKnowledgeManagementCommand2004">(<a href="#ref-cerutiKnowledgeManagementCommand2004" role="doc-biblioref">Ceruti, Wilcox, and Powers 2004</a>)</span>, it was still defined as “Knowledge Rich Acquisition of Knowledge from Experts who are Non-logicians” by the Cycorp. <span class="citation" data-cites="matthewsKRAKENKnowledgeRich2004">(<a href="#ref-matthewsKRAKENKnowledgeRich2004" role="doc-biblioref">Matthews et al. 2004</a>)</span></p></div><div id="fn17"><p><sup>17</sup>&nbsp;But there’s also this:</p>
<blockquote class="blockquote">
<p>Once, developing a scenario for a terrorist attack on Hoover Dam, it hypothesized a school of 1,000 al Qaeda-trained dolphins bearing explosives.</p>
<p><span class="citation" data-cites="hawkinsPredictingTerroristsNext2003">(<a href="#ref-hawkinsPredictingTerroristsNext2003" role="doc-biblioref">Hawkins 2003</a>)</span></p>
</blockquote>
</div></div><p>Looking at the list, we see that much of Cycorp funding came from the American intelligence community, especially between 2001 and 2010, during the heights of the <a href="https://en.wikipedia.org/wiki/War_on_terror">War on Terror</a>, as the American state struggled to expand its sovereign eye over the sprawling cyberspace. Indeed, one of the early success was when it “predicted anthrax might be sent through the mail six months before trove of knowledge about past terrorist activities, tactics, and weapons”. <span class="citation" data-cites="hawkinsPredictingTerroristsNext2003">(<a href="#ref-hawkinsPredictingTerroristsNext2003" role="doc-biblioref">Hawkins 2003</a>)</span> Though the success did not help anyone, it was great advertisement.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> Corroborating, Lenat in a <a href="https://youtu.be/gAtn-4fhuWA?si=gAQ-TISZxxgeD1VN&amp;t=1856">2006 Google Talk</a> showed screenshots of Cyc answering “Which American city would be most vulnerable to an anthrax attack during summer?”. (The answer was “Phoenix”.)</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Cyc_anthrax_phoenix.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Screenshot of Cyc answering “Which American city would be most vulnerable to an anthrax attack during summer?”. The system replied “Phoenix”, with reasoning. <span class="citation" data-cites="lenatGoogleTechTalksComputers2006">(<a href="#ref-lenatGoogleTechTalksComputers2006" role="doc-biblioref">D. B. Lenat 2006</a>)</span></figcaption>
</figure>
</div>
<p>The total funding of the project is hard to know, although we know that in 2002, its <a href="https://stanfordmag.org/contents/wise-up-dumb-machine">total cost had been $60M</a>, of which <a href="https://web.archive.org/web/20120502151103/http://www.opencyc.org/cyc/company/news/APArticle060902">$25M came from the military</a>, so I think it’s fair to say 50% came from the military. This is corroborated in 2005:</p>
<blockquote class="blockquote">
<p>In 1996, we got our first substantial government contract,” Lenat recalls. Since then, Cycorp has collected about half of its revenue from U.S. government agencies and the rest from companies, mostly for building “semantic maps” that help users pull information from various databases with a single query. By taking on paying projects, Cycorp has been able to stay profitable and debt-free. All of the firm’s stock is owned by its employees, making Cycorp answerable only to Cycorp. “But,” Lenat admits, “we have had to tack with the funding winds. Maybe 50 percent of the funding we get pushes us forward in the direction that we need to go.”</p>
<p>Cycorp doesn’t even want to be distracted by the rigors of the retail software business; instead, it licenses Cyc for use in third-party software packages… The time may come, Lenat says, when a greatly expanded Cyc will underlie countless software applications. But reaching that goal could easily take another two decades.</p>
<p><span class="citation" data-cites="woodCycorpCostCommon2005">(<a href="#ref-woodCycorpCostCommon2005" role="doc-biblioref">Wood 2005</a>)</span></p>
</blockquote>
<p>Out of all these applications, only two had been reported in detail.</p>
</section>
<section id="sec-terrorism-knowledge-base" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-terrorism-knowledge-base">Terrorism Knowledge Base</h3>
<p>The first application is the <a href="https://en.wikipedia.org/wiki/MIPT_Terrorism_Knowledge_Base">Terrorism Knowledge Base</a> (TKB), created in 2004 and shut down in 2008. <span class="citation" data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span> During the aftermath of 9/11, the American government funded a massive expansion of surveillance and data processing, and the Cycorp took on several of such contracts. The TKB is the only one about which we know in some detail.</p>
<p>The system can be browsed like a local Wikipedia – if Wikipedia were focused entirely on terrorism. TKB contained &gt;2000 terrorists, &gt;700 terrorist groups, &gt;6500 terrorist attacks, and &gt;200,000 assertions such as “Xavier Djaffor participated in the Jihad from 1996 to 2000” and “<a href="https://en.wikipedia.org/wiki/Lashkar-e-Taiba">Lashkar-e-Taiba</a> is an Islamist terror group founded in 1990”.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/TKB_browser.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Browsing the TKB about <a href="https://en.wikipedia.org/wiki/Imad_Mughniyeh">Imad Fayez Mughniyeh</a>. <span class="citation" data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span></figcaption>
</figure>
</div>
<p>A user query would be processed in 4 steps.</p>
<ol type="1">
<li>User enters question in formal, but still natural, English.</li>
<li>Cyc parses the question by keyword matching, template matching, and syntactic rules, then applies domain and common sense constraints to fix the parse, then retrieves some CycL fragments that are the closest matches to what the user entered.</li>
<li>The user clicks on the fragments they meant. Cyc synthesizes a full query in CycL. The user optionally modifies the CycL query.</li>
<li>Cyc runs inference engines to retrieve the answer along with a logic chain for the answer.</li>
</ol>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/TKB_query.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Asking the TKB about “What terrorists and biological agents are such that the terrorist is capable of learning to make the biological agent?”. Its returns included <a href="https://en.wikipedia.org/wiki/Masami_Tsuchiya_(terrorist)">土谷正実</a> from <a href="https://en.wikipedia.org/wiki/Aum_Shinrikyo">Aum Shinrikyo</a>. <span class="citation" data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span></figcaption>
</figure>
</div>
<p>Like querying, data entry also has a light amount of parsing, and the system attempts to fill a form with it. The user can then fix the form. An intelligence specialist, lightly trained in using it, could enter up to 100 assertions per hour.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/TKB_data_entry.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The data entry form. The system is prompting the user to disambiguate “skating boy” between “boy who is a doer of skating” and “boy who performs skating professionally”. <span class="citation" data-cites="cycorpTerrorismKnowledgeBase2008">(<a href="#ref-cycorpTerrorismKnowledgeBase2008" role="doc-biblioref">Cycorp 2008</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="cleveland-clinic" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="cleveland-clinic">Cleveland Clinic</h3>
<p>The second application took place at the Cleveland Clinic, sometime during 2007–2010.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> The system was called “Semantic Research Assistant” (SRA), and it could answer queries about <a href="https://en.wikipedia.org/wiki/Cardiothoracic_surgery">cardiothoracic surgery</a>, <a href="https://en.wikipedia.org/wiki/Cardiac_catheterization">cardiac catheterization</a>, and <a href="https://en.wikipedia.org/wiki/Percutaneous_coronary_intervention">percutaneous coronary intervention</a> – basically, surgery-relevant questions about the heart.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;This paper is the only substantial public information about this collaboration I can find. Therefore, I can only confirm that Cyc had been used in Cleveland Clinic during 2007–2010. It may have lasted to at least 2021, since <span class="citation" data-cites="cycorpCycTechnologyOverview2021">(<a href="#ref-cycorpCycTechnologyOverview2021" role="doc-biblioref">Cycorp 2021</a>)</span> still cited this application.</p></div></div><div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Semantic_Research_Assistant.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The 4-step process of the SRA. It is essentially the same process as in the TKB. <span class="citation" data-cites="lenatHarnessingCycAnswer2010">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">D. B. Lenat et al. 2010, fig. 2</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/SRA_logical_proof.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">A logical justification constructed by Cyc by backward chaining on a user query. The original query is not shown, but it probably should be “For each instance of pericardial aortic valve replacement event in 2008, which event was it, and which type of pericardial aortic valve prosthesis was it?”. <span class="citation" data-cites="lenatHarnessingCycAnswer2010">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">D. B. Lenat et al. 2010, fig. 4</a>)</span></figcaption>
</figure>
</div>
<p>There was one subsequent report on the system in 2012, which described the SemanticDB project at Cleveland, of which Cyc was only a part. The SemanticDB system contains a database of 120M semantic triples (which Lenat had long dismissed as being too limited). In the system, Cyc parses <a href="https://en.wikipedia.org/wiki/Cohort_analysis">cohort identification queries</a> written in English into formal queries, then queries the database in <a href="https://en.wikipedia.org/wiki/SPARQL">SPARQL</a>, does some further inference, and shows the result. <span class="citation" data-cites="pierceSemanticDBSemanticWeb2012">(<a href="#ref-pierceSemanticDBSemanticWeb2012" role="doc-biblioref">Pierce et al. 2012</a>)</span> Unsurprisingly, Oracle Semantic Technologies <a href="https://download.oracle.com/otndocs/tech/semantic_web/pdf/oow10_semtech_clvclnc_booth.pdf">was also involved</a>.</p>
<p>In a <a href="https://web.archive.org/web/20210430003227/https://files.gotocon.com/uploads/slides/conference_13/724/original/AI_GOTO%20Lenat%20keynote%2030%20April%202019%20hc.pdf">presentation in 2019</a>, Lenat claimed it required 120K new assertions for the Cleveland project, or 0.5% of the total knowledge base. However, 95% of the assertions that Cyc called up for answering queries for the project required knowledge Cyc already had, indicating large knowledge reuse (indicating that the whole project required about 2M rules).</p>
</section>
<section id="is-that-all" class="level3">
<h3 class="anchored" data-anchor-id="is-that-all">Is that all?</h3>
<p>Well, I tried my best to look for more applications, but the fact is that there were so few of them. Out of all the confirmed instances of applications, the above two were the <em>only</em> ones reported in detail.</p>
<p>So far, we have only considered applications of the full Cyc system. We have mentioned that there were both the OpenCyc and the ResearchCyc, two smaller versions of the full Cyc. Did they have substantial applications? As far as I can discern, no. Indeed, just as OpenCyc failed to bring about the Semantic Web or impress the general public, ResearchCyc failed to bring about a revolution in knowledge engineering or impress the general academia, or essentially anyone outside of Cycorp itself. Even Ernest Davis and Gary Marcus, highly sympathetic to the symbolic approach to AI, found little evidence for the success of Cyc, not because Cyc had provably failed, but simply because there was too little evidence in any direction, success or failure.</p>
<blockquote class="blockquote">
<p>… it is in fact very difficult for an outsider to determine what has been accomplished here. In its first 15 years, CYC published astonishingly little. Since about 2002, somewhat more has been published, but still very little, considering the size of the project. No systematic evaluation of the contents, capacities, and limitations of CYC has been published. A number of organizations have done private evaluations but the results were not published.</p>
<p>It is not, for example, at all clear what fraction of CYC actually deals with commonsense inference, and what fraction deals with specialized applications such as medical records or terrorism. It is even less clear what fraction of commonsense knowledge of any kind is in CYC. … There are not even very many specific examples of commonsense reasoning carried out by CYC that have been published.</p>
<p><span class="citation" data-cites="davisCommonsenseReasoningCommonsense2015">(<a href="#ref-davisCommonsenseReasoningCommonsense2015" role="doc-biblioref">Davis and Marcus 2015</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>On the whole, it is fair to say that the AI community regards CYC as a very elaborate failure. Domingos (2015 p.&nbsp;51) characterizes it as “the most notorious failure in the history of AI”. Domingos is a researcher in machine learning and has little use for any kind of knowledge-based methods, so his phrasing is certainly harsh, but in our experience, this opinion, more or less, is common even in the knowledge representation (KR) community. … it would be very helpful, and it would, we believe, significantly improve CYC’s standing in the AI community, if the CYC team could demonstrate some specific task where CYC really visibly shines. … let them design their own task. As Watson demonstrates, passing a self-imposed task can be impressive enough, depending on the task, and in any case it is much better than nothing. At the moment, a person who is asked, “What interesting thing has been done with CYC?” is largely at a loss for an answer.</p>
<p><span class="citation" data-cites="davisEvaluatingCYCPreliminary2016">(<a href="#ref-davisEvaluatingCYCPreliminary2016" role="doc-biblioref">Davis 2016</a>)</span></p>
</blockquote>
<p>There is a kind of insularity in Cycorp that starts to affect you if you look too closely into it. I know I was affected. Many times I had opened a paper that purported to show the application of Cyc, and was disappointed to find that it was yet another paper about the application of a method to ingest knowledge <em>into</em> Cyc, rather than a method to apply knowledge <em>out from</em> Cyc. I came to dread the literature review, as Cyc in my mind took on the sinister appearance of a black hole at the center of the knowledge graph, a cocoon that would never metamorphose into a butterfly.</p>
</section>
</section>
<section id="everyone-can-only-see-their-own-dream" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="everyone-can-only-see-their-own-dream">Everyone can only see their own dream</h2>
<section id="lenats-tenets" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="lenats-tenets">Lenat’s tenets</h3>
<p>In <span class="citation" data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">D. B. Lenat and Feigenbaum 1991</a>)</span>, a paper coauthored with Feigenbaum, Lenat gave the most comprehensive statement for where he stands philosophically, which he held onto for the rest of his life:</p>
<ul>
<li>Knowledge Principle. A system exhibits intelligent understanding and action at a high level of competence primarily because of the knowledge that it can bring to bear: the concepts, facts, representations, methods, models, metaphors, and heuristics about its domain of endeavor.</li>
<li>Explicit Knowledge Principle. While knowledge may be compiled to opaque lumps of code for efficiency, there should always be a declarative version of that, so that they can be subject to meta-reasoning.</li>
<li>Breadth Hypothesis. Intelligent performance often requires the problem solver to fall back on increasingly general knowledge, and/or to analogize to specific knowledge from far-flung domains.</li>
<li>Empirical Inquiry Hypothesis. The most profitable way to investigate AI is to embody our hypotheses in programs, and gather data by running the programs. The surprises usually suggest revisions that start the cycle over again. Progress depends on these experiments being able to falsify our hypotheses. Falsification is the most common and yet most crucial of surprises. In particular, these programs must be capable of behavior not expected by the experimenter.</li>
<li>Difficult Problems Hypothesis. There are too many ways to solve simple problems. Raising the level and breadth of competence we demand of a system makes it easier to test – and raise – its intelligence.</li>
<li>Knowledge Is All There Is Hypothesis. No sophisticated, as-yet-unknown <em>control structure</em> is required for intelligent behavior.</li>
<li>The Local Consistency Hypothesis. There is no need–and probably not even any possibility–of achieving a global consistent unification of several expert systems’ KBs (or, equivalently, for one very large KB). Large systems need local consistency.</li>
<li>The Coherence Hypothesis. Moreover, whenever two large internally consistent chunks C1, C2 are similar, their heuristics and analogies should cohere; e.g., if the “going up” metaphor usually means “getting better” for C1, then it should again mean “getting better” for C2, or else it should not apply at all there.</li>
</ul>
<p>Lenat is the very example of a <a href="https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox">hedgehog</a>: a single philosophy, a vision for AGI, pursued for 40 years. One does not pursue a single vision without rejecting alternative visions, and Lenat has been explicit in rejecting every alternative route to AGI, using his sharp tongue <span class="citation" data-cites="lenatThresholdsKnowledge1991 thompsonKnowItAllMachine2001 lenatVoiceTurtleWhatever2008 lenatGettingGenerativeAI2023">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">D. B. Lenat and Feigenbaum 1991, sec. A.2</a>; <a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>; <a href="#ref-lenatVoiceTurtleWhatever2008" role="doc-biblioref">D. B. Lenat 2008</a>; <a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span>:</p>
<ul>
<li>Logical AI in the style of Simon and Newell’s General Problem Solver. Such an elegant framework would not work beyond toy problem domains, by the Knowledge Principle.</li>
<li>Highly accurate models of human behavior, in the style of Simon and Newell’s <em>Human Problem Solving</em> or the <a href="https://en.wikipedia.org/wiki/Soar_(cognitive_architecture)">SOAR architecture</a>. Duplicating human cognitive architecture constitutes cargo cult science. AGI need not have the <a href="https://en.wikipedia.org/wiki/The_Magical_Number_Seven%2C_Plus_or_Minus_Two">magic number 7 ± 2</a>.</li>
<li>Physical embodiment. It might be great fun to make robots, but physical embodiment is neither necessary nor sufficient for “grounding” the knowledge base, by the <a href="https://en.wikipedia.org/wiki/Physical_symbol_system">Physical Symbol System Hypothesis</a>. A “mystical worship of physical embodiment” would only delay AGI. In particular, the <a href="https://en.wikipedia.org/wiki/Subsumption_architecture">subsumption architecture</a> does not lead to AGI.</li>
<li>Genetic algorithms and other evolutionary algorithms. It gets stuck in local minima too often, and runs too slowly.</li>
<li>Creating tiny morsels of little expert systems, and hope that bit by bit, AGI would emerge out of that. Remember that plateau-hopping requires breadth. Without an overarching plan, they will not fit together, like how the 1980s expert systems could never talk to each other.</li>
<li>Logical machine learning without a large knowledge base already in place. It makes for good demos, but quickly exhausts itself. These are examples of the illusory hope for “free lunch” or elegant “Maxwell’s equations of thinking”, a severe case of laziness and “physics envy”. Researchers should stop sitting on their asses mad with “physics envy”, and start the dirty work of coding.</li>
<li>Statistical machine learning, pattern matching, neural networks, and other self-organization methods. Just wait for enough compute and data, then magically a large model would learn on its own? Yet more wishful thinking for “free lunch”, caused by laziness and “physics envy”.</li>
<li>Any form of machine learning without a large knowledge base to begin with. This is impossible because learning is possible only at the fringe of knowing. Any attempt to learn without a large starting knowledge base is, again, trying to get a “free lunch”.</li>
<li>Wait until philosophers have figured out the one true ontology for the world, then build the Cyc accordingly. Philosophers suffered from “Hamlet syndrome”, unwilling to take decisive action, satisfied with publishing tiny morsels of ontologies that don’t cover the whole world, or grand ontologies that cover a caricature of the whole world.</li>
</ul>
<p>Lenat had his own grand historical vision for AI, which I call the 3 Optima Theory.</p>
<p>With a little hard work (about 6 person-months), one can get a knowledge-free system working, such as self-organized neural networks, Simon and Newell’s General Problem Solver, etc. This allows the researcher to publish a quick paper, a student to earn their PhD degree, and so on. Putting in more hard work does not result in a better system, but usually makes things worse as the code becomes bloated and unmanageable. Academic myopia stops people from trying to get out of this local maximum, since people just want to get published papers.</p>
<p>With a lot more hard work (about 10 person-years), one can get a system with a lot of specialized knowledge working. This is where the commercialized expert systems live. However, the general consensus is that as an expert system grows beyond 10K rules, it starts to suffer from its weight of all the rules. Standard expert systems were built for special fields, so people would use a simple language that works, but eventually collapses under the weight of 100K rules. Commercial myopia stops people from trying to get out of this local maximum, since people just want to sell products, and 50K rules is good enough for the customer.</p>
<p>The problem is that all these efforts are wasted. Specialized expert systems cannot be glued together efficiently, because each of them lives in a differently simplified world. That is, “plateau-hopping requires breadth”. The AI field as a whole would stagnate. The only way out of this is to go for the full common sense, to invest in the 2000 person-years of effort, and make a Cyc. After that, all the expert systems can interface with Cyc, and with each other using CycL, and all the computers can be preinstalled with their digital common sense.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Lenat_3_optima_theory.svg" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Lenat’s 3 Optima Theory for the grand history of AI.</figcaption>
</figure>
</div>
</section>
<section id="a-hostile-assessment-of-cyc" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="a-hostile-assessment-of-cyc">A hostile assessment of Cyc</h3>
<blockquote class="blockquote">
<p><strong>bogosity</strong>: At CMU, bogosity is measured with a bogometer; in a seminar, when a speaker says something bogus, a listener might raise his hand and say “My bogometer just triggered”… The agreed-upon unit of bogosity is the microLenat.</p>
<p><strong>microLenat</strong>: The unit of bogosity. Abbreviated µL or mL in ASCII. Consensus is that this is the largest unit practical for everyday use. The microLenat, originally invented by David Jefferson, was promulgated as an attack against noted computer scientist Doug Lenat by a tenured graduate student at CMU. Doug had failed the student on an important exam because the student gave only “AI is bogus” as his answer to the questions. The slur is generally considered unmerited, but it has become a running gag nevertheless.</p>
<p>— <a href="http://www.catb.org/jargon/html/M/microLenat.html">The Jargon File</a></p>
</blockquote>
<p>It is hard to interpret the state of Cyc today, if we take Lenat’s word for it:<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;Feel free to stop reading at this point. It is about to get polemical. Indeed, I have noticed that articles have the nefarious tendency to start from the personal (“the <a href="https://en.wikipedia.org/wiki/Narrative_hook">hook</a>”), then become informative, and then subtly slide to the polemical (“the call to action”). In fact, I have mastered the art of first scrolling the page with my eyes blurred so that the <a href="https://en.wikipedia.org/wiki/Hook_(boxing)">hook</a> cannot land on my head, and then, as soon as the text utters the first syllable of the ca–[tab closed]</p></div></div><ul>
<li>There were 150 technical challenges to knowledge engineering and representation at the start of Cyc in 1984, but they were all solved by 1990. <span class="citation" data-cites="lenatBuildingMachineSmart2009 lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatBuildingMachineSmart2009" role="doc-biblioref">D. B. Lenat 2009</a>, <a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">2022a</a>)</span></li>
<li>Cyc could already be tutored in (constrained) natural language in 2001. <span class="citation" data-cites="anthesComputerizingCommonSense2002">(<a href="#ref-anthesComputerizingCommonSense2002" role="doc-biblioref">Anthes 2002b</a>)</span></li>
<li>The upper ontology has remained stable for years as of 2015. <span class="citation" data-cites="lenat50ShadesSymbolic2015">(<a href="#ref-lenat50ShadesSymbolic2015" role="doc-biblioref">D. B. Lenat 2015</a>)</span></li>
<li>The knowledge pump is 95% primed in 2015, when there were just 15M assertions, <span class="citation" data-cites="lenat50ShadesSymbolic2015">(<a href="#ref-lenat50ShadesSymbolic2015" role="doc-biblioref">D. B. Lenat 2015</a>)</span>, and as of 2021, there were over 25M assertions <span class="citation" data-cites="cycorpCycTechnologyOverview2021">(<a href="#ref-cycorpCycTechnologyOverview2021" role="doc-biblioref">Cycorp 2021</a>)</span>.</li>
<li>SubLisp is easy to learn, and knowledge engineering in SubLisp is 1000× more efficient than in a modern language like Python. <span class="citation" data-cites="lenatCycQuestSolve2021">(<a href="#ref-lenatCycQuestSolve2021" role="doc-biblioref">D. B. Lenat 2021</a>)</span></li>
<li>The Cycorp had been profitable since its inception, had never taken on debt, had been almost entirely employee-owned, had always had only around 50–200 employees, a mostly flat corporate structure, and could remain profitable entirely on doing business with non-government corporations in 2022. <span class="citation" data-cites="lenatCycQuestSolve2021 lenatCreating30MillionRuleSystem2022">(<a href="#ref-lenatCycQuestSolve2021" role="doc-biblioref">D. B. Lenat 2021</a>, <a href="#ref-lenatCreating30MillionRuleSystem2022" role="doc-biblioref">2022a</a>)</span></li>
<li>Cyc has natural language understanding of pragmatics, while statistical machine learning systems have none. <span class="citation" data-cites="lenatSometimesVeneerIntelligence2017 lenatNotGoodGold2019">(<a href="#ref-lenatSometimesVeneerIntelligence2017" role="doc-biblioref">D. B. Lenat 2017</a>, <a href="#ref-lenatNotGoodGold2019" role="doc-biblioref">2019a</a>)</span></li>
</ul>
<p>At this point, Cyc is supposed to be out there doing true machine learning (not the shallow veneer of “machine learning” that those statistical and connectionist researchers are faking). Reading and studying human text written in natural language. True intelligence. General intelligence. The foundation of a thousand expert systems. The backbone of the Semantic Web. The flowering of a new age of reason.</p>
<p>And yet 9 years after Cyc is declared “done”, Cyc is still stuck inside the walls of Cycorp doing nothing of the kind. What is stopping Cyc from learning?</p>
<p>The finances are healthy. Cycorp is not subject to perverse interests of the market or middle managers. There are fewer employees than <a href="https://en.wikipedia.org/wiki/Dunbar's_number">Dunbar’s number</a>. They are aligned to the corporate mission. SubLisp is a great language. All technical challenges to knowledge engineering and representation had been solved by 1990. The knowledge pump is over 160% primed.</p>
<p>What. Is. Stopping. Cyc. From. Learning??</p>
<p>According to his final work, coauthored with Gary Marcus, the last holdup was natural language understanding (NLU). Entry had been accelerated, but was still quite manual:</p>
<blockquote class="blockquote">
<p>This process has been accelerated by gamification, NLU, etc., but each axiom is hand-checked for default correctness, generality, and best placement into the microtheories (contexts) it applies to, before entering it into the Cyc knowledge base.</p>
<p><span class="citation" data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span></p>
</blockquote>
<p>The knowledge pump had been thoroughly primed, but Cyc still couldn’t learn by reading human texts, because NLU remained unsolved. Cyc can read CycL perfectly well – the interlingua, its mother tongue – but it is stubbornly difficult to parse English into the interlingua. But why is parsing natural languages hard? Why, indeed, is NLU so difficult, even “AI-complete”?</p>
<p>This situation is clarified if we consider a classic model of machine translation, that of the <a href="https://en.wikipedia.org/wiki/Vauquois_triangle">Vauquois triangle</a>: We need to translate from one language (let’s say, English) to another (let’s say, Japanese). We can translate word-for-word, which corresponds to the base of the triangle. This direct approach is of course simple but brittle. Not only does it ignore different word-ordering across languages, it would also completely fail to disambiguate homonyms, such as “fly” as a noun versus “fly” as a verb. Stopgap measures such as using n-grams lead to combinatorial explosions.</p>
<p>At a higher level, we can take account of syntax. We first parse the English sentence into a syntax tree, do a word-replacement, transform the syntax tree according to Japanese syntax, and finally generate the Japanese sentence from it. This approach would solve the “fly” vs “fly” problem, since one is a verb and another is a noun, which would be clear according to the syntax tree. However, this fails to disambiguate the word “pen” in the two sentences:</p>
<ul>
<li>The ink is in the <u>pen</u>.</li>
<li>The sheep is in the <u>pen</u>.</li>
</ul>
<p>In both cases, the word “pen” has exactly the same syntactic category, but has a different meaning, and it requires some understanding of how the world works (i.e.&nbsp;that the writing-pen is too small to hold a sheep, while it is highly unlikely for someone to put ink into an animal-pen). Similarly, in the <a href="https://en.wikipedia.org/wiki/Winograd_schema_challenge">Winograd schema challenge</a>, the task is to disambiguate what a pronoun refers to, such as deciding what the word “they” refers to in each of the two sentences:</p>
<ul>
<li>The city councilmen refused the demonstrators a permit because <u>they</u> feared violence.</li>
<li>The city councilmen refused the demonstrators a permit because <u>they</u> advocated violence.</li>
</ul>
<p>Indeed, Lenat had often claimed that the Winograd schema challenge is a touchstone for true language understanding, something that would prove that Cyc really understands, while unmasking the other systems’ veneers of intelligence. And how must the Winograd schema challenge be solved? Lenat’s solution is to go to the pinnacle of the triangle – interlingua, the King of Kings, the Language among Languages, a completely universal conceptual representation for all humans might wish to mean by their speech.</p>
<p>Lenat intended the CycL to be the interlingua.</p>
<p>Within this framework, the problems of machine translation and understanding are unified: To understand English, it remains to parse English into the interlingua. To translate Japanese to English, it remains to parse Japanese to interlingua, then verbalize interlingua into English.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/Vauquois triangle.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">The Vauquois triangle of translation.</figcaption>
</figure>
</div>
<p>Recall that Lenat had always argued that “just letting a system learn on its own by natural language understanding” is a free lunch, and that NLU requires a significant portion (~10–50%) of common sense already encoded. But if that’s the case, then there is a near-contradiction here:</p>
<ol type="1">
<li>The CycL language is enough to represent common sense language about the world. On the Vauquois triangle, all natural languages are joined at the top by a common interlingua, which is the CycL.</li>
<li>By the Winograd schema challenge, translation requires common sense. Therefore, Japanese → English translation requires common sense.</li>
<li>CycL → English requires no common sense. The result would sound kind of wooden and robotic, but it doesn’t require any understanding: Just follow the syntax substitution rules. Indeed, CycL → English was already working since ~2000. Therefore, CycL → English requires <em>no</em> common sense.</li>
<li>Therefore, by the conservation of common sense, it takes <em>exactly</em> the same amount of common sense to perform Japanese → English translation and Japanese → CycL translation. Indeed, this is why they considered NLU an “AI-complete” problem.</li>
<li>By the no free lunch hypothesis, neural networks trained from scratch can’t have common sense. Thus, neural networks should fail at Japanese → CycL translation.</li>
<li>But in Lenat’s last paper, he argued that the only problem stopping Cyc from learning from natural language was that NLU did not work well enough yet, and <em>hoped</em> that neural networks could perform the natural language → CycL translation. Indeed, they considered ChatGPT and <a href="https://en.wikipedia.org/wiki/Google_Bard">Google Bard</a> to often be better at NLU than Cyc.</li>
</ol>
<p>On the topic of interlingua, it is interesting that ABBYY<!-- todo: link to ABBYY --> was almost a twin of Cycorp. Whereas Cycorp began building an ontology for the common sense world since 1984, spent $200M, and got stuck on NLU since around 2010, ABBYY began building an interlingua-based machine translation system since the 1990s, and spent over $80M. By the early 2010s, they realized that they could not compete with Google statistical machine translation, and pivoted to doing NLU with semantic graph technology based on the knowledge base they produced for the sake of interlingua. <span class="citation" data-cites="skorinkinABBYYsBitterLesson2024">(<a href="#ref-skorinkinABBYYsBitterLesson2024" role="doc-biblioref">Skorinkin 2024</a>)</span></p>
<p>Indeed, interlingua-based machine translation projects used to be common, but essentially went extinct (except for ABBYY) after the rise of statistical machine translation in the 1990s <span class="citation" data-cites="hutchinsMachineTranslationHistory2023">(<a href="#ref-hutchinsMachineTranslationHistory2023" role="doc-biblioref">Hutchins 2023</a>)</span>.</p>
<p>I suspect that it is not simply the problem of getting a better English → CycL translator, and then Cyc would finally begin learning, but that much knowledge in sentences doesn’t translate to interlingua.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> If the failures of all interlingua machine translation systems is not enough evidence, then consider some more facts about Cyc’s NLU:</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;Or that the interlingua exists, but it is not symbolic-logical, but linear-algebraic. Indeed, the successful neural machine translation systems’ latent spaces may be that consummate interlingua so devoutly wished for, but such an interlingua is very far from what you’d see in Cyc or ABBYY.</p></div></div><ul>
<li>Exhaustively searching the literature, I only found four (four!!) examples that Cycorp gave for English → CycL: “A girl is on a white lounge chair” <span class="citation" data-cites="prattCYCReportPratt1994">(<a href="#ref-prattCYCReportPratt1994" role="doc-biblioref">Pratt 1994</a>)</span>, “Bill Clinton sleeps.”, “An AI researcher is a kind of computer scientist.” <span class="citation" data-cites="pantonCommonSenseReasoning2006">(<a href="#ref-pantonCommonSenseReasoning2006" role="doc-biblioref">Panton et al. 2006</a>)</span>, and “Did you touch a blue object located in the capital of France on September 25th, 2022?” <span class="citation" data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span>. They were quite easy and unambiguous examples, almost as if they began with a CycL sentence, and then converted it to English. None involves the “many AI-complete elements, such as correct disambiguation, understanding of idioms, metaphor, sarcasm, foreshadowing, irony, subtext, and so on.” <span class="citation" data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span>.</li>
<li><span class="citation" data-cites="guhaEnablingAgentsWork1994">(<a href="#ref-guhaEnablingAgentsWork1994" role="doc-biblioref">Guha and Lenat 1994</a>)</span> stated that in 1994-03 “The Syntax module can properly handle about 75% of the sentences found in the news stories of a typical issue of the newspaper USA Today. And in cases in which Cyc knows all the proper nouns in the sentence, the Semantics module can properly handle most of the sentences parsable by the syntax module… as good as what our knowledge enterers independently come up with, when asked to manually translate the material into CycL.”.</li>
<li><span class="citation" data-cites="pantonCommonSenseReasoning2006">(<a href="#ref-pantonCommonSenseReasoning2006" role="doc-biblioref">Panton et al. 2006</a>)</span> stated that in 2006, a search-and-verify system for English → CycL, that combined syntactic parsing, statistical parsing, and Cyc verification, resulted in “sentences that were correct, according to human review, approximately 50% of the time”.</li>
<li><span class="citation" data-cites="sarjantAllYouCan2009">(<a href="#ref-sarjantAllYouCan2009" role="doc-biblioref">Sarjant et al. 2009</a>)</span> increased the common-sense knowledge in ResearchCyc by 30% in 2009, by guess-and-verify, where the Cyc does verification, and the guess was done by simplistic methods like regex parsing, <a href="https://en.wikipedia.org/wiki/Help:Infobox">infobox</a> pairing, etc.</li>
<li>Some governmental experimental uses of Cyc, such as ESSENCE-II and Total Information Awareness, might have involved some NLU, but I cannot find details concerning how much NLU was involved.</li>
<li>Only <em>one</em> of the commercial applications of Cyc may have plausibly required NLU.
<ul>
<li>In <a href="https://cyc.com/mathcraft/">MathCraft</a>, the “learning by teaching” game powered by Cyc, students are only allowed to pick from choices generated by the Cyc itself. There is no free-form natural language input at all.</li>
<li>In the Cleveland Clinic application <span class="citation" data-cites="lenatHarnessingCycAnswer2010">(<a href="#ref-lenatHarnessingCycAnswer2010" role="doc-biblioref">D. B. Lenat et al. 2010</a>)</span>, the user enters queries already in a constrained language (like “aortic valve replacement patients with a pericardial aortic valve”), and then compose a CycL translation by clicking from Cyc’s parser’s suggestions.</li>
<li>The same is true for the Terrorism Knowledge Base application.</li>
<li>“Maintaining persistent user models in order to support extended, months-long online chats with their famous characters” (<a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">Source</a>) seems to involve NLU. However, there is no information whatsoever as to how much NLU this project involved. I can’t find out which company hired Cycorp for this, or who the “famous characters” were. It is not even clear if users are allowed to enter free-form input during online chat, or if it is another application like MathCraft, where you can only pick from choices generated by Cyc itself. Besides, even if this case involved free-form input, there is still a problem. We know that self-organized NN can work as chatbots. So if Lenat was right to say that self-organized NN don’t understand (as he had been saying for 40 years), then this application didn’t require NLU after all.</li>
</ul></li>
</ul>
<p>Let’s take it another way: If there is No Free Lunch to NLU, then what is Cyc’s score on Winograd schema benchmark? Where is the Cyc-translator? Forget about Google Neural Translate – is it even better than ABBYY’s? Where’s ChatCyc? Why does none of Cyc’s commercial applications involve NLU in any significant amount, while most commercial applications of NLU use statistical or neural machine learning?</p>
<p>All evidences point to the conclusion that a sentence that can be parsed to CycL is already bureaucratic and formalistic, with the mark of interlingua written on its brow. For those, Cyc could already understand in the early 2000s, and yet, Cyc is still here, not machine-learning, lacking … what? <code>&lt;sarcasm&gt;</code>A sarcasm parser?<code>&lt;/sarcasm&gt;</code></p>
<div class="callout callout-style-default callout-note callout-titled" title="An information-theoretic estimate">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
An information-theoretic estimate
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The <a href="https://yuxi-liu-wired.github.io/essays/posts/perplexity-turing-test/#entropy-of-natural-languages">minimal description length of English</a> is about 0.8 bits per character, and each English sentence contains about 100 characters, giving 80 bits of incompressible information. Since there are 27 characters (we are just counting the 26 lowercase letters and the whitespace), this means that the compressible information is about <span class="math inline">\(\log_2 27 - 0.8 = 4.0\)</span> bits per character.</p>
<p>We can think of writing as a process of selection: Out of all the possible letters you can pick next, you picked this <em>particular</em> letter, and you do it again and again. Intelligence is good selection, the careful elimination of bad choices. In this particular case, the bad choices to be eliminated come out to about 3.2 bits per character.</p>
<p>Now, one might argue that the last bit is the deepest, but it is hard to square the two claims:</p>
<ul>
<li>Self-organized NN could compress natural text to a bit rate of ~1.0 bits per character, so they already capture <span class="math inline">\(3.0/3.2 = 94\%\)</span> of the selection.</li>
<li>Self-organized NN don’t understand.</li>
</ul>
<p>Stated in another way, this means 94% of the selection that humans perform while writing is mindless, mere remembering and espousing without understanding or inferring. This is even harder to square with Lenat’s assertion that 99% of language understanding is pragmatics, which self-organized NN don’t have. <span class="citation" data-cites="lenatSometimesVeneerIntelligence2017">(<a href="#ref-lenatSometimesVeneerIntelligence2017" role="doc-biblioref">D. B. Lenat 2017</a>)</span></p>
<p>So does this mean that 93% of pragmatics is mindless, or are we going to just throw out the information-theoretic understanding of language as another incommensurable paradigm shift? It would fit the theme of Cyc. Indeed, they had long excised the remnants of probabilistic reasoning when they removed <a href="https://en.wikipedia.org/wiki/Mycin">certainty factors</a> at some point before 1989, since they found it inadequate compared to reasoning by logical unification <span class="citation" data-cites="guhaReCycLingPaper1993">(<a href="#ref-guhaReCycLingPaper1993" role="doc-biblioref">Guha and Lenat 1993</a>)</span>. Cry “Kuhnian” and let slip the dogs of logic…</p>
</div>
</div>
</div>
<p>Take another look at <a href="#cycops-what-is-it-good-for">the list of known applications of Cyc</a>. Then take a look at what was even planned for as “short-term uses” of Cyc even back in 1993:</p>
<blockquote class="blockquote">
<p>the most promising short-term uses of Cyc are not what are traditionally considered AI problems. Instead, they are in relatively “mundane” problems such as making spreadsheets smarter, providing better access to a heterogeneous set of databases, directed marketing of goods and services, etc.</p>
<p><span class="citation" data-cites="guhaReCycLingPaper1993">(<a href="#ref-guhaReCycLingPaper1993" role="doc-biblioref">Guha and Lenat 1993</a>)</span></p>
</blockquote>
<p>It is quite prophetic that such “short-term uses” of Cyc are still the <em>only</em> uses of Cyc so far. Is 35 years considered “short-term”? Does this look like a path towards AGI, or does this look no different from building custom-made expert systems for specialized purposes, something that those generic professionals of Oracle, IBM, or Accenture have been doing for decades?</p>
<blockquote class="blockquote">
<p>Even though most people don’t think of the CyCorp… as an expert systems company, effectively, that’s what we’re doing… we’ve stayed in business all these years when none of you have. We are the last surviving large expert system company.</p>
<p><span class="citation" data-cites="allenExpertSystemsPioneer2018b">(<a href="#ref-allenExpertSystemsPioneer2018b" role="doc-biblioref"><em>Expert <span>Systems Pioneer Meeting</span>, Day 1 Session 1: <span>Purpose</span>, Structure and Introductions</em> 2018</a>)</span></p>
</blockquote>
<p>Perhaps they did have a product differentiation in being able to consistently find particularly good knowledge engineers, and in programming in SubLisp, a particularly efficient language compared to Python or Java. Perhaps their product differentiation is in niche expert systems that really require higher-order statements. Perhaps these are what allowed them to stay in business despite having just a hundred-person crew. It is not something to be dismissed, but is this a path towards AGI, or a veneer of AGI cast over enterprise solutions? <code>&lt;sarcasm&gt;</code>An IBM Data Integration Solutions<sup>®</sup> with better cover art?<code>&lt;/sarcasm&gt;</code></p>
<p>Perhaps Lenat could be eternally optimistic despite all the missed predictions and the lack of AGI. Perhaps it is a <a href="https://en.wikipedia.org/wiki/Selection_bias">selection effect</a>. One does not undertake a 40-year project for AGI without being delusionally optimistic about the prospect – the same could be said of great leaders and startup founders.</p>
<p>Or perhaps he was disconnected from what was really going on down there at Cycorp. In a <a href="https://news.ycombinator.com/item?id=21781597">HackerNews discussion</a>, some ex-Cyclists wrote what they thought about Cycorp.</p>
<p>On the pro side were many. The corporate culture was highly intellectual and philosophical, as one expects for a company that does computable ontology for a living: “it can be pretty fun to be in meetings where you try to explain Davidsonian ontology to perplexed business people”. The company had solved many technical problems in large scale inference, and remained profitable, with successful commercial applications.</p>
<p>On the con side were many. The codebase was creaking under 30 years of technical debt:</p>
<blockquote class="blockquote">
<p>I spent some entire days just scrolling through different versions of entire systems that duplicate massive chunks of functionality, written 20 years apart, with no indication of which (if any) still worked or were the preferred way to do things.</p>
</blockquote>
<p>The technical solutions and commercial applications were closely guarded secrets, so the outside world does not know. Lenat was unimpressed with open source and so did not commit resources to OpenCyc, tutorials, easier third-party integration, software development kit, or other outreach projects (except those regularly scheduled newspaper reports for publicity). The Cyc culture was also insular, with a true believer’s mentality:</p>
<blockquote class="blockquote">
<p>… veterans there sort of feel like the broader AI community turned their back on symbolic reasoning in the 80s (fair) and they’re generally not very impressed by the current trends within the AI community, particularly w.r.t. advances in ML (perhaps unfairly so), so they’re going to just keep doing their thing until they can’t be ignored anymore.</p>
</blockquote>
<p>Most pertinent to the dream of AGI, it was unclear, down in the trenches, whether Cyc was really doing common sense reasoning, or just a particularly good base for developing expert systems from. It also wasn’t clear if common sense reasoning was really necessary for the successful commercial projects in the first place. This has caused some Cyclists to become ex-Cyclists from disillusionment.</p>
<blockquote class="blockquote">
<p>I personally suspect that some of Cycorp’s clients would do better with domain-specific solutions because they don’t realize how much of their problem could be solved that way and how much of the analysis coming from Cyc is actually the result of subject matter experts effectively building domain-specific solutions the hard way inside of Cyc. With a lot of Cycorp projects, it’s hard to point your finger at exactly where the “AI” is happening… The degree to which it’s effective seemed to me to be a case-by-case thing. While working there I tended to suspect that Cyc people underestimated the degree to which you could get a large fraction of their results using something like <a href="https://en.wikipedia.org/wiki/Datomic">Datomic</a> and it was an open question (to me at least) whether the extra 10% or whatever was worth how much massively more complicated it is to work with Cyc.</p>
</blockquote>
<p>Structurally, the Cycorp had two levels. At the upper level are Lenat, Witbrock, and such keepers of the faith, who kept the ceaseless striving for that elusive dream alive. At the lower level are the working ontological engineers who just had to deliver the product, AGI or not.</p>
<blockquote class="blockquote">
<p>It turns out there’s a kind of reality distortion field around the management there, despite their best intentions - partially maintained by the management’s own steadfast belief in the idea that what Cyc does is what it ought to be doing, but partially maintained by a layer of people that actively isolate the management from understanding the dirty work that goes into actually making projects work or appear to. So while a certain amount of “common sense” knowledge factors into the reasoning processes, a great amount of Cyc’s output at the project level really comes from hand-crafted algorithms implemented either in the inference engine or the ontology.</p>
<p>Over the years, the Cyc as its actually implemented has drifted pretty far from the Cyc that people like Doug Lenat believe in, and the degree to which they’re willing or able to acknowledge that seems to sort of drift around, often dependent on factors like mood. Doug would show up and be very confused about why some things were hard because he just believes that Cyc works differently than it does in practice, and people had project deadlines, so they often implemented features via hacks to shape inference or hand-built algorithms to deliver answers that Doug thought ought to be derived from principles via inference. Doug thinks way more stuff that Cyc does is something that it effectively learned to do by automatically deriving a way to solve the general form of a problem, rather than a programmer up late hand-coding things to make a demo work the next day, and the programmers aren’t going to tell him because there’s a demo tomorrow too and it’s not working yet.</p>
</blockquote>
<p>But that’s enough about unverified rumors, and I apologize. It would have been better for epistemic hygiene if there were more open information about Cycorp. Let’s turn to a Cycoanalysis of Lenat’s rhetorics, which is more well-documented.</p>
</section>
<section id="lenat-against-the-world" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="lenat-against-the-world">Lenat against the world</h3>
<p>According to multiple reports, Lenat was charismatic, able to sell his vision of AGI to many people. Having read most documents produced by Lenat or Cycorp over the 40 years, I have discovered that there is a consistent list of themes that Lenat just kept repeating over his career. Each theme has a double structure: a technical statement that has an emotionally neutral valence, and a moral coloring that provides the call to action, the charisma, the coherence for the employees to align to his vision.</p>
<table class="caption-top table">
<caption>The doubled structure of Lenat’s rhetoric</caption>
<colgroup>
<col style="width: 61%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>technical statement</th>
<th>moral coloring</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>the No Free Lunch Hypothesis</td>
<td>we do honest hard work like the proverbial ant, you are lazy like the proverbial cricket (and the AI Winter is coming)</td>
</tr>
<tr class="even">
<td>there are no “Maxwell’s Equations of Thought”</td>
<td>we are self-assured, you suffer from physics-envy</td>
</tr>
<tr class="odd">
<td>the Empirical Inquiry Hypothesis; Cyc is unaesthetic; we are building the Cyc profitably, not publishing academic papers</td>
<td>we are strong engineers that get things done, you are weak aesthetes playing the academic game</td>
</tr>
<tr class="even">
<td>the Physical Symbol System Hypothesis</td>
<td>we are building real intelligence, you are just playing with robots</td>
</tr>
<tr class="odd">
<td>the Breadth Hypothesis</td>
<td>our systems are intelligent, yours are autistic idiot savants</td>
</tr>
<tr class="even">
<td>the Explicit Knowledge Principle</td>
<td>our systems understand deeply, yours pattern-match shallowly</td>
</tr>
<tr class="odd">
<td>we think you are putting pattern-matchers in places that require deep understanding</td>
<td>we are trustworthy, you are reckless</td>
</tr>
<tr class="even">
<td>we believe the Cyc is the only current effort towards AGI</td>
<td>we are ambitious, you are academic careerists</td>
</tr>
<tr class="odd">
<td>writing the Cyc costs a lot and is unpopular with the academia</td>
<td>we rebel and think freely, you sheepishly follow the crowd</td>
</tr>
<tr class="even">
<td>Cycorp hires anyone – including high school dropouts – good at encoding common sense</td>
<td>we are egalitarian, you are elitists</td>
</tr>
<tr class="odd">
<td>Cycorp has always had just ~100 people, and has been mostly forgotten now</td>
<td>we are the elect, you will see</td>
</tr>
</tbody>
</table>
<p>And more than technical, moral, and personal conviction is on the line: If Cyc really would take 1000 person-years (20 years with 50 philosopher PhDs), then it would cost about $100 million just in human labor. The Cycorp, if it were to survive, has a strong commercial interest in rejecting all alternatives. It can be very hard to get someone to understand something, when their <a href="https://en.wikipedia.org/wiki/Product_differentiation">product differentiation</a> depends on them not understanding it.</p>
<p>Lenat’s rejections progressed with time as each new challenger arose, applying the same tenets in different decades.</p>
<p>In the 1980s, like other expert systems people, he aimed his rejection at the previous logical AI methods exemplified by Simon and Newell. Logical AI was a dream that a graduate student might build an AGI during a thesis period, if only they knew the “Maxwell’s equations of thinking”. Of course, such attempts failed, because there are no such equations. He took a little effort towards rejecting the <em>other</em> logical AI approach exemplified by <span class="citation" data-cites="newellHumanProblemSolving1972">(<a href="#ref-newellHumanProblemSolving1972" role="doc-biblioref">Newell and Simon 1972</a>)</span>, by constructing models that reproduced every little detail of how humans really perform in psychometric experiments, such as their reaction times, their uhhs and oopses. Admitting its interest to psychologists, he considered it a distraction for machine intelligence.</p>
<p>In the 1990s, as the expert system hype died down, he turned his criticism towards expert systems. He recalled that, back when he was young, before academia had rejected him, he thought automated discovery with AI, such as AM and EURISKO, would lead the way to self-improving learning machines. But then he was disabused of this. BACON discovered Kepler’s three laws “only” from data, but that’s because Pat Langley was careful in presenting nothing but the relevant data. The cost to discover Kepler’s laws on the filtered dataset? A few CPU-hours. The cost to filter the dataset? 10 Kepler-years. Similarly, AM started out with the set-theory axioms and discovered prime numbers and some famous conjectures, but quickly ended up enumerating boring complications. Lenat had to keep adding in more heuristics to get something out of it. Similarly, EURISKO would run overnight and Lenat would check its outputs in the morning, remove some bad ideas, add some good ones, and so on. The <em>Traveller</em> 1981 win was “60/40% Lenat/EURISKO” after all.</p>
<p>Generalizing, Lenat argued that there is a common thread across all these machine learning systems. They would all start out discovering many interesting basic things, but quickly “run out of steam” enumerating boring complications. Lenat called it as systems putting up a “veneer of intelligence” while they were really just “discharging potential energy that was stored in them”. That is, the creators secretly put into the program with their own expert knowledge somehow, either through the right rules, heuristics, dataset, features, or some other thing. Once the expert knowledge is “exhausted”, no more discoveries could be made, and the veneer wears off. However, it makes for impressive demos, leading to cycles of hype and bust. The only escape is to prime the knowledge pump. If the knowledge base is large enough, then it wouldn’t run out of steam. <span class="citation" data-cites="lenatVoiceTurtleWhatever2008">(<a href="#ref-lenatVoiceTurtleWhatever2008" role="doc-biblioref">D. B. Lenat 2008</a>)</span></p>
<p>Lenat’s approach was not welcomed by the academics, and the feeling was mutual. AI researchers thought the Cyc project was hyped, and were unhappy with the secretive nature of Cycorp. Philosophers considered the Cyc project premature – how could Lenat build an ontology for the world when philosophers hadn’t even figured out what the ontology is? Lenat shot back, calling academics lazy, abstract, and unable to persist through decades of hard engineering work. <span class="citation" data-cites="thompsonKnowItAllMachine2001">(<a href="#ref-thompsonKnowItAllMachine2001" role="doc-biblioref">Thompson 2001</a>)</span> Among the academics, the only one that still supported him was Marvin Minsky, who had no problem calling the rest of AI research “brain-dead since the 1970s”, especially robotics: “Graduate students are wasting 3 years of their lives soldering and repairing robots, instead of making them smart.”. <span class="citation" data-cites="baardAIFounderBlasts2003">(<a href="#ref-baardAIFounderBlasts2003" role="doc-biblioref">Baard 2003</a>)</span></p>
<p>Interesting. Why the hate towards robotics? Well, during the 1990s, there were two main challengers to his idea of a symbolic-logical system. On the one side, there was the challenge of bottom-up non-symbolic reasoning promoted by Rodney Brooks’ subsumption architecture <span class="citation" data-cites="brooksElephantsDonPlay1990">(<a href="#ref-brooksElephantsDonPlay1990" role="doc-biblioref">Brooks 1990</a>)</span>, and the statistical machine learning methods like support vector machines. He did not have much to say about the statistical methods – not yet – but he did reject the subsumption architecture as a mistaken attempt to reach AGI through robotics, much as Minsky did. Motors, sensors, etc, are simply not needed – common sense, specified in logical language, is all you need. The hard work needed to get the robots to do anything is, you see, the <em>wrong</em> kind of hard work.</p>
<p>On the other side, though most expert system researchers had shrunken their ambition in the winter chill, from AGI down to mere commercial survival, a few still believed in the mission. They thought that by building little systems, brick by brick, we can build towards a generally intelligent system. This is basically a “Society of Mind” approach of Marvin Minsky, and even though Lenat and Minsky liked each other’s research, Lenat rejected this approach as well. One cannot settle for building common sense bit by bit, expecting a finished system to emerge, but must braid the whole thing under one roof, one upper ontology. The ontology does not have to be perfect or efficient, but there has to be one. Without it, the Society of Mind would fragment into a Tower of Babel, with little expert systems of incompatible ontologies, just like how Feigenbaum’s dream of a “Library of Congress” of knowledge bases failed to materialize.</p>
<p>In the 2000s, big data arrived with the Internet, and statistical learning became dominant. No doubt trying to preempt customers’ “Why don’t I just Google it?”, he turned his firepower towards statistical learning systems. He never tired of pointing out that, if you make an even slightly complex query like “Is the Space Needle taller than the Eiffel Tower?”, Google will happily serve up results saying “The Space Needle is 605 feet high.” and “The Eiffel Tower is 1,063 feet high.”, unable to actually answer your question. Despite having 15,000 servers, Google only ran dumb statistical algorithms, while Cyc running on a single server could answer it. Google-style statistical machine learning, like its trillion-token statistical machine translations systems <span class="citation" data-cites="brantsLargeLanguageModels2007">(<a href="#ref-brantsLargeLanguageModels2007" role="doc-biblioref">Brants et al. 2007</a>)</span>, was just pattern matching, yet another grasping after a free lunch. Such systems could not truly understand. As an alternative, he held out Cyc as the foundation to the <a href="https://en.wikipedia.org/wiki/Semantic_Web">Semantic Web</a>, which would build a system that <em>would</em> truly understand.</p>
<p>Curiously, right from the start, Lenat considered self-organized neural networks as not a viable path towards general intelligence, but for <em>the same reasons</em> as to why the logical AI programs of Simon and Newell would fail! From our perspective, they couldn’t be more different, yet to Lenat, neural nets, General Problem Solvers, n-gram models, whatever, are all just “explicit-knowledge-free systems”, <a href="https://en.wikipedia.org/wiki/Neats_and_scruffies">too neat, not scruffy</a>, and would fail for the exact same reason.</p>
<blockquote class="blockquote">
<p>they are unaesthetic! And they entail person-centuries of hard knowledge-entry work. Until we are forced to them, Occam’s Razor encourages us to try more elegant solutions, such as training a neural net “from scratch”; or getting an infant-simulator and then “talking to it”. Only as these fail do we turn, unhappily, to the “hand-craft a huge KB” tactic.</p>
<p>…</p>
<p>Our position regarding the aesthetes: There is a methodological difference between our “scruffy” way of doing AI and the aesthetes’ “neat” way… If only there were a secret ingredient for intelligence–Maxwell’s equations of thought. If only we could axiomatize the world in a small set of axioms, and deduce everything. If only our learning program could start from scratch. If only our neural nets were big or cerebellar or hyperlinear enough. If only the world were like that. But it isn’t. The evidence indicates that almost all the power is in the bulk knowledge. As Whitehead remarked, “God is in the details.”</p>
<p><span class="citation" data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">D. B. Lenat and Feigenbaum 1991</a>)</span></p>
</blockquote>
<p>With the second neural network winter, he did not pay more attention to them, but as they arose yet again in the 2010s, with some exasperation, he would remind the world that, no, nothing has changed. Neural nets had already failed and they would fail. Thinking that “one large net for everything” would just work is yet another example of the logical AI fallacy that “If only we have the Maxwell’s equations of learning, it will just work!”. They are always “remembering and espousing”, but never “understanding and inferring”, and can only ever be the “right brain” to Cyc’s “left brain” <span class="citation" data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span>. As Deep Learning kept blowing past expectations, he rehashed the same 1980s argument with escalating apocalypse:</p>
<blockquote class="blockquote">
<p>If computers were human, they’d present themselves as autistic, schizophrenic, or otherwise brittle. It would be unwise or dangerous for that person to take care of children and cook meals, but it’s on the horizon for home robots. That’s like saying, ‘We have an important job to do, but we’re going to hire dogs and cats to do it.’</p>
<p><span class="citation" data-cites="loveMostAmbitiousArtificial2014">(<a href="#ref-loveMostAmbitiousArtificial2014" role="doc-biblioref">Love 2014</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>No matter how good your elegant theory of&nbsp;<em>syntax</em>&nbsp;and&nbsp;<em>semantics</em>&nbsp;is, there’s always this annoying residue of&nbsp;<em>pragmatics</em>, which ends up being the lower 99% of the iceberg.&nbsp; You can wish it weren’t so, and ignore it, which is easy to do because it’s out of sight (it’s not explicitly there in the letters, words, and sentences on the page, it’s lurking in the empty spaces around the letters, words, and sentences.)&nbsp; But lacking it, to any noticeable degree, gets a person labeled&nbsp;<em>autistic</em>. They may be otherwise quite smart and charming (such as Raymond in&nbsp;<em>Rain Man</em> and Chauncey Gardiner in&nbsp;<em>Being There</em>), but it would be frankly dangerous to let them drive your car, mind your baby, cook your meals, act as your physician, manage your money, etc.&nbsp;And yet those are the very applications the world is blithely handing over to severely autistic AI programs!</p>
<p><span class="citation" data-cites="lenatSometimesVeneerIntelligence2017">(<a href="#ref-lenatSometimesVeneerIntelligence2017" role="doc-biblioref">D. B. Lenat 2017</a>)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>We would not be comfortable giving a severely neurologically-impaired person – say someone with no functioning left brain hemisphere – real-time decision-making authority over our family members’ health, our life savings, our cars, or our missile defense systems. Yet we are hurtling in that direction with today’s AI’s which are impaired in almost exactly that same fashion! They – those people and those AI programs – have trouble doing multi-step abstract reasoning, and that limitation makes their decision-making and behavior brittle, especially when confronted by unfamiliar, unexpected and unusual situations… Machine learning algorithms have scarcely changed at all, in the last 40 years… Current AI’s can form and recognize patterns, but they don’t really <em>understand</em> anything. That’s what we humans use our left brain hemispheres for.</p>
<p>… Researchers and application builders tolerate their AI systems having just the thinnest veneer of intelligence, and that may be adequate for fast internet searching or party conversation or New York Times op-ed pieces, but that simple representation leads to inferences and answers which fall far short of the levels of competence and insight and adaptability that expert humans routinely achieve at complicated tasks, and leads to shallow explanations and justifications of those answers. There is a way out of that trap, though it’s not pleasant or elegant or easy. The solution is not a machine-learning-like “free lunch” or one clap-of-thunder insight about a clever algorithm: it requires a lot of hard work…</p>
<p><span class="citation" data-cites="lenatNotGoodGold2019">(<a href="#ref-lenatNotGoodGold2019" role="doc-biblioref">D. B. Lenat 2019a</a>)</span></p>
</blockquote>
<p>Concurrently, on the Cycorp website, two white papers published in 2021-04 reiterated their product differentiation against the false promises of <a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">neural networks</a> and <a href="https://cyc.com/wp-content/uploads/2021/04/bayesnetspaper.pdf">Bayesian networks</a>.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> Neural networks posed a great commercial threat to their business, and the Bayesian networks, by promising to half-open the neural network black box, threatened their business as well. The papers argued that since both were not rule-based logical systems, they were not Actually Intelligent. After such fear-uncertainty-doubt, they reassured the reader that true AI needs both the left brain and the right brain, and they sell the finest left brains on the planet.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;Read <a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">it</a> yourself (<span class="math inline">\(\sim 3 \times 10^6 \mathrm{\mu Lenat}\)</span>) to see how hard they had to work that product differentiation. Calling it “<strong>A</strong>ctually <strong>I</strong>ntelligent”, claiming “ML can <em>never</em> give an explicit step-by-step explanation of its line of reasoning behind a conclusion, but Cyc <em>always</em> can.”, and insinuating that <em>nobody</em> could do Natural Language Understanding yet because none of those newfangled neural networks had any pragmatics… And this was uploaded in 2021-04, a year after GPT-3! It was written in the same <a href="https://en.wikipedia.org/wiki/Fear%2C_uncertainty%2C_and_doubt">FUD</a>dy voice of those that still sold machine translation services after Google Neural Translate, transcription services after OpenAI Whisper, or copywriting services after ChatGPT.</p></div></div><p>The same accusation of brain-damage that he leveled at neural networks was in fact a rehash of the exact same argument he had made against statistical machine learning systems like Cleverbot, Google, and Amazon recommender systems <span class="citation" data-cites="loveMostAmbitiousArtificial2014">(<a href="#ref-loveMostAmbitiousArtificial2014" role="doc-biblioref">Love 2014</a>)</span>, since he made no distinction between statistical methods, be it keyword matching, n-gram models, or neural networks. They are all the same veneer of intelligence, same free lunch, same shallowness.</p>
<p>Lenat could apply the same criticisms with the same counterexamples over his 40 years of career, without needing to inspect the details of these machine learning architectures, because he had the following fully general proof, which you can discover by intersecting the previous paragraphs in this section:</p>
<ol type="1">
<li>Unless common sense is fully represented and integrated, an AI system is an idiot savant at most. <span class="citation" data-cites="pantonCommonSenseReasoning2006">(<a href="#ref-pantonCommonSenseReasoning2006" role="doc-biblioref">Panton et al. 2006</a>)</span></li>
<li>Machine-learning common sense from scratch is impossible, because learning occurs at the fringe of what one already knows. <span class="citation" data-cites="lenatCycLargescaleInvestment1995">(<a href="#ref-lenatCycLargescaleInvestment1995" role="doc-biblioref">D. B. Lenat 1995b</a>)</span></li>
<li>Therefore…</li>
</ol>
<p>Hedgehogs. Hedgehogs are all the same. They have one big idea, one big proof, one big theory, and continue going on with it for decades. Chomsky did it, Minsky did it, and Lenat did it too. Benefit: If they got it right, they really got it right. Cost: If they got it wrong, then they would sound like a broken record.</p>
<p>For example, Lenat called expert systems “brittle” and “idiot savants” in the 1990s <span class="citation" data-cites="lenatArtificialIntelligence1995">(<a href="#ref-lenatArtificialIntelligence1995" role="doc-biblioref">D. B. Lenat 1995a</a>)</span>, and statistical machine learning systems “brittle” (probably also “idiot savant”) in the 2000s, and neural networks “brittle” and “autistic” since 2015 until his death.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;Some words and phrases reappear so often in Lenat’s writings that I termed them “Lenatisms” (<span class="math inline">\(\sim 10^6 \mathrm{\mu Lenat/word}\)</span>) and came to hate them as much as I hate GPTisms like “delve” and “crucial”: free lunch, hard work, physics envy, Maxwell’s equations, clever algorithm, measles, idiot-savant, autistic, veneer of intelligence, shallow, pattern matching, brittle, understand, trustworthy, left brain, hemisphere.</p></div></div><p>Similarly, he kept talking about the Winograd schema challenge, and how logically encoded common sense is the only way to solve it. He started talking about it in the 1990s <span class="citation" data-cites="lenatCycLargescaleInvestment1995">(<a href="#ref-lenatCycLargescaleInvestment1995" role="doc-biblioref">D. B. Lenat 1995b</a>)</span>. He was still telling Stephen Wolfram in 2019 that, surely, if Cyc would team up with Wolfram Alpha, then they could finally solve the challenge for good <span class="citation" data-cites="wolframRememberingDougLenat2023">(<a href="#ref-wolframRememberingDougLenat2023" role="doc-biblioref">Wolfram 2023</a>)</span>. He was <em>still</em> <a href="https://x.com/CycorpAI/status/1238183980580642816">tweeting about it even on 2020-03-12</a>, about <span class="citation" data-cites="sakaguchiWinoGrandeAdversarialWinograd2021">(<a href="#ref-sakaguchiWinoGrandeAdversarialWinograd2021" role="doc-biblioref">Sakaguchi et al. 2021</a>)</span> which showed stated that modern LLMs (GPT-2, BERT, and a few others) was underperforming on WinoGrande, a larger version of the previous Winograd benchmark. Etc, etc.</p>
<p>Not just his arguments were repetitive, but his “war stories” too. In 1994, Cyc could retrieve images by semantic search, so for example, it would retrieve an image of a rock climber if queried “an adventurous man” <span class="citation" data-cites="lenatArtificialIntelligence1995">(<a href="#ref-lenatArtificialIntelligence1995" role="doc-biblioref">D. B. Lenat 1995a</a>)</span>. Great demo for 1994, and he would harp on this throughout the 2000s in his presentations, presumably to product-differentiate against Google-like Image Search engines. Similarly, he first recounted in 1987 of an expert system that diagnosed his rusty car with measles <span class="citation" data-cites="lenatThresholdsKnowledge1991">(<a href="#ref-lenatThresholdsKnowledge1991" role="doc-biblioref">D. B. Lenat and Feigenbaum 1991</a>)</span>, then again in <span class="citation" data-cites="lenatArtificialIntelligence1995">(<a href="#ref-lenatArtificialIntelligence1995" role="doc-biblioref">D. B. Lenat 1995a</a>)</span>, then again and again throughout his presentations in the 2000s, and he was <em>still</em> telling the story (and about the Winograd schema) <a href="https://voicesinai.com/episode/episode-89-a-conversation-with-doug-lenat/">in 2019</a>.</p>
<p>In his last paper, coauthored with Gary Marcus, he updated his critique of statistical machine learning to the LLM age. Again the shallowness, brittleness, free lunch, etc.</p>
<blockquote class="blockquote">
<p>Given the arduous nature of the reasoning required… it is understandable almost all AI researchers and developers have gone in the opposite direction, abandoning or trivializing symbolic representation and reasoning, and instead seeking one or another sort of “free lunch” in the form of perceptrons, multi-layer neural networks and, most recently, LLMs… limiting an AI to such a narrow “baby talk” language would be a huge barrier to it ever becoming a trustworthy general AI.</p>
<p><span class="citation" data-cites="lenatGettingGenerativeAI2023">(<a href="#ref-lenatGettingGenerativeAI2023" role="doc-biblioref">D. B. Lenat and Marcus 2023</a>)</span></p>
</blockquote>
<p>I am struck by the irony that a veteran of logical AI would call neural networks “brittle”, or make an appeal to sunk cost. Lenat had devoted 2000 person-years to the project, therefore a “free lunch” shouldn’t work, nevermind the fact that these “free” lunches took 20 years of gritty battles to build the datasets,<!-- todo: add the link to essay on datasets once it's done. --> struggles with the cussedness of CUDA,<!-- todo: add the link to essay on CUDA once it's done. --> waking up to yet another divergent overnight training run, staring at tensors filled with <code>NaNs</code>, and eventually <a href="https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/">cost $100 million</a> per serving, roughly the total budget of Cycorp through its life. How dare you to go “free lunch” on us… But <em>de mortuis nil nisi bonum</em>.</p>
<p>Lenat died in 2023, unmourned on <a href="https://lucid.ai/">Lucid AI</a> and <a href="https://cyc.com/">Cycorp</a>, who, like ABBYY, still proudly advertise their product differentiation to this day.</p>
</section>
</section>
<section id="in-lieu-of-a-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="in-lieu-of-a-conclusion">In lieu of a conclusion</h2>
<p>Napoleon died in 1821. Wellington was greatly saddened.</p>
<blockquote class="blockquote">
<p>Plutarch has related that Julius Caesar wept for the death of Pompey; Aurelian did not weep for the death of John, but he felt what a man would feel when rid of an incurable disease that had become a part of his life.</p>
<p>— Borges, <em>The Theologians</em></p>
</blockquote>
<p><em>Come as you are, as you were<br>
As I want you to be<br>
As a friend, as a friend<br>
As an old enemy<br>
Take your time, hurry up<br>
Choice is yours, don’t be late<br>
Take a rest, as a friend<br>
As an old memoria…</em></p>
</section>



<div id="quarto-appendix" class="default"><section id="updates" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">Updates</h2><div class="quarto-appendix-contents">

<p>Some reviews:</p>
<ul>
<li><a href="https://x.com/layer07_yuxi/status/1907179435079717134">on Twitter</a></li>
<li><a href="https://old.reddit.com/r/mlscaling/comments/1juosnn/cyc_obituary_for_the_greatest_monument_to_logical/">On r/mlscaling</a></li>
<li><a href="https://news.ycombinator.com/item?id=43625474">Hacker News</a></li>
<li><a href="https://lobste.rs/s/ab6qap/obituary_for_cyc">Lobste.rs</a></li>
</ul>
<p>On advice of Gwern, I emailed the Cycorp with some questions, but they never replied.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Fulltext of the email">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fulltext of the email
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To Whom It May Concern,</p>
<p>I hope this message finds you well.</p>
<p>I am writing to inquire about several aspects of Cycorp’s ongoing work and organizational status, particularly with respect to the Cyc knowledge base and its applications. I would greatly appreciate your response to the following questions:</p>
<ol type="1">
<li><p>Knowledge Base Size and Cost What is the current approximate size of the Cyc knowledge base (e.g., number of assertions or facts), and what is the estimated average cost per new fact entry (either historical or current)?</p></li>
<li><p>Organizational Leadership Who currently leads Cycorp, both at the executive level and in terms of technical or research direction?</p></li>
<li><p>Open Source Strategy Does Cycorp have any plans to release additional components of Cyc as open source in the future, following the prior release of OpenCyc?</p></li>
<li><p>Benchmarking and Evaluation Has Cycorp evaluated its system on publically available and standardized linguistic benchmarks, such as MMLU, GPQA, ARC, or other reasoning-intensive tasks? If so, could you share any published or unpublished results?</p></li>
<li><p>Ongoing Partnerships Is Cycorp still maintaining any collaborations or deployments in clinical or hospital environments, similar to the previously reported instance with the Cleveland Clinic?</p></li>
</ol>
<p>I am asking in the context of a comparative evaluation of symbolic and sub-symbolic systems, and I would be grateful for any technical details or references you may be able to provide.</p>
<p>Thank you very much for your time and attention.</p>
<p>–</p>
<p>Regards,<br>
Yuxi Liu<br>
UC Berkeley CS PhD<br>
<a href="https://yuxi-liu-wired.github.io/" class="uri">https://yuxi-liu-wired.github.io/</a></p>
</div>
</div>
</div>


<!-- -->


</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-abbottIntegratedBiologicalWarfare2007" class="csl-entry" role="listitem">
Abbott, Frank T, Apperson H Johnson, Stephen D Prior, and Donald D Steiner. 2007. <span>“Integrated <span>Biological Warfare Technology Platform</span> (<span>IBWTP</span>). <span>Intelligent Software Supporting Situation Awareness</span>, <span>Response</span>, and <span>Operations</span>,”</span> January.
</div>
<div id="ref-anthesCycUse2002" class="csl-entry" role="listitem">
Anthes, Gary. 2002a. <span>“Cyc in Use.”</span> <em>Computerworld</em>, April. <a href="https://www.computerworld.com/article/1354418/cyc-in-use.html">https://www.computerworld.com/article/1354418/cyc-in-use.html</a>.
</div>
<div id="ref-anthesComputerizingCommonSense2002" class="csl-entry" role="listitem">
———. 2002b. <span>“Computerizing <span>Common Sense</span>.”</span> <em>Computerworld</em>, October. <a href="https://web.archive.org/web/20021019152738/http://www.computerworld.com/news/2002/story/0,11280,69881,00.html">https://web.archive.org/web/20021019152738/http://www.computerworld.com/news/2002/story/0,11280,69881,00.html</a>.
</div>
<div id="ref-baardAIFounderBlasts2003" class="csl-entry" role="listitem">
Baard, Mark. 2003. <span>“<span>AI Founder Blasts Modern Research</span>.”</span> <em>Wired</em>, May. <a href="https://www.wired.com/2003/05/ai-founder-blasts-modern-research/">https://www.wired.com/2003/05/ai-founder-blasts-modern-research/</a>.
</div>
<div id="ref-brantsLargeLanguageModels2007" class="csl-entry" role="listitem">
Brants, Thorsten, Ashok Popat, Peng Xu, Franz Josef Och, and Jeffrey Dean. 2007. <span>“Large Language Models in Machine Translation.”</span> In <em>Proceedings of the 2007 <span>Joint Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> and <span>Computational Natural Language Learning</span> (<span>EMNLP-CoNLL</span>)</em>, 858–67. <a href="https://aclanthology.org/D07-1090.pdf">https://aclanthology.org/D07-1090.pdf</a>.
</div>
<div id="ref-brooksElephantsDonPlay1990" class="csl-entry" role="listitem">
Brooks, Rodney A. 1990. <span>“Elephants Don’t Play Chess.”</span> <em>Robotics and Autonomous Systems</em> 6 (1-2): 3–15. <a href="https://doi.org/10/bk6">https://doi.org/10/bk6</a>.
</div>
<div id="ref-buchananDENDRALMetaDENDRALTheir1981" class="csl-entry" role="listitem">
Buchanan, Bruce G., and Edward A. Feigenbaum. 1981. <span>“<span>DENDRAL</span> and <span>Meta-DENDRAL</span>: <span>Their</span> Applications Dimension.”</span> In <em>Readings in Artificial Intelligence</em>, 313–22. Elsevier. <a href="https://www.sciencedirect.com/science/article/pii/B978093461303350026X">https://www.sciencedirect.com/science/article/pii/B978093461303350026X</a>.
</div>
<div id="ref-cerutiKnowledgeManagementCommand2004" class="csl-entry" role="listitem">
Ceruti, Marion G., Dwight R. Wilcox, and Brenda J. Powers. 2004. <span>“Knowledge Management for Command and Control.”</span> In <em>Proceedings of the 9th <span>Command</span> and <span>Control Research</span> and <span>Technology Symposium</span> (<span>CCRTS</span> 04), <span>San Diego</span></em>. <a href="http://dodccrp.org/events/9th_ICCRTS/CD/papers/176.pdf">http://dodccrp.org/events/9th_ICCRTS/CD/papers/176.pdf</a>.
</div>
<div id="ref-conesaUsabilityUpperLevel2010" class="csl-entry" role="listitem">
Conesa, Jordi, Veda C. Storey, and Vijayan Sugumaran. 2010. <span>“Usability of Upper Level Ontologies: <span>The</span> Case of <span>ResearchCyc</span>.”</span> <em>Data &amp; Knowledge Engineering</em>, Including <span>Special Section</span>: 12th <span>International Conference</span> on <span>Applications</span> of <span>Natural Language</span> to <span>Information Systems</span> (<span>NLDB</span>’07) – <span>Three</span> selected and extended papers, 69 (4): 343–56. <a href="https://doi.org/10.1016/j.datak.2009.08.002">https://doi.org/10.1016/j.datak.2009.08.002</a>.
</div>
<div id="ref-crensonBigBrotherCould2003" class="csl-entry" role="listitem">
Crenson, Sharon L. 2003. <span>“Big <span>Brother</span> Could Be Watching You.”</span> <em>Portsmouth Herald</em>, February. <a href="https://www.seacoastonline.com/story/news/2003/02/13/big-brother-could-be-watching/51278510007/">https://www.seacoastonline.com/story/news/2003/02/13/big-brother-could-be-watching/51278510007/</a>.
</div>
<div id="ref-cycorpTerrorismKnowledgeBase2008" class="csl-entry" role="listitem">
Cycorp. 2008. <span>“Terrorism <span>Knowledge Base</span> (<span>TKB</span>): <span>Final Technical Report</span>.”</span> AFRL-RI-RS-TR-2008-125. <a href="http://archive.org/details/DTIC_ADA481467">http://archive.org/details/DTIC_ADA481467</a>.
</div>
<div id="ref-cycorpCycTechnologyOverview2021" class="csl-entry" role="listitem">
———. 2021. <span>“Cyc <span>Technology Overview</span>.”</span> <a href="https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf">https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf</a>.
</div>
<div id="ref-davisEvaluatingCYCPreliminary2016" class="csl-entry" role="listitem">
Davis, Ernest. 2016. <span>“Evaluating <span>CYC</span>: <span>Preliminary Notes</span>.”</span> <a href="https://cs.nyu.edu/~davise/papers/CYCEval.pdf">https://cs.nyu.edu/~davise/papers/CYCEval.pdf</a>.
</div>
<div id="ref-davisCommonsenseReasoningCommonsense2015" class="csl-entry" role="listitem">
Davis, Ernest, and Gary Marcus. 2015. <span>“Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence.”</span> <em>Communications of the ACM</em> 58 (9): 92–103. <a href="https://doi.org/10.1145/2701413">https://doi.org/10.1145/2701413</a>.
</div>
<div id="ref-ermanHearsayIISpeechUnderstandingSystem1980" class="csl-entry" role="listitem">
Erman, Lee D., Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy. 1980. <span>“The <span>Hearsay-II Speech-Understanding System</span>: <span>Integrating Knowledge</span> to <span>Resolve Uncertainty</span>.”</span> <em>ACM Computing Surveys</em> 12 (2): 213–53. <a href="https://doi.org/10.1145/356810.356816">https://doi.org/10.1145/356810.356816</a>.
</div>
<div id="ref-allenExpertSystemsPioneer2018b" class="csl-entry" role="listitem">
<em>Expert <span>Systems Pioneer Meeting</span>, Day 1 Session 1: <span>Purpose</span>, Structure and Introductions</em>. 2018. <span>AI</span>: <span>Expert Systems Pioneer Meeting</span>. Mountain View, CA, USA: Computer History Museum.
</div>
<div id="ref-foxvogCyc2010" class="csl-entry" role="listitem">
Foxvog, Douglas. 2010. <span>“Cyc.”</span> In <em>Theory and <span>Applications</span> of <span>Ontology</span>: <span>Computer Applications</span></em>, edited by Roberto Poli, Michael Healy, and Achilles Kameas, 259–78. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-90-481-8847-5_12">https://doi.org/10.1007/978-90-481-8847-5_12</a>.
</div>
<div id="ref-friedlandProjectHaloDigital2004" class="csl-entry" role="listitem">
Friedland, Noah S., Paul G. Allen, Gavin Matthews, Michael Witbrock, David Baxter, Jon Curtis, Blake Shepard, Pierluigi Miraglia, Jurgen Angele, and Steffen Staab. 2004. <span>“Project Halo: <span>Towards</span> a Digital Aristotle.”</span> <em>AI Magazine</em> 25 (4): 29–29. <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1783">https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1783</a>.
</div>
<div id="ref-guhaReCycLingPaper1993" class="csl-entry" role="listitem">
Guha, R. V., and Douglas B. Lenat. 1993. <span>“Re: <span>CycLing</span> Paper Reviews.”</span> <em>Artificial Intelligence</em> 61 (1): 149–74. <a href="https://doi.org/10.1016/0004-3702(93)90100-P">https://doi.org/10.1016/0004-3702(93)90100-P</a>.
</div>
<div id="ref-guhaEnablingAgentsWork1994" class="csl-entry" role="listitem">
———. 1994. <span>“Enabling Agents to Work Together.”</span> <em>Communications of the ACM</em> 37 (7): 126–42. <a href="https://doi.org/10.1145/176789.176804">https://doi.org/10.1145/176789.176804</a>.
</div>
<div id="ref-haaseInventionExplorationDiscovery1990" class="csl-entry" role="listitem">
Haase, Kenneth W. 1990. <span>“Invention and Exploration in Discovery.”</span> Thesis, Massachusetts Institute of Technology. <a href="https://dspace.mit.edu/handle/1721.1/14257">https://dspace.mit.edu/handle/1721.1/14257</a>.
</div>
<div id="ref-hawkinsPredictingTerroristsNext2003" class="csl-entry" role="listitem">
Hawkins, Dana. 2003. <span>“Predicting Terrorists Next Moves with Common-Sense Computing.”</span> <em>U.S. News &amp; World Report</em>, April. <a href="https://web.archive.org/web/20070328224448/http://www.usnews.com/usnews/culture/articles/030407/7data.b.htm">https://web.archive.org/web/20070328224448/http://www.usnews.com/usnews/culture/articles/030407/7data.b.htm</a>.
</div>
<div id="ref-hiltzikBirthThinkingMachine2001" class="csl-entry" role="listitem">
Hiltzik, Michael a. 2001. <span>“Birth of a <span>Thinking Machine</span>.”</span> <em>Los Angeles Times</em>, June. <a href="https://www.latimes.com/archives/la-xpm-2001-jun-21-mn-12881-story.html">https://www.latimes.com/archives/la-xpm-2001-jun-21-mn-12881-story.html</a>.
</div>
<div id="ref-hutchinsMachineTranslationHistory2023" class="csl-entry" role="listitem">
Hutchins, W. John. 2023. <span>“Machine <span>Translation</span>: <span>History</span> of <span>Research</span> and <span>Applications</span>.”</span> In <em>Routledge <span>Encyclopedia</span> of <span>Translation Technology</span></em>, 2nd ed. Routledge.
</div>
<div id="ref-johnsonMachineryMindNew1986" class="csl-entry" role="listitem">
Johnson, George. 1986. <em>Machinery of the Mind: Inside the New Science of Artificial Intelligence</em>. Redmond, Washington: Tempus.
</div>
<div id="ref-kingstonHighPerformanceKnowledge2001" class="csl-entry" role="listitem">
Kingston, J. 2001. <span>“High <span>Performance Knowledge Bases</span>: Four Approaches to Knowledge Acquisition, Representation and Reasoning for Workaround Planning.”</span> <em>Expert Systems with Applications</em> 21 (4): 181–90. <a href="https://doi.org/10.1016/S0957-4174(01)00038-0">https://doi.org/10.1016/S0957-4174(01)00038-0</a>.
</div>
<div id="ref-knightAI30Years2016" class="csl-entry" role="listitem">
Knight, Will. 2016. <span>“An <span>AI</span> with 30 <span>Years</span>’ <span>Worth</span> of <span>Knowledge Finally Goes</span> to <span>Work</span>.”</span> <em>MIT Technology Review</em>, March. <a href="https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/">https://www.technologyreview.com/2016/03/14/108873/an-ai-with-30-years-worth-of-knowledge-finally-goes-to-work/</a>.
</div>
<div id="ref-langleyDataDrivenDiscoveryPhysical1981" class="csl-entry" role="listitem">
Langley, Pat. 1981. <span>“Data-<span>Driven Discovery</span> of <span>Physical Laws</span>.”</span> <em>Cognitive Science</em> 5 (1): 31–54. <a href="https://doi.org/10.1111/j.1551-6708.1981.tb00869.x">https://doi.org/10.1111/j.1551-6708.1981.tb00869.x</a>.
</div>
<div id="ref-lenatDimensionsContextspace1998" class="csl-entry" role="listitem">
Lenat, Doug. 1998. <span>“The Dimensions of Context-Space.”</span> Cycorp technical report. <a href="https://www.academia.edu/download/82146661/lenat2.pdf">https://www.academia.edu/download/82146661/lenat2.pdf</a>.
</div>
<div id="ref-lenatAMArtificialIntelligence1976" class="csl-entry" role="listitem">
Lenat, Douglas B. 1976. <em><span>AM</span>: An Artificial Intelligence Approach to Discovery in Mathematics as Heuristic Search.</em> Stanford University. <a href="https://search.proquest.com/openview/7bf7ad428cf88bc7205ff59d0edaed81/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y">https://search.proquest.com/openview/7bf7ad428cf88bc7205ff59d0edaed81/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a>.
</div>
<div id="ref-lenatUbiquityDiscovery1977" class="csl-entry" role="listitem">
———. 1977. <span>“The Ubiquity of Discovery.”</span> <em>Artificial Intelligence</em> 9 (3): 257–85. <a href="https://www.sciencedirect.com/science/article/pii/0004370277900248">https://www.sciencedirect.com/science/article/pii/0004370277900248</a>.
</div>
<div id="ref-lenatPlausibleMutationDNA1980" class="csl-entry" role="listitem">
———. 1980. <span>“The <span>Plausible Mutation</span> of <span>DNA</span>.”</span> <a href="https://apps.dtic.mil/sti/citations/tr/ADA096802">https://apps.dtic.mil/sti/citations/tr/ADA096802</a>.
</div>
<div id="ref-lenatEURISKOProgramThat1983" class="csl-entry" role="listitem">
———. 1983a. <span>“<span>EURISKO</span>: A Program That Learns New Heuristics and Domain Concepts: The Nature of Heuristics <span>III</span>: Program Design and Results.”</span> <em>Artificial Intelligence</em> 21 (1-2): 61–98. <a href="https://www.sciencedirect.com/science/article/pii/S0004370283800058">https://www.sciencedirect.com/science/article/pii/S0004370283800058</a>.
</div>
<div id="ref-lenatTheoryFormationHeuristic1983" class="csl-entry" role="listitem">
———. 1983b. <span>“Theory Formation by Heuristic Search: <span>The</span> Nature of Heuristics <span>II</span>: Background and Examples.”</span> <em>Artificial Intelligence</em> 21 (1-2): 31–59. <a href="https://www.sciencedirect.com/science/article/pii/S0004370283800046">https://www.sciencedirect.com/science/article/pii/S0004370283800046</a>.
</div>
<div id="ref-lenatRoleHeuristicsLearning1983" class="csl-entry" role="listitem">
———. 1983c. <span>“The Role of Heuristics in Learning by Discovery: <span>Three</span> Case Studies.”</span> In <em>Machine <span>Learning</span></em>, edited by Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, 243–306. San Francisco (CA): Morgan Kaufmann. <a href="https://doi.org/10.1016/B978-0-08-051054-5.50013-3">https://doi.org/10.1016/B978-0-08-051054-5.50013-3</a>.
</div>
<div id="ref-lenatComputerSoftwareIntelligent1984" class="csl-entry" role="listitem">
———. 1984. <span>“Computer Software for Intelligent Systems.”</span> <em>Scientific American</em> 251 (3): 204–14. <a href="https://www.jstor.org/stable/24920354">https://www.jstor.org/stable/24920354</a>.
</div>
<div id="ref-lenatArtificialIntelligence1995" class="csl-entry" role="listitem">
———. 1995a. <span>“Artificial <span>Intelligence</span>.”</span> <em>Scientific American</em> 273 (3): 80–82. <a href="https://www.jstor.org/stable/24981725">https://www.jstor.org/stable/24981725</a>.
</div>
<div id="ref-lenatCycLargescaleInvestment1995" class="csl-entry" role="listitem">
———. 1995b. <span>“Cyc: A Large-Scale Investment in Knowledge Infrastructure.”</span> <em>Communications of the ACM</em> 38 (11): 33–38. <a href="https://doi.org/10.1145/219717.219745">https://doi.org/10.1145/219717.219745</a>.
</div>
<div id="ref-lenatAppliedOntologyIssues2005" class="csl-entry" role="listitem">
———. 2005. <span>“Applied Ontology Issues.”</span> <em>Applied Ontology: An Interdisciplinary Journal of Ontological Analysis and Conceptual Modeling</em> 1 (1): 9–12. <a href="https://doi.org/10.3233/APO-2005-000002">https://doi.org/10.3233/APO-2005-000002</a>.
</div>
<div id="ref-lenatGoogleTechTalksComputers2006" class="csl-entry" role="listitem">
———. 2006. <span>“Google <span>TechTalks</span>: <span>Computers</span> Versus <span>Common Sense</span>.”</span> <a href="https://www.youtube.com/watch?v=gAtn-4fhuWA">https://www.youtube.com/watch?v=gAtn-4fhuWA</a>.
</div>
<div id="ref-lenatVoiceTurtleWhatever2008" class="csl-entry" role="listitem">
———. 2008. <span>“The Voice of the Turtle: <span>Whatever</span> Happened to Ai?”</span> <em>AI Magazine</em> 29 (2): 11–22. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v29i2.2106">https://onlinelibrary.wiley.com/doi/abs/10.1609/aimag.v29i2.2106</a>.
</div>
<div id="ref-lenatBuildingMachineSmart2009" class="csl-entry" role="listitem">
———. 2009. <span>“Building a <span>Machine Smart Enough</span> to <span>Pass</span> the <span>Turing Test</span>.”</span> In <em>Parsing the <span>Turing Test</span>: <span>Philosophical</span> and <span>Methodological Issues</span> in the <span>Quest</span> for the <span>Thinking Computer</span></em>, edited by Robert Epstein, Gary Roberts, and Grace Beber, 261–82. Dordrecht: Springer Netherlands. <a href="https://doi.org/10.1007/978-1-4020-6710-5_16">https://doi.org/10.1007/978-1-4020-6710-5_16</a>.
</div>
<div id="ref-lenat50ShadesSymbolic2015" class="csl-entry" role="listitem">
———. 2015. <span>“50 <span>Shades</span> of <span>Symbolic Representation</span> and <span>Reasoning</span>.”</span> <a href="https://www.youtube.com/watch?v=4mv0nCS2mik">https://www.youtube.com/watch?v=4mv0nCS2mik</a>.
</div>
<div id="ref-lenatSometimesVeneerIntelligence2017" class="csl-entry" role="listitem">
———. 2017. <span>“Sometimes the <span>Veneer</span> of <span>Intelligence</span> Is <span>Not Enough</span>.”</span> <em>COGNITIVE WORLD</em>. <a href="https://cognitiveworld.com/articles/sometimes-veneer-intelligence-not-enough">https://cognitiveworld.com/articles/sometimes-veneer-intelligence-not-enough</a>.
</div>
<div id="ref-lenatNotGoodGold2019" class="csl-entry" role="listitem">
———. 2019a. <span>“Not <span>Good As Gold</span>: <span>Today</span>’s <span>AI</span>’s <span>Are Dangerously Lacking In AU</span> (<span>Artificial Understanding</span>).”</span> <em>Forbes</em>. <a href="https://www.forbes.com/sites/cognitiveworld/2019/02/18/not-good-as-gold-todays-ais-are-dangerously-lacking-in-au-artificial-understanding/">https://www.forbes.com/sites/cognitiveworld/2019/02/18/not-good-as-gold-todays-ais-are-dangerously-lacking-in-au-artificial-understanding/</a>.
</div>
<div id="ref-lenatEpisode89Conversation2019" class="csl-entry" role="listitem">
———. 2019b. <span>“Episode 89: <span>A Conversation</span> with <span>Doug Lenat</span>.”</span> <a href="https://voicesinai.com/episode/episode-89-a-conversation-with-doug-lenat/">https://voicesinai.com/episode/episode-89-a-conversation-with-doug-lenat/</a>.
</div>
<div id="ref-lenatCycQuestSolve2021" class="csl-entry" role="listitem">
———. 2021. <span>“Cyc and the <span>Quest</span> to <span>Solve Common Sense Reasoning</span> in <span>AI</span>.”</span> <a href="https://lexfridman.com/douglas-lenat/">https://lexfridman.com/douglas-lenat/</a>.
</div>
<div id="ref-lenatCreating30MillionRuleSystem2022" class="csl-entry" role="listitem">
———. 2022a. <span>“Creating a 30-<span>Million-Rule System</span>: <span>MCC</span> and <span>Cycorp</span>.”</span> <em>IEEE Annals of the History of Computing</em> 44 (1): 44–56. <a href="https://doi.org/10.1109/MAHC.2022.3149468">https://doi.org/10.1109/MAHC.2022.3149468</a>.
</div>
<div id="ref-lenatACS2022Invited2022" class="csl-entry" role="listitem">
———. 2022b. <span>“[<span>ACS</span> 2022] <span>Invited Talk</span>: <span>Computers</span> Versus <span>Common Sense</span>.”</span> <a href="https://www.youtube.com/watch?v=VjkbmLjwXO8">https://www.youtube.com/watch?v=VjkbmLjwXO8</a>.
</div>
<div id="ref-lenatWhyAmEurisko1984" class="csl-entry" role="listitem">
Lenat, Douglas B., and John Seely Brown. 1984. <span>“Why <span>AM</span> and <span>EURISKO</span> Appear to Work.”</span> <em>Artificial Intelligence</em> 23 (3): 269–94. <a href="https://doi.org/10.1016/0004-3702(84)90016-X">https://doi.org/10.1016/0004-3702(84)90016-X</a>.
</div>
<div id="ref-lenatReinforcingMathKnowledge2014" class="csl-entry" role="listitem">
Lenat, Douglas B., and Paula J. Durlach. 2014. <span>“Reinforcing <span>Math Knowledge</span> by <span>Immersing Students</span> in a <span>Simulated Learning-By-Teaching Experience</span>.”</span> <em>International Journal of Artificial Intelligence in Education</em> 24 (3): 216–50. <a href="https://doi.org/10.1007/s40593-014-0016-x">https://doi.org/10.1007/s40593-014-0016-x</a>.
</div>
<div id="ref-lenatThresholdsKnowledge1991" class="csl-entry" role="listitem">
Lenat, Douglas B., and Edward A. Feigenbaum. 1991. <span>“On the Thresholds of Knowledge.”</span> <em>Artificial Intelligence</em> 47 (1): 185–250. <a href="https://doi.org/10.1016/0004-3702(91)90055-O">https://doi.org/10.1016/0004-3702(91)90055-O</a>.
</div>
<div id="ref-lenatEfficientPathfindingVery2007" class="csl-entry" role="listitem">
Lenat, Douglas B., Keith Goolsbey, Kevin Knight, and Pace Smith. 2007. <span>“Efficient <span>Pathfinding</span> in <span>Very Large Data Spaces</span> : <span>Defense Technical Information Center</span>.”</span> ADA475387. <a href="https://archive.org/details/DTIC_ADA475387/page/n1/mode/2up">https://archive.org/details/DTIC_ADA475387/page/n1/mode/2up</a>.
</div>
<div id="ref-lenatBuildingLargeKnowledgebased1989" class="csl-entry" role="listitem">
Lenat, Douglas B., and R. V. Guha. 1989. <em>Building Large Knowledge-Based Systems: Representation and Inference in the <span>Cyc</span> Project</em>. Reading, Mass: Addison-Wesley Pub. Co.
</div>
<div id="ref-lenatCycMidtermReport1990" class="csl-entry" role="listitem">
Lenat, Douglas B., and Ramanathan V. Guha. 1990. <span>“Cyc: <span>A</span> Midterm Report.”</span> <em>AI Magazine</em> 11 (3): 32–32. <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/842">https://ojs.aaai.org/index.php/aimagazine/article/view/842</a>.
</div>
<div id="ref-lenatGettingGenerativeAI2023" class="csl-entry" role="listitem">
Lenat, Douglas B., and Gary Marcus. 2023. <span>“Getting from <span>Generative AI</span> to <span>Trustworthy AI</span>: <span>What LLMs</span> Might Learn from <span>Cyc</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2308.04445">https://doi.org/10.48550/arXiv.2308.04445</a>.
</div>
<div id="ref-lenatCycUsingCommon1985" class="csl-entry" role="listitem">
Lenat, Douglas B., Mayank Prakash, and Mary Shepherd. 1985. <span>“Cyc: <span>Using</span> Common Sense Knowledge to Overcome Brittleness and Knowledge Acquisition Bottlenecks.”</span> <em>AI Magazine</em> 6 (4): 65–65. <a href="https://ojs.aaai.org/index.php/aimagazine/article/view/510/0">https://ojs.aaai.org/index.php/aimagazine/article/view/510/0</a>.
</div>
<div id="ref-lenatHarnessingCycAnswer2010" class="csl-entry" role="listitem">
Lenat, Douglas B., Michael Witbrock, David Baxter, Eugene Blackstone, Chris Deaton, Dave Schneider, Jerry Scott, and Blake Shepard. 2010. <span>“Harnessing <span>Cyc</span> to Answer Clinical Researchers’ Ad Hoc Queries.”</span> <em>AI Magazine</em> 31 (3): 13–32. <a href="https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299">https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299</a>.
</div>
<div id="ref-loveMostAmbitiousArtificial2014" class="csl-entry" role="listitem">
Love, Dylan. 2014. <span>“The <span>Most Ambitious Artificial Intelligence Project In The World Has Been Operating In Near Secrecy For</span> 30 <span>Years</span>.”</span> <em>Business Insider</em>, July. <a href="https://www.businessinsider.com/cycorp-ai-2014-7">https://www.businessinsider.com/cycorp-ai-2014-7</a>.
</div>
<div id="ref-lyonsArtificialIntelligenceGets1998" class="csl-entry" role="listitem">
Lyons, Daniel. 1998. <span>“Artificial Intelligence Gets Real.”</span> <em>Forbes</em>, 176–82. <a href="https://www.forbes.com/global/1998/1130/0118096a.html">https://www.forbes.com/global/1998/1130/0118096a.html</a>.
</div>
<div id="ref-matthewsKRAKENKnowledgeRich2004" class="csl-entry" role="listitem">
Matthews, G., J. Curtis, K. Hines, R. C. Kahlert, and P. Miraglia. 2004. <span>“<span>KRAKEN</span> - <span>Knowledge Rich Acquisition</span> of <span>Knowledge</span> from <span>Experts Who Are Non-Logicians</span>.”</span> AFRL-IF-RS-TR-2004-314.
</div>
<div id="ref-metzOneGeniusLonely2016" class="csl-entry" role="listitem">
Metz, Cade. 2016. <span>“One <span>Genius</span>’ <span>Lonely Crusade</span> to <span>Teach</span> a <span>Computer Common Sense</span>.”</span> <em>Wired</em>, March. <a href="https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine/">https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine/</a>.
</div>
<div id="ref-newellHumanProblemSolving1972" class="csl-entry" role="listitem">
Newell, Allen, and Herbert Alexander Simon. 1972. <em>Human Problem Solving</em>.
</div>
<div id="ref-pantonCommonSenseReasoning2006" class="csl-entry" role="listitem">
Panton, Kathy, Cynthia Matuszek, Douglas B. Lenat, Dave Schneider, Michael Witbrock, Nick Siegel, and Blake Shepard. 2006. <span>“Common <span>Sense Reasoning</span> – <span>From Cyc</span> to <span>Intelligent Assistant</span>.”</span> In <em>Ambient <span>Intelligence</span> in <span>Everyday Life</span>: <span>Foreword</span> by <span>Emile Aarts</span></em>, edited by Yang Cai and Julio Abascal, 1–31. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/11825890_1">https://doi.org/10.1007/11825890_1</a>.
</div>
<div id="ref-paulheimHowMuchTriple2018" class="csl-entry" role="listitem">
Paulheim, Heiko. 2018. <span>“How Much Is a Triple.”</span> In <em><span>IEEE International Semantic Web Conference</span></em>. <a href="http://www.heikopaulheim.com/docs/iswc_bluesky_cost2018.pdf">http://www.heikopaulheim.com/docs/iswc_bluesky_cost2018.pdf</a>.
</div>
<div id="ref-pierceSemanticDBSemanticWeb2012" class="csl-entry" role="listitem">
Pierce, Christopher D., David Booth, Chimezie Ogbuji, Chris Deaton, Eugene Blackstone, and Doug Lenat. 2012. <span>“<span>SemanticDB</span>: <span>A Semantic Web Infrastructure</span> for <span>Clinical Research</span> and <span>Quality Reporting</span>.”</span> <em>Current Bioinformatics</em> 7 (3): 267–77. <a href="https://doi.org/10.2174/157489312802460730">https://doi.org/10.2174/157489312802460730</a>.
</div>
<div id="ref-portDuelingBrainscapesArtificial1997" class="csl-entry" role="listitem">
Port, Otis. 1997. <span>“Dueling <span>Brainscapes In Artificial Intelligence</span>.”</span> <em>Bloomberg.com</em>, June. <a href="https://web.archive.org/web/20031224235959/http://businessweek.com/1997/25/b353210.htm">https://web.archive.org/web/20031224235959/http://businessweek.com/1997/25/b353210.htm</a>.
</div>
<div id="ref-prattCYCReportPratt1994" class="csl-entry" role="listitem">
Pratt, Vaughan. 1994. <span>“<span>CYC Report</span> – <span>V</span>. <span>Pratt</span>.”</span> <a href="http://boole.stanford.edu/cyc.html">http://boole.stanford.edu/cyc.html</a>.
</div>
<div id="ref-richmanAllenClaimsSuccess2003" class="csl-entry" role="listitem">
Richman, Dan. 2003. <span>“Allen Claims Success in Work on Computers That Can Reason.”</span> <em>Seattle Post-Intelligencer</em>, June. <a href="https://www.seattlepi.com/business/article/allen-claims-success-in-work-on-computers-that-1117119.php">https://www.seattlepi.com/business/article/allen-claims-success-in-work-on-computers-that-1117119.php</a>.
</div>
<div id="ref-ritchieAMCaseStudy1984" class="csl-entry" role="listitem">
Ritchie, Graeme D., and F. Keith Hanna. 1984. <span>“<span>AM</span>: <span>A</span> Case Study in <span>AI</span> Methodology.”</span> <em>Artificial Intelligence</em> 23 (3): 249–68. <a href="https://www.sciencedirect.com/science/article/pii/0004370284900158">https://www.sciencedirect.com/science/article/pii/0004370284900158</a>.
</div>
<div id="ref-rolandStrategicComputingDARPA2002" class="csl-entry" role="listitem">
Roland, Alex, and Philip Shiman. 2002. <em>Strategic Computing: <span>DARPA</span> and the Quest for Machine Intelligence, 1983-1993</em>. History of Computing. Cambridge, Mass: MIT Press.
</div>
<div id="ref-sakaguchiWinoGrandeAdversarialWinograd2021" class="csl-entry" role="listitem">
Sakaguchi, Keisuke, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. <span>“<span>WinoGrande</span>: An Adversarial Winograd Schema Challenge at Scale.”</span> <em>Communications of the ACM</em> 64 (9): 99–106. <a href="https://doi.org/10.1145/3474381">https://doi.org/10.1145/3474381</a>.
</div>
<div id="ref-sarjantAllYouCan2009" class="csl-entry" role="listitem">
Sarjant, Samuel, Catherine Legg, Michael Robinson, and Olena Medelyan. 2009. <span>“" <span>All You Can Eat</span>" <span>Ontology-Building</span>: <span>Feeding Wikipedia</span> to <span>Cyc</span>.”</span> In <em>2009 <span>IEEE</span>/<span>WIC</span>/<span>ACM International Joint Conference</span> on <span>Web Intelligence</span> and <span>Intelligent Agent Technology</span></em>, 1:341–48. IEEE. <a href="https://ieeexplore.ieee.org/abstract/document/5286048/">https://ieeexplore.ieee.org/abstract/document/5286048/</a>.
</div>
<div id="ref-shapiroFoundationsFoundationalismCase1991" class="csl-entry" role="listitem">
Shapiro, Stewart. 1991. <em>Foundations <span>Without Foundationalism</span>: <span>A Case</span> for <span>Second-Order Logic</span></em>. Oxford <span>Logic Guides</span> v.17. Oxford: Oxford University Press, UK.
</div>
<div id="ref-shepardKnowledgebasedApproachNetwork2005" class="csl-entry" role="listitem">
Shepard, Blake, Cynthia Matuszek, C. Bruce Fraser, William Wechtenhiser, David Crabbe, Zelal Güngördü, John Jantos, et al. 2005. <span>“A Knowledge-Based Approach to Network Security: Applying <span>Cyc</span> in the Domain of Network Risk Assessment.”</span> In <em>Proceedings of the 17th Conference on <span>Innovative</span> Applications of Artificial Intelligence - <span>Volume</span> 3</em>, 1563–68. <span>IAAI</span>’05. Pittsburgh, Pennsylvania: AAAI Press.
</div>
<div id="ref-shilohHeTaughtAI2023" class="csl-entry" role="listitem">
Shiloh, Kali. 2023. <span>“He <span>Taught AI</span> the <span>Facts</span> of <span>Life</span>.”</span> <em>Stanford Magazine</em>. <a href="https://stanfordmag.org/contents/he-taught-ai-the-facts-of-life">https://stanfordmag.org/contents/he-taught-ai-the-facts-of-life</a>.
</div>
<div id="ref-skorinkinABBYYsBitterLesson2024" class="csl-entry" role="listitem">
Skorinkin, Daniil. 2024. <span>“<span>ABBYY’s Bitter Lesson: How Linguists Lost the Last Battle for NLP</span>.”</span> <em>System Block</em>. <a href="https://sysblok.ru/blog/gorkij-urok-abbyy-kak-lingvisty-proigrali-poslednjuju-bitvu-za-nlp/">https://sysblok.ru/blog/gorkij-urok-abbyy-kak-lingvisty-proigrali-poslednjuju-bitvu-za-nlp/</a>.
</div>
<div id="ref-storkHALsLegacy2001s1998" class="csl-entry" role="listitem">
Stork, David G., ed. 1998. <em><span>HAL</span>’s <span>Legacy</span>: 2001’s <span>Computer</span> as <span>Dream</span> and <span>Reality</span></em>. The MIT Press. <a href="https://doi.org/10.7551/mitpress/3404.001.0001">https://doi.org/10.7551/mitpress/3404.001.0001</a>.
</div>
<div id="ref-thompsonKnowItAllMachine2001" class="csl-entry" role="listitem">
Thompson, Clive. 2001. <span>“The <span>Know-It-All Machine</span>.”</span> <em>Lingua Franca</em> 11 (6). <a href="http://linguafranca.mirror.theinfo.org/print/0109/cover.html">http://linguafranca.mirror.theinfo.org/print/0109/cover.html</a>.
</div>
<div id="ref-TravellerBook51980" class="csl-entry" role="listitem">
<em>Traveller <span>Book</span> 5: <span>High Guard</span></em>. 1980. 2nd ed. Game Designers’ Workshop.
</div>
<div id="ref-walkerHowFeasibleAutomated1987" class="csl-entry" role="listitem">
Walker, Michael G. 1987. <span>“How <span>Feasible Is Automated Discovery</span>?”</span> <em>IEEE Expert</em> 2 (1): 69–82. <a href="https://doi.org/10.1109/MEX.1987.4307036">https://doi.org/10.1109/MEX.1987.4307036</a>.
</div>
<div id="ref-wallichSiliconBabies1991" class="csl-entry" role="listitem">
Wallich, Paul. 1991. <span>“Silicon <span>Babies</span>.”</span> <em>Scientific American</em> 265 (6): 124–35. <a href="https://www.jstor.org/stable/24938839">https://www.jstor.org/stable/24938839</a>.
</div>
<div id="ref-witbrockKnowledgeBegetsKnowledge2005" class="csl-entry" role="listitem">
Witbrock, Michael, Cynthia Matuszek, Antoine Brusseau, Robert C. Kahlert, C. Bruce Fraser, and Douglas B. Lenat. 2005. <span>“Knowledge <span>Begets Knowledge</span>: <span>Steps</span> Towards <span>Assisted Knowledge Acquisition</span> in <span>Cyc</span>.”</span> In <em><span>AAAI Spring Symposium</span>: <span>Knowledge Collection</span> from <span>Volunteer Contributors</span></em>, 99–105. <a href="https://cdn.aaai.org/Symposia/Spring/2005/SS-05-03/SS05-03-015.pdf">https://cdn.aaai.org/Symposia/Spring/2005/SS-05-03/SS05-03-015.pdf</a>.
</div>
<div id="ref-wolframRememberingDougLenat2023" class="csl-entry" role="listitem">
Wolfram, Stephen. 2023. <span>“Remembering <span>Doug Lenat</span> (1950–2023) and <span>His Quest</span> to <span>Capture</span> the <span>World</span> with <span>Logic</span>.”</span> <a href="https://writings.stephenwolfram.com/2023/09/remembering-doug-lenat-1950-2023-and-his-quest-to-capture-the-world-with-logic/">https://writings.stephenwolfram.com/2023/09/remembering-doug-lenat-1950-2023-and-his-quest-to-capture-the-world-with-logic/</a>.
</div>
<div id="ref-woodCycorpCostCommon2005" class="csl-entry" role="listitem">
Wood, Lamont. 2005. <span>“Cycorp: <span>The Cost</span> of <span>Common Sense</span>.”</span> <em>MIT Technology Review</em>, March. <a href="https://www.technologyreview.com/2005/03/01/274581/cycorp-the-cost-of-common-sense-2/">https://www.technologyreview.com/2005/03/01/274581/cycorp-the-cost-of-common-sense-2/</a>.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Cyc"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Obituary for the greatest monument to logical AGI"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-04-01"</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2025-05-30"</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [AI, scaling, history]</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "After 40 years, 30 million rules, 200 million dollars, 2000 person-years, and many promises, Cyc has failed to reach intellectual maturity, and may never will. Exacerbated by the secrecy and insularity of Cycorp, there remains no evidence of its general intelligence."</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="an">image-alt:</span><span class="co"> "A symbolic representation of the Cyc project. The vanishing point covered by mist represents the project that keeps growing without ever reaching completion, a destination that is a mirage. The rows of pyramids make up a larger pyramid, due to perspective. This creates the sense of a recursive pyramid, a hierarchy of hierarchies, representing the hierarchy of microtheories in Cyc's ontology. The pyramid is also a monument to the dead, of vast ambitions that failed to secure their design purposes. For Egyptian pyramids, the purpose was a good afterlife. For Cyc, the purpose was AGI.\nA one-point perspective image of a path towards infinity, banked by two rows of pyramids. The pyramids are stepped pyramids made of concrete. The path fades into a distant mist. The point of view is above the tops of the pyramids. Monochromatic photo. Created by GPT-4o."</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner/banner_3.png"</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "done"</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "likely"</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 5</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Abstract</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>The legendary Cyc project, Douglas Lenat's 40-year quest to build artificial general intelligence by scaling symbolic logic, has failed. Based on extensive archival research, this essay brings to light its secret history so that it may be widely known.</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>Lenat's journey began with his PhD work on automated mathematical discovery through heuristic search. He observed that such systems initially make promising discoveries but quickly "run out of steam" as they exhaust their initial pool of heuristic rules. His follow-up system EURISKO, famous for winning tournament competitions by finding unconventional tactics, faced similar limitations. These experiences convinced Lenat that true AI needed a vast foundation of common sense knowledge to avoid intellectual exhaustion.</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>In 1984, he launched Cyc to manually encode millions of facts and rules about common sense, predicting that once this "knowledge pump" was primed, the system would begin true machine learning by reading natural language texts and conducting autonomous scientific experiments. Cyc grew to contain approximately 30 million assertions at a cost of $200 million and 2,000 person-years. Yet despite Lenat's repeated predictions of imminent breakthrough, it never came.</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>In terms of funding, Cycorp was probably half-funded by the military and intelligence before 2010, and entirely funded by commercial applications since 2016. Cycorp achieved long-term financial stability that is uncommon for a small technology company, but all known commercial uses of its system involve standard methods in expert systems, data integration, and information retrieval, functionally the same as similar services offered by established corporations like Oracle and IBM. No evidence suggests that Cyc's purported higher intelligence provided any competitive advantage.</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>By academic standards, the Cyc project is highly insular. Publications involving Cyc typically described methods for entering information *into* the system, rarely addressing applications *out of* it. Outside Cycorp, Cyc saw minimal use in AI research or even in knowledge retrieval, its most adjacent field. Academics found the system difficult to use, and it never performed on public benchmarks. Spin-off projects like OpenCyc and various semantic web initiatives all eventually shut down without notable success.</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>The secretive nature of Cyc has multiple causes. Lenat personally did not release the source code of his PhD project or EURISKO, remained unimpressed with open source, and disliked academia as much as academia disliked him. Most open information concerning Cyc had been deliberately removed circa 2015, at the moment when Cycorp pivoted to commercial applications.</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>Lenat had a single philosophical vision for AI that he pursued for 40 years. Guided by it, he had consistently rejected every alternative vision for AI, including the heuristic search approach, the expert systems approach, the robotics approach, and the neural network approach. All were rejected as either "shallow pattern-matching" seeking a "free lunch" on one end, or a "mystical worship of physical embodiment" on the other end.</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>As of 2025, 9 years after the knowledge pump had been primed, there is still no sign that Cyc would ever achieve general intelligence.</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>More archival material available on <span class="co">[</span><span class="ot">GitHub</span><span class="co">](https://github.com/yuxi-liu-wired/cyc-archive/tree/main)</span>.</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="fu">## In lieu of an introduction</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>Many years later, surrounded by the humming servers of the knowledge base, Douglas Lenat was to remember that distant afternoon when he taught Cyc that everyone can only see their own dream.</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="fu">## Automated Mathematician</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>In the land of AI, there had been legends, of people and systems that were before their time, that showed sparks of brilliance but without followups, and Douglas Lenat was a man of three. </span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>Lenat's first legend was his Automated Mathematician (AM), a system that, according to the legends, discovered mathematical concepts autonomously by distilling concepts out of patterns and remixing and stirring together previous concepts <span class="co">[</span><span class="ot">@lenatAMArtificialIntelligence1976</span><span class="co">]</span>. As we will see, this legend is mostly true, though with a caveat that leads to the topic of his second legend.</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>AM was his 1976 PhD thesis project, and it was one of many "automated discovery" systems in the 1970s -- the only one still remembered nowadays. Though neural networks and other "self-organized" machine learning methods had mostly <span class="co">[</span><span class="ot">died out in the 1960s</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/)</span>, machine learning did not die, and indeed, was going through a logical spring, under the banner of "automated discovery".</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="fu">### Automated discovery</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>Nowadays, when we think of "machine learning", we think of random forests, neural networks, and such. But back then, machine learning was understood as "logical AI gone meta". Well, to go meta, we must first understand: What *is* logical AI? Simon and Newell, the two giants of logical AI, had shown the way: logical AI is heuristic search. </span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To play chess, one simply writes a program that does <span class="co">[</span><span class="ot">alpha-beta search</span><span class="co">](https://en.wikipedia.org/wiki/Alpha-beta_pruning)</span> over the game tree, guided by heuristics of piece-worth, mobility, and such.</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To solve planar geometry problems, one simply writes a program that constructs a path in the space of geometric arguments, until one connects the axioms with the conclusions.</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To solve problems of type X, one simply writes a program that walks through the space of possible solutions to X, and code in the heuristic rules, its north star, so that it will not be lost in the space of solutions.</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>Well, chess is a game, and so is Euclidean geometry, symbolic integration, and... perhaps scientific discovery itself? The famed scientific method, to deserve the name "method", ought to be just one more problem for logical AI to solve. It merely remained to go meta, to heuristically search over the space of heuristic searches.</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>During 1977--1983, <span class="co">[</span><span class="ot">Pat Langley</span><span class="co">](http://www.isle.org/~langley/)</span> wrote a series of programs named BACON, after <span class="co">[</span><span class="ot">Francis Bacon</span><span class="co">](https://en.wikipedia.org/wiki/Francis_Bacon)</span>, a father of scientific empiricism. It purported to discover scientific laws from mere data. For example, when given a table of the moles $N$, pressures $P$, volumes $V$, and temperatures $T$ of gas samples, BACON would first try out simple equations involving two of the terms. It would discover that the data for $PV$ appears more cluster-like than either $P$ or $V$, so it would make a new quantity $PV$, and add that to the table. It would then repeat this process, discovering that $PV/T$ is an interesting quantity, and finally that $PV/NT$ is a *constant* quantity -- the <span class="co">[</span><span class="ot">ideal gas law</span><span class="co">](https://en.wikipedia.org/wiki/Ideal_gas_law)</span> discovered! <span class="co">[</span><span class="ot">@langleyDataDrivenDiscoveryPhysical1981</span><span class="co">]</span></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>In the logical AI framework, the problem space of BACON is the space of all elementary functions involving the table columns, and a *solution* is a (nontrivial) function that results in a nearly constant column. At each step, the program tries out simple combinations of the current columns, and heuristically pick the one most nearly constant.</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>As another example, consider the famed <span class="co">[</span><span class="ot">Dendral and Meta-Dendral</span><span class="co">](https://en.wikipedia.org/wiki/Dendral)</span>. </span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>Dendral was an artificial model of how professional chemists perform <span class="co">[</span><span class="ot">molecular spectroscopy</span><span class="co">](https://en.wikipedia.org/wiki/Molecular_spectroscopy)</span> -- that is, how they read out a molecular structure from looking at a few spiky curves. Its problem space is the space of all possible ways to cut up a molecule into fragments, and ways for the fragments to give and take their atoms. Its heuristics are the rules for generating chemically plausible and implausible cleavages and transfers. For example, it is plausible for a protein to be cleaved at the <span class="in">`-CO-*-NH-`</span> peptide bond, but implausible to be cleaved at the double bond between <span class="in">`C`</span> and <span class="in">`O`</span>. The goal is, given molecular spectroscopic data for a single molecule, to construct a molecular structure and a sequence of cleavages and transfers, such that it would produce the data.</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>With Meta-Dendral, the problem space goes meta, becoming the space of possible chemical *rules*. Its goal is to find plausible rules (plausible according to heuristic meta-rules) that can explain a large collection of molecular structures and their spectroscopic data. Meta-Dendral proved useful for working chemists by discovering some cleavage rules for a certain minor sub-family of <span class="co">[</span><span class="ot">androstanes</span><span class="co">](https://en.wikipedia.org/wiki/Androstane)</span>. <span class="co">[</span><span class="ot">@buchananDENDRALMetaDENDRALTheir1981</span><span class="co">]</span> <span class="co">&lt;!-- todo: For details, see my essay on [expert systems](https://yuxi-liu-wired.github.io/essays/posts/expert-systems/). --&gt;</span></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>In general, such a system begins with some simple rules that allow the system to a low score according to some criteria. and as it runs, it builds, prunes, and modifies the rules, so that in the end, its rule set allows it to achieve a high score. The following tabulates a few <span class="co">[</span><span class="ot">@walkerHowFeasibleAutomated1987</span><span class="co">]</span>:</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> System Name <span class="pp">|</span> Date <span class="pp">|</span> Task <span class="pp">|</span> Data <span class="pp">|</span> Rules <span class="pp">|</span> Discovery Method <span class="pp">|</span></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a><span class="pp">|---|---|---|---|---|---|</span></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Meta-Dendral <span class="pp">|</span> 1976 <span class="pp">|</span> Discover molecular cleavage and transfer rules for mass spectrometry <span class="pp">|</span> Molecular structures and their spectra <span class="pp">|</span> Molecular cleavage and transfer rules <span class="pp">|</span> Use meta-heuristic rules to generate possible rules, and test on data <span class="pp">|</span></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Bacon <span class="pp">|</span> 1977--1983 <span class="pp">|</span> Discover physical laws <span class="pp">|</span> Numeric data from experiments <span class="pp">|</span> Elementary functions <span class="pp">|</span> Symbolic regression <span class="pp">|</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> RX <span class="pp">|</span> 1982 <span class="pp">|</span> Discover drug side effects and interactions <span class="pp">|</span> Patient information database <span class="pp">|</span> Drug effect and interaction rules <span class="pp">|</span> Symbolic regression with time-lag <span class="pp">|</span></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### The working of AM</span></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The honor of your machine is preserved.</span></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Paul Erdős, after examining </span><span class="co">[</span><span class="ot">@lenatAMArtificialIntelligence1976, appendix 4</span><span class="co">]</span><span class="at">.</span></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>Within this context, AM is different. It is still a logical AI with a problem space and a heuristic search. However, there is no data: It was mostly "self-play".</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a>To start AM, Lenat began by entering 115 concepts in set theory and ~250 heuristic rules, and thence AM ran, discovering more and more constructions. Most would be trivial, but a few would be interesting, and these interesting constructions would be stored, allowing further constructions upon them. It secured a place as a minor legend, reportedly rediscovering many concepts, such as the natural numbers, the prime numbers, and the Goldbach conjecture.</span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>A **concept** is essentially a [frame](https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)), which are essentially objects in [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming). A concept has 25 possible **facets**, which are "slots", or "data fields". Not all need to be filled.</span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="An example concept" collapse="true" }</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>The following is an example concept with many facets populated <span class="co">[</span><span class="ot">@lenatRoleHeuristicsLearning1983, table 9.9</span><span class="co">]</span>:</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`NAME`</span>: <span class="in">`Generalize-rare-predicate`</span></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`ABBREVIATION`</span>: <span class="in">`GRP`</span></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`STATEMENT`</span>:</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`English`</span>: If a predicate is rarely true, Then create generalizations of it</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`IF-potentially-relevant`</span>:</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`IF-just-finished-a-task-dealing-with`</span>: a predicate <span class="in">`P`</span></span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`IF-about-to-work-on-task-dealing-with`</span>: an agenda <span class="in">`A`</span></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`IF-in-the-middle-of-a-task-dealing-with`</span>: *never*</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`IF-truly-relevant`</span>: <span class="in">`P`</span> returns True less than <span class="in">`5%`</span> of Average Predicate</span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`IF-resources-available`</span>: at least <span class="in">`10`</span> CPU seconds, at least <span class="in">`300`</span> cells</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`THEN-add-task-to-agenda`</span>: Fill in entries for <span class="in">`Generalizations`</span> slot of <span class="in">`P`</span></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`THEN-conjecture`</span>:</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="ss">        *   </span><span class="in">`P`</span> is less interesting than expected</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="ss">        *   </span><span class="in">`Generalizations`</span> of <span class="in">`P`</span> may be better than <span class="in">`P`</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="ss">        *   </span><span class="in">`Specializations`</span> of <span class="in">`P`</span> may be very bad</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`THEN-modify-slots`</span>:</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="ss">        *   </span>Reduce Worth of <span class="in">`P`</span> by <span class="in">`10%`</span></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a><span class="ss">        *   </span>Reduce Worth of <span class="in">`Specializations(P)`</span> by <span class="in">`50%`</span></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a><span class="ss">        *   </span>Increase Worth of <span class="in">`Generalizations(P)`</span> by <span class="in">`20%`</span></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`THEN-print-to-user`</span>: <span class="in">`English(GRP)`</span> with "a predicate" replaced by <span class="in">`P`</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`THEN-define-new-concepts`</span>:</span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`CODED-IF-PART`</span>: <span class="in">`λ(P) ... &lt;LISP function conjoining all the IF- parts&gt;`</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`CODED-THEN-PART`</span>: <span class="in">`λ(P) ... &lt;LISP function appending all the THEN- parts&gt;`</span></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`CODED-IF-THEN-PARTS`</span>: <span class="in">`λ(P) ... &lt;LISP function combining the previous 2 slots&gt;`</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`COMPILED-CODED-IF-THEN-PARTS`</span>: <span class="in">`#30875`</span></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`SPECIALIZATIONS`</span>: <span class="in">`Generalize-rare-set-predicate`</span></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Boundary-Specializations`</span>: <span class="in">`Enlarge-domain-of-predicate`</span></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`GENERALIZATIONS`</span>: <span class="in">`Modify-predicate`</span>, <span class="in">`Generalize-concept`</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Immediate-Generalizations`</span>: <span class="in">`Generalize-rare-contingent-piece-of-knowledge`</span></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Siblings`</span>: <span class="in">`Generalize-rare-heuristic`</span></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`IS-A`</span>: <span class="in">`Heuristic`</span></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`EXAMPLES`</span>:</span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Good-Examples`</span>: Generalize <span class="in">`Set-Equality`</span> into <span class="in">`Same-Length`</span></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Bad-Examples`</span>: Generalize <span class="in">`Set-Equality`</span> into <span class="in">`Same-First-Element`</span></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`CONJECTURES`</span>: Special cases of this are more powerful than <span class="in">`Generalizations`</span></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Good-Conjec-Units`</span>: <span class="in">`Specialize`</span>, <span class="in">`Generalize`</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`ANALOGIES`</span>: <span class="in">`Weaken-overconstrained-problem`</span></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`WORTH`</span>: <span class="in">`600`</span></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`VIEW`</span>: <span class="in">`Enlarge-structure`</span></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`ORIGIN`</span>: Specialization of <span class="in">`Modify-predicate`</span> via empirical induction</span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Defined-using`</span>: <span class="in">`Specialize`</span></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Creation-date`</span>: <span class="in">`6/1/78 11:30`</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span><span class="in">`HISTORY`</span>:</span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`N-Good-Examples`</span>: <span class="in">`1`</span>, <span class="in">`N-Bad-Examples`</span>: <span class="in">`1`</span></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`N-Good-Conjectures`</span>: <span class="in">`3`</span>, <span class="in">`N-Bad-Conjectures`</span>: <span class="in">`1`</span></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`N-Good-Tasks-Added`</span>: <span class="in">`2`</span>, <span class="in">`N-Bad-Tasks-Added`</span>: <span class="in">`0`</span></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a><span class="ss">    *   </span><span class="in">`Avg-Cpu-Time`</span>: <span class="in">`9.4`</span> seconds, <span class="in">`Avg-List-Cells`</span>: <span class="in">`200`</span></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The network of concepts at the beginning of AM. `|` means "is a". `|||` means "is an example of". [@lenatAMArtificialIntelligence1976, page 106]</span><span class="co">](figure/AM_ontology.png)</span></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a>AM has an **agenda**: a list of **tasks**, each with a list of **reasons** for the task. Each task is of the form "Perform operation <span class="in">`O`</span> to facet <span class="in">`F`</span> of concept <span class="in">`C`</span>". A task's <span class="in">`Worth`</span> is the sum of its reasons' worths. AM always performs the task with the highest worth.</span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>To perform a task, AM looks for **heuristic rules** whose conditions are (mostly) satisfied, and whose worth is (pretty) high. Each heuristic rule is of form "if <span class="in">`&lt;condition&gt;`</span>, then run <span class="in">`&lt;actions&gt;`</span>". Each action has 3 kinds of possible effects:</span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Add a new task to agenda.</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a new concept.</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Add or delete an entry to a facet of a concept.</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a>Some example heuristic rules:</span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If the task is to fill in examples of X, and X is a special case of Y, then for each example of Y, check if it a definition of X. If so, then add it to the list of examples of X.</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If some but not most examples of X are also examples of Y, then create a new concept "X and Y".</span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If very few examples of X are found, then add the following task to the agenda: "Generalize the concept X", for the following reason: "X are quite rare; a slightly less restrictive concept might be more interesting".</span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>At this point, the careful reader would notice several problems:</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a>How does AM know that the concept should be called "Prime Numbers"? Ah, that's because Lenat would regularly interrupt and inspect AM, and if Lenat notices that AM has rediscovered, say, prime numbers, he would rename that from something like <span class="in">`concept-421`</span> to <span class="in">`prime-numbers`</span>.</span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a>To discover prime numbers, AM must have a way to check if a Lisp object is a prime number or not. That is, the definition must also be a program. So...? Ah, the famous <span class="co">[</span><span class="ot">homoiconicity</span><span class="co">](https://en.wikipedia.org/wiki/Homoiconicity)</span> of Lisp came to the rescue! A definition, as stored within a facet of a concept, is a data, but for Lisp, data is program, and program data. Consequently, AM can run a subroutine that enumerates possible programs *as data*, and for each, interpret it *as program*, until AM hits upon a program that works (or times out). </span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a>How does it check that two definitions actually define the same thing? In general, this is impossible by <span class="co">[</span><span class="ot">Rice's theorem</span><span class="co">](https://en.wikipedia.org/wiki/Rice's_theorem)</span>, so Lenat must have used some heuristic rules. I looked, but can't find Lenat explaining this anywhere. It seemed like a trick of the hand.</span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a>But most serious of all issues is that the most critical part of AM was not the concepts it discovered -- after all, mathematicians did not need a computer to inform them that prime numbers are interesting. The most critical part was surely the heuristic rules by which AM worked, and many were entirely hand-waved. The most detailed description was in <span class="co">[</span><span class="ot">@lenatAMArtificialIntelligence1976, appendix 3</span><span class="co">]</span>, and it is still not described to a level of detail that may allow reimplementation.</span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a>Consider rule 75: "A constructive existence conjecture is interesting if it is frequently used." How frequent is "frequent"? What threshold of frequency triggers the increase in interestingness, and by how much? There are even vaguer rules, such as rule 69: "Formulate a parameterized conjecture, a 'template', which gets slowly specialized or instantiated into a definite conjecture.".</span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>Now, these would have not been a problem if AM was just a forgotten system, but it was not. The achievements of AM was impressive enough that it was an instant celebrity among the AI people, winning an <span class="co">[</span><span class="ot">IJCAI Award</span><span class="co">](https://en.wikipedia.org/wiki/IJCAI_Computers_and_Thought_Award)</span> just 1 year after publication <span class="co">[</span><span class="ot">@lenatUbiquityDiscovery1977</span><span class="co">]</span>. Anecdotes even suggested that the AI mathematician was here:<span class="ot">[^ai-ramanujan]</span></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; On one memorable occasion, one of my advisors, George Polya, was looking at its results, and remarked "That reminds me of something a student of a friend of mine once did." He rummaged through an old trunk, found the relevant correspondence, and it turned out that his friend was G. H. Hardy, and the student was Srinivasa Ramanujan! Even though that regularity (involving highly composite numbers) has no practical significance, Polya and I were happy to see AM behaving much like the young self-taught Indian genius had, in his explorations in search for interesting regularities.</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="ot">[^ai-ramanujan]: </span>This anecdote corroborates <span class="co">[</span><span class="ot">@lenatAMArtificialIntelligence1976, appendix 4.6</span><span class="co">]</span>.</span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a><span class="fu">### The end of AM</span></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>The swirling controversy came to a head with the publication of <span class="co">[</span><span class="ot">@ritchieAMCaseStudy1984</span><span class="co">]</span>, which argued that AM was badly documented. That, its control structure probably was not "simply pick the task with the highest worth", but more complicated. This called into question the Lenat claim that the heuristic rules were the load-bearing parts of AM, which Lenat had implied by emphasizing its almost trivially simple control structure. Further, some crucial *quantitative* heuristic rules were probably hidden behind vague *qualitative* handwaves like "A nonconstructive existence conjecture is interesting" (rule 74). Now, handwaving would have not been a problem if they were intended as merely glosses over the source code, but the source code was also unpublished, making it impossible for other other researchers to reproduce or extend the work, or to reinterpret AM's workings.</span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a>In short, because of the various issues, AM was an event that happened, but not an entity that could be built upon. One could not build upon it directly in source code, since it's unavailable. One could not build upon it by reimplementing the pseudocode, since the rules were vaguely specified, and the control structure was probably wrong. One could not build upon it by reimplementing the high-level ideas, since it's unclear which part, out of the several dozen tightly integrated parts of AM, was responsible for AM's good outputs, and which were just implementation details. In our language, there was no ablation study.</span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>Lenat quickly replied with <span class="co">[</span><span class="ot">@lenatWhyAmEurisko1984</span><span class="co">]</span>. He dismissed the criticism as mostly miscommunication, and went on to describe the *real* lesson of AM. If I were to be dab, Lenat was saying that it is fine for AM's source,<span class="ot">[^am-source-code]</span> or even pseudocode, to be unavailable, because Lenat had learned the lessons, and you, dear reader, need only listen to the lessons from him.</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The AM thesis never explained, precisely, how concepts such as 'not very often' and 'related to' were implemented. By and large, these omissions were due to the fact that the code Lenat wrote for these predicates was quite trivial... inevitable, yet regrettable process of simplifying large pieces of code, translating them to brief English phrases. This process left out many exceptional cases, and made the English condensations less accurate... Some problems that Ritchie and Hanna cite... are simply errors of mis-reading what was stated in the thesis or articles... A few of the problems raised in Ritchie and Hanna's article are, annoyingly, genuine inconsistencies in the thesis document, such as whether or not facets had subfacets. These reflect the fact that AM was a running and evolving program, changing daily in small ways even as the thesis document was being written... the changes in representation were driven simply by AM's running out of list space in 1975 </span><span class="co">[</span><span class="ot">INTERLISP</span><span class="co">](https://en.wikipedia.org/wiki/Interlisp)</span><span class="at"> code; we were forced to shift representations time and time again just to gain a few hundred precious list cells.</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatWhyAmEurisko1984</span><span class="co">]</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a><span class="ot">[^am-source-code]: </span>Though Lenat admitted that "the code ought to have been provided" <span class="co">[</span><span class="ot">@lenatWhyAmEurisko1984</span><span class="co">]</span> for AM, he would never publish the code, not with AM, nor with EURISKO. He had often claimed it had been long lost, yet the source code for AM and EURISKO had <span class="co">[</span><span class="ot">recently been found</span><span class="co">](https://white-flame.com/am-eurisko.html)</span>, right where it should be -- the <span class="co">[</span><span class="ot">DBL folder in the Stanford AI Laboratory backup data</span><span class="co">](https://www.saildart.org/DBL)</span>. It could only be found because Lenat had died, thus releasing the password protection on the folder. I wonder if Lenat had lied, and simply wished to protect his source code. It would correlate with his later behavior.</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>And what were the lessons?</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>1th lesson: AM exhausts itself. Roughly speaking, each new interesting discovery depends on ~24 heuristics, and each heuristic has a hand in ~24 discoveries. Therefore, with ~N heuristic rules, there would be ~N interesting discoveries. Because AM cannot discover heuristic rules, with ~300 starting heuristic rules, it would run out of interesting discoveries and "die of boredom". Lenat could help AM by adding new heuristics, and AM would make some new discoveries, but this never lasted.</span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Eventually, AM acquired an uncommon ailment for a computing system: intellectual exhaustion. Having explored the esoteric reaches of mathematics, AM suddenly downshifted into a preoccupation with rudimentary arithmetic. Finally, with the remark, "Warning! No task on the agenda has priority over 200.", the system virtually expired, as though from boredom.</span></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@hiltzikBirthThinkingMachine2001</span><span class="co">]</span></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a>2th lesson: Representation matters a lot. AM worked so well for mathematics, because AM used Lisp code as data. Lisp is the perfect tool if you want to search over the space of interesting mathematical functions. You can modify a Lisp expression, and get a different mathematical function that is possibly interesting. In contrast, if you were to modify assembly code, you'd most likely end up with nonsense. Indeed, Lenat found that he could not extend AM to "go meta" and discover new heuristics, because Lisp is good for math, not "heuretics" (the study of heuristics). Modifying a Lisp expression for a heuristic most likely ends up with nonsense, much like modifying assembly code for a mathematical function.</span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It was only because of the intimate relationship between LISP and Mathematics that the mutation operators (loop unwinding, recursion elimination, composition, argument elimination, function substitution, etc.) turned out to yield a high 'hit rate' of viable, useful new math concepts when applied to previously-known, useful math concepts -- concepts represented as LISP functions. But no such deep relationship existed between LISP and Heuretics, and when the basic automatic programming (mutations) operators were applied to viable, useful heuristics, they almost always produced useless (often worse than useless) new heuristic rules.</span></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We did not perceive until writing this paper that the way in which </span><span class="in">`Similar-To`</span><span class="at">, </span><span class="in">`Not-Often`</span><span class="at">, </span><span class="in">`Notice-Regularity`</span><span class="at">, and scores of other 'primitives' were coded do themselves embody a large amount of heuristic knowledge. We exploited the structure of (or, if you prefer, partially encoded) the domain of elementary mathematics, in the process of making *trivial yet adequate* LISP versions of those extremely complex and subtle notions (such as similarity of concepts).</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatWhyAmEurisko1984</span><span class="co">]</span></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a>How do we know that Lenat learned the lessons?</span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="fu">## EURISKO</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>Though 1981 is still near, EURISKO is already shrouded in a reverential mystery like legends do, among the <span class="co">[</span><span class="ot">Deep Blue</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)), the <span class="co">[</span><span class="ot">Samuel Checkers Player</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)), and the <span class="co">[</span><span class="ot">apprentice's broom</span><span class="co">](https://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice)</span>. A symbol, a moral archetype. A program that discovered loopholes in a sci-fi ship-building tournament, allowing its creator to win twice in a row using fleets so unaesthetic that the people running the tournament threatened to stop the tournaments if it would win thrice, so it retired, the Honorary Admiral, EURISKO the Undefeated.</span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>Brilliant, dramatic, but what do we glean from it, other than a moral play about the power of thinking sideways and <span class="co">[</span><span class="ot">seeing through</span><span class="co">](https://gwern.net/unseeing)</span>? Quite a lot.</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a><span class="fu">### The lessons</span></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>As we saw, AM raised many questions, and Lenat wrote EURISKO to answer them. Specifically, he wanted to see if EURISKO could avoid intellectual exhaustion if it could *also* discover heuristics. If AM exhausted itself because it has used up the worth of its heuristics (1th lesson), then why not let the computer discover more heuristics? Since AM could not efficiently search over heuristic rules using LISP (2th lesson), Lenat designed a new language called RLL ("Representation Language Language"), over which heuristic rules are efficient to search.</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>As in AM, each heuristic in EURISKO had a level of <span class="in">`Worth`</span>. Higher-worth heuristics were more likely to be invoked. Each heuristic had a <span class="in">`CreditTo`</span>, so that if a heuristic rule rose in worth, its <span class="in">`CreditTo`</span> would also rise in worth. When EURISKO was born, and saw itself, all the heuristic rules it had were branded with <span class="in">`CreditTo = DBLenat`</span>, but this would soon change, as heuristics begat heuristics.</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a>Yet while Lenat solved the problems raised by the first two lessons of AM, EURISKO failed, from which Lenat learned two further lessons. As we will see, it turned out that EURISKO *did* exhaust itself eventually after all. Self-discovery of heuristic rules eventually ceased, because self-discovery of heuristic rules relied on meta-heuristic rules about heuristic rules, and *those* rules run out of steam after a dozen or so uses.</span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>Lenat concluded that he could program 100 heuristics and get a system that could discover 1000 rules, or program 100 meta-heuristics and get a system that could discover 1000 heuristics and 10000 rules, but none of these would be truly autonomous, and Lenat wished for more. He wished for something that could be an equal to humanity, that would grow up and explore into the great beyond, where it could no longer rely on humans for help. Lenat concluded that there really is no way to get a working automated discovery program without doing the hard work of hand-coding in a lot of common sense, and that there would be a point at which this system would finally achieve escape velocity, and would never be exhausted again.</span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>Why would common sense help? Lenat observed how humans don't seem to get stuck like EURISKO. He concluded that humans don't run out of steam because they have a vast store of common sense knowledge about the world, from which they can draw upon for analogies, those far-flung flights of fancies that, in sufficient quantities, allow them to generate genuinely new ideas endlessly. For example, one can draw a line of analogy between the military and the medical, so that a doctor can "fighting an infection by an encircling movement with antibiotics". This is the 3th lesson.</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a>Why would analogies give genuinely new ideas? Well, intelligence is messy! If everything is so uniform, then there is no way to make a far-flung analogy -- everything is pretty much the same already. Besides, just look at all the broken dreams of logical AI -- their corpses tell us that no elegant theory of intelligence exists. Programming a genuine AI is a messy job. Messiness is a hideous strength. This is the 4th lesson.</span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The apparent adhoc-ness in both the heuristics' content themselves, and in the control knowledge guiding the application of those heuristics, is clearly the source of many methodological objections to the work. But we believe that this adhocracy -- indeed, adhocracy controlling adhocracy -- may be the source of EURISKO's underlying potential especially as a model for cognition.</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatWhyAmEurisko1984</span><span class="co">]</span></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>Those are the lessons Lenat drew, but what did EURISKO really do?</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>Other than winning the sci-fi naval battle tournaments, EURISKO also worked on number theory, set theory, simulation of evolution, and metal-oxide (MOS) design. In MOS, it designed some new circuit elements that were verified by physical fabrication, such as a more efficient flip-flop that was "difficult to produce masks for and difficult to fabricate, but extremely small and fast".</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Lenat's meta-Darwinian evolution theory" collapse="true" }</span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a>Lenat had a novel idea about how evolution works, one that I have not seen anywhere else. His idea was that purely random mutations are too slow for evolutionary progress, so that evolution must have gone "meta" as well. A substantial portion of the genome is probably not coding for direct protein transcription, but rather for a kind of heuristics, so that instead of trying out all possible mutations, evolution only tries out mutations that are likely useful. And of course, this would have gone meta, with meta-heuristic genes coding for useful ways to mutate the heuristic genes. <span class="co">[</span><span class="ot">@lenatPlausibleMutationDNA1980</span><span class="co">]</span></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Here is a mechanism which embodies the heuristic "If a gene has mutated successfully several times in the recent past, then increase its chance of mutating in the next generation, and conversely." All we need to posit is that somehow a short, noncoding sequence—we'll call it an *asterisk—is* added to a gene each time it mutates </span><span class="sc">\[</span><span class="at">and</span><span class="sc">\]</span><span class="at"> some mechanism (for example, stereochemical) whereby genes with many asterisks are more likely to be mutated, duplicated, and so on, than genes with few or none. Since the asterisks provide no specific benefits to the individual, they will gradually be lost over time, so that when a gene no longer should be mutated, its asterisk count will slowly decline over several generations.</span></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... recombination among </span><span class="co">[</span><span class="ot">introns</span><span class="co">](https://en.wikipedia.org/wiki/Intron)</span><span class="at"> modulates the evolution of a gene. Let's look at an example of this: it is extremely important to keep the </span><span class="co">[</span><span class="ot">`a`</span><span class="co">](https://en.wikipedia.org/wiki/Hemoglobin_subunit_alpha)</span><span class="at">, </span><span class="co">[</span><span class="ot">`b`</span><span class="co">](https://en.wikipedia.org/wiki/Hemoglobin_subunit_beta)</span><span class="at">, and </span><span class="co">[</span><span class="ot">`d`</span><span class="co">](https://en.wikipedia.org/wiki/HBD)</span><span class="at"> globin genes separate, but their internal structure is very similar. To inhibit recombination, the spacers between them can be made very different, and the introns within them can diverge dramatically (since mutations in introns are not as deleterious to the functioning of the gene as mutations to the coding regions). In fact, there is evidence that both of these kinds of divergence do occur for the globins.</span></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatRoleHeuristicsLearning1983</span><span class="co">]</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>At the start, only random mutations would be selected for, but eventually heuristics would arise that create likely advantageous mutations, and then evolution would go meta by one level: it would select for good heuristics. Over the natural history of earth, this can go meta for as many levels as time allows.</span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>The paper goes into further speculative details, and makes a half-humorous--half-horror suggestion that perhaps evolution has worked for so long that the genome now contains a sophisticated intelligent designer, bootstrapped from the billions of years of heuristics-upon-heuristics <span class="co">[</span><span class="ot">backstopped by natural selection</span><span class="co">](https://gwern.net/backstop)</span>. The tiny designer would design the progeny's genome, so that almost nothing in the mutation is "random".</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>Since it has an expectation of how things "should go", it might regard the human experiment as a failure because humans are driving the natural world so far out of its expectation. For example, the designer might have learned a rule "If the environment temperature is varying outside of 20 ± 10 °C</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>, then undo the previous mutation. We have clearly created an offspring that is wandering too much.", then it would find the human experiment a big mistake and start mashing the <span class="in">`undo`</span> button.</span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; By now a large knowledge base may exist about ecology, geology, glaciation, seasons, gravity, predation, symbiosis, causality, conservation, behavior, evolution and knowledge itself. In a small number of generations, man has managed to invalidate many of these bits of knowledge, this model of the world. If the heuristics can trace this breakdown to the increasing size of our brains, they might take quick corrective action, preserving homeostasis and the validity of their knowledge base by drastically decreasing human brain size over just a few generations. While this is of course a fanciful tongue-in-cheek extreme case, it (and the longer example above) demonstrates the power, the coordination, that a body of heuristics could evince if it were guiding the process of evolution.</span></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatRoleHeuristicsLearning1983</span><span class="co">]</span></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>Imagine a species getting so good at adaptation that it is considered a mistake and thus undone. Wouldn't that be the greatest <span class="co">[</span><span class="ot">joke of nature</span><span class="co">](https://en.wiktionary.org/wiki/lusus_naturae)</span>?</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>But we're really here to hear about the cool naval battles, not more abstract logics. Unfurl the photonic sails. We go.</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Evervictorious</span></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a>Lenat did not describe why, one July 4 weekend of 1981, he and EURISKO decided to enter the The Trillion Credit Squadron tournament. Perhaps for glory, or the thrill of the hunt. In any case, they did, and thus carved their legend. </span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a>The tournament was for the tabletop RPG game <span class="co">[</span><span class="ot">*Traveller*</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Traveller_(role-playing_game)). Despite purportedly about space battles in a galactic empire, the game was essentially just a simplified version of WWII-era navy battles draped in space-opera garments. In the tournament, pairs of starship generals line up their fleets and fight until one side loses all their ships or surrenders. Each fleet must be built within a budget of 1 trillion credits (thus the name), 100 ships, and under certain other minor constraints.</span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>At the start, Lenat went through the rule books and coded the rules into EURISKO. Then, every evening EURISKO would run through its tasks, including *Traveller* games, MOS design, math problems, and so on. And every morning, Lenat would check on the last night's results, remove some heuristics that EURISKO discovered that he deemed bad, and add some others. Manual intervention was necessary since otherwise EURISKO can be stuck with bad heuristics for a long time, and because of weird meta-bugs. Lenat estimated that the final EURISKO had accumulated 1300 CPU-hours of runtime in total on a [Xerox 1100 Lisp machine](https://archive.computerhistory.org/resources/access/text/2010/06/102660634-05-07-acc.pdf), and the *Traveller* win was "60/40% Lenat/EURISKO".</span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>Some heuristics that Lenat hardcoded at the start were:</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>R7 (diagonal functions are interesting): If $f$ is an interesting function of type $A \times A \to B$, then $g(a) := f(a,a)$ is possibly an interesting function of type $A \to B$, and should be studied.</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>R9 (extremal sets have possibly interesting preimages): If $f: A \to B$ is an interesting function, and $S \subset B$ is extremal in some sense, then $f^{-1}(S)$ is possibly an interesting subset of $A$ and should be studied.</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>R16 (conjecturing): If the first few examples of a concept $C$ have just been found, then examine a typical one and see what properties it satisfies, then make a conjecture for each of those properties being satisfied by all examples of $C$.</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>These were used by EURISKO for fleet design:</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; One type of craft which is commonly included is a fighter, which is carried into the area by a carrier... Following R7, the possibility was considered of building fighters that could transport themselves into the battle area; due to the way the constraints were set up, this turned out to be a very powerful--if bizarre--design tactic. Essentially, each fighter was equipped with just enough 'sailing' and 'launching' equipment for it not to need a carrier. Once airborne, this excess equipment was jettisoned... This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing.</span></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The constraints specified a minimum fractional tonnage which had to be held back, away from battle </span><span class="sc">\[</span><span class="at">under the pretense of "</span><span class="co">[</span><span class="ot">fuel tenders</span><span class="co">](https://en.wikipedia.org/wiki/Depot_ship)</span><span class="at">"</span><span class="sc">\]</span><span class="at">. R7 caused us to consider using warships for that purpose, and indeed that proved a useful decision: whenever some front-line ships were moderately (but not totally) damaged, they traded places with the tenders in the rear lines. This maneuver was explicitly permitted in the rules, but no one had ever employed it except in desperation near the end of a nearly-stalemated battle, when little besides tenders were left intact. Due to the unintuitive and undesirable power of this design, the tournament directors altered the rules so that in 1982 and succeeding years the act of "trading places" is not so instantaneous. The rules modifications introduced more new synergies (loopholes) than they eliminated, and one of those involved having a ship which, when damaged, fired on (and sunk) itself so as not to reduce the overall fleet agility.</span></span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In the naval fleet design task, R9 was used quite heavily. The functions $f$ in that simulated world apply to the design and behavior of fleets and of individual ships: </span><span class="in">`FleetComposition`</span><span class="at">, </span><span class="in">`Agility`</span><span class="at">, </span><span class="in">`Armor`</span><span class="at">, </span><span class="in">`WeaponVariety`</span><span class="at">, </span><span class="in">`TimeToEngage`</span><span class="at">, etc... the ultimate design did settle on a fleet containing almost all identical ships, each with nearly minimal agility, maximal armor, maximal weapon variety, almost all of which engaged with the enemy immediately, etc. One extremal ship employed in the 1981 tournament was a tiny but incredibly agile ship, with no offense whatsoever, that simply could not be hit. Although this was no longer legal in 1982, a ship with massive offensive capability and no defense was instrumental in that new fleet.</span></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="sc">\[</span><span class="at">For R16,</span><span class="sc">\]</span><span class="at"> once a new design was tested in simulated combat, several characteristics of the conflict were noted (speed of victory, final state of the victor, amount of tactical decision-making required, etc.). These were formed into proto-conjectures, which were then tested by subsequent mock battles, and any which held over most of the simulations were believed as empirically valid.</span></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatTheoryFormationHeuristic1983</span><span class="co">]</span></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a>Other than hard-coded heuristics, EURISKO of course discovered many heuristics on its own, such as the "nearly extreme" rule: In almost all fleet design situations, the right decision is to go for the *nearly* extremal design.</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Thus, the final ships had Agility 2 (slightly above the absolute minimum), one weapon of each type of small weapons (rather than 0 or many), the fleet had almost as many ships as it could legally have but not quite (96 instead of 100), etc. Big weapons (enormous spinal mounts capable of blasting another ship to pieces with a single shot) were gradually phased out, in favor of an enormous number of small missile weapons. The fleet had almost all (75) ships of this type though there was one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship), and a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive.</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatTheoryFormationHeuristic1983</span><span class="co">]</span></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a>One might have questioned the wisdom of running EURISKO every night on *all* the problem domains, from *Traveller* to MOS design, but it found good use of heuristic rules learned in one domain applied to another, what we'd now call "transfer learning":</span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In working on the design of integrated circuits, for example, EURISKO stumbled on the fact that symmetry is a desirable property for such chips, although it did not understand why; when it was later instructed to design fleets for the *Traveller* game, EURISKO decided to make them symmetrical and justified its decision by referring to its earlier experience in designing circuits.</span></span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatComputerSoftwareIntelligent1984</span><span class="co">]</span></span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a>Fortunately for EURISKO and Lenat, the navy battles were "tactically trivial", thus reducing the task to merely <span class="co">[</span><span class="ot">nonlinear optimization</span><span class="co">](https://en.wikipedia.org/wiki/Nonlinear_programming)</span>. Also importantly, the problem was hard enough, and nonlinear enough, for EURISKO to show its edge over linear programming and human intuition.</span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; with 50 parameters per ship, about 10 values for each parameter (sometimes fewer, often an infinite number), and up to 100 distinct ships to design and include in each fleet, any systematic or Monte Carlo analysis of the problem is unlikely to succeed. In fact, the designers had done a detailed linear programming model of the game, and their computer runs convinced them that a fleet of about 20 behemoths was the optimal design. This was close to the starting fleet design the author supplied to EURISKO, and it was also close to the designs that most of the tournament entrants came up with.</span></span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a>At the 1981 championship, EURISKO's fleet consisted of 96 ships:</span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>75 "Eurisko class" ships, which were very slow, heavily armored, and carried many small missiles, like a sea urchin. They formed the bulk of the fleet, and used for pure attrition warfare. It was discovered thanks to the "nearly extreme" heuristic.</span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>7 "Wasp class" ships, which were small (1000 tons) but the most agile (Agility 6). These ships had virtually no offensive capability but were practically impossible to hit, serving as "stalemate guarantors". If all other ships in EURISKO's fleet were destroyed, these agile ships would remain, forcing a draw since enemy ships couldn't destroy what they couldn't hit. This concept emerged from a serendipitous battle where one side survived by having an unhittable "lifeboat" (the "Bee class"). They were carried aboard the Queller and Garter class ships.</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>3 "Bee class" tiny ships (99 tons), which were the original accidentally-discovered stalemate guarantor, the "lifeboat" that EURISKO incorporated into every subsequent design, even while it refined the concept of the stalemate guarantor into the Wasp class. Despite heavy Armor 10, their Agility 0 made them less effective than Wasps at avoiding enemy fire, but presumably their tiny size made them hard to hit. They were carried aboard the Queller class ships.</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>3 "Queller class" ships. They were specifically designed as hard counters to stalemate guarantors. Each carried a single massive particle accelerator, which was not used by human-designed fleets, since that was ineffective against normally sized ships -- but due to its broad beam and ease of aiming, excellent against small targets.</span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>4 "Cisor class" ships. These "monstrous hulks" were heavily armored vessels that were *also* hard counters to stalemate guarantors. It had Agility 0, which means it has no chance of avoiding normal ships, but presumably it would survive long enough to destroy any stalemate guarantor, if any existed on the opposing side.</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>4 "Garter class" ships, which implemented the "warship as fuel tender" concept. They were reasonably agile (Agility 4) and could rotate between combat and reserve roles. When front-line ships became damaged, they would trade places with these capable warships held in reserve, allowing fresh ships to enter combat while damaged ones withdrew for repairs. They were crucial for victory in the final battle.</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Reconstructing EURISKO's fleet" collapse="true" }</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>The fleet was actually precisely reported in <span class="co">[</span><span class="ot">*The Journal of the Travellers' Aid Society*, #10, pp. 38-9</span><span class="co">](https://the-eye.eu/public/Books/rpg.rem.uz/Traveller/00%20-%20Other%20Materials/Journal%20of%20the%20Travellers%20Aid%20Society/JTAS/JTAS%2010.pdf)</span>, which Yuxi had <span class="co">[</span><span class="ot">transcribed to plaintext</span><span class="co">](code/eurisko's%20fleet.txt)</span> (thenceforth "report"). Not being an honorary Admiral, agi convened with a council of 4 other LLMs (Gemini-2.5, Claude-3.7, OpenAI-o1, DeepSeek-R1) to figure out how to correlate Lenat's verbal description in <span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span> (thenceforth "description") with the precise report.</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>In fact, it's quite confusing even in the original paper. All agreed that the 75 "Eurisko class" ships corresponded to the "Eurisko class" in the report. But that's where clarity ended.</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>The "stalemate guarantor" was described as "one ship which was small and super agile and purely defensive (and literally unhittable by any reasonable enemy ship)". However, in the report, *every single class* contained more than one ship. There were two classes that seemed like the stalemate guarantor: the Wasp class with 1000 tons and Agility 6, and the Bee class with 99 tons and Agility 0. The Bee class had the smallest tonnage but no Agility, while the Wasp class was second-smallest and had the highest Agility. The Council voted Wasp as the "stalemate guarantor" at 3 Yea (Yuxi, Claude, o1), 1 Nay (Gemini), and R1 abstaining due to the server being busy (abstaining is typical behavior for the Chinese during <span class="co">[</span><span class="ot">Big-Five votes</span><span class="co">](https://en.wikipedia.org/wiki/Permanent_members_of_the_United_Nations_Security_Council)</span>). Those who voted "Yea" were unable to respond to Gemini's objection as to what EURISKO used the Bee class for, and motioned to discuss the second issue.</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a>The hard counter to the stalemate guarantor was described even less clearly, and it seemed like there were *two* classes of them!</span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a couple monstrous hulks which had no chance of defense against normal ships, but which had weapons just barely accurate enough to hit any enemy ships that were (of course!) small and agile and purely defensive. ... this new ship had moderate size, no armor, the largest possible guidance computer, the slowest possible engines for its size and equipment, and one single, enormous accelerator weapon--a weapon usually ignored because its broad beam glances harmlessly off large armor-plated ships, but which is very easy to aim.</span></span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span></span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a>The first hard counter matched both the Cisor class and the Queller class, both of which had over 19,000 tons and Agility 0. The second hard counter seemed like the Garter class, with 12,000 tons and Agility 4.</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>Yuxi lamented that it would have saved the Council a great deal of trouble if the report had detailed what weapons the ships used, but alas, they only reported on the "batteries". It was at this point that o1 and Gemini raised that the "Batteries Bearing" did not actually stand for "carrying around electric batteries", but rather "the angle which the artillery could span its fire". Yuxi expressed astonishment that they could recall the rules of *Classic Traveller*. "What a nerd!", muttered Yuxi. There was no reaction from the others, since the meeting was entirely text-based. Yuxi then sent the entire <span class="co">[</span><span class="ot">@TravellerBook51980</span><span class="co">]</span> to Gemini for a translation, in response to which Gemini suggested that Yuxi RTFM, "Specifically, pages 21--37 and 50--52 are crucial.". </span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>The Council's final conclusion was that probably there were two types of hard counters, with the Cisor being the "monstrous hulk" and the Queller being the one with the "enormous accelerator".</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>Reading Gemini's statement, Claude raised the issue of what the Garter class was supposed to be. At that point, Yuxi re-attended to the fact that <span class="co">[</span><span class="ot">@lenatTheoryFormationHeuristic1983</span><span class="co">]</span> described a slightly different fleet, one with "fuel tenders", which <span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span> did not describe at all. Upon presenting the quote describing "fuel tenders", Gemini argued that the Garter class is the fuel tender, and the Council concurred.</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>The Council reached no concensus as to the purpose of the Bee, though some suspected that the Bee was *another* type of stalemate guarantor, possibly the *original* accidentally-discovered stalemate guarantor, as described in:</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Some of the strangest elements of the final fleet were discovered accidentally rather than as the result of a long, continuous evolution process. The usefulness of a tiny defensive ship was apprehended after a 'lifeboat' was the only survivor from one side's fleet, yet round after round it could not be hit at all. That design was immortalized into a design strategy ("Include one such ship in your fleet!"), and a very general rule began looking for ships that *could* destroy it.</span></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>Yuxi raised one final issue, that of what the fighter "equipped with just enough 'sailing' and 'launching' equipment for it not to need a carrier" described in <span class="co">[</span><span class="ot">@lenatTheoryFormationHeuristic1983</span><span class="co">]</span> corresponded to. Gemini concluded that no such thing existed in the fleet as reported, since the 3 Bees and 7 Wasps were the only fighters in the fleet, and they were carried by exactly enough carriers (3 Quellers carrying 1 Wasp and 1 Bee each, 4 Garters carrying 1 Wasp each). Yet the description "This design tactic caused the rules publishers to modify the constraints, so that in 1982 one could not legally build such a thing." strongly suggested that EURISKO actually used such a ship at one point in the 1981 competition.</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a>Someone suggested that EURISKO may have used more than one fleet design, and only the design used in the final battle was reported. Yuxi acknowledged absent endorsement and motioned that the meeting adjourn.</span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>The final report from the Council <span class="co">[</span><span class="ot">is attached</span><span class="co">](code/Traveller%20High%20Guard%20Analysis.md)</span>.</span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>Whereas most battles took 2--4 hours, EURISKO's opponents resigned in a few minutes, because typical opponents had around 20 large ships against EURISKO's 96 ships. Each round would destroy EURISKO's 15 ships and 5 of opponent's ships. One round was enough for the opponent to realize this, and resign.</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>That is, except the very last round, where EURISKO faced against a human opponent with *basically the same design*, except they didn't have the stalemate guarantor. So if EURISKO seemed to be losing, it could retreat all its fleet and bring out the stalemate guarantor, repair the fleet to full health, then do it again. EURISKO could keep doing this until a lucky dice roll to win.</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Its second opponent did some calculations and resigned without ever firing a shot. The subsequent opponents resigned during their first or second round of combat with this fleet. EURISKO's few specialty ships remained unused until the final round of the tournament, battling for 1st versus 2nd place. That opponent also had ships with heavy armor, few large weapons, low agility, etc. He was lacking any fast ships or fast-ship-killers, though. The author simply pointed out to him that if EURISKO were losing then (according to the TCS rules) our side need put only our fast ship out the front line, withdraw all the others and repair them, and -- once they were finished repairing themselves -- effectively start the battle all over again. This could go on ad infinitum, until such time as EURISKO appeared to be winning, and in that case we would let the battle continue to termination. The opponent did a few calculations and surrendered without fighting. </span></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The tournament directors were chagrined that a bizarre fleet such as this one captured the day, *and* a similar fleet (though not so extreme) took second place. The rules for future years' TCS tournaments were changed to eliminate the design singularities which EURISKO found. For example, repairing of damaged ships was prohibited, so the utility of the unhittable ship became negligible.</span></span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a>Notice that this complicates the typical narrative about a computer beating out humans. As it turns out, *Traveller* was just a very exploitable game. Even a human discovered that loophole.</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a>At the 1982 championship, rules were changed to plug the loopholes, and the rules were published only a week before the event. Fortunately, loophole-plugging begat even more loopholes, as any programmer who has ever rush-debugged could attest, and EURISKO's general heuristics (such as the "nearly extreme" rule) remained valid, so it worked out another winning fleet quickly without needing another 1300 CPU-hours.</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Coincidentally, just as the defensive ship made a difference in the 1981 final round, the offensive ships made a difference in the 1982 final round. In each case, their presence caused the opponent to resign without firing a shot... Just as most 'experienced' players jeered at the 1981 fleet because it had practically no large weapons, they jeered at the 1982 fleet because it was unarmored *and* it still had no large weapons, even though the rules changes had made them much cheaper.</span></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a>Unfortunately, there is no information whatsoever on the composition of EURISKO's 1982 fleet. It seems to be lost in the star-dust of history.</span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a>As one expect from a program reasoning about and making its own rules, it stumbled into meta-bugs. The simplest example was one that kept triggering itself, creating an infinite loop. <span class="co">[</span><span class="ot">@johnsonMachineryMindNew1986, chapter 10</span><span class="co">]</span> Others were more amusing.</span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; One of the first heuristics that EURISKO synthesized (H59) quickly attained nearly the highest </span><span class="in">`Worth`</span><span class="at"> possible (999). Quite excitedly, we examined it and could not understand at first what it was doing that was so terrific. We monitored it carefully, and finally realized how it worked: whenever a new conjecture was made with high worth, this rule put its own name down as one of the discoverers! It turned out to be particularly difficult to prevent this generic type of finessing of EURISKO's evaluation mechanism. Since the rules had full access to EURISKO's code, they would have access to any safeguards we might try to implement. We finally opted for having a small 'meta-level' of protected code that the rest of the system could not modify.</span></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The second 'bug' is even stranger. A heuristic arose which (as part of a daring but ill-advised experiment EURISKO was conducting) said that all machine-synthesized heuristics were terrible and should be eliminated. Luckily, EURISKO chose this very heuristic as one of the first to eliminate, and the problem solved itself.</span></span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatEURISKOProgramThat1983</span><span class="co">]</span></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Often I'd find it in a mode best described as "dead". Sometime during the night, EURISKO would decide that the best thing to do was to commit suicide and shut itself off. More precisely, it modified its own judgmental rules in a way that valued "making no errors at all" as highly as "making productive new discoveries". As soon as EURISKO did this, it found it could successfully meet its new goal by doing nothing at all for the rest of the night... I eventually had to add a new heuristic to EURISKO-one it couldn't modify in any way-to explicitly forbid this sort of suicide.</span></span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@storkHALsLegacy2001s1998, page 194</span><span class="co">]</span></span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The only known image of EURISKO, reasoning about the *Traveller* game, probably on the Xerox 1100 Lisp machine. It was probably running over the [Interlisp Operating System](https://en.wikipedia.org/wiki/Interlisp). Lenat claimed EURISKO ran for 1300 CPU-hours in total. [@lenatComputerSoftwareIntelligent1984]</span><span class="co">](figure/Eurisko_GUI.png)</span></span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>In his review article, Lenat made a brief philosophical comment that EURISKO is the new perceptron:</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... the paradigm underlying AM and EURISKO may be thought of as the new generation of perceptrons, perceptrons based on collections or societies of evolving, self-organizing, symbolic knowledge structures. In classical perceptrons, all knowledge had to be encoded as topological networks of linked neurons, with weights on the links. The representation scheme being used by EURISKO provides much more powerful linkages, taking the form of heuristics about concepts, including heuristics for how to use and evolve heuristics. Both types of perceptrons rely on the law of large numbers, on a kind of local-global property of achieving adequate performance through the interactions of many small, relatively simple parts.</span></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The classical perceptrons did hill-climbing, in spaces whose topology was defined explicitly by weights on arcs between nodes (nodes which did straightforward Boolean combinations plus thresholding). The EURISKO style of system does hill-climbing at both the object- (performance-program) and meta- (control decision) levels, in spaces whose terrain is defined implicitly, symbolically, by the contents of the nodes (nodes which are full-fledged concepts, at both object- and meta-levels). The new scheme fully exploits the same source of power (synergy through abundance) yet it is free from many of the limitations of the classical perceptron scheme.</span></span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatWhyAmEurisko1984</span><span class="co">]</span></span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a>If this sounds familiar, it is because this Lenat had the same idea as Marvin Minsky, a friend of his,<span class="ot">[^minsky-anecdote]</span> and he was writing in 1984, at the second coming of neural networks. Minsky would soon write his *Society of Mind* in 1986, which the Cyc project resembled, then [re-reject neural networks by writing a long epilogue in 1988](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/#minsky-and-papert-struck-back) to his infamous *Perceptrons* (1969), often blamed for the first neural network winter. Indeed, Lenat's objection to neural networks was essentially the same as Minsky's, if you compare that with Minsky's epilogue. Lenat's approach to Cyc was the same as the Society of Mind of Minsky. Reciprocating, Minsky had often called the field of AI "brain-dead", holding Lenat's Cyc as the only one worth mentioning. <span class="co">[</span><span class="ot">@baardAIFounderBlasts2003</span><span class="co">]</span></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>Continuing the trend of AM, Lenat never published the source code for EURISKO,<span class="ot">[^am-source-code]</span> and indeed, the only known attempt at reimplementation was <span class="co">[</span><span class="ot">@haaseInventionExplorationDiscovery1990</span><span class="co">]</span>, which had no offspring.</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a><span class="ot">[^minsky-anecdote]</span>:</span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a>    I'm not sure where to put this anecdote about Minsky that Lenat told, but I want to put it somewhere:</span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; ... when he was at Lincoln Labs about 50 years ago. And in those days computer time was so precious that you submitted a deck of computer cards and the very first card said ‘how many CPU seconds to allow the program to run?’ And so he built a program that essentially would beg for time. So it would say ‘give 30 seconds’ on the job control card, but then once it started, all it would do is sit there for 15 seconds doing nothing. Then it would ring a bell on the Teletype console in the machine room and call the operator’s attention and say ‘I need 20 more seconds please.’ Then it would just sit there for another 15 seconds and do that again and say ‘I need another minute please.’ And so at the end finally after like half an hour, the operator just killed that particular job. And Marvin would storm into the poor operator’s room and say "Hey I put 15 seconds on the job control card. You're charging me for half an hour of CPU time," and the poor operator would say "well your program kept asking for it," and Marvin would say, "it always does that."</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; [@lenatEpisode89Conversation2019]</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a><span class="in">    Though now that I've finished the essay, this feels like a metaphor for the Cyc project itself.</span></span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## The saga of Cyc</span></span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; He divided the universe into forty categories or classes, which were then subdivided into differences, and subdivided in turn into species. To each class he assigned a monosyllable of two letters; to each difference, a consonant; to each species, a vowel. For example, </span><span class="in">`de`</span><span class="at"> means element; </span><span class="in">`deb`</span><span class="at">, the first of the elements, fire; </span><span class="in">`deba`</span><span class="at">, a portion of the element of fire, a flame. In a similar language invented by Letellier (1850),</span><span class="ot">[^letellier-1850]</span><span class="at"> </span><span class="in">`a`</span><span class="at"> means animal; </span><span class="in">`ab`</span><span class="at">, mammalian; </span><span class="in">`abo`</span><span class="at">, carnivorous; </span><span class="in">`aboj`</span><span class="at">, feline; </span><span class="in">`aboje`</span><span class="at">, cat; </span><span class="in">`abi`</span><span class="at">, herbivorous; </span><span class="in">`abiv`</span><span class="at">, equine; etc... children could learn this language without knowing that it was artificial; later, in school, they would discover that it was also a universal key and a secret encyclopedia.</span></span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Having defined Wilkins' procedure, we must examine a problem that is impossible or difficult to postpone: the merit of the forty-part table on which the language is based. Let us consider the eighth category: stones. Wilkins divides them into common (flint, gravel, slate); moderate (marble, amber, coral); precious (pearl, opal); transparent (amethyst, sapphire); and insoluble (coal, fuller's earth, and arsenic). The ninth category is almost as alarming as the eighth. It reveals that metals can be imperfect (vermilion, quicksilver); artificial (bronze, brass); recremental (filings, rust); and natural (gold, tin, copper). The whale appears in the sixteenth category: it is a viviparous, oblong fish.</span></span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Borges, *The [analytical language](https://en.wikipedia.org/wiki/An_Essay_Towards_a_Real_Character,_and_a_Philosophical_Language) of [John Wilkins](https://en.wikipedia.org/wiki/John_Wilkins)*</span></span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a><span class="ot">[^letellier-1850]: </span>Took some work to figure out what this is. It was by Charles Louis Augustin Letellier, published in 1852. The full title is *Cours complet de langue universelle : offrant en même temps une méthode. pour apprendre les langues, et pour comparer. toutes les littératures mortes et vivantes* <span class="sc">\[</span>Complete course in universal language: offering at the same time a method for learning languages, and for comparing all literatures, dead and living<span class="sc">\]</span>.</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a>Unfortunately, EURISKO ran out of steam just like AM. Taking the 4 lessons of AM and EURISKO, Lenat concluded that there would be no free lunch. Intelligence is a lot of work. You need to put in the right representational language for reasoning and discovery -- not just about Lisp or RLL or mathematics, but a lot more. You need to put in a lot of loosely organized, kind of correct heuristic rules -- not just a few eternal truths of discovery. You need to put in a lot of facts. You need to interact with the program, to help it along and to be helped along, not just to sit and watch.</span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>So Lenat began paying for his lunch.</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a>In 1984, he started the Cyc project, an ambitious attempt to scale symbolic AI up to the real world. Like most expert systems, the Cyc project consists of a giant knowledge base that encodes all of common sense, upon which inference engines run. Unlike most expert systems, the ambition of Cyc was universal: Its knowledge base would not be restricted to expert knowledge in a particular domain, but *all* common sense knowledge in *all* domains that humans have ever common-sensed. It would take a decade, but considering the payoff, it would be completely worth it.</span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; AI has for many years understood enough about representation and inference to tackle this project, but no one has sat down and done it... only by pulling together the latest human interface tools, Lisp machines, ideas of enforced semantics, and funding for a decade-long effort could we attempt a project of this scale.</span></span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatCycUsingCommon1985</span><span class="co">]</span></span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a>The game plan was simple:</span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>"Prime the knowledge pump" by manually encoding a large enough knowledge base of common senses in a logical language. Also, construct a translator between the logical language of the Cyc and the natural language of humans.</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Obtain an AI with common sense and natural language, allowing it to learn by reading what people have written down and conversing with people.</span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>When it reaches the human frontier of knowledge, it will start performing experiments to go beyond it.</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>This would solve in one go three problems that plagued the 1980s expert systems:</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>No more of the famous brittleness of expert systems, such as stating that a rusty car had measles just because the user stated that it had reddish spots, because Cyc would have *all* the common senses.</span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>No more of "running out of steam" like EURISKO and AM. Once it has enough knowledge, it would be able to machine-learn, and thus get past the knowledge bottleneck.</span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>No more of siloed experts unable to communicate across the vast gap between their little knowledge domains. Expert systems would finally be able to talk with each other if they would all be based on Cyc's universal knowledge base.</span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a>A metaphor that Lenat had used often is that of priming a water pump. The more knowledge Cyc has, the easier it is for Cyc to learn more. At first, it must be spoon-fed knowledge with every entry entered by hand. As it builds a basic understanding of the world, it would be able to parse sentences half-way from natural language to logic, and the ontologists would help finish the job, and the more it knew, the better it could parse, saving more time, until it would start parsing without human help. On that day, the "knowledge pump" would finally, triumphantly, be primed, and Cyc would start pumping and pumping, and more knowledge would just keep pouring out without any exhaustion, ushering a new golden age.</span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cyc by 1993</span></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a>The first published plan of Cyc was in 1985 <span class="co">[</span><span class="ot">@lenatCycUsingCommon1985</span><span class="co">]</span>, with 3 stages:</span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>(1985--1988): by hand, encode 400 encyclopedia articles. They estimated there are about 400 "kinds" of articles, and they planned to get one in each kind.</span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>(1988--1993): encode 30000 encyclopedia articles. This step should be fast, since each new article can be copy-and-edited from a previously encoded similar one.</span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>(1993--?): use Cyc to solve AI problems and apply Cyc for commercial purposes.</span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a>They noted that most articles had just 1 paragraph, and 1 paragraph took about 1 person-day to encode, which allowed them to estimate that phases 1 and 2 would take 150 person-years to complete, so with a crew of 20, the plan seemed doable.</span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a>At this point, Cyc was still based on frames, like AM and EURISKO. Each frame corresponded roughly to a concept. Each new article encoded took "dozens" of new frames, but they expected it to drop to around 0.1 frames per article after the first 1000s of articles, indicating they expected the final Cyc to contain about 50K concepts.</span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>Such a project was way beyond a typical academic project, or even a commercial project. Fortunately, the Japan scare of the 1980s created a flood of funding for AI projects, and Cyc got funding for the next 10 years, freed from both academic and commercial interests.</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The state of knowledge base of Cyc one year after its founding, in 1985. [@lenatCycUsingCommon1985, Figure 1]</span><span class="co">](figure/Cyc_Upper_Ontology_1985.png)</span></span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a>5 years later, Lenat optimistically wrote the "midterm" report that the project was still on schedule, that the knowledge pump would be primed by 1995. <span class="co">[</span><span class="ot">@lenatCycMidtermReport1990</span><span class="co">]</span></span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a>The most important change was what Lenat called "from the black to the white". Instead of encoding what is written in the encyclopedias, they really should encode what is *not* written. Encyclopedias don't teach what everyone knows, but to make sense of it, one must already know what "everyone knows". What Cyc should have is not the black ink, but the white space around the black ink.</span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Lenat began building Cyc by setting himself a seemingly modest challenge. He picked a pair of test sentences that Cyc would eventually have to understand: "Napoleon died in 1821. Wellington was greatly saddened." To comprehend them, Cyc would need to grasp such basic concepts as death, time, warfare, and France, as well as the sometimes counterintuitive aspects of human emotion, such as why Wellington would be saddened by his enemy's demise. Lenat and a few collaborators began writing these concepts down and constructing a huge branching-tree chart to connect them. They produced a gigantic list of axiomatic statements--fundamental assumptions--that described each concept in Cyc's database: its properties, how it interacted with other things. "We took enormous pieces of white paper," Lenat remembers, "and filled walls, maybe 150 feet long by about 8 feet high, with little notes and circles and arrows and whatnot."</span></span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@thompsonKnowItAllMachine2001</span><span class="co">]</span></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a>They have also estimated that it takes about 80 minutes to encode a single rule, including the overhead for knowledge elicitation up front, and the overhead for debugging and testing <span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span>.</span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a>With these lessons learned, they confidently claimed that they have encoded 1M assertions, 50K concepts, with 0.1% of common sense done. They expected that 10--50% was necessary for knowledge pump to be primed, which they expected to be done by 1995 -- still on schedule,<span class="ot">[^everyone-friends-of-doug-lenat]</span> and they looked forward to the day, around 2000, when "no one would even think about having a computer that doesn’t have Cyc running on it" <span class="co">[</span><span class="ot">@lenatBuildingLargeKnowledgebased1989</span><span class="co">]</span>.</span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a><span class="ot">[^everyone-friends-of-doug-lenat]</span>:</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a>    Presumably, at that point Cyc would stop imagining everyone is a friend of Lenat but not of gravity. Growing up truly comes with some terrible disillusionments.</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; When a natural-language understanding researcher was first trying to represent the concept of falling, he translated "X fell" as "Gravity carried X downward." Elsewhere in his system was a rule that if you fell in a river and could not swim, and had no friends to rescue you, you drowned. Since gravity as a force of nature has neither arms, legs nor friends, it meets its unfortunate -- and improbable -- end.</span></span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; ... sometime in 1994 or 1995 by Lenat's hopeful reckoning -- Cyc reaches the breakeven level of about 10 million facts. At that point, it will be able to pick up new knowledge more readily by reading than by having knowledge engineers spoon-feed it. A wider information base may also save Cyc from such gaffes as concluding (from everybody it knew about) that all humans in the world are friends of Doug Lenat.</span></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; [@wallichSiliconBabies1991]</span></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a>They expected that the finished Cyc would know 1M concepts. Why 1M? They held their own "mini-<span class="co">[</span><span class="ot">Dartmouth conference</span><span class="co">](https://en.wikipedia.org/wiki/Dartmouth_Conference)</span>" and found that multiple estimates all suggest the 1M number <span class="co">[</span><span class="ot">@lenatThresholdsKnowledge1991</span><span class="co">]</span>:</span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Alan Kay: 30K encyclopedia articles with 30 concepts per article gives 0.9M concepts.</span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Japanese Electronic Dictionary Research Project: in several languages, an educated speaker knows about 200K words.</span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Marvin Minsky: 0.2M waking hours between birth and age 21. Assuming 4 new concepts per hour, then 0.8M concepts.</span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There are about 1 trillion brain cells. Assuming each brain cell is responsible for a one-step inference between two concepts, then there are 1M concepts.</span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The user interface of Cyc as of 1989. It has a similar style as EURISKO's, and thus it probably ran on the Interlisp OS. It was described as being able to run on "[Symbolics Machines](https://en.wikipedia.org/wiki/Symbolics) of all sorts, [Sun3](https://en.wikipedia.org/wiki/Sun-3)s, [TI Explorer II](https://en.wikipedia.org/wiki/Texas_Instruments_Explorer)+s, [MacIIs (with Ivory boards)](https://en.wikipedia.org/wiki/MacIvory), and (with some conversion) [VAX machines](https://en.wikipedia.org/wiki/VAX)". These were dominant high-end Lisp computers of the late 1980s: dedicated Lisp Machines (Symbolics, TI), powerful Unix workstations (Sun), high-end PC enhanced with Lisp hardware (MacIvory), and minicomputers (VAX). [@lenatBuildingLargeKnowledgebased1989, figure 8-1]</span><span class="co">](figure/Cyc_GUI_1989.png)</span></span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A small fragment of Cyc's upper ontology as of 1990. [@lenatCycMidtermReport1990, figure 2]</span><span class="co">](figure/Cyc_Upper_Ontology_1990.png)</span></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cyc by 2000</span></span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a>Around 1990, a large rewrite occurred, resetting some progress. This was expected. As Lenat later claimed, they had met about 150 technical obstacles along the way, and had cleared them all away by 1990, and it remained to just add more knowledge. <span class="co">[</span><span class="ot">@lenatBuildingMachineSmart2009; @lenatCreating30MillionRuleSystem2022</span><span class="co">]</span> The most important technical obstacles that they conquered were as follows.</span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a>Instead of an object-oriented frame-and-slots language like EURISKO and AM, use a fully general higher-order logic language. This is necessary because people can do common sense higher-order reasoning: modals, reflection, pros and cons, counterfactual hypotheticals, contexts as first-class objects in our ontology, several different useful "species" of negation, etc. <span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a>Instead of searching for the right representation, use as many representations for concepts and rules as you need. Each is represented in at least two ways. This is necessary for efficient inference. Similarly, use as many inference engines as needed, since the general logic engine is too slow. They had already 20 at that point, and they would eventually end up with &gt;1100. <span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a>Don't try to make the perfect "<span class="co">[</span><span class="ot">upper ontology</span><span class="co">](https://en.wikipedia.org/wiki/Upper_ontology)</span>". It just has to be good enough. A "suboptimal" one only causes a constant factor $O(1)$ of waste in computational space and time.</span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We even wasted quite a bit of time trying to get the very most general tip of Cyc's concept network "right"... at the 1986 Fifth-Generation Project conference in Tokyo, when we saw the ontology built by Japan’s answer to Cyc, named Electronic Dictionary Research (EDR). Their topmost distinction was between things with souls and things without souls. And large trees were in the former category, whereas small trees were in the latter category... They and their EDR system knew that both types of trees needed water and sunlight and had roots, etc., they just had to represent each of those assertions as two separate rules instead of one, as we did in Cyc. No big deal.</span></span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The important lesson was: Making suboptimal ontology choices just means that your ontology and knowledge base might have to be bigger, more verbose, to make up for those missed generalization opportunities.</span></span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a>In the progress report in 1995, they stated that they had manually re-entered in the new language 100K concepts and 1M assertions into Cyc in the new language, at the price of 100 person-years. Furthermore, the knowledge pump was close to being primed, and they expect Cyc to start learning on its own by reading ("natural language understanding") and discovery ("machine learning") sometime in the next 10 years. <span class="co">[</span><span class="ot">@lenatCycLargescaleInvestment1995</span><span class="co">]</span> In another essay, Lenat optimistically predicted that "The goal of a general artificial intelligence is in sight, and the 21st-century world will be radically changed as a result." <span class="co">[</span><span class="ot">@lenatArtificialIntelligence1995</span><span class="co">]</span></span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a>Before this point, the Cyc project was part of a <span class="co">[</span><span class="ot">private-public consortium</span><span class="co">](#sec-cycops-overview)</span> and funded accordingly. After this point, the consortium was mostly over, so Lenat spun out Cyc into a for-profit company, Cycorp, to continue the work. Most academic publications ceased at this point, and I had to rely on OSINT/cyberstalking at this point to piece together what happened afterwords, by watching every Lenat talk, reading every news report, and digging up every gossip by ex-Cyclists in long-dead forums.<span class="ot">[^cyberstalking-the-cyc]</span></span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a><span class="ot">[^cyberstalking-the-cyc]: </span>Most papers published after that point were slim on details, and mostly about yet new exciting ways for them to ingest more data from the Internet, or about yet more ways to use their knowledge base. I could find no information about how the inference engines worked, and very little details of commercial applications. Most of the "applications" were vaporware, with dead links everywhere.</span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cyc in the 2000s</span></span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a>It was 2001. The Internet was thriving even though <span class="co">[</span><span class="ot">most dot-com startups had died</span><span class="co">](https://en.wikipedia.org/wiki/Dot-com_bubble)</span>, and Lenat planned to harness the wisdom of the online crowd -- much like Wikipedia, the Free Encyclopedia, which launched in the same year. Cyc at that point had 1.5M assertions, and had begun to be partly "tutored" in natural language by the Cyclists, which was more pleasant than typing everything up in pure CycL. Lenat optimistically predicted that the Internet crowd would be entering 10M assertions in 2002, accelerating from there, so that Cyc would know 100M assertions by 2007, at which point it would know as many things as a typical human. Then by 2011, Cyc would have learned, by reading and chatting with people, all that humanity collectively knows. From there on, Cyc will extend the knowledge frontier by running novel experiments in a research lab. <span class="co">[</span><span class="ot">@thompsonKnowItAllMachine2001; @anthesComputerizingCommonSense2002</span><span class="co">]</span> Other reports suggested that AGI, or what he called a "generally intelligent artifact" would arrive by 2020 or 2025. <span class="co">[</span><span class="ot">@portDuelingBrainscapesArtificial1997; @lyonsArtificialIntelligenceGets1998</span><span class="co">]</span></span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; He draws me a graph that shows Cyc's learning curve. From 1985 to 2000, the line curves upward gradually -- the "brain surgery" phase during which the Cyclists input knowledge by hand. But then at 2001, the curve steepens dramatically as the open-source phase takes over, and thousands -- or millions -- more inputters join in. Lenat extends the curve maybe ten years into the future. As the curve reaches the point where Cyc has read everything there is to read and spoken with everyone willing to tell it facts, it will begin to flatten out. "It'll know all there is to know," he says. "At that point, the only way it could learn more is by doing experiments itself."</span></span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@thompsonKnowItAllMachine2001</span><span class="co">]</span></span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a>To drum up the support, Cycorp released OpenCyc in 2001, a small subset of Cyc. Lenat planned to migrate everything to the public mode. But OpenCyc would always lag the true Cyc by 24 to 30 months. <span class="co">[</span><span class="ot">@anthesComputerizingCommonSense2002</span><span class="co">]</span> Unfortunately, this was not to be the case. The last version of OpenCyc was released in 2012, and it quietly shut down with no fanfare, probably in 2017-03, with a <span class="co">[</span><span class="ot">curt message</span><span class="co">](https://web.archive.org/web/20170422212642/http://opencyc.org/)</span>:</span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Part of the Cyc technology was released, starting in 2001, as OpenCyc, which provided an API, RDF endpoint, and data dump, under appropriate Apache and Creative Commons open source licenses. Its distribution was discontinued in early 2017 because such "fragmenting" led to divergence, and led to confusion amongst its users and the technical community generally that that OpenCyc fragment was Cyc.</span></span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a>OpenCyc was primarily a subset of the knowledge base. As described <span class="co">[</span><span class="ot">in the Handbook</span><span class="co">](https://web.archive.org/web/20040310001548/http://www.cyc.com/doc/handbook/oe/10-notable-quoted-collections.html)</span>, the base contained 3 sections:</span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The public section was a "large section, constituting much of the Cyc upper-ontology as well as some middle- and lower-ontology".</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The proprietary section was an "even larger section of the Knowledge Base, subsuming the public release and containing a good deal more, has been sanctioned for release to corporations and individuals who are co-participants with Cyc in various DARPA contracts. This portion of the Knowledge Base is generally referred to as the Integrated Knowledge Base or IKB.".</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A small classified section was for Cycorp itself, "that should not be released to anyone outside the company. Sometimes, this is because the information is pertinent to commercial contracts that are subject to non-disclosure; sometimes, it is because the terms in question are considered experimental in one way or another, and therefore not suitable for immediate release."</span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The knowledge base of OpenCyc as of 2010. ([Source](https://web.archive.org/web/20100526095325/http://www.opencyc.com/))</span><span class="co">](figure/opencyc-kb-browser.gif)</span></span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a>Concurrent with OpenCyc, there was also ResearchCyc, which contained the non-proprietary parts of Cyc, but only available for research purposes. It shutdown <span class="co">[</span><span class="ot">sometime in 2019</span><span class="co">](https://web.archive.org/web/20190601000000*/http://www.cyc.com/researchcyc)</span>, without even a curt message.</span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a>As one can expect from Lenat's previous non-releases, there was and has never been any release of Cyc itself, especially because Cyc is a commercial endeavor, as is necessary to sustain the 2000 person-year project.</span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>In 2001, at the start of the <span class="co">[</span><span class="ot">Semantic Web</span><span class="co">](https://en.wikipedia.org/wiki/Semantic_Web)</span> hype (the original "Web 3.0", before it came to mean cryptographic blockchains), Cycorp began seriously engaging with the many Semantic Web initiatives.</span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a>Squinting a bit, the visions of the semantic web and the vision of the Cyc were the same: Both wish to draw connections between data, such that computer programs can chain together multiple operations on data, synthesize them, and give the user what they meant, instead of what they literally typed out. Cycorp regarded the semantic web effort as doing essentially the same thing, on the Internet scale, except with a less expressive frame language (not even first-order logic). During the 2000s, papers from Cycorp often talked of integrating Cyc with the semantic web by encoding knowledge in DAML, RDF, OWL, XML, or some other boring acronym. </span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a>Cycorp participated in the <span class="co">[</span><span class="ot">Standard Upper Ontology Working Group (SUO WG)</span><span class="co">](https://web.archive.org/web/20120609070903/http://suo.ieee.org/)</span>, which, like most Working Groups, petered out in 2003 among motions, procedures, and consensus-buildings, filled with meticulous wisdoms and benevolent safeguarding of the metaphysics of mankind. What it did provide is <span class="co">[</span><span class="ot">the earliest copy</span><span class="co">](https://web.archive.org/web/20030515043435/http://www.cyc.com/SUO/opencyc-ontology.txt)</span> of OpenCyc I have found, in 2003, which I have <span class="co">[</span><span class="ot">backed up</span><span class="co">](https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/opencyc-ontology.txt)</span> for safekeeping.</span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>With the breakout success of the <span class="co">[</span><span class="ot">ESP game</span><span class="co">](https://en.wikipedia.org/wiki/ESP_game)</span> in 2003, "games with a purpose" were all the rage, so Cyc attempted to keep up with the times with its own <span class="co">[</span><span class="ot">FACTory</span><span class="co">](https://web.archive.org/web/20120902022607/http://game.cyc.com/)</span> in 2005. It never got out of the <span class="in">`Beta version 1.0.1`</span> and shut down sometime after 2012. According to the <span class="co">[</span><span class="ot">tutorial</span><span class="co">](https://web.archive.org/web/20061205102605/http://207.207.9.186/helpfiles/HowToPlay.html)</span>, it was a single player game, where the player just selects whether the statement is true, false, doesn't make sense, or the player doesn't know. The answers are scored based on the majority of answers. Though Lenat stated in a lecture that they had players that raked up 50 hours a month to the point that Lenat felt concerned about it, I could not find any mention of how much data they have gathered from this game in total. Nevertheless, this appeared to be the only time Cyc crowd-sourced data.</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a>In 2006, Cycorp spun out The Cyc Foundation, a non-profit organization promote the OpenCyc + Semantic Web combo. As usual, nothing ever came of it. The <span class="co">[</span><span class="ot">last blogpost went up in 2011-06</span><span class="co">](https://web.archive.org/web/20110610022626/http://www.cycfoundation.org/blog/?p=45)</span>, and the website shutdown in 2015.</span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The Cyc + Wikipedia = "Cyclopedia" mockup [posted in 2007 by The Cyc Foundation](https://web.archive.org/web/20071102082440/http://www.cycfoundation.org/blog/?page_id=15). Despite being "fairly close to releasing a beta version of Cyclopedia", they never did.</span><span class="co">](figure/Cyclopedia_mockeup.png)</span></span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a>In 2008, Cycorp tried again by putting up a copy of OpenCyc with a user interface and an API, branded as "<span class="co">[</span><span class="ot">OpenCyc for the Semantic Web</span><span class="co">](https://web.archive.org/web/20080826011848/http://sw.opencyc.org/)</span>". The API would allow web agents to call on OpenCyc and use the replies to do Semantic Web things, making it "the backbone of semantic web"... at least that was the hope. An exhaustive Google search turned up zero actual applications. It shut down in 2017 when OpenCyc did.</span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a>Though deprived of its purported backbone, from our vantage point, the Semantic Web has arrive as promised, but instead of the dream of Cyc-like thinkers performing long queries over databases, we have forgetful agent swarms talking with each other with API calls, dying, <span class="co">[</span><span class="ot">REST</span><span class="co">](https://en.wikipedia.org/wiki/REST)</span>ing, and reincarnating, a game of <span class="co">[</span><span class="ot">*Memento*</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Memento_(film)) on the global scale.</span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>In 2008, Cycorp tried again by joining the Large Knowledge Collider (LarKC)<span class="ot">[^larkc]</span> project at Europe, "a platform for massive distributed incomplete reasoning that will remove the scalability barriers of currently existing reasoning systems for the Semantic Web". The <span class="co">[</span><span class="ot">hope</span><span class="co">](https://active09.ijs.si/wp-content/uploads/2009/06/michael-witbrock.pdf)</span> was to produce a common knowledge base with over 1 billion triples, such that it can easily be scaled up further, and easy to produce expert systems based on it. A few papers and conferences later, the project <span class="co">[</span><span class="ot">ended in 2011</span><span class="co">](https://web.archive.org/web/20150206003436/http://www.larkc.eu/)</span>.</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a><span class="ot">[^larkc]: </span>The name is an obvious shade to the <span class="co">[</span><span class="ot">Large Hadron Collider</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Large_Hadron_Collider#Inaugural_tests_(2008)), which began operation in 2009 and was briefly in the popular imagination.</span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>There was *another* <span class="co">[</span><span class="ot">blog</span><span class="co">](https://web.archive.org/web/20120126094742/http://blog.cyc.com/)</span> by Cycorp starting in 2008, that stopped updating in 2011 after 11 unremarkable posts. It was supposedly written by Cycorp, though its style was the same forgettably respectable style as that of the Cyc Foundation and parliamentary proceedings.</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a>There was a Twitter bot <span class="in">`@cyc_ai​`</span>, which started in 2008 and <span class="co">[</span><span class="ot">stopped in 2011</span><span class="co">](https://web.archive.org/web/20150524003739/https://twitter.com/cyc_ai)</span> after 15764 inane tweets in the format of "I just leaned <span class="in">`&lt;statement&gt;`</span>, true or false?". According to Internet Archive, it shut down sometime before 2017.</span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cyc is done?</span></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a>As you might have noticed if you clicked on any of the above links, almost all the links are dead now. By checking the last known good copy of the various websites on Internet Archive, I noticed that there was a "massive extinction event" during 2014--2016, when Cycorp purged most of the open information about Cyc from the Internet. No more <span class="co">[</span><span class="ot">OpenCyc</span><span class="co">](https://web.archive.org/web/20100526095325/http://www.opencyc.com/)</span>, <span class="co">[</span><span class="ot">tutorials</span><span class="co">](https://web.archive.org/web/20091110010118/http://www.cyc.com/cycdoc/walkthroughs/oeintro_cats_frames_long.html)</span>, <span class="co">[</span><span class="ot">references</span><span class="co">](https://web.archive.org/web/20090219032021/http://www.cyc.com:80/cycdoc/ref/subl-reference.html)</span>, <span class="co">[</span><span class="ot">vocabulary lists</span><span class="co">](https://web.archive.org/web/20090213151624/http://www.cyc.com:80/cycdoc/vocab/actor-vocab-complete.html)</span>, <span class="co">[</span><span class="ot">The Ontological Engineer's handbook (version 0.7)</span><span class="co">](https://web.archive.org/web/20040107124240/http://www.cyc.com/doc/handbook/oe/oe-handbook-toc-opencyc.html)</span>...<span class="ot">[^ontological-engineers-handbook]</span> everything was gone, except marketing material. Fortunately, the Internet Archive exists, and I scraped much of the primary material from it and <span class="co">[</span><span class="ot">uploaded them to GitHub</span><span class="co">](https://github.com/yuxi-liu-wired/cyc-archive/tree/main)</span> for safekeeping.</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a><span class="ot">[^ontological-engineers-handbook]</span>:</span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a>    There is a funny anecdote about this book:</span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; I coined the phrase "ontological engineer" in the mid-1980’s, shortly after embarking on the construction of Cyc, because I didn’t like the pejorative tone of "knowledge enterer" or "knowledge worker", and the term "knowledge engineer" had already been taken (to mean someone who builds expert systems). Based on that, when Addison-Wesley published our 1989 Cyc book (*Building Large Knowledge-Based Systems*), the editor playfully inserted a forward reference, at the front, under Other Publications, to the 1997 *Ontological Engineer's Handbook*. Of course the joke was on us when, starting in 1997, we began to be deluged by requests for that nonexistent work.</span></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; [@lenatAppliedOntologyIssues2005]</span></span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a>Why? </span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a>I believe it was for commercial reasons. This mass extinction event closely corresponded to the commercialization wave in 2016, when Lenat finally declared the Cyc project "done" and set about commercializing it, both via Cycorp and via Lucid.ai, a company founded a year before.</span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "Part of the reason is the doneness of Cyc," explains Lenat, who left his post as a professor at Stanford to start the project in late 1984. "Not that there’s nothing else to do," he says. But he notes that most of what is left to be added is relevant to a specific area of expertise, such as finance or oncology. Among other projects, </span><span class="sc">\[</span><span class="at">Lucid</span><span class="sc">\]</span><span class="at"> is developing a personal assistant equipped with Cyc's general knowledge. This could perhaps lead to something similar to Siri. ... the CEO of Lucid says the new company is in talks with various others interested in using the Cyc knowledge base. Lucid has been working with the Cleveland Clinic, for example, to help automate the process of finding patients for clinical studies.</span></span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@knightAI30Years2016</span><span class="co">]</span></span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a>This timing of commercialization coincides suspiciously with the end of almost all projects except ResearchCyc. Even <span class="co">[</span><span class="ot">the documentation</span><span class="co">](https://web.archive.org/web/20160825080811/http://www.cyc.com/documentation/)</span> <span class="co">[</span><span class="ot">went offline</span><span class="co">](https://web.archive.org/web/20170802104610/http://www.cyc.com/documentation/)</span> in 2016 and could now only be accessed for commercially registered accounts! It looks like a total internal pivot as the company focused on commercialization and shut down all services that were not high priority for the bottom line.</span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a><span class="fu">### How expensive was the lunch?</span></span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a>After cyberstalking the Cyc for days, I had captured every numerical datapoint that has ever dripped out of Cycorp over its entire existence, and combined them into a <span class="co">[</span><span class="ot">spreadsheet that you can analyze yourself</span><span class="co">](https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/Cycorp%20claims.xlsx)</span>. In short, Cyc has grown to 30M assertions over the years, and is still incomplete, if completeness were measured by its original standard -- a self-learning AGI. Despite Lenat's 2016 claim that it was "done", there is no self-learning or AGI in sight.</span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The progress of Cyc project over the period of 1989--2022. The number of assertions grew to 30M, the cost grew to \$200M, with 2000 person-years. Source: [my obsessive cyberstalking](https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/Cycorp%20claims.xlsx).</span><span class="co">](figure/Cyc_progress_history.svg)</span></span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a>Looking at this diagram, we notice 3 things:</span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a>One, the growth of assertions is roughly exponential, doubling every 6 years. At this rate, in 2032 Cyc can expect to reach 100M assertions, the hoped-for point at which Cyc would know as much as a typical human. This might be the dream of Lenat, but I bet it will just become a slightly bigger enterprise solution to expertise management.</span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a>Two, the cost of human labor has remained stable throughout the existence of Cycorp, at \$100K/person-year.</span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a>Three, The cost of assertions was \$5/assertion from 1990--2010, but \$0.7/assertion after 2015, around the time when Lenat declared the project mostly complete. This matches the estimate of <span class="co">[</span><span class="ot">@paulheimHowMuchTriple2018</span><span class="co">]</span>. It's unclear what caused the 7-fold increase in efficiency. Perhaps this is a success of "priming the knowledge pump", or this might just because after 2015, most new assertions came from adding new assertions specialized to particular commercial applications, and thus were easier to handle. Formalizing business rules already written down is easier than extracting the intuitive metaphysics of fruits vs vegetables, after all. In any case, a 7-fold efficiency improvement over 30 years is unimpressive, and indistinguishable from more prosaic arguments about <span class="co">[</span><span class="ot">learning curve</span><span class="co">](https://en.wikipedia.org/wiki/Learning_curve)</span> effects observed in most industries: The more you make, the cheaper you can make.</span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview of Cyc, the system</span></span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a>The Cyc system consists of three main kinds of parts: </span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The iterations of CycL, based on SubLisp, which is itself based on Lisp.</span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The knowledge bases, consisting of millions of assertions written in CycL.</span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The ~1000 inference engines, written in SubLisp.</span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>To increase speed and compatibility, the programs written in CycL are compiled to Java, which is then compiled to bytecode.</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a>The CycL language is intended to be the high-level language that humans can read and write, at the "Epistemological Level" (EL). Most inference engines do not process CycL directly, but process low-level translations of CycL at the "Heuristic Level" (HL). Each sentence at the EL can be translated into a multitude of HL sentences, since different translations allow different inference engines to process it, so that hopefully at least one of those would process it efficiently. The two sides are connected by an interface called the "Canonicalizer".</span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">How Cyc processes a user's input as of 1990. The user enters at the EL-KB (epistemological level knowledge base), which gets translated down to the HL, where multiple modules  for generating and comparing arguments for and against a given proposition are run. [@lenatCycMidtermReport1990, figure 1]</span><span class="co">](figure/Cyc_EL_HL.png)</span></span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The expressiveness-efficiency tradeoff. The HL is highly efficient for the machine to infer with, but very hard for humans to express what they are thinking of in. It is the opposite for the EL, and even more so for natural languages like English. [Source](https://web.archive.org/web/20041225143106/http://www.cyc.com:80/cycdoc/handbook/images/elhl.gif)</span><span class="co">](figure/EL_vs_HL.gif)</span></span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a><span class="fu">### CycL language</span></span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>Since the CycL language has had been developed since the late 1980s, and Cycorp published very little, I am basing this section mostly on <span class="co">[</span><span class="ot">the Ontological Engineer's Handbook (version 0.7)</span><span class="co">](https://web.archive.org/web/20040107124240/http://www.cyc.com/doc/handbook/oe/oe-handbook-toc-opencyc.html)</span>, which seemed to contain the most details, even though technically it only describes the 2002 version of CycL.</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>Like Common Lisp, the CycL language has almost no syntax:</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>An **expression** is from one of the following types: <span class="in">`(#$relation &lt;arg1&gt; … &lt;argn&gt;)`</span>, <span class="in">`?variable`</span>, <span class="in">`#$Term`</span>, text string, and rational number. </span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A <span class="in">`#$relation`</span> is either a function or a predicate. It always begins with a lowercase.</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If <span class="in">`#$relation`</span> is a function, then <span class="in">`(#$relation &lt;arg1&gt; … &lt;argn&gt;)`</span> is an **object**, aka a **term**. Most of Cyc's knowledge base consists of assertions. Each assertion has a truth value.</span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If <span class="in">`#$relation`</span> is a predicate, then <span class="in">`(#$relation &lt;arg1&gt; … &lt;argn&gt;)`</span> is an **assertion**, aka a **sentence**. Most of Cyc's knowledge base consists of assertions.</span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For a few special cases of <span class="in">`#$relation`</span> such as <span class="in">`#$implies`</span>, <span class="in">`#$forAll`</span>, and <span class="in">`#$thereExists`</span>,<span class="ot">[^skolemization]</span> <span class="in">`(#$relation &lt;arg1&gt; … &lt;argn&gt;)`</span> is a **rule** of inference.</span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each assertion has a truth value.</span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>There are 5 truth values: default true, monotonically true, default false, monotonically false, and undefined. The "default true" is used to allow for "nonmonotonic reasoning". For example, it allows us to say that "Pingu is a bird, so Pingu can fly", and then, if someone else adds "and Pingu is a penguin", we can say, "in that case, Pingu cannot fly". Since the "Pingu can fly" statement is only "default true" and "Pingu cannot fly" is only "default false", we would not suffer from the principle of explosion.</span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A <span class="in">`#$Term`</span>, aka an **atomic term**, aka a **concept**, can be thought of as a "word" in the vocabulary of CycL. It begins with an uppercase. Interpreted in object-oriented programming, some terms correspond to classes, while others correspond to objects. For example, <span class="in">`#$Socrates`</span> is an object, while <span class="in">`#$Human`</span> is a class. The difference is that we can say <span class="in">`(#$isa #$Socrates #$Human)`</span> but not <span class="in">`(#$isa #$Socrates #$Plato)`</span>. The concepts can be as abstract as <span class="in">`#$AnimalWalkingProcess`</span> (the concept of any possible walking by any animal) and as granular as <span class="in">`#$Walking00036`</span> (the walk I took on the afternoon of 1897-02-03 in Paris).</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a><span class="ot">[^skolemization]: </span>A technical detail is that as soon as you enter an expression that uses <span class="in">`#$thereExists`</span>, Cyc automatically "<span class="co">[</span><span class="ot">Skolemizes</span><span class="co">](https://en.wikipedia.org/wiki/Skolem_normal_form)</span>" that expression, because existential quantifiers are technically inconvenient. For example, if I were to say "There exists a time at which Socrates dies.", then it would be very awkward when someone asks "When?" and I have no recourse but to reply "... there exists!". Skolemization means that I would instead define a function <span class="in">`#$DateOfDeath`</span>. Then any time someone asks "When?", I need simply reply "<span class="in">`(#$DateOfDeath #$Socrates)`</span>!".</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a>This is enough for us to start writing some assertions that would imply "Socrates is mortal".</span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$Socrates #$MaleHuman)</span></span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$MaleHuman #$Predicate)</span></span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a><span class="in">(#$genls #$MaleHuman #$Human) ; #$genls means "generalizes"</span></span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a><span class="in">(#$genls #$MaleHuman #$MaleAnimal) ; Multiple generalization is common</span></span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="in">(#$genls #$Person #$Individual)</span></span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$Individual #$FirstOrderCollection) ; Things can go meta</span></span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$FirstOrderCollection #$SecondOrderCollection)</span></span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$FirstOrderCollection #$MetaClass)</span></span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$MetaClass #$MetaClass) ; Very meta</span></span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a><span class="in">(#$forAll ?X</span></span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$sameAs (#$MotherFn (#$MotherFn ?X)) </span></span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a><span class="in">    (#$MaternalGrandMotherFn ?X)))</span></span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="in">; Humans are mortal</span></span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$Mortal #$Predicate)</span></span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies (#$isa ?X #$Human) (#$Mortal ?X))</span></span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a><span class="in">; For X to be mortal means there exists a death event X is subject to</span></span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies</span></span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$Mortal ?X)</span></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$exists ?DE</span></span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a><span class="in">    (#$and</span></span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a><span class="in">      (#$isa ?DE #$DeathEvent)</span></span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a><span class="in">      (#$subject ?DE ?X))))</span></span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a>Assertions can be packaged into a **microtheory** (**Mt**, aka a **context**), which could be thought of as <span class="co">[</span><span class="ot">scopes</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Scope_(computer_science)) or <span class="co">[</span><span class="ot">modules</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Module_(programming)). Each assertion can belong to exactly one microtheory, though you can have two assertions that are literally typed out using the same symbols occur in two microtheories -- and they would count as two logical assertions.</span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a>Assertions can, and often need, to be automatically inferred across microtheories. For example, "Socrates is alive." is true in the context of 500 BC, but not in the context of 1995. Given an assertion to the effect of "Socrates was born in 470 BC and died in 399 BC", an automatic process would allow the system to automatically infer assertions "Socrates is alive" for every microtheory from "The time is 470 BC" to "The time is 399 BC", and "Socrates is not alive" for the other microtheories.</span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a>As one might expect from the above example, it is unnecessary, and indeed impractical, to create all microtheories that one might ever use. Indeed, many microtheories are created and destroyed for one-time-use "on the spot". For a single problem-and-answer session with a user, Cyc typically sets up a temporary context that is deleted after the session, or saved if the user wishes to continue the session later.</span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a>Microtheories are necessary because human beliefs are incompatible, and there are a lot of humans. For example (very relevant, considering <span class="co">[</span><span class="ot">how much Cyc was involved with the War on Terror</span><span class="co">](#sec-cycops-overview)</span>), <span class="in">`#$ChristianMt`</span> and <span class="in">`#$IslamMt`</span> could have:</span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$ChristianMt #$Microtheory)</span></span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$IslamMt #$Microtheory)</span></span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a><span class="in">; #$ist means "is true in the context of"</span></span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$ChristianMt)</span></span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$not (#$sameAs #$God #$Allah)))</span></span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$ChristianMt)</span></span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$sonOf #$Jesus #$God))</span></span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a><span class="in">; the Trinity is left as an exercise</span></span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$IslamMt)</span></span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$sameAs #$God #$Allah))</span></span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a><span class="in">; In Islam, God has no son</span></span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$IslamMt)</span></span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$not (#$sonOf ?X #$God)))</span></span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$IslamMt)</span></span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$prophetOf #$Jesus #$God))</span></span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a>Microtheories can contain each other. For example, both are <span class="in">`#$AbrahamicMt`</span>, which allows us to make assertions that apply to both.</span>
<span id="cb14-705"><a href="#cb14-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-706"><a href="#cb14-706" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$AbrahamicMt #$Microtheory)</span></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a><span class="in">(#$genls #$ChristianMt #$AbrahamicMt)</span></span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a><span class="in">(#$genls #$IslamMt #$AbrahamicMt)</span></span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a><span class="in">; Abraham is a prophet</span></span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$AbrahamicMt)</span></span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a><span class="in">    (#$prophetOf #$Abraham #$God))</span></span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a><span class="in">; God exists uniquely</span></span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a><span class="in">(#$ist (#$AbrahamicMt)</span></span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$exists ?X</span></span>
<span id="cb14-718"><a href="#cb14-718" aria-hidden="true" tabindex="-1"></a><span class="in">    (#$and</span></span>
<span id="cb14-719"><a href="#cb14-719" aria-hidden="true" tabindex="-1"></a><span class="in">      (#$isa ?X #$God)</span></span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a><span class="in">      (#$not</span></span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a><span class="in">        (#$exists ?Y</span></span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a><span class="in">          (#$and</span></span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a><span class="in">            (#$isa ?Y #$God)</span></span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a><span class="in">            (#$not</span></span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a><span class="in">              (#$sameAs ?X ?Y))))))))</span></span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a>As of 2010, there were over 20K microtheories arranged in a hierarchy, with <span class="in">`BaseKB`</span> at the top. It was unfortunately a mess, as Lenat promised. Some microtheories were 50 levels deep down the hierarchy!</span>
<span id="cb14-729"><a href="#cb14-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-730"><a href="#cb14-730" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... determining the hierarchical structure of the Mt’s is difficult, manual, and error prone because all of the Mt’s in ResearchCyc are direct subtypes of </span><span class="in">`BaseKB`</span><span class="at">, even if they are also its indirect subtypes. For example, Mt </span><span class="in">`ClothingGMt`</span><span class="at"> describes general information about clothing and is a direct subtype of </span><span class="in">`BaseKB`</span><span class="at">. It is also an indirect subtype of </span><span class="in">`BaseKB`</span><span class="at"> because it is a subtype of </span><span class="in">`ArtifactGMt`</span><span class="at">, also a subtype of </span><span class="in">`ArtifactGVocabularyMt`</span><span class="at">, which, in turn, is a subtype of </span><span class="in">`BaseKB`</span><span class="at">... Choosing the right microtheory can easily take several minutes for a trained ontologist who is familiar with the Cyc ontology and experienced in how best to organize knowledge for maximum utility.</span></span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@conesaUsabilityUpperLevel2010</span><span class="co">]</span></span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ontology</span></span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a>The Ontology of Cyc is the graph of all concepts in Cyc, with one directed edge per <span class="in">`(#$genls #$Thing1 #$Thing2)`</span> assertion. At least, that's the simplest possible way to say it. In fact, the graph is more complicated, since CycL is a higher-order logic, which allows it to talk about the relation between relations between predicates, etc. For example, both <span class="in">`#$SetOrCollection`</span> and <span class="in">`#$MathematicalObject`</span> are sub-concepts of <span class="in">`#$MathematicalThing`</span>, but we also need to specify that any <span class="in">`#$MathematicalThing`</span> is either a <span class="in">`#$SetOrCollection`</span> xor a <span class="in">`#$MathematicalObject`</span>.</span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a>The ontology contains multiple levels. At the upper level are the most metaphysical concepts, starting with <span class="in">`#$Thing`</span>, going down to the middle level of <span class="in">`#$Language`</span> and <span class="in">`#$MilitaryOrganization`</span>. </span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The upper ontology of Cyc as of 2010. [@foxvogCyc2010, figure 1]</span><span class="co">](figure/Cyc_Upper_Ontology_2010.png)</span></span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-742"><a href="#cb14-742" aria-hidden="true" tabindex="-1"></a>Below the upper ontology are domain-specific knowledge, divided into large microtheories (minitheories?). There would be knowledge about mortgages, computer security, weapons systems, pathology, etc. Below those are the domain-specific facts and data, divided into microtheories. The entire thing is structured like a pyramid.</span>
<span id="cb14-743"><a href="#cb14-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a><span class="al">![The cyc ontology pyramid.](figure/Cyc_ontology_pyramid.png)</span></span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a>As an example, here is how <span class="in">`#$Philosopher`</span> is described in <span class="co">[</span><span class="ot">OpenCyc</span><span class="co">](https://github.com/yuxi-liu-wired/cyc-archive/blob/main/other_files/opencyc-ontology.txt)</span>:</span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A specialization of </span><span class="in">`#$Person`</span><span class="at">; in the context of </span><span class="in">`#$HumanActivitiesMt`</span><span class="at"> this collection is an instance of </span><span class="in">`#$PersonTypeByActivity`</span><span class="at">, in the context of </span><span class="in">`#$JobMt`</span><span class="at"> it is an instance of </span><span class="in">`#$PersonTypeByOccupation`</span><span class="at">. Each instance of </span><span class="in">`#$Philosopher`</span><span class="at"> is a person who habitually thinks about philosophical matters such as what is or might be, what we can know, how we can know anything, etc. In the contemporary era most philosophers are academics or professionals, but a significant number (now and historically) don't fit this profile.</span></span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a>And here's the <span class="in">`#$Thing`</span>: </span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="in">`#$Thing`</span><span class="at"> is the "universal collection": the collection which, by definition, contains everything there is. Every thing in the Cyc ontology -- every </span><span class="in">`#$Individual`</span><span class="at"> (of any kind), every </span><span class="in">`#$Set-Mathematical`</span><span class="at">, and every </span><span class="in">`#$Collection`</span><span class="at"> -- is an instance of (see </span><span class="in">`#$isa`</span><span class="at">) </span><span class="in">`#$Thing`</span><span class="at">. Similarly, every collection is a subcollection of (see </span><span class="in">`#$genls`</span><span class="at">) </span><span class="in">`#$Thing`</span><span class="at">. Trivially, </span><span class="in">`#$Thing`</span><span class="at"> is both an instance of and a subcollection of itself, and is not a subcollection of any other collection. (Note that the above reference to "every thing in the Cyc ontology" is *not* meant to be limited to things actually *reified* in the Cyc system, but includes (e.g.) every instance -- reified or not, known or not -- of every collection recognized by Cyc.)</span></span>
<span id="cb14-753"><a href="#cb14-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-754"><a href="#cb14-754" aria-hidden="true" tabindex="-1"></a>And if the parenthetical note sounds a bit theological,<span class="ot">[^meinong-jungle]</span> note that among those that Cycorp had hired included philosophers, botanists, chemists, and of course, theologians. Interestingly, they didn't ask botanists to encode what they know about botany, but about what they know about non-botany. Lenat's theory was that botanists' understanding of botany is *not* commonsensical. Instead, what botany he wanted Cyc to encode is common sense botany: how non-botanists think about plants -- even, and especially, those botanical beliefs that a botanist would consider wrong, such as <span class="in">`(#$not (#$isa #$Banana #$BotanicalBerry))`</span>.</span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a><span class="ot">[^meinong-jungle]: </span>I did not find any mention of Meinong in Lenat's corpus or Cycorp's communications, but this description sounds close to <span class="co">[</span><span class="ot">Meinong's Jungle</span><span class="co">](https://en.wikipedia.org/wiki/Meinong's_jungle)</span>, where every thing exists, even logically impossible things such as "a square circle". The metaphysical difficulties are many, but presumably Cyc has tamed the jungle in practice.</span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a>Unfortunately for ontology, while everyone has a common sense, few can pull it out of their heads and push it into a computer. Let's consider how Cyc encodes events via "<span class="co">[</span><span class="ot">Davidsonian semantics</span><span class="co">](https://web.archive.org/web/20040310001317/http://www.cyc.com/doc/handbook/oe/08-cycl-representation-choices.html)</span>", since it's how <span class="co">[</span><span class="ot">Donald Davidson</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Donald_Davidson_(philosopher)) represented <span class="co">[</span><span class="ot">events</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Event_(philosophy)) -- yes, Cyc hired a lot of philosophy PhDs. While everyone can reason about events commonsensically, implicitly, making it explicit required a Davidson.<span class="ot">[^aristotle-bronze]</span></span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a><span class="ot">[^aristotle-bronze]</span>:</span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a>    This reminds me of an event very early on in the history of Cyc:</span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-763"><a href="#cb14-763" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Cyc apparently believed that if a bronze statue were melted into slag, it would remain a statue. What had gone wrong? ... Cyc had been told that bronze was a material that retained its essential property -- its "bronzeness," as it were -- no matter what state it was in, solid or liquid. But now Cyc was trying to apply that fact to the *statue* aspect of "bronze statue". Cyc hadn't been told anything about statues that would invalidate its conclusion; nobody had ever thought it necessary to tell Cyc, for example, that statues are only statues if they're more or less in their original form.</span></span>
<span id="cb14-764"><a href="#cb14-764" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt;</span></span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; [@thompsonKnowItAllMachine2001]</span></span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a><span class="in">    To a philosopher, this would have been funny, because Cyc had just been ambushed by Aristotle's bronze statue! See especially [*Physics* 195a6-8](https://www.loebclassics.com/view/aristotle-physics/1934/pb_LCL228.137.xml) and [*Metaphysics* 1045a26-29](https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.01.0052:book%3D8:section%3D1045a). Aristotle solved it by [hylomorphism](https://en.wikipedia.org/wiki/Hylomorphism). I wonder how Cyc solved it?</span></span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a>To represent "John gave Mary a book.", you can write something like <span class="in">`(#$Give #$John #$Mary #$Book1)`</span>, but this prevents you from adding more details, because for the assertion to be syntactically correct, you must define <span class="in">`#$Give`</span> to be a predicate with arity exactly 3. And then you'd be stuck if you want to write "John gave Mary a book last August.", since there is no arity in <span class="in">`#$Give`</span> to insert the "last August" into. And even if you redefine <span class="in">`#$Give`</span> to have arity 4, what if you *also* want to say it happened in the library, or that it was a gift, or that John was happy about it? You'd have to change the definition of the predicate <span class="in">`#$Give`</span> again and again, and there is no end to this.</span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a>To solve this problem, Cyc treats the event itself as an <span class="in">`#$Event`</span>. That is, it reifies the process as an object. Now, instead of cramming everything into one assertion, you can construct an endless sequence of assertions, limited only by your patience:</span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-774"><a href="#cb14-774" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa #$Event123 #$Event)</span></span>
<span id="cb14-775"><a href="#cb14-775" aria-hidden="true" tabindex="-1"></a><span class="in">(#$Sender #$Event123 #$John)</span></span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a><span class="in">(#$Receiver #$Event123 #$Mary)</span></span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a><span class="in">(#$GivenObject #$Event123 #$Book134)</span></span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a>Such problems are fairly subtle, and the world is a very big place. Fortunately, Lenat was a master of ontology, so it all worked out in the end.</span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... during a short stint working with Doug Lenat's Cyc project. At the time, they were trying to encode all of botany and had a small staff of professional botanists doing knowledge entry. Naturally it was quite difficult for the botanists to try to translate their knowledge into the formalisms required by Cyc, and they would regularly puzzle over various questions... and if they could not come to a consensus, would have to take it before the Master, Doug Lenat, who would think for a bit, maybe draw some diagrams on a whiteboard, and come up with the Right Representation.</span></span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- </span><span class="co">[</span><span class="ot">AMMDI: alpha ontologist</span><span class="co">](https://hyperphor.com/ammdi/alpha-ontologist)</span><span class="at"> (2023-10-07)</span></span>
<span id="cb14-786"><a href="#cb14-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-787"><a href="#cb14-787" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference engines</span></span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a>To a mathematician, 1 and 1 trillion are the same -- both are finite. To computer scientists, even $x^2$ and $x^3$ are different. Much work on Cyc was not on building the 100M-assertion knowledge base, but on building inference engines that allow fast inferences when there are 100M assertions to pick from.</span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a>Every logical system must face an impossible trilemma: </span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>an **expressive** language that can represent what people would ever want to say in practice;</span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>an **efficient** inference engine on the language that runs fast enough for practical inferences;</span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>a **complete** inference engine that can perform all inferences that are logically valid.</span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a>Why a trilemma?</span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-799"><a href="#cb14-799" aria-hidden="true" tabindex="-1"></a>Suppose we want to allow Cyc express all common sense assertions, then since humans in their daily life say high-order statements like "Are you implying that you *meant* to make me upset by spreading rumors about her, when you had known all along that I would soon hear about it?", Cyc needs to use a higher-order language. Now this immediately makes it impossible to have an inference engine that is both complete and computable, since we have a Gödel-style incompleteness theorem <span class="co">[</span><span class="ot">@shapiroFoundationsFoundationalismCase1991, theorem 4.14</span><span class="co">]</span>.</span>
<span id="cb14-800"><a href="#cb14-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-801"><a href="#cb14-801" aria-hidden="true" tabindex="-1"></a>Lenat chose to use a fully higher-order language, giving up completeness, and then try his best improving efficiency.</span>
<span id="cb14-802"><a href="#cb14-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-803"><a href="#cb14-803" aria-hidden="true" tabindex="-1"></a>Like most expert systems, the Cyc has a general <span class="co">[</span><span class="ot">resolution</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Resolution_(logic))-based inference engine. Unlike most expert systems, its knowledge base is large and higher-ordered, so its general engine runs too slowly for most queries. Thus, the developers kept adding more specialized modules ("pattern-specific heuristic modules"), each capable of efficiently inferring on a few microtheories. If a specialized engine fails to make progress, a more general engine can be the fall-back, all the way up to the most general one.</span>
<span id="cb14-804"><a href="#cb14-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-805"><a href="#cb14-805" aria-hidden="true" tabindex="-1"></a>As the simplest example of how inference engine can work, consider the following example of backward-chaining inference, in a semantic web. The system is asked basically "Why does Clyde want to possess a crescent wrench?", and it eventually replies "Because Clyde has not eaten lately.". Lenat expected that the ontology of Cyc would eventually power generally intelligent agents, which would use forward-chaining to construct goals from current states, and use backward-chaining to explain why others have their goals.</span>
<span id="cb14-806"><a href="#cb14-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-807"><a href="#cb14-807" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A sketch of how to reason by backward-chaining together triples in a semantic web. Figure from [@wallichSiliconBabies1991]. I can't help but wonder if Lenat meant for a subtle joke against the *Elephants don't play chess* by [@brooksElephantsDonPlay1990] by adding in a "dead end" branch that ends up concluding Clyde is an elephant.</span><span class="co">](figure/Cyc_triple_reasoning.png)</span></span>
<span id="cb14-808"><a href="#cb14-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-809"><a href="#cb14-809" aria-hidden="true" tabindex="-1"></a>Most inference engines are highly specialized. They can only reason within a few microtheories, but very well. Here is a toy example for reasoning with mutually exclusive categories. We can define <span class="in">`#$Mutex`</span> as </span>
<span id="cb14-810"><a href="#cb14-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-811"><a href="#cb14-811" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-812"><a href="#cb14-812" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies</span></span>
<span id="cb14-813"><a href="#cb14-813" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$and (#$isa ?X ?A) (#$Mutex ?A ?B))</span></span>
<span id="cb14-814"><a href="#cb14-814" aria-hidden="true" tabindex="-1"></a><span class="in">         (#$not (#$isa ?X ?B)))</span></span>
<span id="cb14-815"><a href="#cb14-815" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-816"><a href="#cb14-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-817"><a href="#cb14-817" aria-hidden="true" tabindex="-1"></a>Then this allows the general reasoning engine to reason about mutual exclusivity by always reducing to this definition. However, we can accelerate this by adding in some "lemmas":</span>
<span id="cb14-818"><a href="#cb14-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-819"><a href="#cb14-819" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-820"><a href="#cb14-820" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies (#$Mutex ?A ?B) (#$Mutex ?B ?A))</span></span>
<span id="cb14-821"><a href="#cb14-821" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies</span></span>
<span id="cb14-822"><a href="#cb14-822" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$and (#$Mutex ?A ?B) (#$genls ?C ?A))</span></span>
<span id="cb14-823"><a href="#cb14-823" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$Mutex ?C ?B))</span></span>
<span id="cb14-824"><a href="#cb14-824" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies</span></span>
<span id="cb14-825"><a href="#cb14-825" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$and (#$Mutex ?A ?B) (#$isa ?X ?A))</span></span>
<span id="cb14-826"><a href="#cb14-826" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$not (#$isa ?X ?B)))</span></span>
<span id="cb14-827"><a href="#cb14-827" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-828"><a href="#cb14-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-829"><a href="#cb14-829" aria-hidden="true" tabindex="-1"></a>After producing enough lemmas, we can then write a specialized inference engine that would be invoked whenever <span class="in">`#$Mutex`</span> appears in an expression, and it would attempt some inference and simplifications by applying these lemmas. Indeed, most inference engines were constructed in this way: Cyc would try to solve a problem, and fails by timing out. The ontologists at Cyc would call up a human expert and ask, "How did you do this?" and the expert would explain how they would solve it with quick rules of thumb, which the ontologists would write into Cyc, resulting in more assertions, and possibly more inference engines. This is essentially the same as "knowledge elicitation" used for making expert systems.</span>
<span id="cb14-830"><a href="#cb14-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-831"><a href="#cb14-831" aria-hidden="true" tabindex="-1"></a>Other than these inference engines (or "workers"), there are also many "tacticians", modules that pick engines that are probably good for solving a problem, and a general all-powerful strategist at the very top. It was reported in that there were 1 strategist, 4 tacticians, and 1097 workers. The strategist and tacticians have parameters that can vary, presumably by the user or the ontologists in order to adapt to specific tasks. <span class="co">[</span><span class="ot">@lenatEfficientPathfindingVery2007</span><span class="co">]</span></span>
<span id="cb14-832"><a href="#cb14-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-833"><a href="#cb14-833" aria-hidden="true" tabindex="-1"></a>The control structure of Cyc is a commercial secret.<span class="ot">[^sublisp-control-structure]</span> I could only find two brief explanations of it, one in a <span class="co">[</span><span class="ot">2010 Cycorp white paper</span><span class="co">](https://web.archive.org/web/20130708080327/http://www.cyc.com/sites/default/files/storage/white-papers/The%20Cyc%20Blackboard%20System%20v1.0.pdf)</span>, another in an essay published near the end of his life <span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span>. According to these, Cyc coordinates the ~1000 inference engines with blackboard architecture similar to the one used in Hearsay-II speech recognition system <span class="co">[</span><span class="ot">@ermanHearsayIISpeechUnderstandingSystem1980</span><span class="co">]</span>. Lenat described it as:</span>
<span id="cb14-834"><a href="#cb14-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-835"><a href="#cb14-835" aria-hidden="true" tabindex="-1"></a><span class="ot">[^sublisp-control-structure]: </span><span class="co">[</span><span class="ot">The SubLisp reference</span><span class="co">](https://web.archive.org/web/20130707133552/http://www.cyc.com/documentation/control-structure)</span> was titled "control structure", and got me excited for a moment. But I was disappointed to find that it described the control structure of the SubLisp programming language (mostly the same as that of Common Lisp), *not* of the Cyc system.</span>
<span id="cb14-836"><a href="#cb14-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-837"><a href="#cb14-837" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; there is a whole battery of specialized inference engines and representations... and, when making progress, broadcasting the results... Whenever progress is made, all of them stop and work on the now-simpler subproblem. Some of the inference engines are very general, and work on general representations--e.g., a theorem prover that works on first-order logic. The more specialized inference engines are much faster whenever they do apply... In 1986, Cyc had two such representations; by 1990, there were 20, each with its own inference engine; today Cyc has over 1100. They work together as a community of agents, communicating by posting their intermediate results on a sort of blackboard that all the other agents can watch and react to if/when/as they see an opportunity that fits their specialty.</span></span>
<span id="cb14-838"><a href="#cb14-838" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-839"><a href="#cb14-839" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-840"><a href="#cb14-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-841"><a href="#cb14-841" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The blackboard architecture used in Hearsay-II. In this example, a 3-dimensional blackboard allows different modules to read and write to different parts of the blackboard, and collaboratively recognize speech. [@lenatComputerSoftwareIntelligent1984]</span><span class="co">](figure/Blackboard_architecture.png)</span></span>
<span id="cb14-842"><a href="#cb14-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-843"><a href="#cb14-843" aria-hidden="true" tabindex="-1"></a>You can imagine a shared working space -- the blackboard -- where agents can scan and find tasks to do, do them, and then post the outputs to the blackboard. Each agent has their own "places to watch". They notice (or get notified) what is posted to their watched areas, and ignore what is outside.<span class="ot">[^cyc-blackboard]</span> When an agent completes whatever task it is running, it posts the outputs to particular areas of the board to notify certain agents, "Hey, check this out! I bet you'll find this helpful.". To prevent race conditions or general conflict between agents, an agent can claim a lock on something, so that other agents can't work on it until it times out, or releases the lock.</span>
<span id="cb14-844"><a href="#cb14-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-845"><a href="#cb14-845" aria-hidden="true" tabindex="-1"></a><span class="ot">[^cyc-blackboard]: </span>Lenat seemed to suggest that each inference engine watches *the entire board*, but every large-scale blackboard system I know of doesn't do that. Hearsay-II certainly did not. It is slow and creates much of unintended complexities. Imagine a microservice where every process pings every process whenever it outputs something, or a conference where everyone broadcasts to everyone.</span>
<span id="cb14-846"><a href="#cb14-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-847"><a href="#cb14-847" aria-hidden="true" tabindex="-1"></a>You can imagine this as a <span class="co">[</span><span class="ot">microservices framework</span><span class="co">](https://en.wikipedia.org/wiki/Microservices)</span>, with over 1100 microservices, <span class="co">[</span><span class="ot">multicast</span><span class="co">](https://en.wikipedia.org/wiki/Multicast)</span>, with SubLisp as the <span class="co">[</span><span class="ot">serialization language</span><span class="co">](https://en.wikipedia.org/wiki/Serialization)</span>. Indeed, <span class="co">[</span><span class="ot">apparently</span><span class="co">](https://www.planettechnews.com/planettech-interviews-michael-stewart-founder-chairman-and-ceo-of-lucid-ai/)</span> some of the inference engines used neural networks, so their numerical outputs *definitely* had to be serialized somehow into SubLisp.</span>
<span id="cb14-848"><a href="#cb14-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-849"><a href="#cb14-849" aria-hidden="true" tabindex="-1"></a>The blackboard is also connected to the "external world" through API calls. For example, SQL queries run by an external database can return data that is posted to a region of the blackboard, and what Cyc writes to the blackboard can be read out for external use.</span>
<span id="cb14-850"><a href="#cb14-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-851"><a href="#cb14-851" aria-hidden="true" tabindex="-1"></a>According to a 2006 lecture <span class="co">[</span><span class="ot">@lenatGoogleTechTalksComputers2006</span><span class="co">]</span>, the blackboard has 12 "dimensions", because each assertion occurs in a kind of context, and there are 12 dimensions to the context: <span class="in">`Anthropacity`</span>, <span class="in">`Time`</span>, <span class="in">`GeoLocation`</span>, <span class="in">`TypeOfPlace`</span>, <span class="in">`TypeOfTime`</span>, <span class="in">`Culture`</span>, <span class="in">`Sophistication/Security`</span>, <span class="in">`Topic`</span>, <span class="in">`TopicGranularity`</span>, <span class="in">`Modality/Disposition/Epistemology`</span>, <span class="in">`Argument-Preference`</span>, and <span class="in">`Justification`</span>. For example, "Ronald Reagan is president" is true in the context <span class="in">`Time = 1985, GeoLocation = UnitedStates`</span>, etc. These 12 dimensions were detailed in a 1998 technical report, though in that report, <span class="in">`Anthropacity`</span> was called a "pseudo-dimension", and replaced by <span class="in">`Let's`</span>. <span class="co">[</span><span class="ot">@lenatDimensionsContextspace1998</span><span class="co">]</span></span>
<span id="cb14-852"><a href="#cb14-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-853"><a href="#cb14-853" aria-hidden="true" tabindex="-1"></a>Other than the slightly anarchic microservice structure, Cyc could also use <span class="co">[</span><span class="ot">Simple Hierarchical Ordered Planner</span><span class="co">](https://www.cs.umd.edu/projects/shop/)</span> to run the engines in order, if the workflow for how the engines should be run is known.</span>
<span id="cb14-854"><a href="#cb14-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-855"><a href="#cb14-855" aria-hidden="true" tabindex="-1"></a>It was known very early on that the most general inference engine is the slowest, which is why they settled on a multi-layered structure of inference engines. Each inference is handled by the lowest engines first, and upon failing, the higher-levels try it. </span>
<span id="cb14-856"><a href="#cb14-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-857"><a href="#cb14-857" aria-hidden="true" tabindex="-1"></a>Further, to allow the tacticians to know who to call, and the engines to know whether it is powerful enough to handle the query, the user can specify in a query over 150 adjustable parameters, such as max time limit, max number of answers desired, max number of backward-chaining steps, whether to introduce new terms, etc. In 2007, they found 6 combinations of these parameters that could answer almost all queries they tested with, which allowed them to clean up the user interface. Not a form with 150 blanks, but just a pick-1-in-6. <span class="co">[</span><span class="ot">@lenatACS2022Invited2022</span><span class="co">]</span></span>
<span id="cb14-858"><a href="#cb14-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-859"><a href="#cb14-859" aria-hidden="true" tabindex="-1"></a>Though there is a most general inference engine on the very top, they had noticed by 2007 that was so slow that turning it off made the system go *faster*! So they turned it off entirely in 2010. <span class="co">[</span><span class="ot">@lenatACS2022Invited2022; @lenatGettingGenerativeAI2023</span><span class="co">]</span></span>
<span id="cb14-860"><a href="#cb14-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-861"><a href="#cb14-861" aria-hidden="true" tabindex="-1"></a>The heaven is high and the emperor is far away. The monologic is dead. Long live the swarm.</span>
<span id="cb14-862"><a href="#cb14-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-863"><a href="#cb14-863" aria-hidden="true" tabindex="-1"></a><span class="fu">### Querying</span></span>
<span id="cb14-864"><a href="#cb14-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-865"><a href="#cb14-865" aria-hidden="true" tabindex="-1"></a>As a user, the knowledge base would be worth nothing if you cannot ask it questions and receive answers. The Cyc system, like most logical programming systems, answers queries by <span class="co">[</span><span class="ot">logical unification</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Unification_(computer_science)). That is, it converts a query into a logical formula $\phi(x)$ with a certain unknown $x$, then returns all $x$ that makes $\phi(x)$ true, or if no such $x$ exists, then returns <span class="in">`FALSE`</span>.</span>
<span id="cb14-866"><a href="#cb14-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-867"><a href="#cb14-867" aria-hidden="true" tabindex="-1"></a>For a query like </span>
<span id="cb14-868"><a href="#cb14-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-869"><a href="#cb14-869" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-870"><a href="#cb14-870" aria-hidden="true" tabindex="-1"></a><span class="in">(#$geopoliticalSubdivision #$Canada ?WHAT)</span></span>
<span id="cb14-871"><a href="#cb14-871" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-872"><a href="#cb14-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-873"><a href="#cb14-873" aria-hidden="true" tabindex="-1"></a>the Cyc system applies various inference engines to find all bindings (aka substitutions) of the variable <span class="in">`?WHAT`</span> that make the statement true in the knowledge base. The results will include all Canadian provinces and territories that have been asserted explicitly or can be inferred through rules.</span>
<span id="cb14-874"><a href="#cb14-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-875"><a href="#cb14-875" aria-hidden="true" tabindex="-1"></a>Cyc provides 3 types of queries:</span>
<span id="cb14-876"><a href="#cb14-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-877"><a href="#cb14-877" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`ASK`</span>: General-purpose queries that generate bindings for free variables along with a formal proof.</span>
<span id="cb14-878"><a href="#cb14-878" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`PROVE`</span>: Conditional queries that handle universal quantification (<span class="in">`#$forAll`</span>) by constructing temporary microtheories for temporary hypotheticals.</span>
<span id="cb14-879"><a href="#cb14-879" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`QUERY`</span>: A wrapper around both <span class="in">`ASK`</span> and <span class="in">`PROVE`</span> in the Cyc Browser.</span>
<span id="cb14-880"><a href="#cb14-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-881"><a href="#cb14-881" aria-hidden="true" tabindex="-1"></a>To illustrate how querying works in practice, consider a scenario where we want to find all Canadian provinces. An <span class="in">`ASK`</span> query would look like:</span>
<span id="cb14-882"><a href="#cb14-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-883"><a href="#cb14-883" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-884"><a href="#cb14-884" aria-hidden="true" tabindex="-1"></a><span class="in">(#$isa ?WHAT #$CanadianProvince)</span></span>
<span id="cb14-885"><a href="#cb14-885" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-886"><a href="#cb14-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-887"><a href="#cb14-887" aria-hidden="true" tabindex="-1"></a>Cyc processes this by looking for all terms (symbols that begin with <span class="in">`#$`</span>) that satisfy the predicate when plugged into the variable <span class="in">`?WHAT`</span>. The system might use transitivity rules, inheritance mechanisms, and various specialized inference engines to gather results. Behind the scenes, Cyc might employ backward-chaining to start from the query and work backward to known facts, or forward-chaining to derive new facts from existing ones, depending on which approach its tacticians determine is more efficient. The reply would be something like</span>
<span id="cb14-888"><a href="#cb14-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-889"><a href="#cb14-889" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-890"><a href="#cb14-890" aria-hidden="true" tabindex="-1"></a><span class="in">((?WHAT . #$Manitoba-CanadianProvince))</span></span>
<span id="cb14-891"><a href="#cb14-891" aria-hidden="true" tabindex="-1"></a><span class="in">((?WHAT . #$BritishColumbia-CanadianProvince))</span></span>
<span id="cb14-892"><a href="#cb14-892" aria-hidden="true" tabindex="-1"></a><span class="in">((?WHAT . #$Alberta-CanadianProvince))</span></span>
<span id="cb14-893"><a href="#cb14-893" aria-hidden="true" tabindex="-1"></a><span class="in">((?WHAT . #$Ontario-CanadianProvince))</span></span>
<span id="cb14-894"><a href="#cb14-894" aria-hidden="true" tabindex="-1"></a><span class="in">((?WHAT . #$Quebec-CanadianProvince))</span></span>
<span id="cb14-895"><a href="#cb14-895" aria-hidden="true" tabindex="-1"></a><span class="in">((?WHAT . #$NovaScotia-CanadianProvince))</span></span>
<span id="cb14-896"><a href="#cb14-896" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb14-897"><a href="#cb14-897" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-898"><a href="#cb14-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-899"><a href="#cb14-899" aria-hidden="true" tabindex="-1"></a>When Cyc performs a query, it can also provide a deductive trace (forma proof) showing how it reached its conclusion. For example, when proving that Manitoba is a Canadian province, the deductive trace might look like:</span>
<span id="cb14-900"><a href="#cb14-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-901"><a href="#cb14-901" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-902"><a href="#cb14-902" aria-hidden="true" tabindex="-1"></a><span class="in">lispCopyQuery: (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)</span></span>
<span id="cb14-903"><a href="#cb14-903" aria-hidden="true" tabindex="-1"></a><span class="in">  1. (#$isa #$Manitoba-CanadianProvince #$CanadianProvince)</span></span>
<span id="cb14-904"><a href="#cb14-904" aria-hidden="true" tabindex="-1"></a><span class="in">     [Direct assertion in PoliticalGeographyMt]</span></span>
<span id="cb14-905"><a href="#cb14-905" aria-hidden="true" tabindex="-1"></a><span class="in">  2. Therefore, Manitoba-CanadianProvince is a CanadianProvince.</span></span>
<span id="cb14-906"><a href="#cb14-906" aria-hidden="true" tabindex="-1"></a><span class="in">     [TRUE]</span></span>
<span id="cb14-907"><a href="#cb14-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-908"><a href="#cb14-908" aria-hidden="true" tabindex="-1"></a><span class="in">Query: (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)</span></span>
<span id="cb14-909"><a href="#cb14-909" aria-hidden="true" tabindex="-1"></a><span class="in">  1. (isa Manitoba-CanadianProvince CanadianProvince)</span></span>
<span id="cb14-910"><a href="#cb14-910" aria-hidden="true" tabindex="-1"></a><span class="in">     [Established above]</span></span>
<span id="cb14-911"><a href="#cb14-911" aria-hidden="true" tabindex="-1"></a><span class="in">  2. (implies </span></span>
<span id="cb14-912"><a href="#cb14-912" aria-hidden="true" tabindex="-1"></a><span class="in">      (#$isa ?X #$CanadianProvince)</span></span>
<span id="cb14-913"><a href="#cb14-913" aria-hidden="true" tabindex="-1"></a><span class="in">      (geopoliticalSubdivision Canada ?X))</span></span>
<span id="cb14-914"><a href="#cb14-914" aria-hidden="true" tabindex="-1"></a><span class="in">     [Rule in PoliticalGMt]</span></span>
<span id="cb14-915"><a href="#cb14-915" aria-hidden="true" tabindex="-1"></a><span class="in">  3. Therefore, (#$geopoliticalSubdivision #$Canada #$Manitoba-CanadianProvince)</span></span>
<span id="cb14-916"><a href="#cb14-916" aria-hidden="true" tabindex="-1"></a><span class="in">     [Modus Ponens: 1,2]</span></span>
<span id="cb14-917"><a href="#cb14-917" aria-hidden="true" tabindex="-1"></a><span class="in">     [TRUE]</span></span>
<span id="cb14-918"><a href="#cb14-918" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-919"><a href="#cb14-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-920"><a href="#cb14-920" aria-hidden="true" tabindex="-1"></a>For more complex queries involving logical implications, <span class="in">`PROVE`</span> creates hypothetical microtheories to test universal claims. Such microtheories are useful, since the hypotheticals being considered are usually temporary and should not be mixed with the permanent assertions in the knowledge base. For example, to ask "Is every Canadian province a political subdivision of Canada?", we use:</span>
<span id="cb14-921"><a href="#cb14-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-922"><a href="#cb14-922" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-923"><a href="#cb14-923" aria-hidden="true" tabindex="-1"></a><span class="in">(#$implies</span></span>
<span id="cb14-924"><a href="#cb14-924" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$isa ?WHAT #$CanadianProvince)</span></span>
<span id="cb14-925"><a href="#cb14-925" aria-hidden="true" tabindex="-1"></a><span class="in">  (#$geopoliticalSubdivision #$Canada ?WHAT))</span></span>
<span id="cb14-926"><a href="#cb14-926" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-927"><a href="#cb14-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-928"><a href="#cb14-928" aria-hidden="true" tabindex="-1"></a><span class="in">`PROVE`</span> handles this by creating a temporary "possible world" where it assumes the existence of a generic Canadian province and tests whether it must also be a political subdivision of Canada.</span>
<span id="cb14-929"><a href="#cb14-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-930"><a href="#cb14-930" aria-hidden="true" tabindex="-1"></a>As previously stated, each time the user begins a question-and-answer session with the Cyc system, it creates a new microtheory just for this session, which allows it to keep track of what has been asked and responded to, so that the user can refer to things like "your previous answer". Such a microtheory is usually deleted after the session is over, though it can also be saved if the user wishes to continue the session at a later date.</span>
<span id="cb14-931"><a href="#cb14-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-932"><a href="#cb14-932" aria-hidden="true" tabindex="-1"></a>The querying interface also provides controls for managing the inference process, such as limiting search depth, maximal time allowed, the inference engines to be used, the parameters for the strategist and the tacticians, etc. These can help the Cyc system to solve difficult queries when the default settings are insufficient.</span>
<span id="cb14-933"><a href="#cb14-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-934"><a href="#cb14-934" aria-hidden="true" tabindex="-1"></a><span class="fu">### Natural language processing</span></span>
<span id="cb14-935"><a href="#cb14-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-936"><a href="#cb14-936" aria-hidden="true" tabindex="-1"></a>When it comes to the use of natural languages, there are two problems. The easy problem is to convert sentences in CycL into natural language. This is fairly simple and has been set up since around late 1990s, simply by constructing assertions between a concept and a concept of the English word for the concept. <span class="co">[</span><span class="ot">@guhaReCycLingPaper1993</span><span class="co">]</span> For example, <span class="in">`(#$genls #$Dog #$Mammal)`</span> would be transformed to "Dogs are mammals.", and so on. Concretely, it involves entering sentences of the form (simplified):</span>
<span id="cb14-937"><a href="#cb14-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-938"><a href="#cb14-938" aria-hidden="true" tabindex="-1"></a><span class="in">```lisp</span></span>
<span id="cb14-939"><a href="#cb14-939" aria-hidden="true" tabindex="-1"></a><span class="in">(#$rootString #$TheEnglishWordPen "pen")</span></span>
<span id="cb14-940"><a href="#cb14-940" aria-hidden="true" tabindex="-1"></a><span class="in">(#$denotation #$TheEnglishWordPen #$WritingPen)</span></span>
<span id="cb14-941"><a href="#cb14-941" aria-hidden="true" tabindex="-1"></a><span class="in">(#$rootString #$TheFrenchWordPlume "plume")</span></span>
<span id="cb14-942"><a href="#cb14-942" aria-hidden="true" tabindex="-1"></a><span class="in">(#$denotation #$TheFrenchWordPlume #$WritingPen)</span></span>
<span id="cb14-943"><a href="#cb14-943" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb14-944"><a href="#cb14-944" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-945"><a href="#cb14-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-946"><a href="#cb14-946" aria-hidden="true" tabindex="-1"></a>and similarly, Cyc could order the words correctly using logical assertions for English grammar in the form of <span class="co">[</span><span class="ot">government and binding theory</span><span class="co">](https://en.wikipedia.org/wiki/Government_and_binding_theory)</span>.</span>
<span id="cb14-947"><a href="#cb14-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-948"><a href="#cb14-948" aria-hidden="true" tabindex="-1"></a>Sure, such generated language will sound a bit wooden and formalistic, and would not win any literary award, but it solves the problem. The hard problem is to parse sentences in natural language into CycL, along with all its messiness.</span>
<span id="cb14-949"><a href="#cb14-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-950"><a href="#cb14-950" aria-hidden="true" tabindex="-1"></a>Information about how Cyc solves the hard problem is sparse, but from what I gathered, it uses a tiered system from the fast and inaccurate to the slow and accurate.</span>
<span id="cb14-951"><a href="#cb14-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-952"><a href="#cb14-952" aria-hidden="true" tabindex="-1"></a>Underlying all of its NLP is an English dictionary, also represented as concepts and assertions in CycL. The dictionary contains about 200K words and phrases, and more assertions about them. For example, the word "light" is represented as a concept <span class="in">`Light-theWord`</span>, and there would be assertions stating that it can be a verb, or a noun, or an adjective, etc.</span>
<span id="cb14-953"><a href="#cb14-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-954"><a href="#cb14-954" aria-hidden="true" tabindex="-1"></a>At the fastest tier are keyword matching, or concept spotting. For example, for parsing terrorism articles, it can just quickly match for the existence of keywords in a sentence. Seeing a sentence that looks like "... al-Qaeda ... embassy ... grenade ... suicide attack...", the system can assume, with high probability, what the sentence means, and generate its CycL representation.</span>
<span id="cb14-955"><a href="#cb14-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-956"><a href="#cb14-956" aria-hidden="true" tabindex="-1"></a>At a slower tier is using extraction templates. For example, "<span class="sc">\[</span>A<span class="sc">\]</span> was involved in kidnapping <span class="sc">\[</span>B<span class="sc">\]</span>" would be matched to a template that looks for the fragment "involved in kidnapping", which then would parse <span class="sc">\[</span>A<span class="sc">\]</span> as the <span class="in">`Perpetrator`</span> while <span class="sc">\[</span>B<span class="sc">\]</span> as the <span class="in">`Victim`</span>. </span>
<span id="cb14-957"><a href="#cb14-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-958"><a href="#cb14-958" aria-hidden="true" tabindex="-1"></a>At a slower tier is example-based machine translation by syntax templates. For example, in "... the heart of Baghdad ...", the first pass parses Badhdad as a city, then a syntax template activates and it parses the "heart" as <span class="in">`Downtown`</span>. This can be coded as</span>
<span id="cb14-959"><a href="#cb14-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-960"><a href="#cb14-960" aria-hidden="true" tabindex="-1"></a><span class="in">```xml</span></span>
<span id="cb14-961"><a href="#cb14-961" aria-hidden="true" tabindex="-1"></a>&lt;<span class="kw">template</span>&gt;</span>
<span id="cb14-962"><a href="#cb14-962" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">nlPattern</span><span class="ot"> class=</span><span class="st">"140080"</span>&gt;the heart of $City#0&lt;/<span class="kw">nlPattern</span>&gt;</span>
<span id="cb14-963"><a href="#cb14-963" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">cyclPattern</span>&gt;</span>
<span id="cb14-964"><a href="#cb14-964" aria-hidden="true" tabindex="-1"></a>        (#$equalSymbols ?D (#$DowntownFn $City#0))</span>
<span id="cb14-965"><a href="#cb14-965" aria-hidden="true" tabindex="-1"></a>    &lt;/<span class="kw">cyclPattern</span>&gt;</span>
<span id="cb14-966"><a href="#cb14-966" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">variable</span>&gt;?D&lt;/<span class="kw">variable</span>&gt;</span>
<span id="cb14-967"><a href="#cb14-967" aria-hidden="true" tabindex="-1"></a>    &lt;<span class="kw">type</span>&gt;#$Downtown&lt;/<span class="kw">type</span>&gt;</span>
<span id="cb14-968"><a href="#cb14-968" aria-hidden="true" tabindex="-1"></a>&lt;/<span class="kw">template</span>&gt;</span>
<span id="cb14-969"><a href="#cb14-969" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-970"><a href="#cb14-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-971"><a href="#cb14-971" aria-hidden="true" tabindex="-1"></a>At the slowest tier is <span class="co">[</span><span class="ot">full syntax tree parsing</span><span class="co">](https://web.archive.org/web/20130511232427/http://www.cyc.com/cyc/nl)</span>. In this tier, the sentence is fully parsed to a syntax tree using a manually written transformational grammar under the <span class="co">[</span><span class="ot">government and binding theory</span><span class="co">](https://en.wikipedia.org/wiki/Government_and_binding_theory)</span>. It is then parsed semantically using <span class="co">[</span><span class="ot">Montague grammar</span><span class="co">](https://en.wikipedia.org/wiki/Montague_grammar)</span>.</span>
<span id="cb14-972"><a href="#cb14-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-973"><a href="#cb14-973" aria-hidden="true" tabindex="-1"></a>Whereas the CycL-to-English part is already workable mostly in 2001, the English-to-CycL part is still ongoing work, a consummation devoutly to be wished. Even though Lenat originally thought natural language understanding would be finished soon after priming the knowledge pump, it has already been 8 years since the priming, and Cyc is still not reading the world's writings and learning autonomously.</span>
<span id="cb14-974"><a href="#cb14-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-975"><a href="#cb14-975" aria-hidden="true" tabindex="-1"></a><span class="fu">### Machine learning</span></span>
<span id="cb14-976"><a href="#cb14-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-977"><a href="#cb14-977" aria-hidden="true" tabindex="-1"></a>Cyc has used statistical and machine learning methods in minor parts, such as using neural networks, n-gram methods, random forest, support vector machines, etc, to automatically extract templates for natural language processing, classify the microtheories that a sentence belongs to, etc.</span>
<span id="cb14-978"><a href="#cb14-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-979"><a href="#cb14-979" aria-hidden="true" tabindex="-1"></a>However, none of these were at all what Lenat meant when he talked of "machine learning", by which he meant a Cyc machine, with knowledge pump fully primed, would begin to perform experiments and learn by automated discovery, much like AM and EURISKO were meant to do. While this has always been the "phase 3" of the Cyc project, to this day, Cyc has made no progress in this area.</span>
<span id="cb14-980"><a href="#cb14-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-981"><a href="#cb14-981" aria-hidden="true" tabindex="-1"></a><span class="fu">## CycOps, what is it good for?</span></span>
<span id="cb14-982"><a href="#cb14-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-983"><a href="#cb14-983" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overview {#sec-cycops-overview}</span></span>
<span id="cb14-984"><a href="#cb14-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-985"><a href="#cb14-985" aria-hidden="true" tabindex="-1"></a>In America, at least before the Deep Learning era, the government funded most large-scale efforts in AI, usually in the context of military and intelligence applications. This was true for the Cyc project. Indeed, it initiated in 1984-07 within the <span class="co">[</span><span class="ot">Microelectronics and Computer Consortium</span><span class="co">](https://en.wikipedia.org/wiki/Microelectronics_and_Computer_Technology_Corporation)</span> (MCC), which, like the <span class="co">[</span><span class="ot">Strategic Computing Initiative</span><span class="co">](https://en.wikipedia.org/wiki/Strategic_Computing_Initiative)</span>,<span class="co">&lt;!-- todo: link to SCI --&gt;</span> was formed in reaction to the threat of the Japanese <span class="co">[</span><span class="ot">Fifth Generation Computer Systems</span><span class="co">](https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems)</span> project<span class="co">&lt;!-- todo: link to SCI#FGCS instead --&gt;</span>. Though not *directly* funded by the government, its head was <span class="co">[</span><span class="ot">Bobby Inman</span><span class="co">](https://en.wikipedia.org/wiki/Bobby_Ray_Inman)</span>, who had previously held high positions in the Navy, the NSA, and the CIA, so draw your own conclusion.</span>
<span id="cb14-986"><a href="#cb14-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-987"><a href="#cb14-987" aria-hidden="true" tabindex="-1"></a>In 1995-01, as MCC wound down, the Cyc group formed Cycorp Inc., a for-profit company, to continue their mission. Immediately after that point, academic publication and generally candid conversation almost ceased. Who bought the services of Cyc, and for what? The details are slim. Trade secrets, no doubt. Confirmed results:<span class="ot">[^policy-wonk]</span></span>
<span id="cb14-988"><a href="#cb14-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-989"><a href="#cb14-989" aria-hidden="true" tabindex="-1"></a><span class="ot">[^policy-wonk]: </span>I swear I'm not a policy wonk, but 3 days of cyc-berstalking does take its toll.</span>
<span id="cb14-990"><a href="#cb14-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-991"><a href="#cb14-991" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Lycos search engine</span><span class="co">](https://en.wikipedia.org/wiki/Lycos)</span>, to disambiguate search terms. It ended in 2001. (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://web.archive.org/web/20150905165226/http://www.cyc.com/about/media-coverage/computer-save-world/)</span>)</span>
<span id="cb14-992"><a href="#cb14-992" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Department of Defense, in 2001, to "clean dataset". <span class="co">[</span><span class="ot">@thompsonKnowItAllMachine2001</span><span class="co">]</span></span>
<span id="cb14-993"><a href="#cb14-993" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Glaxo Wellcome, <span class="co">[</span><span class="ot">DEC</span><span class="co">](https://en.wikipedia.org/wiki/Digital_Equipment_Corporation)</span>, IBM, and <span class="co">[</span><span class="ot">United Healthcare</span><span class="co">](https://en.wikipedia.org/wiki/UnitedHealth_Group)</span>, sometime before 1997, for unknown purposes. <span class="co">[</span><span class="ot">@portDuelingBrainscapesArtificial1997</span><span class="co">]</span></span>
<span id="cb14-994"><a href="#cb14-994" aria-hidden="true" tabindex="-1"></a><span class="ss">  *  </span>For Glaxo Wellcome, and United Healthcare, it was probably for the purpose described below.</span>
<span id="cb14-995"><a href="#cb14-995" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>For DEC and IBM, probably to produce specialized expert systems -- after all, DEC was famous for using the expert system <span class="co">[</span><span class="ot">XCON</span><span class="co">](https://en.wikipedia.org/wiki/Xcon)</span> that saved them $40M/year. <span class="co">[</span><span class="ot">@rolandStrategicComputingDARPA2002, page 191</span><span class="co">]</span></span>
<span id="cb14-996"><a href="#cb14-996" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>GlaxoSmithKline, before 2001, to "clean dataset" <span class="co">[</span><span class="ot">@thompsonKnowItAllMachine2001</span><span class="co">]</span>, probably meaning to manage a thesaurus of pharmaceutical chemicals and health-care words. (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://web.archive.org/web/20130720051038/http://www.cyc.com/enterprise-solutions/success-stories/pharmaceutical-thesaurus-management)</span>) Probably started in 1997 <span class="co">[</span><span class="ot">@portDuelingBrainscapesArtificial1997</span><span class="co">]</span></span>
<span id="cb14-997"><a href="#cb14-997" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">CycSecure</span><span class="co">](https://web.archive.org/web/20101224222259/http://cyc.com/cyc/applications/CycSecure/)</span>, a network vulnerability assessment tool, first beta in 2002. <span class="co">[</span><span class="ot">@anthesCycUse2002</span><span class="co">]</span> Trialed at the US Strategic Command Computer Emergency Response Team at some unknown point before 2005. <span class="co">[</span><span class="ot">@shepardKnowledgebasedApproachNetwork2005</span><span class="co">]</span></span>
<span id="cb14-998"><a href="#cb14-998" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Paul Allen</span><span class="co">](https://en.wikipedia.org/wiki/Paul_Allen)</span> (co-founder of Microsoft) had funded Cycorp sometime before 2001 for unknown purposes and for an unknown sum. <span class="co">[</span><span class="ot">@hiltzikBirthThinkingMachine2001</span><span class="co">]</span> In 2003, he funded it by \$0.7 million as part of his project of "Digital Aristotle", to create a tutoring AI. <span class="co">[</span><span class="ot">@richmanAllenClaimsSuccess2003; @friedlandProjectHaloDigital2004</span><span class="co">]</span></span>
<span id="cb14-999"><a href="#cb14-999" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>DARPA's <span class="co">[</span><span class="ot">High Performance Knowledge Base</span><span class="co">](https://en.wikipedia.org/wiki/High_Performance_Knowledge_Bases)</span> program (unknown duration, but must be within 1997--1999, the duration of the full project) and KRAKEN<span class="ot">[^kraken-acronym]</span> under the Rapid Knowledge Formation program (2000--2004). <span class="co">[</span><span class="ot">@kingstonHighPerformanceKnowledge2001; @matthewsKRAKENKnowledgeRich2004; @witbrockKnowledgeBegetsKnowledge2005</span><span class="co">]</span></span>
<span id="cb14-1000"><a href="#cb14-1000" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) program, sometime between 2001 and 2004. Its title is self-explanatory. <span class="co">[</span><span class="ot">@lenatBuildingMachineSmart2009</span><span class="co">]</span> (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://logicmoo.org/xwiki/bin/download/Main/References/Citations/WebHome/AQUAINT-Kickoff-Overview-Prange.pdf?rev=1.1)</span>)</span>
<span id="cb14-1001"><a href="#cb14-1001" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The <span class="co">[</span><span class="ot">Terrorism Knowledge Base</span><span class="co">](https://en.wikipedia.org/wiki/MIPT_Terrorism_Knowledge_Base)</span> (2004--2008), <span class="co">[</span><span class="ot">detailed below</span><span class="co">](#sec-terrorism-knowledge-base)</span>.</span>
<span id="cb14-1002"><a href="#cb14-1002" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Cleveland Clinic, starting in 2007, to parse clinician queries into database queries and returns the result. <span class="co">[</span><span class="ot">@lenatHarnessingCycAnswer2010; @pierceSemanticDBSemanticWeb2012</span><span class="co">]</span></span>
<span id="cb14-1003"><a href="#cb14-1003" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Goldman Sachs, sometime around 2016, to "monitor the inner workings of its technological infrastructure" and detect insider trading. <span class="co">[</span><span class="ot">@metzOneGeniusLonely2016; @shilohHeTaughtAI2023</span><span class="co">]</span></span>
<span id="cb14-1004"><a href="#cb14-1004" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The CIA and the Department of Defense, at an unknown time, for unspecified purposes, though probably to identify terrorist threats in connection with the Terrorist Knowledge Base. <span class="co">[</span><span class="ot">@shilohHeTaughtAI2023</span><span class="co">]</span></span>
<span id="cb14-1005"><a href="#cb14-1005" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The NSA, to "identify terrorist threats in international communications data". <span class="co">[</span><span class="ot">@metzOneGeniusLonely2016</span><span class="co">]</span></span>
<span id="cb14-1006"><a href="#cb14-1006" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The <span class="co">[</span><span class="ot">Total Information Awareness</span><span class="co">](https://en.wikipedia.org/wiki/Total_Information_Awareness)</span> project funded Cycorp for \$9.8 million in 2003 for a "prototype database" and a system that could "identify phone-calling patterns as they might exist among potential terrorists overseas". <span class="co">[</span><span class="ot">@crensonBigBrotherCould2003</span><span class="co">]</span></span>
<span id="cb14-1007"><a href="#cb14-1007" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Electronic Surveillance System for the Early Notification of Community-Based Epidemics-II (ESSENCE-II), around 2006. Its title is self-explanatory. <span class="co">[</span><span class="ot">@abbottIntegratedBiologicalWarfare2007</span><span class="co">]</span></span>
<span id="cb14-1008"><a href="#cb14-1008" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Seven unnamed big companies, for unspecified expert system applications <span class="co">[</span><span class="ot">@cycorpCycTechnologyOverview2021</span><span class="co">]</span>, but probably "common-sense platform for their applications, and as an interlingua to fully, semantically integrate all the data they generate and all the data they license from third parties". <span class="co">[</span><span class="ot">@lenatCreating30MillionRuleSystem2022</span><span class="co">]</span> </span>
<span id="cb14-1009"><a href="#cb14-1009" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Unnamed semiconductor foundry, for an expert system for root-cause analysis of fabrication yields. (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://web.archive.org/web/20130805145356/http://www.cyc.com/enterprise-solutions/success-stories/semiconductor-yield-management)</span>)</span>
<span id="cb14-1010"><a href="#cb14-1010" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Unnamed big bank, for an expert system for IT support and inventory management. (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://web.archive.org/web/20130805145023/http://www.cyc.com/enterprise-solutions/success-stories/it-inventory-catalog-and-configuration-management)</span>)</span>
<span id="cb14-1011"><a href="#cb14-1011" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Unnamed big bank, for an expert system for IT personnel expertise management. (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://web.archive.org/web/20130619021204/http://www.cyc.com/enterprise-solutions/success-stories/help-desk-expertise-management)</span>)</span>
<span id="cb14-1012"><a href="#cb14-1012" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Unnamed oil company, for an expert system for monitoring and predicting breakdowns at oil pumping facilities. (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://web.archive.org/web/20130805144921/http://www.cyc.com/enterprise-solutions/success-stories/facilities-status-monitoring-and-alerting)</span>)</span>
<span id="cb14-1013"><a href="#cb14-1013" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A learn-by-teaching game for 6th-grade arithmetics. It began in 2014 as project "BELLA", funded by the <span class="co">[</span><span class="ot">Advanced Distributed Learning Initiative</span><span class="co">](https://en.wikipedia.org/wiki/Advanced_Distributed_Learning)</span> under the <span class="co">[</span><span class="ot">Department of Defense</span><span class="co">](https://en.wikipedia.org/wiki/United_States_Department_of_Defense)</span> <span class="co">[</span><span class="ot">@lenatReinforcingMathKnowledge2014</span><span class="co">]</span>, and was commercialized in 2016 as <span class="co">[</span><span class="ot">MathCraft</span><span class="co">](https://cyc.com/mathcraft/)</span>. In the game, the student takes on the role of a tutor to an AI avatar, who would make various mistakes that the student would correct. It uses a small fragment of Cyc to model both the student's and the avatar's states of knowledge. Though it <span class="co">[</span><span class="ot">looks very polished</span><span class="co">](https://www.youtube.com/watch?v=pbrp7MzBDm0)</span>, I did not find confirmed instances of MathCraft being used in a school.</span>
<span id="cb14-1014"><a href="#cb14-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1015"><a href="#cb14-1015" aria-hidden="true" tabindex="-1"></a><span class="ot">[^kraken-acronym]: </span>Despite "KRAKEN" supposedly "not an abbreviation and does not require further definition" <span class="co">[</span><span class="ot">@cerutiKnowledgeManagementCommand2004</span><span class="co">]</span>, it was still defined as "Knowledge Rich Acquisition of Knowledge from Experts who are Non-logicians" by the Cycorp. <span class="co">[</span><span class="ot">@matthewsKRAKENKnowledgeRich2004</span><span class="co">]</span></span>
<span id="cb14-1016"><a href="#cb14-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1017"><a href="#cb14-1017" aria-hidden="true" tabindex="-1"></a>Looking at the list, we see that much of Cycorp funding came from the American intelligence community, especially between 2001 and 2010, during the heights of the <span class="co">[</span><span class="ot">War on Terror</span><span class="co">](https://en.wikipedia.org/wiki/War_on_terror)</span>, as the American state struggled to expand its sovereign eye over the sprawling cyberspace. Indeed, one of the early success was when it "predicted anthrax might be sent through the mail six months before trove of knowledge about past terrorist activities, tactics, and weapons". <span class="co">[</span><span class="ot">@hawkinsPredictingTerroristsNext2003</span><span class="co">]</span> Though the success did not help anyone, it was great advertisement.<span class="ot">[^explosive-dolphins]</span> Corroborating, Lenat in a <span class="co">[</span><span class="ot">2006 Google Talk</span><span class="co">](https://youtu.be/gAtn-4fhuWA?si=gAQ-TISZxxgeD1VN&amp;t=1856)</span> showed screenshots of Cyc answering "Which American city would be most vulnerable to an anthrax attack during summer?". (The answer was "Phoenix".)</span>
<span id="cb14-1018"><a href="#cb14-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1019"><a href="#cb14-1019" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Screenshot of Cyc answering "Which American city would be most vulnerable to an anthrax attack during summer?". The system replied "Phoenix", with reasoning. [@lenatGoogleTechTalksComputers2006]</span><span class="co">](figure/Cyc_anthrax_phoenix.png)</span></span>
<span id="cb14-1020"><a href="#cb14-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1021"><a href="#cb14-1021" aria-hidden="true" tabindex="-1"></a><span class="ot">[^explosive-dolphins]</span>:</span>
<span id="cb14-1022"><a href="#cb14-1022" aria-hidden="true" tabindex="-1"></a>    But there's also this:</span>
<span id="cb14-1023"><a href="#cb14-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1024"><a href="#cb14-1024" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; Once, developing a scenario for a terrorist attack on Hoover Dam, it hypothesized a school of 1,000 al Qaeda-trained dolphins bearing explosives.</span></span>
<span id="cb14-1025"><a href="#cb14-1025" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; </span></span>
<span id="cb14-1026"><a href="#cb14-1026" aria-hidden="true" tabindex="-1"></a><span class="in">    &gt; [@hawkinsPredictingTerroristsNext2003]</span></span>
<span id="cb14-1027"><a href="#cb14-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1028"><a href="#cb14-1028" aria-hidden="true" tabindex="-1"></a>The total funding of the project is hard to know, although we know that in 2002, its <span class="co">[</span><span class="ot">total cost had been \$60M</span><span class="co">](https://stanfordmag.org/contents/wise-up-dumb-machine)</span>, of which <span class="co">[</span><span class="ot">\$25M came from the military</span><span class="co">](https://web.archive.org/web/20120502151103/http://www.opencyc.org/cyc/company/news/APArticle060902)</span>, so I think it's fair to say 50\% came from the military. This is corroborated in 2005:</span>
<span id="cb14-1029"><a href="#cb14-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1030"><a href="#cb14-1030" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; In 1996, we got our first substantial government contract," Lenat recalls. Since then, Cycorp has collected about half of its revenue from U.S. government agencies and the rest from companies, mostly for building "semantic maps" that help users pull information from various databases with a single query. By taking on paying projects, Cycorp has been able to stay profitable and debt-free. All of the firm’s stock is owned by its employees, making Cycorp answerable only to Cycorp. "But," Lenat admits, "we have had to tack with the funding winds. Maybe 50 percent of the funding we get pushes us forward in the direction that we need to go."</span></span>
<span id="cb14-1031"><a href="#cb14-1031" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1032"><a href="#cb14-1032" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Cycorp doesn’t even want to be distracted by the rigors of the retail software business; instead, it licenses Cyc for use in third-party software packages... The time may come, Lenat says, when a greatly expanded Cyc will underlie countless software applications. But reaching that goal could easily take another two decades.</span></span>
<span id="cb14-1033"><a href="#cb14-1033" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1034"><a href="#cb14-1034" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@woodCycorpCostCommon2005</span><span class="co">]</span></span>
<span id="cb14-1035"><a href="#cb14-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1036"><a href="#cb14-1036" aria-hidden="true" tabindex="-1"></a>Out of all these applications, only two had been reported in detail.</span>
<span id="cb14-1037"><a href="#cb14-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1038"><a href="#cb14-1038" aria-hidden="true" tabindex="-1"></a><span class="fu">### Terrorism Knowledge Base {#sec-terrorism-knowledge-base}</span></span>
<span id="cb14-1039"><a href="#cb14-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1040"><a href="#cb14-1040" aria-hidden="true" tabindex="-1"></a>The first application is the <span class="co">[</span><span class="ot">Terrorism Knowledge Base</span><span class="co">](https://en.wikipedia.org/wiki/MIPT_Terrorism_Knowledge_Base)</span> (TKB), created in 2004 and shut down in 2008. <span class="co">[</span><span class="ot">@cycorpTerrorismKnowledgeBase2008</span><span class="co">]</span> During the aftermath of 9/11, the American government funded a massive expansion of surveillance and data processing, and the Cycorp took on several of such contracts. The TKB is the only one about which we know in some detail.</span>
<span id="cb14-1041"><a href="#cb14-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1042"><a href="#cb14-1042" aria-hidden="true" tabindex="-1"></a>The system can be browsed like a local Wikipedia -- if Wikipedia were focused entirely on terrorism. TKB contained &gt;2000 terrorists, &gt;700 terrorist groups, &gt;6500 terrorist attacks, and &gt;200,000 assertions such as "Xavier Djaffor participated in the Jihad from 1996 to 2000" and "<span class="co">[</span><span class="ot">Lashkar-e-Taiba</span><span class="co">](https://en.wikipedia.org/wiki/Lashkar-e-Taiba)</span> is an Islamist terror group founded in 1990".</span>
<span id="cb14-1043"><a href="#cb14-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1044"><a href="#cb14-1044" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Browsing the TKB about [Imad Fayez Mughniyeh](https://en.wikipedia.org/wiki/Imad_Mughniyeh). [@cycorpTerrorismKnowledgeBase2008]</span><span class="co">](figure/TKB_browser.png)</span></span>
<span id="cb14-1045"><a href="#cb14-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1046"><a href="#cb14-1046" aria-hidden="true" tabindex="-1"></a>A user query would be processed in 4 steps.</span>
<span id="cb14-1047"><a href="#cb14-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1048"><a href="#cb14-1048" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>User enters question in formal, but still natural, English.</span>
<span id="cb14-1049"><a href="#cb14-1049" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Cyc parses the question by keyword matching, template matching, and syntactic rules, then applies domain and common sense constraints to fix the parse, then retrieves some CycL fragments that are the closest matches to what the user entered.</span>
<span id="cb14-1050"><a href="#cb14-1050" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The user clicks on the fragments they meant. Cyc synthesizes a full query in CycL. The user optionally modifies the CycL query.</span>
<span id="cb14-1051"><a href="#cb14-1051" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Cyc runs inference engines to retrieve the answer along with a logic chain for the answer.</span>
<span id="cb14-1052"><a href="#cb14-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1053"><a href="#cb14-1053" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Asking the TKB about "What terrorists and biological agents are such that the terrorist is capable of learning to make the biological agent?". Its returns included [土谷正実](https://en.wikipedia.org/wiki/Masami_Tsuchiya_(terrorist)) from [Aum Shinrikyo](https://en.wikipedia.org/wiki/Aum_Shinrikyo). [@cycorpTerrorismKnowledgeBase2008]</span><span class="co">](figure/TKB_query.png)</span></span>
<span id="cb14-1054"><a href="#cb14-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1055"><a href="#cb14-1055" aria-hidden="true" tabindex="-1"></a>Like querying, data entry also has a light amount of parsing, and the system attempts to fill a form with it. The user can then fix the form. An intelligence specialist, lightly trained in using it, could enter up to 100 assertions per hour.</span>
<span id="cb14-1056"><a href="#cb14-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1057"><a href="#cb14-1057" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The data entry form. The system is prompting the user to disambiguate "skating boy" between "boy who is a doer of skating" and "boy who performs skating professionally". [@cycorpTerrorismKnowledgeBase2008]</span><span class="co">](figure/TKB_data_entry.png)</span></span>
<span id="cb14-1058"><a href="#cb14-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1059"><a href="#cb14-1059" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cleveland Clinic</span></span>
<span id="cb14-1060"><a href="#cb14-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1061"><a href="#cb14-1061" aria-hidden="true" tabindex="-1"></a>The second application took place at the Cleveland Clinic, sometime during 2007--2010.<span class="ot">[^cleveland-clinic-cyc]</span> The system was called "Semantic Research Assistant" (SRA), and it could answer queries about <span class="co">[</span><span class="ot">cardiothoracic surgery</span><span class="co">](https://en.wikipedia.org/wiki/Cardiothoracic_surgery)</span>, <span class="co">[</span><span class="ot">cardiac catheterization</span><span class="co">](https://en.wikipedia.org/wiki/Cardiac_catheterization)</span>, and <span class="co">[</span><span class="ot">percutaneous coronary intervention</span><span class="co">](https://en.wikipedia.org/wiki/Percutaneous_coronary_intervention)</span> -- basically, surgery-relevant questions about the heart. </span>
<span id="cb14-1062"><a href="#cb14-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1063"><a href="#cb14-1063" aria-hidden="true" tabindex="-1"></a><span class="ot">[^cleveland-clinic-cyc]: </span>This paper is the only substantial public information about this collaboration I can find. Therefore, I can only confirm that Cyc had been used in Cleveland Clinic during 2007--2010. It may have lasted to at least 2021, since <span class="co">[</span><span class="ot">@cycorpCycTechnologyOverview2021</span><span class="co">]</span> still cited this application.</span>
<span id="cb14-1064"><a href="#cb14-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1065"><a href="#cb14-1065" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">The 4-step process of the SRA. It is essentially the same process as in the TKB. [@lenatHarnessingCycAnswer2010, figure 2]</span><span class="co">](figure/Semantic_Research_Assistant.png)</span></span>
<span id="cb14-1066"><a href="#cb14-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1067"><a href="#cb14-1067" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">A logical justification constructed by Cyc by backward chaining on a user query. The original query is not shown, but it probably should be "For each instance of pericardial aortic valve replacement event in 2008, which event was it, and which type of pericardial aortic valve prosthesis was it?". [@lenatHarnessingCycAnswer2010, figure 4]</span><span class="co">](figure/SRA_logical_proof.png)</span></span>
<span id="cb14-1068"><a href="#cb14-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1069"><a href="#cb14-1069" aria-hidden="true" tabindex="-1"></a>There was one subsequent report on the system in 2012, which described the SemanticDB project at Cleveland, of which Cyc was only a part. The SemanticDB system contains a database of 120M semantic triples (which Lenat had long dismissed as being too limited). In the system, Cyc parses <span class="co">[</span><span class="ot">cohort identification queries</span><span class="co">](https://en.wikipedia.org/wiki/Cohort_analysis)</span> written in English into formal queries, then queries the database in <span class="co">[</span><span class="ot">SPARQL</span><span class="co">](https://en.wikipedia.org/wiki/SPARQL)</span>, does some further inference, and shows the result. <span class="co">[</span><span class="ot">@pierceSemanticDBSemanticWeb2012</span><span class="co">]</span> Unsurprisingly, Oracle Semantic Technologies <span class="co">[</span><span class="ot">was also involved</span><span class="co">](https://download.oracle.com/otndocs/tech/semantic_web/pdf/oow10_semtech_clvclnc_booth.pdf)</span>.</span>
<span id="cb14-1070"><a href="#cb14-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1071"><a href="#cb14-1071" aria-hidden="true" tabindex="-1"></a>In a <span class="co">[</span><span class="ot">presentation in 2019</span><span class="co">](https://web.archive.org/web/20210430003227/https://files.gotocon.com/uploads/slides/conference_13/724/original/AI_GOTO%20Lenat%20keynote%2030%20April%202019%20hc.pdf)</span>, Lenat claimed it required 120K new assertions for the Cleveland project, or 0.5% of the total knowledge base. However, 95% of the assertions that Cyc called up for answering queries for the project required knowledge Cyc already had, indicating large knowledge reuse (indicating that the whole project required about 2M rules).</span>
<span id="cb14-1072"><a href="#cb14-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1073"><a href="#cb14-1073" aria-hidden="true" tabindex="-1"></a><span class="fu">### Is that all?</span></span>
<span id="cb14-1074"><a href="#cb14-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1075"><a href="#cb14-1075" aria-hidden="true" tabindex="-1"></a>Well, I tried my best to look for more applications, but the fact is that there were so few of them. Out of all the confirmed instances of applications, the above two were the *only* ones reported in detail.</span>
<span id="cb14-1076"><a href="#cb14-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1077"><a href="#cb14-1077" aria-hidden="true" tabindex="-1"></a>So far, we have only considered applications of the full Cyc system. We have mentioned that there were both the OpenCyc and the ResearchCyc, two smaller versions of the full Cyc. Did they have substantial applications? As far as I can discern, no. Indeed, just as OpenCyc failed to bring about the Semantic Web or impress the general public, ResearchCyc failed to bring about a revolution in knowledge engineering or impress the general academia, or essentially anyone outside of Cycorp itself. Even Ernest Davis and Gary Marcus, highly sympathetic to the symbolic approach to AI, found little evidence for the success of Cyc, not because Cyc had provably failed, but simply because there was too little evidence in any direction, success or failure.</span>
<span id="cb14-1078"><a href="#cb14-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1079"><a href="#cb14-1079" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... it is in fact very difficult for an outsider to determine what has been accomplished here. In its first 15 years, CYC published astonishingly little. Since about 2002, somewhat more has been published, but still very little, considering the size of the project. No systematic evaluation of the contents, capacities, and limitations of CYC has been published. A number of organizations have done private evaluations but the results were not published.</span></span>
<span id="cb14-1080"><a href="#cb14-1080" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1081"><a href="#cb14-1081" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It is not, for example, at all clear what fraction of CYC actually deals with commonsense inference, and what fraction deals with specialized applications such as medical records or terrorism. It is even less clear what fraction of commonsense knowledge of any kind is in CYC. ... There are not even very many specific examples of commonsense reasoning carried out by CYC that have been published.</span></span>
<span id="cb14-1082"><a href="#cb14-1082" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1083"><a href="#cb14-1083" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@davisCommonsenseReasoningCommonsense2015</span><span class="co">]</span></span>
<span id="cb14-1084"><a href="#cb14-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1085"><a href="#cb14-1085" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; On the whole, it is fair to say that the AI community regards CYC as a very elaborate failure. Domingos (2015 p. 51) characterizes it as "the most notorious failure in the history of AI". Domingos is a researcher in machine learning and has little use for any kind of knowledge-based methods, so his phrasing is certainly harsh, but in our experience, this opinion, more or less, is common even in the knowledge representation (KR) community. ... it would be very helpful, and it would, we believe, significantly improve CYC’s standing in the AI community, if the CYC team could demonstrate some specific task where CYC really visibly shines. ... let them design their own task. As Watson demonstrates, passing a self-imposed task can be impressive enough, depending on the task, and in any case it is much better than nothing. At the moment, a person who is asked, "What interesting thing has been done with CYC?" is largely at a loss for an answer.</span></span>
<span id="cb14-1086"><a href="#cb14-1086" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1087"><a href="#cb14-1087" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@davisEvaluatingCYCPreliminary2016</span><span class="co">]</span></span>
<span id="cb14-1088"><a href="#cb14-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1089"><a href="#cb14-1089" aria-hidden="true" tabindex="-1"></a>There is a kind of insularity in Cycorp that starts to affect you if you look too closely into it. I know I was affected. Many times I had opened a paper that purported to show the application of Cyc, and was disappointed to find that it was yet another paper about the application of a method to ingest knowledge *into* Cyc, rather than a method to apply knowledge *out from* Cyc. I came to dread the literature review, as Cyc in my mind took on the sinister appearance of a black hole at the center of the knowledge graph, a cocoon that would never metamorphose into a butterfly.</span>
<span id="cb14-1090"><a href="#cb14-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1091"><a href="#cb14-1091" aria-hidden="true" tabindex="-1"></a><span class="fu">## Everyone can only see their own dream</span></span>
<span id="cb14-1092"><a href="#cb14-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1093"><a href="#cb14-1093" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lenat's tenets</span></span>
<span id="cb14-1094"><a href="#cb14-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1095"><a href="#cb14-1095" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@lenatThresholdsKnowledge1991</span><span class="co">]</span>, a paper coauthored with Feigenbaum, Lenat gave the most comprehensive statement for where he stands philosophically, which he held onto for the rest of his life:</span>
<span id="cb14-1096"><a href="#cb14-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1097"><a href="#cb14-1097" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Knowledge Principle. A system exhibits intelligent understanding and action at a high level of competence primarily because of the knowledge that it can bring to bear: the concepts, facts, representations, methods, models, metaphors, and heuristics about its domain of endeavor.</span>
<span id="cb14-1098"><a href="#cb14-1098" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explicit Knowledge Principle. While knowledge may be compiled to opaque lumps of code for efficiency, there should always be a declarative version of that, so that they can be subject to meta-reasoning.</span>
<span id="cb14-1099"><a href="#cb14-1099" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Breadth Hypothesis. Intelligent performance often requires the problem solver to fall back on increasingly general knowledge, and/or to analogize to specific knowledge from far-flung domains.</span>
<span id="cb14-1100"><a href="#cb14-1100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Empirical Inquiry Hypothesis. The most profitable way to investigate AI is to embody our hypotheses in programs, and gather data by running the programs. The surprises usually suggest revisions that start the cycle over again. Progress depends on these experiments being able to falsify our hypotheses. Falsification is the most common and yet most crucial of surprises. In particular, these programs must be capable of behavior not expected by the experimenter.</span>
<span id="cb14-1101"><a href="#cb14-1101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Difficult Problems Hypothesis. There are too many ways to solve simple problems. Raising the level and breadth of competence we demand of a system makes it easier to test -- and raise -- its intelligence.</span>
<span id="cb14-1102"><a href="#cb14-1102" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Knowledge Is All There Is Hypothesis. No sophisticated, as-yet-unknown *control structure* is required for intelligent behavior.</span>
<span id="cb14-1103"><a href="#cb14-1103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The Local Consistency Hypothesis. There is no need--and probably not even any possibility--of achieving a global consistent unification of several expert systems' KBs (or, equivalently, for one very large KB). Large systems need local consistency.</span>
<span id="cb14-1104"><a href="#cb14-1104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The Coherence Hypothesis. Moreover, whenever two large internally consistent chunks C1, C2 are similar, their heuristics and analogies should cohere; e.g., if the "going up" metaphor usually means "getting better" for C1, then it should again mean "getting better" for C2, or else it should not apply at all there.</span>
<span id="cb14-1105"><a href="#cb14-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1106"><a href="#cb14-1106" aria-hidden="true" tabindex="-1"></a>Lenat is the very example of a <span class="co">[</span><span class="ot">hedgehog</span><span class="co">](https://en.wikipedia.org/wiki/The_Hedgehog_and_the_Fox)</span>: a single philosophy, a vision for AGI, pursued for 40 years. One does not pursue a single vision without rejecting alternative visions, and Lenat has been explicit in rejecting every alternative route to AGI, using his sharp tongue <span class="co">[</span><span class="ot">@lenatThresholdsKnowledge1991, section A.2; @thompsonKnowItAllMachine2001; @lenatVoiceTurtleWhatever2008; @lenatGettingGenerativeAI2023</span><span class="co">]</span>:</span>
<span id="cb14-1107"><a href="#cb14-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1108"><a href="#cb14-1108" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Logical AI in the style of Simon and Newell's General Problem Solver. Such an elegant framework would not work beyond toy problem domains, by the Knowledge Principle.</span>
<span id="cb14-1109"><a href="#cb14-1109" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Highly accurate models of human behavior, in the style of Simon and Newell's *Human Problem Solving* or the <span class="co">[</span><span class="ot">SOAR architecture</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Soar_(cognitive_architecture)). Duplicating human cognitive architecture constitutes cargo cult science. AGI need not have the <span class="co">[</span><span class="ot">magic number 7 ± 2</span><span class="co">](https://en.wikipedia.org/wiki/The_Magical_Number_Seven%2C_Plus_or_Minus_Two)</span>.</span>
<span id="cb14-1110"><a href="#cb14-1110" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Physical embodiment. It might be great fun to make robots, but physical embodiment is neither necessary nor sufficient for "grounding" the knowledge base, by the <span class="co">[</span><span class="ot">Physical Symbol System Hypothesis</span><span class="co">](https://en.wikipedia.org/wiki/Physical_symbol_system)</span>. A "mystical worship of physical embodiment" would only delay AGI. In particular, the <span class="co">[</span><span class="ot">subsumption architecture</span><span class="co">](https://en.wikipedia.org/wiki/Subsumption_architecture)</span> does not lead to AGI.</span>
<span id="cb14-1111"><a href="#cb14-1111" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Genetic algorithms and other evolutionary algorithms. It gets stuck in local minima too often, and runs too slowly.</span>
<span id="cb14-1112"><a href="#cb14-1112" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Creating tiny morsels of little expert systems, and hope that bit by bit, AGI would emerge out of that. Remember that plateau-hopping requires breadth. Without an overarching plan, they will not fit together, like how the 1980s expert systems could never talk to each other.</span>
<span id="cb14-1113"><a href="#cb14-1113" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Logical machine learning without a large knowledge base already in place. It makes for good demos, but quickly exhausts itself. These are examples of the illusory hope for "free lunch" or elegant "Maxwell's equations of thinking", a severe case of laziness and "physics envy". Researchers should stop sitting on their asses mad with "physics envy", and start the dirty work of coding.</span>
<span id="cb14-1114"><a href="#cb14-1114" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Statistical machine learning, pattern matching, neural networks, and other self-organization methods. Just wait for enough compute and data, then magically a large model would learn on its own? Yet more wishful thinking for "free lunch", caused by laziness and "physics envy".</span>
<span id="cb14-1115"><a href="#cb14-1115" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Any form of machine learning without a large knowledge base to begin with. This is impossible because learning is possible only at the fringe of knowing. Any attempt to learn without a large starting knowledge base is, again, trying to get a "free lunch".</span>
<span id="cb14-1116"><a href="#cb14-1116" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Wait until philosophers have figured out the one true ontology for the world, then build the Cyc accordingly. Philosophers suffered from "Hamlet syndrome", unwilling to take decisive action, satisfied with publishing tiny morsels of ontologies that don't cover the whole world, or grand ontologies that cover a caricature of the whole world.</span>
<span id="cb14-1117"><a href="#cb14-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1118"><a href="#cb14-1118" aria-hidden="true" tabindex="-1"></a>Lenat had his own grand historical vision for AI, which I call the 3 Optima Theory.</span>
<span id="cb14-1119"><a href="#cb14-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1120"><a href="#cb14-1120" aria-hidden="true" tabindex="-1"></a>With a little hard work (about 6 person-months), one can get a knowledge-free system working, such as self-organized neural networks, Simon and Newell's General Problem Solver, etc. This allows the researcher to publish a quick paper, a student to earn their PhD degree, and so on. Putting in more hard work does not result in a better system, but usually makes things worse as the code becomes bloated and unmanageable. Academic myopia stops people from trying to get out of this local maximum, since people just want to get published papers.</span>
<span id="cb14-1121"><a href="#cb14-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1122"><a href="#cb14-1122" aria-hidden="true" tabindex="-1"></a>With a lot more hard work (about 10 person-years), one can get a system with a lot of specialized knowledge working. This is where the commercialized expert systems live. However, the general consensus is that as an expert system grows beyond 10K rules, it starts to suffer from its weight of all the rules. Standard expert systems were built for special fields, so people would use a simple language that works, but eventually collapses under the weight of 100K rules. Commercial myopia stops people from trying to get out of this local maximum, since people just want to sell products, and 50K rules is good enough for the customer. </span>
<span id="cb14-1123"><a href="#cb14-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1124"><a href="#cb14-1124" aria-hidden="true" tabindex="-1"></a>The problem is that all these efforts are wasted. Specialized expert systems cannot be glued together efficiently, because each of them lives in a differently simplified world. That is, "plateau-hopping requires breadth". The AI field as a whole would stagnate. The only way out of this is to go for the full common sense, to invest in the 2000 person-years of effort, and make a Cyc. After that, all the expert systems can interface with Cyc, and with each other using CycL, and all the computers can be preinstalled with their digital common sense.</span>
<span id="cb14-1125"><a href="#cb14-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1126"><a href="#cb14-1126" aria-hidden="true" tabindex="-1"></a><span class="al">![Lenat's 3 Optima Theory for the grand history of AI.](figure/Lenat_3_optima_theory.svg)</span></span>
<span id="cb14-1127"><a href="#cb14-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1128"><a href="#cb14-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">### A hostile assessment of Cyc</span></span>
<span id="cb14-1129"><a href="#cb14-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1130"><a href="#cb14-1130" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **bogosity**: At CMU, bogosity is measured with a bogometer; in a seminar, when a speaker says something bogus, a listener might raise his hand and say "My bogometer just triggered"... The agreed-upon unit of bogosity is the microLenat.</span></span>
<span id="cb14-1131"><a href="#cb14-1131" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1132"><a href="#cb14-1132" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **microLenat**: The unit of bogosity. Abbreviated µL or mL in ASCII. Consensus is that this is the largest unit practical for everyday use. The microLenat, originally invented by David Jefferson, was promulgated as an attack against noted computer scientist Doug Lenat by a tenured graduate student at CMU. Doug had failed the student on an important exam because the student gave only "AI is bogus" as his answer to the questions. The slur is generally considered unmerited, but it has become a running gag nevertheless.</span></span>
<span id="cb14-1133"><a href="#cb14-1133" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1134"><a href="#cb14-1134" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- </span><span class="co">[</span><span class="ot">The Jargon File</span><span class="co">](http://www.catb.org/jargon/html/M/microLenat.html)</span></span>
<span id="cb14-1135"><a href="#cb14-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1136"><a href="#cb14-1136" aria-hidden="true" tabindex="-1"></a>It is hard to interpret the state of Cyc today, if we take Lenat's word for it:<span class="ot">[^a-hostile-review]</span></span>
<span id="cb14-1137"><a href="#cb14-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1138"><a href="#cb14-1138" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>There were 150 technical challenges to knowledge engineering and representation at the start of Cyc in 1984, but they were all solved by 1990. <span class="co">[</span><span class="ot">@lenatBuildingMachineSmart2009; @lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-1139"><a href="#cb14-1139" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Cyc could already be tutored in (constrained) natural language in 2001. <span class="co">[</span><span class="ot">@anthesComputerizingCommonSense2002</span><span class="co">]</span></span>
<span id="cb14-1140"><a href="#cb14-1140" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The upper ontology has remained stable for years as of 2015. <span class="co">[</span><span class="ot">@lenat50ShadesSymbolic2015</span><span class="co">]</span></span>
<span id="cb14-1141"><a href="#cb14-1141" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The knowledge pump is 95\% primed in 2015, when there were just 15M assertions, <span class="co">[</span><span class="ot">@lenat50ShadesSymbolic2015</span><span class="co">]</span>, and as of 2021, there were over 25M assertions <span class="co">[</span><span class="ot">@cycorpCycTechnologyOverview2021</span><span class="co">]</span>.</span>
<span id="cb14-1142"><a href="#cb14-1142" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>SubLisp is easy to learn, and knowledge engineering in SubLisp is 1000× more efficient than in a modern language like Python. <span class="co">[</span><span class="ot">@lenatCycQuestSolve2021</span><span class="co">]</span></span>
<span id="cb14-1143"><a href="#cb14-1143" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The Cycorp had been profitable since its inception, had never taken on debt, had been almost entirely employee-owned, had always had only around 50--200 employees, a mostly flat corporate structure, and could remain profitable entirely on doing business with non-government corporations in 2022. <span class="co">[</span><span class="ot">@lenatCycQuestSolve2021; @lenatCreating30MillionRuleSystem2022</span><span class="co">]</span></span>
<span id="cb14-1144"><a href="#cb14-1144" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Cyc has natural language understanding of pragmatics, while statistical machine learning systems have none. <span class="co">[</span><span class="ot">@lenatSometimesVeneerIntelligence2017; @lenatNotGoodGold2019</span><span class="co">]</span></span>
<span id="cb14-1145"><a href="#cb14-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1146"><a href="#cb14-1146" aria-hidden="true" tabindex="-1"></a><span class="ot">[^a-hostile-review]: </span>Feel free to stop reading at this point. It is about to get polemical. Indeed, I have noticed that articles have the nefarious tendency to start from the personal ("the <span class="co">[</span><span class="ot">hook</span><span class="co">](https://en.wikipedia.org/wiki/Narrative_hook)</span>"), then become informative, and then subtly slide to the polemical ("the call to action"). In fact, I have mastered the art of first scrolling the page with my eyes blurred so that the <span class="co">[</span><span class="ot">hook</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Hook_(boxing)) cannot land on my head, and then, as soon as the text utters the first syllable of the ca--<span class="sc">\[</span>tab closed<span class="sc">\]</span></span>
<span id="cb14-1147"><a href="#cb14-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1148"><a href="#cb14-1148" aria-hidden="true" tabindex="-1"></a>At this point, Cyc is supposed to be out there doing true machine learning (not the shallow veneer of "machine learning" that those statistical and connectionist researchers are faking). Reading and studying human text written in natural language. True intelligence. General intelligence. The foundation of a thousand expert systems. The backbone of the Semantic Web. The flowering of a new age of reason.</span>
<span id="cb14-1149"><a href="#cb14-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1150"><a href="#cb14-1150" aria-hidden="true" tabindex="-1"></a>And yet 9 years after Cyc is declared "done", Cyc is still stuck inside the walls of Cycorp doing nothing of the kind. What is stopping Cyc from learning?</span>
<span id="cb14-1151"><a href="#cb14-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1152"><a href="#cb14-1152" aria-hidden="true" tabindex="-1"></a>The finances are healthy. Cycorp is not subject to perverse interests of the market or middle managers. There are fewer employees than <span class="co">[</span><span class="ot">Dunbar's number</span><span class="co">](https://en.wikipedia.org/wiki/Dunbar's_number)</span>. They are aligned to the corporate mission. SubLisp is a great language. All technical challenges to knowledge engineering and representation had been solved by 1990. The knowledge pump is over 160\% primed.</span>
<span id="cb14-1153"><a href="#cb14-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1154"><a href="#cb14-1154" aria-hidden="true" tabindex="-1"></a>What. Is. Stopping. Cyc. From. Learning??</span>
<span id="cb14-1155"><a href="#cb14-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1156"><a href="#cb14-1156" aria-hidden="true" tabindex="-1"></a>According to his final work, coauthored with Gary Marcus, the last holdup was natural language understanding (NLU). Entry had been accelerated, but was still quite manual:</span>
<span id="cb14-1157"><a href="#cb14-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1158"><a href="#cb14-1158" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This process has been accelerated by gamification, NLU, etc., but each axiom is hand-checked for default correctness, generality, and best placement into the microtheories (contexts) it applies to, before entering it into the Cyc knowledge base.</span></span>
<span id="cb14-1159"><a href="#cb14-1159" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1160"><a href="#cb14-1160" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatGettingGenerativeAI2023</span><span class="co">]</span></span>
<span id="cb14-1161"><a href="#cb14-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1162"><a href="#cb14-1162" aria-hidden="true" tabindex="-1"></a>The knowledge pump had been thoroughly primed, but Cyc still couldn't learn by reading human texts, because NLU remained unsolved. Cyc can read CycL perfectly well -- the interlingua, its mother tongue -- but it is stubbornly difficult to parse English into the interlingua. But why is parsing natural languages hard? Why, indeed, is NLU so difficult, even "AI-complete"?</span>
<span id="cb14-1163"><a href="#cb14-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1164"><a href="#cb14-1164" aria-hidden="true" tabindex="-1"></a>This situation is clarified if we consider a classic model of machine translation, that of the <span class="co">[</span><span class="ot">Vauquois triangle</span><span class="co">](https://en.wikipedia.org/wiki/Vauquois_triangle)</span>: We need to translate from one language (let's say, English) to another (let's say, Japanese). We can  translate word-for-word, which corresponds to the base of the triangle. This direct approach is of course simple but brittle. Not only does it ignore different word-ordering across languages, it would also completely fail to disambiguate homonyms, such as "fly" as a noun versus "fly" as a verb. Stopgap measures such as using n-grams lead to combinatorial explosions.</span>
<span id="cb14-1165"><a href="#cb14-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1166"><a href="#cb14-1166" aria-hidden="true" tabindex="-1"></a>At a higher level, we can take account of syntax. We first parse the English sentence into a syntax tree, do a word-replacement, transform the syntax tree according to Japanese syntax, and finally generate the Japanese sentence from it. This approach would solve the "fly" vs "fly" problem, since one is a verb and another is a noun, which would be clear according to the syntax tree. However, this fails to disambiguate the word "pen" in the two sentences:</span>
<span id="cb14-1167"><a href="#cb14-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1168"><a href="#cb14-1168" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The ink is in the <span class="dt">&lt;</span><span class="kw">u</span><span class="dt">&gt;</span>pen<span class="dt">&lt;/</span><span class="kw">u</span><span class="dt">&gt;</span>.</span>
<span id="cb14-1169"><a href="#cb14-1169" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The sheep is in the <span class="dt">&lt;</span><span class="kw">u</span><span class="dt">&gt;</span>pen<span class="dt">&lt;/</span><span class="kw">u</span><span class="dt">&gt;</span>.</span>
<span id="cb14-1170"><a href="#cb14-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1171"><a href="#cb14-1171" aria-hidden="true" tabindex="-1"></a>In both cases, the word "pen" has exactly the same syntactic category, but has a different meaning, and it requires some understanding of how the world works (i.e. that the writing-pen is too small to hold a sheep, while it is highly unlikely for someone to put ink into an animal-pen). Similarly, in the <span class="co">[</span><span class="ot">Winograd schema challenge</span><span class="co">](https://en.wikipedia.org/wiki/Winograd_schema_challenge)</span>, the task is to disambiguate what a pronoun refers to, such as deciding what the word "they" refers to in each of the two sentences:</span>
<span id="cb14-1172"><a href="#cb14-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1173"><a href="#cb14-1173" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The city councilmen refused the demonstrators a permit because <span class="dt">&lt;</span><span class="kw">u</span><span class="dt">&gt;</span>they<span class="dt">&lt;/</span><span class="kw">u</span><span class="dt">&gt;</span> feared violence.</span>
<span id="cb14-1174"><a href="#cb14-1174" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The city councilmen refused the demonstrators a permit because <span class="dt">&lt;</span><span class="kw">u</span><span class="dt">&gt;</span>they<span class="dt">&lt;/</span><span class="kw">u</span><span class="dt">&gt;</span> advocated violence.</span>
<span id="cb14-1175"><a href="#cb14-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1176"><a href="#cb14-1176" aria-hidden="true" tabindex="-1"></a>Indeed, Lenat had often claimed that the Winograd schema challenge is a touchstone for true language understanding, something that would prove that Cyc really understands, while unmasking the other systems' veneers of intelligence. And how must the Winograd schema challenge be solved? Lenat's solution is to go to the pinnacle of the triangle -- interlingua, the King of Kings, the Language among Languages, a completely universal conceptual representation for all humans might wish to mean by their speech.</span>
<span id="cb14-1177"><a href="#cb14-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1178"><a href="#cb14-1178" aria-hidden="true" tabindex="-1"></a>Lenat intended the CycL to be the interlingua.</span>
<span id="cb14-1179"><a href="#cb14-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1180"><a href="#cb14-1180" aria-hidden="true" tabindex="-1"></a>Within this framework, the problems of machine translation and understanding are unified: To understand English, it remains to parse English into the interlingua. To translate Japanese to English, it remains to parse Japanese to interlingua, then verbalize interlingua into English.</span>
<span id="cb14-1181"><a href="#cb14-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1182"><a href="#cb14-1182" aria-hidden="true" tabindex="-1"></a><span class="al">![The Vauquois triangle of translation.](figure/Vauquois%20triangle.png)</span></span>
<span id="cb14-1183"><a href="#cb14-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1184"><a href="#cb14-1184" aria-hidden="true" tabindex="-1"></a>Recall that Lenat had always argued that "just letting a system learn on its own by natural language understanding" is a free lunch, and that NLU requires a significant portion (~10--50%) of common sense already encoded. But if that's the case, then there is a near-contradiction here:</span>
<span id="cb14-1185"><a href="#cb14-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1186"><a href="#cb14-1186" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The CycL language is enough to represent common sense language about the world. On the Vauquois triangle, all natural languages are joined at the top by a common interlingua, which is the CycL.</span>
<span id="cb14-1187"><a href="#cb14-1187" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>By the Winograd schema challenge, translation requires common sense. Therefore, Japanese → English translation requires common sense.</span>
<span id="cb14-1188"><a href="#cb14-1188" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>CycL → English requires no common sense. The result would sound kind of wooden and robotic, but it doesn't require any understanding: Just follow the syntax substitution rules. Indeed, CycL → English was already working since ~2000. Therefore, CycL → English requires *no* common sense.</span>
<span id="cb14-1189"><a href="#cb14-1189" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Therefore, by the conservation of common sense, it takes *exactly* the same amount of common sense to perform Japanese → English translation and Japanese → CycL translation. Indeed, this is why they considered NLU an "AI-complete" problem.</span>
<span id="cb14-1190"><a href="#cb14-1190" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>By the no free lunch hypothesis, neural networks trained from scratch can't have common sense. Thus,  neural networks should fail at Japanese → CycL translation.</span>
<span id="cb14-1191"><a href="#cb14-1191" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>But in Lenat's last paper, he argued that the only problem stopping Cyc from learning from natural language was that NLU did not work well enough yet, and *hoped* that neural networks could perform the natural language → CycL translation. Indeed, they considered ChatGPT and <span class="co">[</span><span class="ot">Google Bard</span><span class="co">](https://en.wikipedia.org/wiki/Google_Bard)</span> to often be better at NLU than Cyc.</span>
<span id="cb14-1192"><a href="#cb14-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1193"><a href="#cb14-1193" aria-hidden="true" tabindex="-1"></a>On the topic of interlingua, it is interesting that ABBYY<span class="co">&lt;!-- todo: link to ABBYY --&gt;</span> was almost a twin of Cycorp. Whereas Cycorp began building an ontology for the common sense world since 1984, spent \$200M, and got stuck on NLU since around 2010, ABBYY began building an interlingua-based machine translation system since the 1990s, and spent over \$80M. By the early 2010s, they realized that they could not compete with Google statistical machine translation, and pivoted to doing NLU with semantic graph technology based on the knowledge base they produced for the sake of interlingua. <span class="co">[</span><span class="ot">@skorinkinABBYYsBitterLesson2024</span><span class="co">]</span></span>
<span id="cb14-1194"><a href="#cb14-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1195"><a href="#cb14-1195" aria-hidden="true" tabindex="-1"></a>Indeed, interlingua-based machine translation projects used to be common, but essentially went extinct (except for ABBYY) after the rise of statistical machine translation in the 1990s <span class="co">[</span><span class="ot">@hutchinsMachineTranslationHistory2023</span><span class="co">]</span>.</span>
<span id="cb14-1196"><a href="#cb14-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1197"><a href="#cb14-1197" aria-hidden="true" tabindex="-1"></a>I suspect that it is not simply the problem of getting a better English → CycL translator, and then Cyc would finally begin learning, but that much knowledge in sentences doesn't translate to interlingua.<span class="ot">[^interlingua-latent-space]</span> If the failures of all interlingua machine translation systems is not enough evidence, then consider some more facts about Cyc's NLU:</span>
<span id="cb14-1198"><a href="#cb14-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1199"><a href="#cb14-1199" aria-hidden="true" tabindex="-1"></a><span class="ot">[^interlingua-latent-space]: </span>Or that the interlingua exists, but it is not symbolic-logical, but linear-algebraic. Indeed, the successful neural machine translation systems' latent spaces may be that consummate interlingua so devoutly wished for, but such an interlingua is very far from what you'd see in Cyc or ABBYY.</span>
<span id="cb14-1200"><a href="#cb14-1200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1201"><a href="#cb14-1201" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Exhaustively searching the literature, I only found four (four!!) examples that Cycorp gave for English → CycL: "A girl is on a white lounge chair" <span class="co">[</span><span class="ot">@prattCYCReportPratt1994</span><span class="co">]</span>, "Bill Clinton sleeps.", "An AI researcher is a kind of computer scientist." <span class="co">[</span><span class="ot">@pantonCommonSenseReasoning2006</span><span class="co">]</span>, and "Did you touch a blue object located in the capital of France on September 25th, 2022?" <span class="co">[</span><span class="ot">@lenatGettingGenerativeAI2023</span><span class="co">]</span>. They were quite easy and unambiguous examples, almost as if they began with a CycL sentence, and then converted it to English. None involves the "many AI-complete elements, such as correct disambiguation, understanding of idioms, metaphor, sarcasm, foreshadowing, irony, subtext, and so on." <span class="co">[</span><span class="ot">@lenatGettingGenerativeAI2023</span><span class="co">]</span>.</span>
<span id="cb14-1202"><a href="#cb14-1202" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">@guhaEnablingAgentsWork1994</span><span class="co">]</span> stated that in 1994-03 "The Syntax module can properly handle about 75% of the sentences found in the news stories of a typical issue of the newspaper USA Today. And in cases in which Cyc knows all the proper nouns in the sentence, the Semantics module can properly handle most of the sentences parsable by the syntax module... as good as what our knowledge enterers independently come up with, when asked to manually translate the material into CycL.".</span>
<span id="cb14-1203"><a href="#cb14-1203" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">@pantonCommonSenseReasoning2006</span><span class="co">]</span> stated that in 2006, a search-and-verify system for English → CycL, that combined syntactic parsing, statistical parsing, and Cyc verification, resulted in "sentences that were correct, according to human review, approximately 50% of the time".</span>
<span id="cb14-1204"><a href="#cb14-1204" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">@sarjantAllYouCan2009</span><span class="co">]</span> increased the common-sense knowledge in ResearchCyc by 30% in 2009, by guess-and-verify, where the Cyc does verification, and the guess was done by simplistic methods like regex parsing, <span class="co">[</span><span class="ot">infobox</span><span class="co">](https://en.wikipedia.org/wiki/Help:Infobox)</span> pairing, etc.</span>
<span id="cb14-1205"><a href="#cb14-1205" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Some governmental experimental uses of Cyc, such as ESSENCE-II and Total Information Awareness, might have involved some NLU, but I cannot find details concerning how much NLU was involved.</span>
<span id="cb14-1206"><a href="#cb14-1206" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Only *one* of the commercial applications of Cyc may have plausibly required NLU.</span>
<span id="cb14-1207"><a href="#cb14-1207" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>In <span class="co">[</span><span class="ot">MathCraft</span><span class="co">](https://cyc.com/mathcraft/)</span>, the "learning by teaching" game powered by Cyc, students are only allowed to pick from choices generated by the Cyc itself. There is no free-form natural language input at all.</span>
<span id="cb14-1208"><a href="#cb14-1208" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>In the Cleveland Clinic application <span class="co">[</span><span class="ot">@lenatHarnessingCycAnswer2010</span><span class="co">]</span>, the user enters queries already in a constrained language (like "aortic valve replacement patients with a pericardial aortic valve"), and then compose a CycL translation by clicking from Cyc's parser's suggestions.</span>
<span id="cb14-1209"><a href="#cb14-1209" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The same is true for the Terrorism Knowledge Base application.</span>
<span id="cb14-1210"><a href="#cb14-1210" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>"Maintaining persistent user models in order to support extended, months-long online chats with their famous characters" (<span class="co">[</span><span class="ot">Source</span><span class="co">](https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf)</span>) seems to involve NLU. However, there is no information whatsoever as to how much NLU this project involved. I can't find out which company hired Cycorp for this, or who the "famous characters" were. It is not even clear if users are allowed to enter free-form input during online chat, or if it is another application like MathCraft, where you can only pick from choices generated by Cyc itself. Besides, even if this case involved free-form input, there is still a problem. We know that self-organized NN can work as chatbots. So if Lenat was right to say that self-organized NN don't understand (as he had been saying for 40 years), then this application didn't require NLU after all.</span>
<span id="cb14-1211"><a href="#cb14-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1212"><a href="#cb14-1212" aria-hidden="true" tabindex="-1"></a>Let's take it another way: If there is No Free Lunch to NLU, then what is Cyc's score on Winograd schema benchmark? Where is the Cyc-translator? Forget about Google Neural Translate -- is it even better than ABBYY's? Where's ChatCyc? Why does none of Cyc's commercial applications involve NLU in any significant amount, while most commercial applications of NLU use statistical or neural machine learning?</span>
<span id="cb14-1213"><a href="#cb14-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1214"><a href="#cb14-1214" aria-hidden="true" tabindex="-1"></a>All evidences point to the conclusion that a sentence that can be parsed to CycL is already bureaucratic and formalistic, with the mark of interlingua written on its brow. For those, Cyc could already understand in the early 2000s, and yet, Cyc is still here, not machine-learning, lacking ... what? <span class="in">`&lt;sarcasm&gt;`</span>A sarcasm parser?<span class="in">`&lt;/sarcasm&gt;`</span></span>
<span id="cb14-1215"><a href="#cb14-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1216"><a href="#cb14-1216" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="An information-theoretic estimate" collapse="true" }</span>
<span id="cb14-1217"><a href="#cb14-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1218"><a href="#cb14-1218" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">minimal description length of English</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perplexity-turing-test/#entropy-of-natural-languages)</span> is about 0.8 bits per character, and each English sentence contains about 100 characters, giving 80 bits of incompressible information. Since there are 27 characters (we are just counting the 26 lowercase letters and the whitespace), this means that the compressible information is about $\log_2 27 - 0.8 = 4.0$ bits per character.</span>
<span id="cb14-1219"><a href="#cb14-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1220"><a href="#cb14-1220" aria-hidden="true" tabindex="-1"></a>We can think of writing as a process of selection: Out of all the possible letters you can pick next, you picked this *particular* letter, and you do it again and again. Intelligence is good selection, the careful elimination of bad choices. In this particular case, the bad choices to be eliminated come out to about 3.2 bits per character.</span>
<span id="cb14-1221"><a href="#cb14-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1222"><a href="#cb14-1222" aria-hidden="true" tabindex="-1"></a>Now, one might argue that the last bit is the deepest, but it is hard to square the two claims:</span>
<span id="cb14-1223"><a href="#cb14-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1224"><a href="#cb14-1224" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Self-organized NN could compress natural text to a bit rate of ~1.0 bits per character, so they already capture $3.0/3.2 = 94\%$ of the selection.</span>
<span id="cb14-1225"><a href="#cb14-1225" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Self-organized NN don't understand.</span>
<span id="cb14-1226"><a href="#cb14-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1227"><a href="#cb14-1227" aria-hidden="true" tabindex="-1"></a>Stated in another way, this means 94\% of the selection that humans perform while writing is mindless, mere remembering and espousing without understanding or inferring. This is even harder to square with Lenat's assertion that 99\% of language understanding is pragmatics, which self-organized NN don't have. <span class="co">[</span><span class="ot">@lenatSometimesVeneerIntelligence2017</span><span class="co">]</span></span>
<span id="cb14-1228"><a href="#cb14-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1229"><a href="#cb14-1229" aria-hidden="true" tabindex="-1"></a>So does this mean that 93% of pragmatics is mindless, or are we going to just throw out the information-theoretic understanding of language as another incommensurable  paradigm shift? It would fit the theme of Cyc. Indeed, they had long excised the remnants of probabilistic reasoning when they removed <span class="co">[</span><span class="ot">certainty factors</span><span class="co">](https://en.wikipedia.org/wiki/Mycin)</span> at some point before 1989, since they found it inadequate compared to reasoning by logical unification <span class="co">[</span><span class="ot">@guhaReCycLingPaper1993</span><span class="co">]</span>. Cry "Kuhnian" and let slip the dogs of logic...</span>
<span id="cb14-1230"><a href="#cb14-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1231"><a href="#cb14-1231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1232"><a href="#cb14-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1233"><a href="#cb14-1233" aria-hidden="true" tabindex="-1"></a>Take another look at <span class="co">[</span><span class="ot">the list of known applications of Cyc</span><span class="co">](#cycops-what-is-it-good-for)</span>. Then take a look at what was even planned for as "short-term uses" of Cyc even back in 1993:</span>
<span id="cb14-1234"><a href="#cb14-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1235"><a href="#cb14-1235" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; the most promising short-term uses of Cyc are not what are traditionally considered AI problems. Instead, they are in relatively "mundane" problems such as making spreadsheets smarter, providing better access to a heterogeneous set of databases, directed marketing of goods and services, etc.</span></span>
<span id="cb14-1236"><a href="#cb14-1236" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1237"><a href="#cb14-1237" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@guhaReCycLingPaper1993</span><span class="co">]</span></span>
<span id="cb14-1238"><a href="#cb14-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1239"><a href="#cb14-1239" aria-hidden="true" tabindex="-1"></a>It is quite prophetic that such "short-term uses" of Cyc are still the *only* uses of Cyc so far. Is 35 years considered "short-term"? Does this look like a path towards AGI, or does this look no different from building custom-made expert systems for specialized purposes, something that those generic professionals of Oracle, IBM, or Accenture have been doing for decades?</span>
<span id="cb14-1240"><a href="#cb14-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1241"><a href="#cb14-1241" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Even though most people don't think of the CyCorp... as an expert systems company, effectively, that's what we're doing... we've stayed in business all these years when none of you have. We are the last surviving large expert system company.</span></span>
<span id="cb14-1242"><a href="#cb14-1242" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1243"><a href="#cb14-1243" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@allenExpertSystemsPioneer2018b</span><span class="co">]</span></span>
<span id="cb14-1244"><a href="#cb14-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1245"><a href="#cb14-1245" aria-hidden="true" tabindex="-1"></a>Perhaps they did have a product differentiation in being able to consistently find particularly good knowledge engineers, and in programming in SubLisp, a particularly efficient language compared to Python or Java. Perhaps their product differentiation is in niche expert systems that really require higher-order statements. Perhaps these are what allowed them to stay in business despite having just a hundred-person crew. It is not something to be dismissed, but is this a path towards AGI, or a veneer of AGI cast over enterprise solutions? <span class="in">`&lt;sarcasm&gt;`</span>An IBM Data Integration Solutions<span class="dt">&lt;</span><span class="kw">sup</span><span class="dt">&gt;</span>®<span class="dt">&lt;/</span><span class="kw">sup</span><span class="dt">&gt;</span> with better cover art?<span class="in">`&lt;/sarcasm&gt;`</span></span>
<span id="cb14-1246"><a href="#cb14-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1247"><a href="#cb14-1247" aria-hidden="true" tabindex="-1"></a>Perhaps Lenat could be eternally optimistic despite all the missed predictions and the lack of AGI. Perhaps it is a <span class="co">[</span><span class="ot">selection effect</span><span class="co">](https://en.wikipedia.org/wiki/Selection_bias)</span>. One does not undertake a 40-year project for AGI without being delusionally optimistic about the prospect -- the same could be said of great leaders and startup founders.</span>
<span id="cb14-1248"><a href="#cb14-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1249"><a href="#cb14-1249" aria-hidden="true" tabindex="-1"></a>Or perhaps he was disconnected from what was really going on down there at Cycorp. In a <span class="co">[</span><span class="ot">HackerNews discussion</span><span class="co">](https://news.ycombinator.com/item?id=21781597)</span>, some ex-Cyclists wrote what they thought about Cycorp.</span>
<span id="cb14-1250"><a href="#cb14-1250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1251"><a href="#cb14-1251" aria-hidden="true" tabindex="-1"></a>On the pro side were many. The corporate culture was highly intellectual and philosophical, as one expects for a company that does computable ontology for a living: "it can be pretty fun to be in meetings where you try to explain Davidsonian ontology to perplexed business people". The company had solved many technical problems in large scale inference, and remained profitable, with successful commercial applications.</span>
<span id="cb14-1252"><a href="#cb14-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1253"><a href="#cb14-1253" aria-hidden="true" tabindex="-1"></a>On the con side were many. The codebase was creaking under 30 years of technical debt:</span>
<span id="cb14-1254"><a href="#cb14-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1255"><a href="#cb14-1255" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I spent some entire days just scrolling through different versions of entire systems that duplicate massive chunks of functionality, written 20 years apart, with no indication of which (if any) still worked or were the preferred way to do things.</span></span>
<span id="cb14-1256"><a href="#cb14-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1257"><a href="#cb14-1257" aria-hidden="true" tabindex="-1"></a>The technical solutions and commercial applications were closely guarded secrets, so the outside world does not know. Lenat was unimpressed with open source and so did not commit resources to OpenCyc, tutorials, easier third-party integration, software development kit, or other outreach projects (except those regularly scheduled newspaper reports for publicity). The Cyc culture was also insular, with a true believer's mentality:</span>
<span id="cb14-1258"><a href="#cb14-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1259"><a href="#cb14-1259" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... veterans there sort of feel like the broader AI community turned their back on symbolic reasoning in the 80s (fair) and they're generally not very impressed by the current trends within the AI community, particularly w.r.t. advances in ML (perhaps unfairly so), so they're going to just keep doing their thing until they can't be ignored anymore.</span></span>
<span id="cb14-1260"><a href="#cb14-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1261"><a href="#cb14-1261" aria-hidden="true" tabindex="-1"></a>Most pertinent to the dream of AGI, it was unclear, down in the trenches, whether Cyc was really doing common sense reasoning, or just a particularly good base for developing expert systems from. It also wasn't clear if common sense reasoning was really necessary for the successful commercial projects in the first place. This has caused some Cyclists to become ex-Cyclists from disillusionment.</span>
<span id="cb14-1262"><a href="#cb14-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1263"><a href="#cb14-1263" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; I personally suspect that some of Cycorp's clients would do better with domain-specific solutions because they don't realize how much of their problem could be solved that way and how much of the analysis coming from Cyc is actually the result of subject matter experts effectively building domain-specific solutions the hard way inside of Cyc. With a lot of Cycorp projects, it's hard to point your finger at exactly where the "AI" is happening... The degree to which it's effective seemed to me to be a case-by-case thing. While working there I tended to suspect that Cyc people underestimated the degree to which you could get a large fraction of their results using something like </span><span class="co">[</span><span class="ot">Datomic</span><span class="co">](https://en.wikipedia.org/wiki/Datomic)</span><span class="at"> and it was an open question (to me at least) whether the extra 10% or whatever was worth how much massively more complicated it is to work with Cyc.</span></span>
<span id="cb14-1264"><a href="#cb14-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1265"><a href="#cb14-1265" aria-hidden="true" tabindex="-1"></a>Structurally, the Cycorp had two levels. At the upper level are Lenat, Witbrock, and such keepers of the faith, who kept the ceaseless striving for that elusive dream alive. At the lower level are the working ontological engineers who just had to deliver the product, AGI or not.</span>
<span id="cb14-1266"><a href="#cb14-1266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1267"><a href="#cb14-1267" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It turns out there's a kind of reality distortion field around the management there, despite their best intentions - partially maintained by the management's own steadfast belief in the idea that what Cyc does is what it ought to be doing, but partially maintained by a layer of people that actively isolate the management from understanding the dirty work that goes into actually making projects work or appear to. So while a certain amount of "common sense" knowledge factors into the reasoning processes, a great amount of Cyc's output at the project level really comes from hand-crafted algorithms implemented either in the inference engine or the ontology.</span></span>
<span id="cb14-1268"><a href="#cb14-1268" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1269"><a href="#cb14-1269" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Over the years, the Cyc as its actually implemented has drifted pretty far from the Cyc that people like Doug Lenat believe in, and the degree to which they're willing or able to acknowledge that seems to sort of drift around, often dependent on factors like mood. Doug would show up and be very confused about why some things were hard because he just believes that Cyc works differently than it does in practice, and people had project deadlines, so they often implemented features via hacks to shape inference or hand-built algorithms to deliver answers that Doug thought ought to be derived from principles via inference. Doug thinks way more stuff that Cyc does is something that it effectively learned to do by automatically deriving a way to solve the general form of a problem, rather than a programmer up late hand-coding things to make a demo work the next day, and the programmers aren't going to tell him because there's a demo tomorrow too and it's not working yet.</span></span>
<span id="cb14-1270"><a href="#cb14-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1271"><a href="#cb14-1271" aria-hidden="true" tabindex="-1"></a>But that's enough about unverified rumors, and I apologize. It would have been better for epistemic hygiene if there were more open information about Cycorp. Let's turn to a Cycoanalysis of Lenat's rhetorics, which is more well-documented.</span>
<span id="cb14-1272"><a href="#cb14-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1273"><a href="#cb14-1273" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lenat against the world</span></span>
<span id="cb14-1274"><a href="#cb14-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1275"><a href="#cb14-1275" aria-hidden="true" tabindex="-1"></a>According to multiple reports, Lenat was charismatic, able to sell his vision of AGI to many people. Having read most documents produced by Lenat or Cycorp over the 40 years, I have discovered that there is a consistent list of themes that Lenat just kept repeating over his career. Each theme has a double structure: a technical statement that has an emotionally neutral valence, and a moral coloring that provides the call to action, the charisma, the coherence for the employees to align to his vision.</span>
<span id="cb14-1276"><a href="#cb14-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1277"><a href="#cb14-1277" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> technical statement <span class="pp">|</span> moral coloring <span class="pp">|</span></span>
<span id="cb14-1278"><a href="#cb14-1278" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-------|</span></span>
<span id="cb14-1279"><a href="#cb14-1279" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> the No Free Lunch Hypothesis <span class="pp">|</span> we do honest hard work like the proverbial ant, you are lazy like the proverbial cricket (and the AI Winter is coming) <span class="pp">|</span></span>
<span id="cb14-1280"><a href="#cb14-1280" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> there are no "Maxwell's Equations of Thought" <span class="pp">|</span> we are self-assured, you suffer from physics-envy <span class="pp">|</span></span>
<span id="cb14-1281"><a href="#cb14-1281" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> the Empirical Inquiry Hypothesis; Cyc is unaesthetic; we are building the Cyc profitably, not publishing academic papers <span class="pp">|</span> we are strong engineers that get things done, you are weak aesthetes playing the academic game <span class="pp">|</span></span>
<span id="cb14-1282"><a href="#cb14-1282" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> the Physical Symbol System Hypothesis <span class="pp">|</span> we are building real intelligence, you are just playing with robots <span class="pp">|</span></span>
<span id="cb14-1283"><a href="#cb14-1283" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> the Breadth Hypothesis <span class="pp">|</span> our systems are intelligent, yours are autistic idiot savants <span class="pp">|</span></span>
<span id="cb14-1284"><a href="#cb14-1284" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> the Explicit Knowledge Principle <span class="pp">|</span> our systems understand deeply, yours pattern-match shallowly <span class="pp">|</span></span>
<span id="cb14-1285"><a href="#cb14-1285" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> we think you are putting pattern-matchers in places that require deep understanding <span class="pp">|</span> we are trustworthy, you are reckless <span class="pp">|</span></span>
<span id="cb14-1286"><a href="#cb14-1286" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> we believe the Cyc is the only current effort towards AGI <span class="pp">|</span> we are ambitious, you are academic careerists <span class="pp">|</span></span>
<span id="cb14-1287"><a href="#cb14-1287" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> writing the Cyc costs a lot and is unpopular with the academia <span class="pp">|</span> we rebel and think freely, you sheepishly follow the crowd <span class="pp">|</span></span>
<span id="cb14-1288"><a href="#cb14-1288" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Cycorp hires anyone -- including high school dropouts -- good at encoding common sense <span class="pp">|</span> we are egalitarian, you are elitists <span class="pp">|</span></span>
<span id="cb14-1289"><a href="#cb14-1289" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Cycorp has always had just ~100 people, and has been mostly forgotten now <span class="pp">|</span> we are the elect, you will see <span class="pp">|</span></span>
<span id="cb14-1290"><a href="#cb14-1290" aria-hidden="true" tabindex="-1"></a>: The doubled structure of Lenat's rhetoric</span>
<span id="cb14-1291"><a href="#cb14-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1292"><a href="#cb14-1292" aria-hidden="true" tabindex="-1"></a>And more than technical, moral, and personal conviction is on the line: If Cyc really would take 1000 person-years (20 years with 50 philosopher PhDs), then it would cost about \$100 million just in human labor. The Cycorp, if it were to survive, has a strong commercial interest in rejecting all alternatives. It can be very hard to get someone to understand something, when their <span class="co">[</span><span class="ot">product differentiation</span><span class="co">](https://en.wikipedia.org/wiki/Product_differentiation)</span> depends on them not understanding it.</span>
<span id="cb14-1293"><a href="#cb14-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1294"><a href="#cb14-1294" aria-hidden="true" tabindex="-1"></a>Lenat's rejections progressed with time as each new challenger arose, applying the same tenets in different decades.</span>
<span id="cb14-1295"><a href="#cb14-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1296"><a href="#cb14-1296" aria-hidden="true" tabindex="-1"></a>In the 1980s, like other expert systems people, he aimed his rejection at the previous logical AI methods exemplified by Simon and Newell. Logical AI was a dream that a graduate student might build an AGI during a thesis period, if only they knew the "Maxwell's equations of thinking". Of course, such attempts failed, because there are no such equations. He took a little effort towards rejecting the *other* logical AI approach exemplified by <span class="co">[</span><span class="ot">@newellHumanProblemSolving1972</span><span class="co">]</span>, by constructing models that reproduced every little detail of how humans really perform in psychometric experiments, such as their reaction times, their uhhs and oopses. Admitting its interest to psychologists, he considered it a distraction for machine intelligence.</span>
<span id="cb14-1297"><a href="#cb14-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1298"><a href="#cb14-1298" aria-hidden="true" tabindex="-1"></a>In the 1990s, as the expert system hype died down, he turned his criticism towards expert systems. He recalled that, back when he was young, before academia had rejected him, he thought automated discovery with AI, such as AM and EURISKO, would lead the way to self-improving learning machines. But then he was disabused of this. BACON discovered Kepler's three laws "only" from data, but that's because Pat Langley was careful in presenting nothing but the relevant data. The cost to discover Kepler's laws on the filtered dataset? A few CPU-hours. The cost to filter the dataset? 10 Kepler-years. Similarly, AM started out with the set-theory axioms and discovered prime numbers and some famous conjectures, but quickly ended up enumerating boring complications. Lenat had to keep adding in more heuristics to get something out of it. Similarly, EURISKO would run overnight and Lenat would check its outputs in the morning, remove some bad ideas, add some good ones, and so on. The *Traveller* 1981 win was "60/40% Lenat/EURISKO" after all.</span>
<span id="cb14-1299"><a href="#cb14-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1300"><a href="#cb14-1300" aria-hidden="true" tabindex="-1"></a>Generalizing, Lenat argued that there is a common thread across all these machine learning systems. They would all start out discovering many interesting basic things, but quickly "run out of steam" enumerating boring complications. Lenat called it as systems putting up a "veneer of intelligence" while they were really just "discharging potential energy that was stored in them". That is, the creators secretly put into the program with their own expert knowledge somehow, either through the right rules, heuristics, dataset, features, or some other thing. Once the expert knowledge is "exhausted", no more discoveries could be made, and the veneer wears off. However, it makes for impressive demos, leading to cycles of hype and bust. The only escape is to prime the knowledge pump. If the knowledge base is large enough, then it wouldn't run out of steam. <span class="co">[</span><span class="ot">@lenatVoiceTurtleWhatever2008</span><span class="co">]</span></span>
<span id="cb14-1301"><a href="#cb14-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1302"><a href="#cb14-1302" aria-hidden="true" tabindex="-1"></a>Lenat's approach was not welcomed by the academics, and the feeling was mutual. AI researchers thought the Cyc project was hyped, and were unhappy with the secretive nature of Cycorp. Philosophers considered the Cyc project premature -- how could Lenat build an ontology for the world when philosophers hadn't even figured out what the ontology is? Lenat shot back, calling academics lazy, abstract, and unable to persist through decades of hard engineering work. <span class="co">[</span><span class="ot">@thompsonKnowItAllMachine2001</span><span class="co">]</span> Among the academics, the only one that still supported him was Marvin Minsky, who had no problem calling the rest of AI research "brain-dead since the 1970s", especially robotics: "Graduate students are wasting 3 years of their lives soldering and repairing robots, instead of making them smart.". <span class="co">[</span><span class="ot">@baardAIFounderBlasts2003</span><span class="co">]</span></span>
<span id="cb14-1303"><a href="#cb14-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1304"><a href="#cb14-1304" aria-hidden="true" tabindex="-1"></a>Interesting. Why the hate towards robotics? Well, during the 1990s, there were two main challengers to his idea of a symbolic-logical system. On the one side, there was the challenge of bottom-up non-symbolic reasoning promoted by Rodney Brooks' subsumption architecture <span class="co">[</span><span class="ot">@brooksElephantsDonPlay1990</span><span class="co">]</span>, and the statistical machine learning methods like support vector machines. He did not have much to say about the statistical methods -- not yet -- but he did reject the subsumption architecture as a mistaken attempt to reach AGI through robotics, much as Minsky did. Motors, sensors, etc, are simply not needed -- common sense, specified in logical language, is all you need. The hard work needed to get the robots to do anything is, you see, the *wrong* kind of hard work.</span>
<span id="cb14-1305"><a href="#cb14-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1306"><a href="#cb14-1306" aria-hidden="true" tabindex="-1"></a>On the other side, though most expert system researchers had shrunken their ambition in the winter chill, from AGI down to mere commercial survival, a few still believed in the mission. They thought that by building little systems, brick by brick, we can build towards a generally intelligent system. This is basically a "Society of Mind" approach of Marvin Minsky, and even though Lenat and Minsky liked each other's research, Lenat rejected this approach as well. One cannot settle for building common sense bit by bit, expecting a finished system to emerge, but must braid the whole thing under one roof, one upper ontology. The ontology does not have to be perfect or efficient, but there has to be one. Without it, the Society of Mind would fragment into a Tower of Babel, with little expert systems of incompatible ontologies, just like how Feigenbaum's dream of a "Library of Congress" of knowledge bases failed to materialize.</span>
<span id="cb14-1307"><a href="#cb14-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1308"><a href="#cb14-1308" aria-hidden="true" tabindex="-1"></a>In the 2000s, big data arrived with the Internet, and statistical learning became dominant. No doubt trying to preempt customers' "Why don't I just Google it?", he turned his firepower towards statistical learning systems. He never tired of pointing out that, if you make an even slightly complex query like "Is the Space Needle taller than the Eiffel Tower?", Google will happily serve up results saying "The Space Needle is 605 feet high." and "The Eiffel Tower is 1,063 feet high.", unable to actually answer your question. Despite having 15,000 servers, Google only ran dumb statistical algorithms, while Cyc running on a single server could answer it. Google-style statistical machine learning, like its trillion-token statistical machine translations systems <span class="co">[</span><span class="ot">@brantsLargeLanguageModels2007</span><span class="co">]</span>, was just pattern matching, yet another grasping after a free lunch. Such systems could not truly understand. As an alternative, he held out Cyc as the foundation to the <span class="co">[</span><span class="ot">Semantic Web</span><span class="co">](https://en.wikipedia.org/wiki/Semantic_Web)</span>, which would build a system that *would* truly understand.</span>
<span id="cb14-1309"><a href="#cb14-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1310"><a href="#cb14-1310" aria-hidden="true" tabindex="-1"></a>Curiously, right from the start, Lenat considered self-organized neural networks as not a viable path towards general intelligence, but for *the same reasons* as to why the logical AI programs of Simon and Newell would fail! From our perspective, they couldn't be more different, yet to Lenat, neural nets, General Problem Solvers, n-gram models, whatever, are all just "explicit-knowledge-free systems", <span class="co">[</span><span class="ot">too neat, not scruffy</span><span class="co">](https://en.wikipedia.org/wiki/Neats_and_scruffies)</span>, and would fail for the exact same reason.</span>
<span id="cb14-1311"><a href="#cb14-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1312"><a href="#cb14-1312" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; they are unaesthetic! And they entail person-centuries of hard knowledge-entry work. Until we are forced to them, Occam's Razor encourages us to try more elegant solutions, such as training a neural net "from scratch"; or getting an infant-simulator and then "talking to it". Only as these fail do we turn, unhappily, to the "hand-craft a huge KB" tactic.</span></span>
<span id="cb14-1313"><a href="#cb14-1313" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1314"><a href="#cb14-1314" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ...</span></span>
<span id="cb14-1315"><a href="#cb14-1315" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1316"><a href="#cb14-1316" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Our position regarding the aesthetes: There is a methodological difference between our "scruffy" way of doing AI and the aesthetes' "neat" way... If only there were a secret ingredient for intelligence--Maxwell's equations of thought. If only we could axiomatize the world in a small set of axioms, and deduce everything. If only our learning program could start from scratch. If only our neural nets were big or cerebellar or hyperlinear enough. If only the world were like that. But it isn't. The evidence indicates that almost all the power is in the bulk knowledge. As Whitehead remarked, "God is in the details."</span></span>
<span id="cb14-1317"><a href="#cb14-1317" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1318"><a href="#cb14-1318" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatThresholdsKnowledge1991</span><span class="co">]</span></span>
<span id="cb14-1319"><a href="#cb14-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1320"><a href="#cb14-1320" aria-hidden="true" tabindex="-1"></a>With the second neural network winter, he did not pay more attention to them, but as they arose yet again in the 2010s, with some exasperation, he would remind the world that, no, nothing has changed. Neural nets had already failed and they would fail. Thinking that "one large net for everything" would just work is yet another example of the logical AI fallacy that "If only we have the Maxwell's equations of learning, it will just work!". They are always "remembering and espousing", but never "understanding and inferring", and can only ever be the "right brain" to Cyc's "left brain" <span class="co">[</span><span class="ot">@lenatGettingGenerativeAI2023</span><span class="co">]</span>. As Deep Learning kept blowing past expectations, he rehashed the same 1980s argument with escalating apocalypse:</span>
<span id="cb14-1321"><a href="#cb14-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1322"><a href="#cb14-1322" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; If computers were human, they’d present themselves as autistic, schizophrenic, or otherwise brittle. It would be unwise or dangerous for that person to take care of children and cook meals, but it’s on the horizon for home robots. That’s like saying, 'We have an important job to do, but we’re going to hire dogs and cats to do it.'</span></span>
<span id="cb14-1323"><a href="#cb14-1323" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1324"><a href="#cb14-1324" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@loveMostAmbitiousArtificial2014</span><span class="co">]</span></span>
<span id="cb14-1325"><a href="#cb14-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1326"><a href="#cb14-1326" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; No matter how good your elegant theory of&nbsp;_syntax_&nbsp;and&nbsp;_semantics_&nbsp;is, there’s always this annoying residue of&nbsp;_pragmatics_, which ends up being the lower 99% of the iceberg.&nbsp; You can wish it weren’t so, and ignore it, which is easy to do because it’s out of sight (it’s not explicitly there in the letters, words, and sentences on the page, it’s lurking in the empty spaces around the letters, words, and sentences.)&nbsp; But lacking it, to any noticeable degree, gets a person labeled&nbsp;_autistic_. They may be otherwise quite smart and charming (such as Raymond in&nbsp;_Rain Man_ and Chauncey Gardiner in&nbsp;_Being There_), but it would be frankly dangerous to let them drive your car, mind your baby, cook your meals, act as your physician, manage your money, etc.&nbsp;And yet those are the very applications the world is blithely handing over to severely autistic AI programs!</span></span>
<span id="cb14-1327"><a href="#cb14-1327" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1328"><a href="#cb14-1328" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatSometimesVeneerIntelligence2017</span><span class="co">]</span></span>
<span id="cb14-1329"><a href="#cb14-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1330"><a href="#cb14-1330" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We would not be comfortable giving a severely neurologically-impaired person -- say someone with no functioning left brain hemisphere -- real-time decision-making authority over our family members' health, our life savings, our cars, or our missile defense systems. Yet we are hurtling in that direction with today’s AI's which are impaired in almost exactly that same fashion! They -- those people and those AI programs -- have trouble doing multi-step abstract reasoning, and that limitation makes their decision-making and behavior brittle, especially when confronted by unfamiliar, unexpected and unusual situations... Machine learning algorithms have scarcely changed at all, in the last 40 years... Current AI’s can form and recognize patterns, but they don’t really *understand* anything. That’s what we humans use our left brain hemispheres for.</span></span>
<span id="cb14-1331"><a href="#cb14-1331" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1332"><a href="#cb14-1332" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; ... Researchers and application builders tolerate their AI systems having just the thinnest veneer of intelligence, and that may be adequate for fast internet searching or party conversation or New York Times op-ed pieces, but that simple representation leads to inferences and answers which fall far short of the levels of competence and insight and adaptability that expert humans routinely achieve at complicated tasks, and leads to shallow explanations and justifications of those answers. There is a way out of that trap, though it’s not pleasant or elegant or easy. The solution is not a machine-learning-like "free lunch" or one clap-of-thunder insight about a clever algorithm: it requires a lot of hard work...</span></span>
<span id="cb14-1333"><a href="#cb14-1333" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1334"><a href="#cb14-1334" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatNotGoodGold2019</span><span class="co">]</span></span>
<span id="cb14-1335"><a href="#cb14-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1336"><a href="#cb14-1336" aria-hidden="true" tabindex="-1"></a>Concurrently, on the Cycorp website, two white papers published in 2021-04 reiterated their product differentiation against the false promises of <span class="co">[</span><span class="ot">neural networks</span><span class="co">](https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf)</span> and <span class="co">[</span><span class="ot">Bayesian networks</span><span class="co">](https://cyc.com/wp-content/uploads/2021/04/bayesnetspaper.pdf)</span>.<span class="ot">[^cyc-white-paper]</span> Neural networks posed a great commercial threat to their business, and the Bayesian networks, by promising to half-open the neural network black box, threatened their business as well. The papers argued that since both were not rule-based logical systems, they were not Actually Intelligent. After such fear-uncertainty-doubt, they reassured the reader that true AI needs both the left brain and the right brain, and they sell the finest left brains on the planet.</span>
<span id="cb14-1337"><a href="#cb14-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1338"><a href="#cb14-1338" aria-hidden="true" tabindex="-1"></a><span class="ot">[^cyc-white-paper]</span>:</span>
<span id="cb14-1339"><a href="#cb14-1339" aria-hidden="true" tabindex="-1"></a>    Read <span class="co">[</span><span class="ot">it</span><span class="co">](https://cyc.com/wp-content/uploads/2021/04/Cyc-Technology-Overview.pdf)</span> yourself ($\sim 3 \times 10^6 \mathrm{\mu Lenat}$) to see how hard they had to work that product differentiation. Calling it "**A**ctually **I**ntelligent", claiming "ML can *never* give an explicit step-by-step explanation of its line of reasoning behind a conclusion, but Cyc *always* can.", and insinuating that *nobody* could do Natural Language Understanding yet because none of those newfangled neural networks had any pragmatics... And this was uploaded in 2021-04, a year after GPT-3! It was written in the same <span class="co">[</span><span class="ot">FUD</span><span class="co">](https://en.wikipedia.org/wiki/Fear%2C_uncertainty%2C_and_doubt)</span>dy voice of those that still sold machine translation services after Google Neural Translate, transcription services after OpenAI Whisper, or copywriting services after ChatGPT.</span>
<span id="cb14-1340"><a href="#cb14-1340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1341"><a href="#cb14-1341" aria-hidden="true" tabindex="-1"></a>The same accusation of brain-damage that he leveled at neural networks was in fact a rehash of the exact same argument he had made against statistical machine learning systems like Cleverbot, Google, and Amazon recommender systems <span class="co">[</span><span class="ot">@loveMostAmbitiousArtificial2014</span><span class="co">]</span>, since he made no distinction between statistical methods, be it keyword matching, n-gram models, or neural networks. They are all the same veneer of intelligence, same free lunch, same shallowness.</span>
<span id="cb14-1342"><a href="#cb14-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1343"><a href="#cb14-1343" aria-hidden="true" tabindex="-1"></a>Lenat could apply the same criticisms with the same counterexamples over his 40 years of career, without needing to inspect the details of these machine learning architectures, because he had the following fully general proof, which you can discover by intersecting the previous paragraphs in this section:</span>
<span id="cb14-1344"><a href="#cb14-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1345"><a href="#cb14-1345" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Unless common sense is fully represented and integrated, an AI system is an idiot savant at most. <span class="co">[</span><span class="ot">@pantonCommonSenseReasoning2006</span><span class="co">]</span></span>
<span id="cb14-1346"><a href="#cb14-1346" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Machine-learning common sense from scratch is impossible, because learning occurs at the fringe of what one already knows. <span class="co">[</span><span class="ot">@lenatCycLargescaleInvestment1995</span><span class="co">]</span></span>
<span id="cb14-1347"><a href="#cb14-1347" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Therefore...</span>
<span id="cb14-1348"><a href="#cb14-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1349"><a href="#cb14-1349" aria-hidden="true" tabindex="-1"></a>Hedgehogs. Hedgehogs are all the same. They have one big idea, one big proof, one big theory, and continue going on with it for decades. Chomsky did it, Minsky did it, and Lenat did it too. Benefit: If they got it right, they really got it right. Cost: If they got it wrong, then they would sound like a broken record.</span>
<span id="cb14-1350"><a href="#cb14-1350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1351"><a href="#cb14-1351" aria-hidden="true" tabindex="-1"></a>For example, Lenat called expert systems "brittle" and "idiot savants" in the 1990s <span class="co">[</span><span class="ot">@lenatArtificialIntelligence1995</span><span class="co">]</span>, and statistical machine learning systems "brittle" (probably also "idiot savant") in the 2000s, and neural networks "brittle" and "autistic" since 2015 until his death.<span class="ot">[^lenetism]</span></span>
<span id="cb14-1352"><a href="#cb14-1352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1353"><a href="#cb14-1353" aria-hidden="true" tabindex="-1"></a><span class="ot">[^lenetism]: </span>Some words and phrases reappear so often in Lenat's writings that I termed them "Lenatisms" ($\sim 10^6 \mathrm{\mu Lenat/word}$) and came to hate them as much as I hate GPTisms like "delve" and "crucial": free lunch, hard work, physics envy, Maxwell's equations, clever algorithm, measles, idiot-savant, autistic, veneer of intelligence, shallow, pattern matching, brittle, understand, trustworthy, left brain, hemisphere.</span>
<span id="cb14-1354"><a href="#cb14-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1355"><a href="#cb14-1355" aria-hidden="true" tabindex="-1"></a>Similarly, he kept talking about the Winograd schema challenge, and how logically encoded common sense is the only way to solve it. He started talking about it in the 1990s <span class="co">[</span><span class="ot">@lenatCycLargescaleInvestment1995</span><span class="co">]</span>. He was still telling Stephen Wolfram in 2019 that, surely, if Cyc would team up with Wolfram Alpha, then they could finally solve the challenge for good <span class="co">[</span><span class="ot">@wolframRememberingDougLenat2023</span><span class="co">]</span>. He was *still* <span class="co">[</span><span class="ot">tweeting about it even on 2020-03-12</span><span class="co">](https://x.com/CycorpAI/status/1238183980580642816)</span>, about <span class="co">[</span><span class="ot">@sakaguchiWinoGrandeAdversarialWinograd2021</span><span class="co">]</span> which showed stated that modern LLMs (GPT-2, BERT, and a few others) was underperforming on WinoGrande, a larger version of the previous Winograd benchmark. Etc, etc.</span>
<span id="cb14-1356"><a href="#cb14-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1357"><a href="#cb14-1357" aria-hidden="true" tabindex="-1"></a>Not just his arguments were repetitive, but his "war stories" too. In 1994, Cyc could retrieve images by semantic search, so for example, it would retrieve an image of a rock climber if queried "an adventurous man" <span class="co">[</span><span class="ot">@lenatArtificialIntelligence1995</span><span class="co">]</span>. Great demo for 1994, and he would harp on this throughout the 2000s in his presentations, presumably to product-differentiate against Google-like Image Search engines. Similarly, he first recounted in 1987 of an expert system that diagnosed his rusty car with measles <span class="co">[</span><span class="ot">@lenatThresholdsKnowledge1991</span><span class="co">]</span>, then again in <span class="co">[</span><span class="ot">@lenatArtificialIntelligence1995</span><span class="co">]</span>, then again and again throughout his presentations in the 2000s, and he was *still* telling the story (and about the Winograd schema) <span class="co">[</span><span class="ot">in 2019</span><span class="co">](https://voicesinai.com/episode/episode-89-a-conversation-with-doug-lenat/)</span>.</span>
<span id="cb14-1358"><a href="#cb14-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1359"><a href="#cb14-1359" aria-hidden="true" tabindex="-1"></a>In his last paper, coauthored with Gary Marcus, he updated his critique of statistical machine learning to the LLM age. Again the shallowness, brittleness, free lunch, etc.</span>
<span id="cb14-1360"><a href="#cb14-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1361"><a href="#cb14-1361" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Given the arduous nature of the reasoning required... it is understandable almost all AI researchers and developers have gone in the opposite direction, abandoning or trivializing symbolic representation and reasoning, and instead seeking one or another sort of "free lunch" in the form of perceptrons, multi-layer neural networks and, most recently, LLMs... limiting an AI to such a narrow "baby talk" language would be a huge barrier to it ever becoming a trustworthy general AI.</span></span>
<span id="cb14-1362"><a href="#cb14-1362" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb14-1363"><a href="#cb14-1363" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@lenatGettingGenerativeAI2023</span><span class="co">]</span></span>
<span id="cb14-1364"><a href="#cb14-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1365"><a href="#cb14-1365" aria-hidden="true" tabindex="-1"></a>I am struck by the irony that a veteran of logical AI would call neural networks "brittle", or make an appeal to sunk cost. Lenat had devoted 2000 person-years to the project, therefore a "free lunch" shouldn't work, nevermind the fact that these "free" lunches took 20 years of gritty battles to build the datasets,<span class="co">&lt;!-- todo: add the link to essay on datasets once it's done. --&gt;</span> struggles with the cussedness of CUDA,<span class="co">&lt;!-- todo: add the link to essay on CUDA once it's done. --&gt;</span> waking up to yet another divergent overnight training run, staring at tensors filled with <span class="dt">&lt;</span><span class="kw">code</span><span class="dt">&gt;</span>NaNs<span class="dt">&lt;/</span><span class="kw">code</span><span class="dt">&gt;</span>, and eventually <span class="co">[</span><span class="ot">cost \$100 million</span><span class="co">](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)</span> per serving, roughly the total budget of Cycorp through its life. How dare you to go "free lunch" on us... But *de mortuis nil nisi bonum*.</span>
<span id="cb14-1366"><a href="#cb14-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1367"><a href="#cb14-1367" aria-hidden="true" tabindex="-1"></a>Lenat died in 2023, unmourned on <span class="co">[</span><span class="ot">Lucid AI</span><span class="co">](https://lucid.ai/)</span> and <span class="co">[</span><span class="ot">Cycorp</span><span class="co">](https://cyc.com/)</span>, who, like ABBYY, still proudly advertise their product differentiation to this day.</span>
<span id="cb14-1368"><a href="#cb14-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1369"><a href="#cb14-1369" aria-hidden="true" tabindex="-1"></a><span class="fu">## In lieu of a conclusion</span></span>
<span id="cb14-1370"><a href="#cb14-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1371"><a href="#cb14-1371" aria-hidden="true" tabindex="-1"></a>Napoleon died in 1821. Wellington was greatly saddened.</span>
<span id="cb14-1372"><a href="#cb14-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1373"><a href="#cb14-1373" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Plutarch has related that Julius Caesar wept for the death of Pompey; Aurelian did not weep for the death of John, but he felt what a man would feel when rid of an incurable disease that had become a part of his life.</span></span>
<span id="cb14-1374"><a href="#cb14-1374" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb14-1375"><a href="#cb14-1375" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; --- Borges, *The Theologians*</span></span>
<span id="cb14-1376"><a href="#cb14-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1377"><a href="#cb14-1377" aria-hidden="true" tabindex="-1"></a>*Come as you are, as you were  </span>
<span id="cb14-1378"><a href="#cb14-1378" aria-hidden="true" tabindex="-1"></a>As I want you to be  </span>
<span id="cb14-1379"><a href="#cb14-1379" aria-hidden="true" tabindex="-1"></a>As a friend, as a friend  </span>
<span id="cb14-1380"><a href="#cb14-1380" aria-hidden="true" tabindex="-1"></a>As an old enemy  </span>
<span id="cb14-1381"><a href="#cb14-1381" aria-hidden="true" tabindex="-1"></a>Take your time, hurry up  </span>
<span id="cb14-1382"><a href="#cb14-1382" aria-hidden="true" tabindex="-1"></a>Choice is yours, don't be late  </span>
<span id="cb14-1383"><a href="#cb14-1383" aria-hidden="true" tabindex="-1"></a>Take a rest, as a friend  </span>
<span id="cb14-1384"><a href="#cb14-1384" aria-hidden="true" tabindex="-1"></a>As an old memoria...*</span>
<span id="cb14-1385"><a href="#cb14-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1386"><a href="#cb14-1386" aria-hidden="true" tabindex="-1"></a><span class="fu">## Updates {.appendix}</span></span>
<span id="cb14-1387"><a href="#cb14-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1388"><a href="#cb14-1388" aria-hidden="true" tabindex="-1"></a>Some reviews:</span>
<span id="cb14-1389"><a href="#cb14-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1390"><a href="#cb14-1390" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">on Twitter</span><span class="co">](https://x.com/layer07_yuxi/status/1907179435079717134)</span></span>
<span id="cb14-1391"><a href="#cb14-1391" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">On r/mlscaling</span><span class="co">](https://old.reddit.com/r/mlscaling/comments/1juosnn/cyc_obituary_for_the_greatest_monument_to_logical/)</span></span>
<span id="cb14-1392"><a href="#cb14-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Hacker News</span><span class="co">](https://news.ycombinator.com/item?id=43625474)</span></span>
<span id="cb14-1393"><a href="#cb14-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="co">[</span><span class="ot">Lobste.rs</span><span class="co">](https://lobste.rs/s/ab6qap/obituary_for_cyc)</span></span>
<span id="cb14-1394"><a href="#cb14-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1395"><a href="#cb14-1395" aria-hidden="true" tabindex="-1"></a>On advice of Gwern, I emailed the Cycorp with some questions, but they never replied.</span>
<span id="cb14-1396"><a href="#cb14-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1397"><a href="#cb14-1397" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Fulltext of the email" collapse="true" }</span>
<span id="cb14-1398"><a href="#cb14-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1399"><a href="#cb14-1399" aria-hidden="true" tabindex="-1"></a>To Whom It May Concern,</span>
<span id="cb14-1400"><a href="#cb14-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1401"><a href="#cb14-1401" aria-hidden="true" tabindex="-1"></a>I hope this message finds you well.</span>
<span id="cb14-1402"><a href="#cb14-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1403"><a href="#cb14-1403" aria-hidden="true" tabindex="-1"></a>I am writing to inquire about several aspects of Cycorp’s ongoing work and organizational status, particularly with respect to the Cyc knowledge base and its applications. I would greatly appreciate your response to the following questions:</span>
<span id="cb14-1404"><a href="#cb14-1404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1405"><a href="#cb14-1405" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Knowledge Base Size and Cost</span>
<span id="cb14-1406"><a href="#cb14-1406" aria-hidden="true" tabindex="-1"></a>What is the current approximate size of the Cyc knowledge base (e.g., number of assertions or facts), and what is the estimated average cost per new fact entry (either historical or current)?</span>
<span id="cb14-1407"><a href="#cb14-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1408"><a href="#cb14-1408" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Organizational Leadership</span>
<span id="cb14-1409"><a href="#cb14-1409" aria-hidden="true" tabindex="-1"></a>Who currently leads Cycorp, both at the executive level and in terms of technical or research direction?</span>
<span id="cb14-1410"><a href="#cb14-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1411"><a href="#cb14-1411" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Open Source Strategy</span>
<span id="cb14-1412"><a href="#cb14-1412" aria-hidden="true" tabindex="-1"></a>Does Cycorp have any plans to release additional components of Cyc as open source in the future, following the prior release of OpenCyc?</span>
<span id="cb14-1413"><a href="#cb14-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1414"><a href="#cb14-1414" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Benchmarking and Evaluation</span>
<span id="cb14-1415"><a href="#cb14-1415" aria-hidden="true" tabindex="-1"></a>Has Cycorp evaluated its system on publically available and standardized linguistic benchmarks, such as MMLU, GPQA, ARC, or other reasoning-intensive tasks? If so, could you share any published or unpublished results?</span>
<span id="cb14-1416"><a href="#cb14-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1417"><a href="#cb14-1417" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Ongoing Partnerships</span>
<span id="cb14-1418"><a href="#cb14-1418" aria-hidden="true" tabindex="-1"></a>Is Cycorp still maintaining any collaborations or deployments in clinical or hospital environments, similar to the previously reported instance with the Cleveland Clinic?</span>
<span id="cb14-1419"><a href="#cb14-1419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1420"><a href="#cb14-1420" aria-hidden="true" tabindex="-1"></a>I am asking in the context of a comparative evaluation of symbolic and sub-symbolic systems, and I would be grateful for any technical details or references you may be able to provide.</span>
<span id="cb14-1421"><a href="#cb14-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1422"><a href="#cb14-1422" aria-hidden="true" tabindex="-1"></a>Thank you very much for your time and attention.</span>
<span id="cb14-1423"><a href="#cb14-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1424"><a href="#cb14-1424" aria-hidden="true" tabindex="-1"></a>--</span>
<span id="cb14-1425"><a href="#cb14-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1426"><a href="#cb14-1426" aria-hidden="true" tabindex="-1"></a>Regards,  </span>
<span id="cb14-1427"><a href="#cb14-1427" aria-hidden="true" tabindex="-1"></a>Yuxi Liu  </span>
<span id="cb14-1428"><a href="#cb14-1428" aria-hidden="true" tabindex="-1"></a>UC Berkeley CS PhD  </span>
<span id="cb14-1429"><a href="#cb14-1429" aria-hidden="true" tabindex="-1"></a><span class="ot">&lt;https://yuxi-liu-wired.github.io/&gt;</span></span>
<span id="cb14-1430"><a href="#cb14-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1431"><a href="#cb14-1431" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>