<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuxi Liu">
<meta name="dcterms.date" content="2023-12-21">
<meta name="description" content="A long, hard stare into the math of Perceptrons, the mythical neural network killer.">

<title>Reading Perceptrons – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Reading Perceptrons – Yuxi on the Wired">
<meta property="og:description" content="A long, hard stare into the math of Perceptrons, the mythical neural network killer.">
<meta property="og:image" content="https://yuxi.ml/essays/posts/reading-perceptron-book/figure/banner_2.png">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta property="og:image:height" content="1024">
<meta property="og:image:width" content="1792">
<meta property="og:image:alt" content="The cover of 'Project SOGO: Principles of Hypnotic Bionics'. It is a combination of 'Project PARA: Principles of Neurodynamics' and 'Shoggoth' from Lovecraft's cosmology. The idea is that the book is mythical like the Necronomicon (see Minsky's quote). In the style of a cold-war era RAND corporation report.">
<meta name="twitter:title" content="Reading Perceptrons – Yuxi on the Wired">
<meta name="twitter:description" content="A long, hard stare into the math of Perceptrons, the mythical neural network killer.">
<meta name="twitter:image" content="https://yuxi.ml/essays/posts/reading-perceptron-book/figure/banner_2.png">
<meta name="twitter:image-height" content="1024">
<meta name="twitter:image-width" content="1792">
<meta name="twitter:image:alt" content="The cover of 'Project SOGO: Principles of Hypnotic Bionics'. It is a combination of 'Project PARA: Principles of Neurodynamics' and 'Shoggoth' from Lovecraft's cosmology. The idea is that the book is mythical like the Necronomicon (see Minsky's quote). In the style of a cold-war era RAND corporation report.">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Reading <em>Perceptrons</em></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          A long, hard stare into the math of <em>Perceptrons</em>, the mythical neural network killer.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
                <div class="quarto-category">math</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Yuxi Liu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 21, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">December 29, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#setup" id="toc-setup" class="nav-link active" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#over-finite-sets" id="toc-over-finite-sets" class="nav-link" data-scroll-target="#over-finite-sets">Over finite sets</a></li>
  <li><a href="#over-infinite-spaces" id="toc-over-infinite-spaces" class="nav-link" data-scroll-target="#over-infinite-spaces">Over infinite spaces</a></li>
  <li><a href="#learning-theory" id="toc-learning-theory" class="nav-link" data-scroll-target="#learning-theory">Learning theory</a></li>
  <li><a href="#chapter-13" id="toc-chapter-13" class="nav-link" data-scroll-target="#chapter-13">Chapter 13</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<blockquote class="blockquote">
<p>It would seem that <em>Perceptrons</em> has much the same role as The Necronomicon – that is, often cited but never read.</p>
<p>Marvin Minsky, 1994. Quoted in <span class="citation" data-cites="berkeleyRevisionistHistoryConnectionism1997">(<a href="#ref-berkeleyRevisionistHistoryConnectionism1997" role="doc-biblioref">Berkeley 1997</a>)</span></p>
</blockquote>
<p>During my early forays into AI, perhaps ten years ago, I heard rumors of how in the 1960s, there was a bitter controversy between the symbolic AI and the neural network AI schools. Furthermore, it was said that a certain book, <em>Perceptions</em>, was the center of the controversy – a deadly weapon that killed neural networks – until their resurgence in the 1980s.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> What, then, is in the mythical book? Recently, I have returned to this alleged episode, and in my research into the bottom of the perceptron controversy, I ended up actually reading the book. Now that I have done so, I proclaim:</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;The story would continue, with the vain dream of Japanese Fifth Generation Project and expert systems, both of which would never come to anything… And that was where the story ended! Deep learning? In 2013, neural nets were only a mirage. Sure, there might have been reports of early success, but it was not rising above the periodic tide of news stories of yet another miracle cancer drug that would never amount to anything, with the same feeling of “Once more unto the breach, my friends…”</p></div></div><blockquote class="blockquote">
<p>You, too, can read the <em>Necronomicon</em>.</p>
</blockquote>
<p>In one sentence, the mathematical portion of <em>Perceptrons</em> is a theory of two-layered perceptrons, mostly by methods typical of discrete mathematics and computational complexity theory, and no empirical results. One should forget about the modern theory and practice of neural networks, read the book in the same frame of mind as one would read a textbook on Turing machines, stack machines, Post tag systems, and other various theoretical objects in computational complexity theory. Indeed, this book is written in the same spirit and style as <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967</a>)</span> and <span class="citation" data-cites="sipserIntroductionTheoryComputation2006">(<a href="#ref-sipserIntroductionTheoryComputation2006" role="doc-biblioref">Sipser 2006</a>)</span>.</p>
<p>Perceptron representation occupies chapters 0–10, and learning is only studied in chapters 11 and 12. The book also contained a chapter 13, but it contains mostly interpretations and brief “lessons” that they wanted the readers to take away from the exact mathematical results. They added some handwritten corrections and updates for a 1972 printing run.</p>
<p>During the 1980s rise of connectionism, Minsky and Papert came out with a new edition in 1988. This is the same as the 1972 printing, except that they added a prologue and an epilogue, where they expounded at length the intended lesson of the book, as they felt that people have failed to learn it, and history was repeating itself.</p>
<p>For those interpretations, see <a href="https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/"><em>The Perceptron Controversy</em></a>.</p>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>Let <span class="math inline">\(R\)</span> be a finite set, where “R” can be read as “region” or “rectangle”.</p>
<div id="def-perceptron" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Perceptron)</strong></span> A perceptron is a binary function of type <span class="math inline">\(\{0, 1\}^R \to \{0, 1\}\)</span>, defined by a weight vector <span class="math inline">\(w\)</span> and a threshold number <span class="math inline">\(b\)</span>:</p>
<p><span class="math display">\[
\Phi(x) := \theta(w^T x + b)
\]</span></p>
<p>where <span class="math inline">\(\theta(t) := 1_{t \geq 0}\)</span> is the 0-1 step function.</p>
</div>
<div id="def-perceptron-machine" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 (perceptron machine)</strong></span> A perceptron machine with <span class="math inline">\(k\)</span> hidden neurons is a function of type <span class="math inline">\(\{0, 1\}^R \to \{0, 1\}\)</span>, defined by</p>
<p><span class="math display">\[
\Phi(x) := \psi_{k+1}(\psi_1(x), \psi_2(x), \dots , \psi_k(x))
\]</span></p>
<p>where <span class="math inline">\(\psi_1, \dots, \psi_k\)</span> are (hidden) perceptrons in the hidden layer, and <span class="math inline">\(\psi_{k+1}\)</span> is the single output perceptron.</p>
</div>
<div id="def-predicate-order" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 (perceptron orders)</strong></span> The order of a hidden perceptron is the number of nonzero weights.</p>
<p>The order of a perceptron machine is the maximum order of its hidden perceptrons.</p>
<p>The order of a boolean function is the minimum order necessary for a perceptron machine that implements it.</p>
</div>
<p>For example, the constant-0 and constant-1 boolean functions are both of order 0.</p>
<p>A key focus of the perceptron controversy is the concept of being “conjunctively local”.</p>
<div id="def-predicate-order" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 (conjunctively local)</strong></span> A family of boolean functions is conjunctively local iff their orders are upper bounded.</p>
</div>
</section>
<section id="over-finite-sets" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="over-finite-sets">Over finite sets</h2>
<p>While their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">computational reduction</a> to the special case of mask perceptron machines. Most of the book considers the problem of classifying subsets of families of planar square grids, such as <span class="math inline">\(\{1, 2, 3\} \times \{1, 2, 3, 4\}\)</span>. A subset is inputted to the perceptron machine by setting inputs in the subset to <span class="math inline">\(1\)</span>, and the rest to <span class="math inline">\(0\)</span>. Consequently, it is natural to consider this special case of perceptron machines.</p>
<p>While their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a <a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">computational reduction</a> to the special case of the mask perceptron machine. Most of the book considers the problem of classifying subsets of families of planar square grids, such as <span class="math inline">\(\{1, 2, 3\} \times \{1, 2, 3, 4\}\)</span>. A subset is input to the perceptron machine by setting inputs in the subset to <span class="math inline">\(1\)</span>, and the rest to <span class="math inline">\(0\)</span>. Consequently, it is natural to consider this special case of perceptron machines.</p>
<div id="def-predicate-order" class="theorem definition page-columns page-full">
<p><span class="theorem-title"><strong>Definition 5 (mask perceptron machine)</strong></span> A mask for <span class="math inline">\(A\subset R\)</span> is a function of type <span class="math inline">\(\{0, 1\}^R \to \mathbb{R}\)</span>, such that <span class="math inline">\(\psi(x) = 1\)</span> if <span class="math inline">\(x_i = 1\)</span> for all <span class="math inline">\(i \in A\)</span>, and otherwise <span class="math inline">\(\psi(x) = 0\)</span>.</p>
<p>A mask perceptron machine is a perceptron machine where each hidden perceptron is a mask, and the threshold of the output perceptron is zero. In other words, it is of form</p>
<p><span class="math display">\[
\Phi(x) = \theta\left(\sum_{i=1}^k a_i \psi_{A_i}(x)\right)
\]</span></p>
<p>where each <span class="math inline">\(\psi_{A_i}\)</span> is a mask, each <span class="math inline">\(a_i\in \mathbb{R}\)</span>, and <span class="math inline">\(k\)</span> is the number of hidden perceptrons.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_fig_0_2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Figure 0.2</figcaption>
</figure>
</div>
</div>
<div id="thm-predicate-order" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Theorem 1.5.1)</strong></span> Restricting to mask perceptron machines does not change perceptron orders. That is, any boolean function implemented by a perceptron machine of order <span class="math inline">\(k\)</span> can be implemented by a mask perceptron machine of order at most <span class="math inline">\(k\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Take one such perceptron machine. The threshold of the output perceptron can be removed by adding a hidden perceptron that always outputs <span class="math inline">\(1\)</span> – in other words, <span class="math inline">\(\psi_\emptyset\)</span>, the mask of the empty set. Now it remains to decompose each hidden perceptron into masks with the same order, or less.</p>
<p>Let <span class="math inline">\(\psi\)</span> be a hidden perceptron with nonzero weights on the input points <span class="math inline">\(x_{i_1}, \dots, x_{i_k}\)</span>; then, its output is determined by the values of <span class="math inline">\(x_{i_1}, \dots, x_{i_k}\)</span>. Therefore, we can partition the binary set <span class="math inline">\(\{0, 1\}^{i_1, \dots, i_k}\)</span> into two subsets <span class="math inline">\(A_0, A_1\)</span>, such that for any input <span class="math inline">\(x\in\{0, 1\}^R\)</span>, we have <span class="math inline">\(\psi(x) = 1\)</span> iff <span class="math inline">\((x_{i_1}, \dots, x_{i_k}) \in A_1\)</span>.</p>
<p>In other words, we only need to look at the binary values <span class="math inline">\(x_{i_1}, \dots, x_{i_k}\)</span> to determine the binary output <span class="math inline">\(\psi(x)\)</span>.</p>
<p>Therefore, we can replace <span class="math inline">\(\psi\)</span> with a boolean formula on <span class="math inline">\(x_{i_1}, \dots, x_{i_k}\)</span>, then expand it to obtain up to <span class="math inline">\(2^k\)</span> masks, each of order at most <span class="math inline">\(k\)</span>.</p>
<p>For example, suppose <span class="math inline">\(\psi\)</span> has nonzero weights on <span class="math inline">\(x_1, x_2\)</span>, and is 1 on all odd-sized subsets, then we can write it as a boolean formula:</p>
<p><span class="math display">\[
\left(x_1 \wedge \neg x_2\right) \vee\left(\neg x_1 \wedge x_2\right) = x_1\left(1-x_2\right)+\left(1-x_1\right) x_2 = x_1 + x_2 - 2 x_1 x_2
\]</span></p>
</div>
<p>The next tool they used is symmetry, formulated in the language of <a href="https://en.wikipedia.org/wiki/Group_action">finite group actions</a>.</p>
<p>Let <span class="math inline">\(S_R\)</span> be the permutation group on the elements of <span class="math inline">\(R\)</span>, and <span class="math inline">\(G\)</span> be a subgroup of <span class="math inline">\(S_R\)</span>. We say that a boolean function <span class="math inline">\(\psi\)</span> is <span class="math inline">\(G\)</span>-invariant iff <span class="math inline">\(\psi \circ g=\psi\)</span> for any <span class="math inline">\(g \in G\)</span>. That is, for any <span class="math inline">\(X \subset R\)</span>, we have <span class="math inline">\(\psi(X) = \psi(g(X))\)</span>. For example, the parity function is <span class="math inline">\(S_R\)</span>-invariant, since any permutation of the set preserves the size, and thus the parity, of any of its subsets.</p>
<div id="thm-group-invariance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (group invariance theorem)</strong></span> If a boolean function is <span class="math inline">\(G\)</span>-invariant, where <span class="math inline">\(G\)</span> is a finite group, then any perceptron machine computing it can be converted to a perceptron machine <span class="math inline">\(\theta(\sum_i a_i \psi_i)\)</span>, such that if <span class="math inline">\(\psi_i = \psi_j \circ g\)</span> for some <span class="math inline">\(g \in G\)</span>, then <span class="math inline">\(a_i = a_j\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Take the group-action average: any mask <span class="math inline">\(\psi\)</span> is equal to <span class="math inline">\(\frac{1}{|G|} \sum_{g\in G} \psi\circ g\)</span>.</p>
</div>
<p>Once the groundwork was laid, they proceeded to prove a wide variety of theorems on the order of particular boolean functions.</p>
<p>Consider the parity function. It is <span class="math inline">\(1\)</span> iff exactly an odd number of inputs are <span class="math inline">\(1\)</span> and the rest are <span class="math inline">\(0\)</span>.</p>
<div id="thm-parity-order" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Theorem 3.1)</strong></span> The parity function has order <span class="math inline">\(|R|\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Since the parity function is <span class="math inline">\(S_R\)</span>-invariant, if it is implemented by a perceptron machine of order <span class="math inline">\(k\)</span>, it is implemented by some mask perceptron machine <span class="math inline">\(\theta(\sum_{A_i \subset R} a_{A_i} \psi_{A_i})\)</span>, where each mask is of size <span class="math inline">\(\leq k\)</span>, and each weight <span class="math inline">\(a_{A_i}\)</span> depends only on the size of <span class="math inline">\(A_i\)</span>. Let <span class="math inline">\(b_{|A_i|} = a_{A_i}\)</span> be those coefficients. It remains to show <span class="math inline">\(b_{|R|} \neq 0\)</span>.</p>
<p>For each <span class="math inline">\(X \subset R\)</span>, we have by explicit computation</p>
<p><span class="math display">\[
\theta\left(\sum_{A_i \subset R} a_{A_i} \psi_{A_i}\right) = 1\left[f(|X|) \geq 0\right]
\]</span></p>
<p>where <span class="math inline">\(f(t) := \sum_{i=0}^{|R|}b_i \binom{t}{i}\)</span> is a polynomial in <span class="math inline">\(t\)</span>. Since this perceptron machine implements the parity function, as <span class="math inline">\(t\)</span> increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(|R|\)</span>, the function <span class="math inline">\(f(t) + \epsilon\)</span> must intersect the <span class="math inline">\(x\)</span>-axis at least <span class="math inline">\(|R|\)</span> times for some real value <span class="math inline">\(\epsilon\)</span>. Since <span class="math inline">\(f\)</span> is a polynomial, it must have at least order <span class="math inline">\(|R|\)</span>, thus <span class="math inline">\(b_{|R|} \neq 0\)</span>.</p>
</div>
<div id="thm-one-in-a-box" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 (Theorem 3.2, one-in-a-box)</strong></span> Let <span class="math inline">\(A_1, A_2, \dots, A_m\)</span> be disjoint subsets of <span class="math inline">\(R\)</span>, each of size <span class="math inline">\(4 m^2\)</span>, and define the predicate <span class="math inline">\(\psi(X) = \forall i, \left|X \cap A_i\right|&gt;0\)</span>; that is, there is at least one point of <span class="math inline">\(X\)</span> in each <span class="math inline">\(A_i\)</span>. The order of <span class="math inline">\(\psi\)</span> is <span class="math inline">\(\geq m\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let the order of <span class="math inline">\(\psi\)</span> be <span class="math inline">\(k\)</span>.</p>
<p>The predicate <span class="math inline">\(\psi\)</span> is invariant with respect to the group <span class="math inline">\(S_{A_1} \times \cdots \times S_{A_m}\)</span>, so by the same construction as the proof of <a href="#thm-group-invariance" class="quarto-xref">Theorem&nbsp;2</a>, there exists a polynomial <span class="math inline">\(P(t_1, \dots, t_m)\)</span>, where <span class="math inline">\(P\)</span> has order <span class="math inline">\(k\)</span>, and</p>
<p><span class="math display">\[
\forall t_1, \dots , t_m \in \{0, 1, \dots , 4m^2\}, P(t_1, \dots , t_m) &lt; 0 \iff t_1 = 0 \vee \cdots \vee t_m = 0
\]</span></p>
<p>Now define <span class="math inline">\(Q(t) := P((t-1)^2, (t-3)^2, \dots, (t-2m+1)^2)\)</span>. By the above equation, <span class="math inline">\(Q &lt; 0\)</span> at <span class="math inline">\(t=1, 3, \dots, 2m - 1\)</span> and <span class="math inline">\(Q \geq 0\)</span> at <span class="math inline">\(t = 0, 2, \dots, 2m\)</span>. Thus, <span class="math inline">\(Q\)</span> has order <span class="math inline">\(\geq 2m\)</span>. Thus, <span class="math inline">\(2k \geq 2m\)</span>.</p>
</div>
<div id="thm-and-or" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5 (Theorem 4.0)</strong></span> There exist predicates <span class="math inline">\(\psi_1\)</span> and <span class="math inline">\(\psi_2\)</span> of order 1 such that <span class="math inline">\(\psi_1 \wedge \psi_2\)</span> and <span class="math inline">\(\psi_1 \vee \psi_2\)</span> are not of finite order. Specifically, if we partition <span class="math inline">\(R\)</span> into three equal subsets <span class="math inline">\(A, B, C\)</span>, then the boolean function does not have bounded order:</p>
<p><span class="math display">\[
(|X \cap A| &gt; |X \cap C|) \wedge (|X \cap B| &gt; |X \cap C|)
\]</span></p>
<p>even though both <span class="math inline">\(|X \cap A| &gt; |X \cap C|\)</span> and <span class="math inline">\(|X \cap B| &gt; |X \cap C|\)</span> are of order <span class="math inline">\(1\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math inline">\(|X \cap A| &gt; |X \cap C|\)</span> is computed by the order-<span class="math inline">\(1\)</span> perceptron machine <span class="math inline">\(\theta\left(\sum_{i\in A} x_i - \sum_{i \in C}x_i\right)\)</span>, and similarly for the other one.</p>
<p>To show the composite boolean function is not of bounded order, suppose otherwise, then by the same argument as the previous proof, we can construct a sequence of polynomials <span class="math inline">\(P_1(a, b, c), P_2(a, b, c), P_3(a, b, c), \dots\)</span>, such that each <span class="math inline">\(P_n\)</span> is the polynomial corresponding to the perceptron machine for the case where <span class="math inline">\(|A| = |B| = |C| = n\)</span>, and each of them has order at most <span class="math inline">\(M\)</span>, for some fixed <span class="math inline">\(M\)</span>.</p>
<p>Being the polynomial corresponding to the perceptron machine for the case where <span class="math inline">\(|A| = |B| = |C| = n\)</span> means precisely that</p>
<p><span class="math display">\[
a &gt; c \wedge b &gt; c \implies P_n(a, b, c) \geq 0; \neg (a &gt; c \wedge b &gt; c) \implies P_n(a, b, c) &lt; 0;
\]</span></p>
<p>for all <span class="math inline">\(a, b, c \in \{0, 1, \dots, n\}\)</span>. This implies that each <span class="math inline">\(P_1, P_2, \dots \neq 0\)</span>. Now, divide each polynomial by the root-sum-square of its coefficients, so that if we interpret the coefficients of each <span class="math inline">\(P_n\)</span> as a point in a high-dimensional space, the point falls on the unit sphere in that space. Since the unit sphere is compact, there exists a limit point, which we write as <span class="math inline">\(P(a, b, c)\)</span>.</p>
<p>By the limit construction, we have</p>
<p><span class="math display">\[
\forall a, b, c \in \mathbb{N}, \quad a &gt; c \wedge b &gt; c \implies P(a, b, c) \geq 0; \neg (a &gt; c \wedge b &gt; c) \implies P(a, b, c) \leq 0;
\]</span></p>
<p>If we color the points <span class="math inline">\(\mathbb{N}^3\)</span> with black for <span class="math inline">\(P &lt; 0\)</span> and white for <span class="math inline">\(P \geq 0\)</span>, then we can see the dusty outlines of a pyramid. We can construct a solid pyramid by zooming.</p>
<p>Let <span class="math inline">\(M'\)</span> be the order of <span class="math inline">\(P\)</span>, then we can “zoom out” by taking the <a href="https://en.wikipedia.org/wiki/Homogeneous_function">projective limit</a> <span class="math inline">\(Q(a, b, c) := \lim_{t \to \infty} t^{-M'} P(ta, tb, tc)\)</span>. This <span class="math inline">\(Q\)</span> is a homogeneous polynomial, and by continuity,</p>
<p><span class="math display">\[
\forall a, b, c \geq 0, \quad a &gt; c \wedge b &gt; c \implies P(a, b, c) \geq 0; a &lt; c \vee b &lt; c \implies P(a, b, c) \leq 0;
\]</span></p>
<p>This implies that <span class="math inline">\(P\)</span> is identically zero on the “creased curve” <span class="math inline">\(\{ a, b, c \geq 0, a = c \vee b = c\}\)</span> in the projective plane, which is not an algebraic curve, therefore it is identically zero, contradiction.</p>
</div>
<div id="thm-connectedness-order" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6 (Theorem 5.1)</strong></span> The connectedness function has order <span class="math inline">\(\Omega(|R|^{1/3})\)</span>.</p>
</div>
<div class="proof page-columns page-full">
<p><span class="proof-title"><em>Proof</em>. </span>If we have a mask perceptron machine that can solve the connectedness problem, then it can be repurposed<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> to solve the one-in-a-box problem of the following kind:</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Repurposing one machine to solve another problem is a common trick in computational complexity, called “<a href="https://en.wikipedia.org/wiki/Reduction_(complexity)">reduction</a>”. For perceptron machines, they called it “Theorem 5.4.1: The Collapsing Theorem”.</p></div></div><div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_fig_5_2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Figure 5.2</figcaption>
</figure>
</div>
<p>In the picture, the rectangle <span class="math inline">\(R\)</span> has width <span class="math inline">\(4m^2\)</span> and height <span class="math inline">\(2m+1\)</span>. We fill in all the odd-numbered rows, and use the machine to solve the one-in-a-box problem for the even-numbered rows. By <a href="#thm-one-in-a-box" class="quarto-xref">Theorem&nbsp;4</a>, the machine has order <span class="math inline">\(\geq m = \Omega(|R|^{1/3})\)</span>.</p>
</div>
<p>In fact, it turns out that essentially the only locally conjunctive topological invariant is the Euler number.</p>
<div id="thm-euler-number" class="theorem">
<p><span class="theorem-title"><strong>Theorem 7 (Theorem 5.8.1)</strong></span> The Euler number itself is <span class="math inline">\(E(X) = \sum_{i \in R} x_i - \sum_{i, j \in R} x_ix_j + \sum_{i, j, k, l  \in R} x_ix_jx_kx_l\)</span>, where the <span class="math inline">\(i, j\in R\)</span> ranges only over adjacent points, and <span class="math inline">\(i, j, k, l  \in R\)</span> ranges only over quadruples that form a square. Thus the Euler number itself has order <span class="math inline">\(4\)</span>.</p>
</div>
<div id="thm-euler-number" class="theorem">
<p><span class="theorem-title"><strong>Theorem 8 (Theorem 5.9)</strong></span> If <span class="math inline">\(\psi\)</span> is a boolean function, and it depends only on the topology of the input figure, and its order is upper-bounded by some number <span class="math inline">\(k\)</span> that does not grow even as <span class="math inline">\(R\)</span> grows into a larger and larger rectangle, then <span class="math inline">\(\psi\)</span> is of form <span class="math inline">\(f \circ E\)</span>, for some function <span class="math inline">\(f: \mathbb{N}\to 2\)</span>.</p>
</div>
</section>
<section id="over-infinite-spaces" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="over-infinite-spaces">Over infinite spaces</h2>
<p>Chapters 6–9 continue in the same style, but move to the case where the input space is made of one or two copies of the infinite line <span class="math inline">\(\mathbb{Z}\)</span>, or the infinite plane <span class="math inline">\(\mathbb{Z}^2\)</span>, and the predicate to recognize is still translation-invariant. In order to avoid blowing up the perceptron machine with infinities, they restricted to the case where the input figure is of finite size, meaning that <span class="math inline">\(\sum_i x_i\)</span> is finite.</p>
<p>Chapter 6 develops the idea of “spectra” of images. For example, the following picture shows that if we were to design a perceptron machine using only masks of size up to 2, and translation-invariant weights, then it cannot possibly distinguish between the two tetris-like pieces, because both figures contain exactly 4 instances of single-square, 1 instance of two squares side by side, and so on.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_chap_6.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">page 100</figcaption>
</figure>
</div>
<p>Sections 6.1–6.5 of the chapter studies the same idea for masks of size up to 4. For example, Section 6.4 shows that “<span class="math inline">\(X\)</span> is the perimeter of a complete circle” is of order <span class="math inline">\(4\)</span>.</p>
<p>Section 6.6 claims that “recognizing figures in context” is generally not locally conjunctive, although they gave only two examples. Specifically, they showed that the predicate “<span class="math inline">\(X\)</span> is a horizontal line across the rectangle” is order 2, the predicate “<span class="math inline">\(X\)</span> contains one horizontal line across the plane” is not locally conjunctive. The same is true for the case with “a hollow square” instead of “a horizontal line”.</p>
<p>Chapter 7 uses the “stratification” construction to explicitly demonstrate that several binocular predicates are of finite order, but requires exponentially growing weights. It is essentially the same idea as <a href="https://en.wikipedia.org/wiki/G%C3%B6del_numbering">Gödel numbering</a>. A single illustrative example suffices to demonstrate the general point.</p>
<div id="exm-line-symmetry" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 1</strong></span> Given a line <span class="math inline">\(\mathbb{Z}\)</span>, how to construct a perceptron machine that detects that input figure is symmetric?</p>
<p>Suppose we know for certain that the input figure has leftmost point <span class="math inline">\(m\)</span> and rightmost point <span class="math inline">\(n\)</span>, then we can test for symmetry by computing the value of:</p>
<p><span class="math display">\[
f_{m, n}(x) := \sum_{i = 0}^{n-m} x_{m+i}(x_{n-i} - 1)
\]</span></p>
<p>We have that <span class="math inline">\(f_{m, n}(x) = 0\)</span> if the figure is symmetric, and <span class="math inline">\(f_{m, n}(x) \leq -1\)</span> otherwise.</p>
<p>Now we define the entire perceptron machine by <span class="math inline">\(\sum_{m \leq n} M_{n - m} x_m x_n (f_{m, n}(x) - 1/2)\)</span>. If the sequence of <span class="math inline">\(M_0, M_1, \dots\)</span> grows as <span class="math inline">\((d!)^2\)</span> roughly <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, then the largest bracket <span class="math inline">\((m, n)\)</span> would “veto” every smaller bracket contained within, and so the entire perceptron machine is effectively first testing for the smallest interval containing the figure, before testing the symmetry within that interval.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Expanding term by term, we have <span class="math inline">\(|f_{m, n}(x) - 1/2| \leq 2(n-m) + \frac{1}{2}\)</span>. Therefore, in order for <span class="math inline">\(M_d\)</span> to “veto” every other bracket within, we need</p>
<p><span class="math display">\[
M_d \frac 12 &gt; \sum_{d' = 1}^{d-1} \left(M_{d'}(\frac 12 + 2d')(d-d' + 1)\right)
\]</span></p>
<p>Here <span class="math inline">\(d\)</span> should be read as “distance between two ends of a bracket”.</p>
<p>To bound the growth rate, we bound the recurrence relation <span class="math inline">\(M_d = \sum_{d' = 1}^{d-1} \left(M_{d'}(4d' + 1)(d-d' + 1)\right)\)</span>. The sum on the right is bounded by</p>
<p><span class="math display">\[
\sum_{d' = 1}^{d-1} \left(M_{d'}(4d' + 1)(d-d' + 1)\right) \in \Theta{\left[
    \sum_{d' = 1}^{d-1} \left(M_{d'}d'\right),
    d^2\sum_{d' = 1}^{d-1} \left(M_{d'}\right)\right]}
\]</span></p>
<p>The lower bound implies <span class="math inline">\(M_d = \Omega((d!)^2 \times d^{-1})\)</span> and the upper bound implies <span class="math inline">\(M_d = O((d!)^2 \times (d+1)^2)\)</span>.</p></div></div></div>
<p>They made similar constructions for perceptron machines that decide whether two infinite lines or planes are translates of each other, whether two planes are translation and approximate dilations of each other, and whether a line or a plane is the translation of a fixed figure.</p>
<p>Chapter 8 studies diameter-limited perceptron machines, meaning that not only are the hidden perceptrons assumed to be masks of bounded size, those masks are assumed to be contained in a circle of radius <span class="math inline">\(M\)</span> for some finite <span class="math inline">\(M\)</span>. It is intended to be a formalization of the intuition that the perceptron machines should model a real pair of eyes scanning a plane, and each retina is a circular disk with finite radius. No results in chapter 8 were used further in the book, and they resemble the case of finite-order perceptron machines, so we do not discuss the results in detail.</p>
<p>Chapter 9 shows that the connectedness problem is easy to solve using small serial programs, a contrast to the case of perceptron machines. The overarching lesson is that connectedness is an “inherently serial” decision problem. The whole chapter is beautiful computational complexity theory, in the same style of solving fun mathematical puzzles, similar to <span class="citation" data-cites="minskyComputationFiniteInfinite1967">(<a href="#ref-minskyComputationFiniteInfinite1967" role="doc-biblioref">Minsky 1967</a>)</span>.</p>
<p>They designed a robot (a finite state machine) that moves around the plane, and can solve the connectedness problem, with a memory size of just two. Namely, it only needs to store up to two locations <span class="math inline">\((x, y), (x', y')\)</span> in its memory during its operation, and it eventually halts in one of three states “empty”, “connected”, and “disconnected”. Notice that a robot with no memory can still remember a finite number of states. It is simply that those memory slots are its finite states, which do not scale with the size of the problem. The little robot with a “memory size of two” really has a memory size of <span class="math inline">\(2 \log_2|R|\)</span> bits, because it can remember two coordinates from the square <span class="math inline">\(R\)</span>, no matter how large the square grows.</p>
<p>They then described a few other more exotic computational models, such as a “pebble machine”, meaning a robot that has no memory, but provided with a single pebble. It can drop the pebble anywhere, and it can pick it up again. One can think of this as an intermediate level between pure finite state machines, which cannot write at all, and a Turing machine, which can write as much as it wants. They left as an exercise for the reader the task of translating the previous robot with two memory slots to this robot with one pebble. They conjectured that a finite state machine (a robot with no pebbles) would be unable to solve the task, but they could not prove it. I did a brief literature search and it seemed to be still unsolved.</p>
</section>
<section id="learning-theory" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="learning-theory">Learning theory</h2>
<p>They claimed that Chapter 10 is part of the learning theory. However, it does not actually involve learning. Whereas in the construction <a href="#exm-line-symmetry" class="quarto-xref">Example&nbsp;1</a>, we saw coefficients growing exponentially on an infinite plane, chapter 10 proves similar results on a finite plane.</p>
<div id="exm-parity-order" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Theorem 10.1)</strong></span> Suppose we have a perceptron machine that tests for parity; then, by <a href="#thm-parity-order" class="quarto-xref">Theorem&nbsp;3</a>, it must have order <span class="math inline">\(|R|\)</span>. As in the construction given in the proof, we use the group invariance theorem to obtain a group-symmetric machine with the form <span class="math inline">\(\theta(\sum_{i=0}^{|R|}\binom{|X|}{i} b_i)\)</span>, where <span class="math inline">\(b_0, b_1, \dots, b_{|R|}\)</span> are real numbers. Then, assuming the machine is “reliable”, we can prove that <span class="math inline">\((-1)^{M} b_{M+1} \geq 2^{M}\)</span> for any <span class="math inline">\(M \in \{0, 1, \dots, |R|-1\}\)</span>.</p>
<p>Since the group-symmetric construction can only average out the most extreme values, this implies that, before the group-symmetric construction, our perceptron machine had even more extreme coefficients.</p>
<p>A “reliable” machine is a <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine</a> with margin <span class="math inline">\(\geq 1/2\)</span>. That is, it is a machine such that <span class="math display">\[
\sum_{i=0}^{|R|}\binom{|X|}{i} b_i \begin{cases}
\geq 1 &amp; \text{ if $|X|$ is odd}
\leq 0 &amp; \text{ if $|X|$ is even}
\end{cases}
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Define <span class="math inline">\(A_n = \sum_{i=0}^n \binom{n}{i}b_i\)</span>. Since the machine is reliable, we have that <span class="math inline">\((-1)^{n}(A_{n+1} - A_n) \geq 1\)</span> for each <span class="math inline">\(n = 0, 1, \dots, |R|-1\)</span>. Simplifying the binomial coefficients, we have <span class="math inline">\(A_{n+1} - A_n = \sum_i \binom{n}{i} b_{i+1}\)</span>. Note that we use the convenient convention that <span class="math inline">\(\binom{x}{y} = 0\)</span> if <span class="math inline">\(x &lt; y\)</span>.</p>
<p>Now fix any <span class="math inline">\(M \in \{0, 1, \dots, |R|-1\}\)</span>, and evaluate the following inequality:</p>
<p><span class="math display">\[
2^{M} = \sum_n \binom{M}{n} \cdot 1 \leq \sum_n \binom{M}{n} (-1)^{n}(A_{n+1} - A_n)
\]</span></p>
<p>By manipulating the binomial coefficients, the right side simplifies to <span class="math inline">\((-1)^M b_{M+1}\)</span>.</p>
</div>
<p>Sections 10.2 and 10.3 construct two pathological examples. By restricting the shapes of hidden perceptron masks, they proved that certain predicates required superexponential weights. Section 10.4 purports to extend the group invariance theorem to the infinite case.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> None of these are used elsewhere in the book. They are virtuoso performances. However, they interpreted this as a serious problem:</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;They purported to show in Theorem 10.4.1 that, if the weights are bounded, then the group invariance theorem applies again, but the proof is false, with a counterexample being <span class="math inline">\(\sum_{n \in \mathbb{Z}}e^{-n^2}x_n - e^{-n^4}x_nx_{n+1}\)</span>. The theorem <em>might</em> still be correct with another proof, but I cannot find one.</p></div></div><blockquote class="blockquote">
<p>A proof, in Chapter 10, that coefficients can grow much faster than exponentially with <span class="math inline">\(|R|\)</span> has serious consequences both practically and conceptually: the use of more memory capacity to store the coefficients than to list all the figures strains the idea that the machine is making some kind of abstraction</p>
</blockquote>
<p>In Chapter 11, they <em>finally</em> began discussing perceptron learning, which is of a very restrictive form.</p>
<div id="def-perceptron-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 (Perceptron learning)</strong></span> To train a perceptron machine is to <em>fix</em> its hidden perceptrons, and adjust the weights and threshold of <em>only</em> the single output perceptron by the perceptron learning rule. For this chapter, it is cleaner to change the convention so that each perceptron outputs <span class="math inline">\(-1, +1\)</span> instead of <span class="math inline">\(0, 1\)</span>.</p>
<p>Since only the output perceptron is adapted, it suffices to discuss the case where there are <em>no</em> hidden perceptrons, and we only need to adapt the weights of a single perceptron. That is, we have a dataset <span class="math inline">\(D\)</span>, and we sample some <span class="math inline">\((x, y) \in D\)</span>, and verify that <span class="math inline">\(y = \theta(\braket{w, x})\)</span>.</p>
<p>If this is true for all <span class="math inline">\((x, y) \in D\)</span>, then the perceptron learning has converged. Otherwise, we update <span class="math inline">\(w\)</span> using <span class="math inline">\(w \leftarrow w + \alpha y x\)</span>, where <span class="math inline">\(\alpha &gt; 0\)</span> is the learning rate.</p>
</div>
<div id="def-perceptron-learning" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 (Perceptron learning theorem)</strong></span> Let <span class="math inline">\(D\)</span> be a dataset with radius <span class="math inline">\(R = \max_{(x, y) \in D} \|x\|\)</span>. If there exists some unit <span class="math inline">\(w^*\)</span> such that <span class="math inline">\(\gamma = \min_{(x, y) \in D} y\braket{w^*, x}\)</span>, then the perceptron learning algorithm converges after making at most <span class="math inline">\((R/\gamma)^2\)</span> updates.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By linearity of the learning rule, we can deal only with the case where <span class="math inline">\(\alpha = 1\)</span>.</p>
<p>By multiplying each <span class="math inline">\(x\)</span> with its <span class="math inline">\(y\)</span>, we can deal only with the case where all <span class="math inline">\(y = +1\)</span>.</p>
<p>By rotating and scaling the space, we can deal only with the case where <span class="math inline">\(w^* = (1, 0, \dots, 0)\)</span>, and <span class="math inline">\(\gamma = 1\)</span>.</p>
<p>Now, each weight update increases the first coordinate of <span class="math inline">\(w\)</span> by at least <span class="math inline">\(1\)</span>, so after <span class="math inline">\(n\)</span> updates, <span class="math inline">\(\|w\| \geq n\)</span>. However, each weight update of <span class="math inline">\(w \leftarrow w + x\)</span> uses a vector <span class="math inline">\(x\)</span> that is pointing in a direction perpendicular to <span class="math inline">\(w\)</span>, or worse, pointing against <span class="math inline">\(w\)</span>. Therefore, by Pythagorean theorem, <span class="math inline">\(\|w\|^2\)</span> increases by at most <span class="math inline">\(\|x\|^2 \leq R^2\)</span>. So after <span class="math inline">\(n\)</span> updates, <span class="math inline">\(\|w\|^2 \leq nR^2\)</span>.</p>
<p>Combining the two results, we have <span class="math inline">\(n \leq R^2\)</span>.</p>
</div>
<p>Modifying the proof slightly, and applying the conclusion of <a href="#exm-parity-order" class="quarto-xref">Example&nbsp;2</a>, we find that starting with the zero weight vector, it takes at least <span class="math inline">\(2^{|R|}/|R|\)</span> steps to learn the parity function.</p>
<p>They then suggested that, since gradient descent is “just” a more efficient perceptron learning rule, it also cannot escape local optima. No “local learning rule” can escape local optima, unlike symbolic programs that are provably capable of finding global optima.</p>
<p>If the dataset is not linearly separable, then the perceptron weights will not converge. However, the perceptron cycling theorem shows that if the dataset is finite, then the perceptron weights will still be trapped within a large but finite disk, no matter how the dataset is sampled.</p>
<p>Chapter 12 is not very mathematical, and consists mainly of quick sketches<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> of other algorithms for learning. Those included: lookup table, nearest neighbor, k-means, ISODATA, maximum likelihood, Bayes, naive Bayes, etc. Sections 12.6 and 12.7 study variations on a toy problem: given a subset of <span class="math inline">\(\{0, 1\}^n\)</span>, decide whether an <span class="math inline">\(n\)</span>-bit word is in it or not. This had relevance to the time-space tradeoff, a perennial topic in computational complexity.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><blockquote class="blockquote"><sup>5</sup>&nbsp;
<p>In this chapter we will study a few of these to indicate points of contact with the perceptron and to reveal deep differences. … The chapter is written more in the spirit of inciting students to research than of offering solutions to problems.</p>
</blockquote>
</div></div></section>
<section id="chapter-13" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="chapter-13">Chapter 13</h2>
<p>Chapter 13 is a summary of the “take-home lessons” for the readers. As the intended lessons were expounded in great length in the epilogue added in 1988, I would not analyze the chapter in detail.</p>
<p>They discussed “Gamba perceptrons”, which is just another name for two-layered perceptrons where the hidden layer consists of perceptrons, instead of merely masks. They made the infamous prediction that not only Gamba perceptrons but also arbitrary multilayer perceptrons would be a “sterile extension”.</p>
<blockquote class="blockquote">
<p>Well, we have considered Gamba machines, which could be described as “two layers of perceptron.” We have not found (by thinking or by studying the literature) any other really interesting class of multilayered machine, at least none whose principles seem to have a significant relation to those of the perceptron. To see the force of this qualification it is worth pondering the fact, trivial in itself, that a universal computer could be built entirely out of linear threshold modules. This does not in any sense reduce the theory of computation and programming to the theory of perceptrons. Some philosophers might like to express the relevant general principle by saying that the computer is so much more than the sum of its parts that the computer scientist can afford to ignore the nature of the components and consider only their connectivity. More concretely, we would call the student’s attention to the following considerations:</p>
<ol type="1">
<li><p>Multilayer machines with loops clearly open all the questions of the general theory of automata.</p></li>
<li><p>A system with no loops but with an order restriction at each layer can compute only predicates of finite order.</p></li>
<li><p>On the other hand, if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head.</p></li>
</ol>
<p>The perceptron has shown itself worthy of study despite (and even because of!) its severe limitations. It has many features to attract attention: its linearity; its intriguing learning theorem; its clear paradigmatic simplicity as a kind of parallel computation. There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) <strong>our intuitive judgment that the extension is sterile</strong>.</p>
</blockquote>
<p>In short, their objection to multilayer perceptrons is that they suffer from Turing completeness, making it hard to prove anything concrete about them, except the trivial claim that they are Turing complete.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> A single perceptron is just a linear classifier, so it is possible to study mathematically. Experimental evidence is no justification, because:</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><blockquote class="blockquote"><sup>6</sup>&nbsp;
<p>Beware of the <a href="https://en.wikipedia.org/wiki/Turing_tarpit">Turing tar-pit</a> in which everything is possible but nothing of interest is easy.</p>
</blockquote>
</div></div><blockquote class="blockquote">
<p>13.5 Why Prove Theorems?</p>
<p><em>Why did you prove all these complicated theorems? Couldn’t you just take a perceptron and see if it can recognize <span class="math inline">\(\psi_{\text {CONNECTED}}\)</span>?</em></p>
<p>No.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/perceptron_page_239.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">page 239</figcaption>
</figure>
</div>
<p>Since a perceptron-only architecture is not the right way, they illustrated what they believe to be the “right” way to do computer vision by describing in detail the scene analysis algorithm. In short, it starts by discovering edges, then performs some computational geometry on them to recover outlines of basic shapes, such as cubes, then fills in the faces, then fills in the bodies between the faces. The algorithm had to do something clever to deal with occlusions. In the end, a complete 3D scene populated with 3D objects is recovered.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="figure/guzman_1968.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Scene analysis. Figure from <span class="citation" data-cites="guzmanDecompositionVisualScene1968">(<a href="#ref-guzmanDecompositionVisualScene1968" role="doc-biblioref">Guzmán 1968</a>)</span></figcaption>
</figure>
</div>
<p>They ended the book with a brief discussion of how they discovered the various results, as well as a list of people they thanked.</p>


<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-berkeleyRevisionistHistoryConnectionism1997" class="csl-entry" role="listitem">
Berkeley, Istvan SN. 1997. <span>“A Revisionist History of Connectionism.”</span> <em>Unpublished Manuscript</em>. <a href="http://www.universelle-automation.de/1969_Boston.pdf">http://www.universelle-automation.de/1969_Boston.pdf</a>.
</div>
<div id="ref-guzmanDecompositionVisualScene1968" class="csl-entry" role="listitem">
Guzmán, Adolfo. 1968. <span>“Decomposition of a Visual Scene into Three-Dimensional Bodies.”</span> In <em>Proceedings of the <span>December</span> 9-11, 1968, Fall Joint Computer Conference, Part <span>I</span> on - <span>AFIPS</span> ’68 (<span>Fall</span>, Part <span>I</span>)</em>, 291. San Francisco, California: ACM Press. <a href="https://doi.org/10.1145/1476589.1476631">https://doi.org/10.1145/1476589.1476631</a>.
</div>
<div id="ref-minskyComputationFiniteInfinite1967" class="csl-entry" role="listitem">
Minsky, Marvin. 1967. <em>Computation: Finite and Infinite Machines</em>. Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-sipserIntroductionTheoryComputation2006" class="csl-entry" role="listitem">
Sipser, Michael. 2006. <em>Introduction to the Theory of Computation</em>. 2nd ed. Boston: Thomson Course Technology.
</div>
</div></section></div></main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Reading *Perceptrons*"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Yuxi Liu"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-12-21"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> "2023-12-29"</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [AI, math]</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        toc: true</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "A long, hard stare into the math of *Perceptrons*, the mythical neural network killer."</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "figure/banner_2.png"</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="an">image-alt:</span><span class="co"> "The cover of 'Project SOGO: Principles of Hypnotic Bionics'. It is a combination of 'Project PARA: Principles of Neurodynamics' and 'Shoggoth' from Lovecraft's cosmology. The idea is that the book is mythical like the Necronomicon (see Minsky's quote). In the style of a cold-war era RAND corporation report."</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "certain"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 3</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../../static/_macros.tex &gt;}}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It would seem that *Perceptrons* has much the same role as The Necronomicon -- that is, often cited but never read.</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Marvin Minsky, 1994. Quoted in </span><span class="co">[</span><span class="ot">@berkeleyRevisionistHistoryConnectionism1997</span><span class="co">]</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>During my early forays into AI, perhaps ten years ago, I heard rumors of how in the 1960s, there was a bitter controversy between the symbolic AI and the neural network AI schools. Furthermore, it was said that a certain book, *Perceptions*, was the center of the controversy -- a deadly weapon that killed neural networks -- until their resurgence in the 1980s.<span class="ot">[^story-continues]</span> What, then, is in the mythical book? Recently, I have returned to this alleged episode, and in my research into the bottom of the perceptron controversy, I ended up actually reading the book. Now that I have done so, I proclaim:</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ot">[^story-continues]: </span>The story would continue, with the vain dream of Japanese Fifth Generation Project and expert systems, both of which would never come to anything... And that was where the story ended! Deep learning? In 2013, neural nets were only a mirage. Sure, there might have been reports of early success, but it was not rising above the periodic tide of news stories of yet another miracle cancer drug that would never amount to anything, with the same feeling of "Once more unto the breach, my friends..."</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; You, too, can read the *Necronomicon*.</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>In one sentence, the mathematical portion of *Perceptrons* is a theory of two-layered perceptrons, mostly by methods typical of discrete mathematics and computational complexity theory, and no empirical results. One should forget about the modern theory and practice of neural networks, read the book in the same frame of mind as one would read a textbook on Turing machines, stack machines, Post tag systems, and other various theoretical objects in computational complexity theory. Indeed, this book is written in the same spirit and style as <span class="co">[</span><span class="ot">@minskyComputationFiniteInfinite1967</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@sipserIntroductionTheoryComputation2006</span><span class="co">]</span>.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Perceptron representation occupies chapters 0--10, and learning is only studied in chapters 11 and 12. The book also contained a chapter 13, but it contains mostly interpretations and brief "lessons" that they wanted the readers to take away from the exact mathematical results. They added some handwritten corrections and updates for a 1972 printing run.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>During the 1980s rise of connectionism, Minsky and Papert came out with a new edition in 1988. This is the same as the 1972 printing, except that they added a prologue and an epilogue, where they expounded at length the intended lesson of the book, as they felt that people have failed to learn it, and history was repeating itself.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>For those interpretations, see <span class="co">[</span><span class="ot">*The Perceptron Controversy*</span><span class="co">](https://yuxi-liu-wired.github.io/essays/posts/perceptron-controversy/)</span>.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setup</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>Let $R$ be a finite set, where "R" can be read as "region" or "rectangle".</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>::: {#def-perceptron}</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="fu">## Perceptron</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>A perceptron is a binary function of type $<span class="sc">\{</span>0, 1<span class="sc">\}</span>^R \to <span class="sc">\{</span>0, 1<span class="sc">\}</span>$, defined by a weight vector $w$ and a threshold number $b$:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>\Phi(x) := \theta(w^T x + b)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>where $\theta(t) := 1_{t \geq 0}$ is the 0-1 step function.</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>::: {#def-perceptron-machine}</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="fu">## perceptron machine</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>A perceptron machine with $k$ hidden neurons is a function of type $<span class="sc">\{</span>0, 1<span class="sc">\}</span>^R \to <span class="sc">\{</span>0, 1<span class="sc">\}</span>$, defined by</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>\Phi(x) := \psi_{k+1}(\psi_1(x), \psi_2(x), \dots , \psi_k(x))</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>where $\psi_1, \dots, \psi_k$ are (hidden) perceptrons in the hidden layer, and $\psi_{k+1}$ is the single output perceptron.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>::: {#def-predicate-order}</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## perceptron orders</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>The order of a hidden perceptron is the number of nonzero weights.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>The order of a perceptron machine is the maximum order of its hidden perceptrons.</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>The order of a boolean function is the minimum order necessary for a perceptron machine that implements it.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>For example, the constant-0 and constant-1 boolean functions are both of order 0.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>A key focus of the perceptron controversy is the concept of being "conjunctively local".</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>::: {#def-predicate-order}</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## conjunctively local</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>A family of boolean functions is conjunctively local iff their orders are upper bounded.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="fu">## Over finite sets</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>While their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a <span class="co">[</span><span class="ot">computational reduction</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Reduction_(complexity)) to the special case of mask perceptron machines. Most of the book considers the problem of classifying subsets of families of planar square grids, such as $<span class="sc">\{</span>1, 2, 3<span class="sc">\}</span> \times <span class="sc">\{</span>1, 2, 3, 4<span class="sc">\}</span>$. A subset is inputted to the perceptron machine by setting inputs in the subset to $1$, and the rest to $0$. Consequently, it is natural to consider this special case of perceptron machines.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>While their definition allows perceptron machines of any real-valued weights and biases, they immediately performed a <span class="co">[</span><span class="ot">computational reduction</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Reduction_(complexity)) to the special case of the mask perceptron machine. Most of the book considers the problem of classifying subsets of families of planar square grids, such as $<span class="sc">\{</span>1, 2, 3<span class="sc">\}</span> \times <span class="sc">\{</span>1, 2, 3, 4<span class="sc">\}</span>$. A subset is input to the perceptron machine by setting inputs in the subset to $1$, and the rest to $0$. Consequently, it is natural to consider this special case of perceptron machines.</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>::: {#def-predicate-order}</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## mask perceptron machine</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>A mask for $A\subset R$ is a function of type $<span class="sc">\{</span>0, 1<span class="sc">\}</span>^R \to \R$, such that $\psi(x) = 1$ if $x_i = 1$ for all $i \in A$, and otherwise $\psi(x) = 0$.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>A mask perceptron machine is a perceptron machine where each hidden perceptron is a mask, and the threshold of the output perceptron is zero. In other words, it is of form</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>\Phi(x) = \theta\lrb{\sum_{i=1}^k a_i \psi_{A_i}(x)}</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>where each $\psi_{A_i}$ is a mask, each $a_i\in \R$, and $k$ is the number of hidden perceptrons.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="al">![Figure 0.2](figure/perceptron_fig_0_2.png)</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>::: {#thm-predicate-order}</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 1.5.1</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>Restricting to mask perceptron machines does not change perceptron orders. That is, any boolean function implemented by a perceptron machine of order $k$ can be implemented by a mask perceptron machine of order at most $k$.</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>Take one such perceptron machine. The threshold of the output perceptron can be removed by adding a hidden perceptron that always outputs $1$ -- in other words, $\psi_\emptyset$, the mask of the empty set. Now it remains to decompose each hidden perceptron into masks with the same order, or less.</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>Let $\psi$ be a hidden perceptron with nonzero weights on the input points $x_{i_1}, \dots, x_{i_k}$; then, its output is determined by the values of $x_{i_1}, \dots, x_{i_k}$. Therefore, we can partition the binary set $<span class="sc">\{</span>0, 1<span class="sc">\}</span>^{i_1, \dots, i_k}$ into two subsets $A_0, A_1$, such that for any input $x\in<span class="sc">\{</span>0, 1<span class="sc">\}</span>^R$, we have $\psi(x) = 1$ iff $(x_{i_1}, \dots, x_{i_k}) \in A_1$.</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>In other words, we only need to look at the binary values $x_{i_1}, \dots, x_{i_k}$ to determine the binary output $\psi(x)$.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>Therefore, we can replace $\psi$ with a boolean formula on $x_{i_1}, \dots, x_{i_k}$, then expand it to obtain up to $2^k$ masks, each of order at most $k$.</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>For example, suppose $\psi$ has nonzero weights on $x_1, x_2$, and is 1 on all odd-sized subsets, then we can write it as a boolean formula:</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\left(x_1 \wedge \neg x_2\right) \vee\left(\neg x_1 \wedge x_2\right) = x_1\left(1-x_2\right)+\left(1-x_1\right) x_2 = x_1 + x_2 - 2 x_1 x_2</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>The next tool they used is symmetry, formulated in the language of <span class="co">[</span><span class="ot">finite group actions</span><span class="co">](https://en.wikipedia.org/wiki/Group_action)</span>.</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>Let $S_R$ be the permutation group on the elements of $R$, and $G$ be a subgroup of $S_R$. We say that a boolean function $\psi$ is $G$-invariant iff $\psi \circ g=\psi$ for any $g \in G$. That is, for any $X \subset R$, we have $\psi(X) = \psi(g(X))$. For example, the parity function is $S_R$-invariant, since any permutation of the set preserves the size, and thus the parity, of any of its subsets.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>::: {#thm-group-invariance}</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a><span class="fu">## group invariance theorem</span></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>If a boolean function is $G$-invariant, where $G$ is a finite group, then any perceptron machine computing it can be converted to a perceptron machine $\theta(\sum_i a_i \psi_i)$, such that if $\psi_i = \psi_j \circ g$ for some $g \in G$, then $a_i = a_j$.</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>Take the group-action average: any mask $\psi$ is equal to $\frac{1}{|G|} \sum_{g\in G} \psi\circ g$.</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>Once the groundwork was laid, they proceeded to prove a wide variety of theorems on the order of particular boolean functions.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>Consider the parity function. It is $1$ iff exactly an odd number of inputs are $1$ and the rest are $0$.</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>::: {#thm-parity-order}</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 3.1</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>The parity function has order $|R|$.</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>Since the parity function is $S_R$-invariant, if it is implemented by a perceptron machine of order $k$, it is implemented by some mask perceptron machine $\theta(\sum_{A_i \subset R} a_{A_i} \psi_{A_i})$, where each mask is of size $\leq k$, and each weight $a_{A_i}$ depends only on the size of $A_i$. Let $b_{|A_i|} = a_{A_i}$ be those coefficients. It remains to show $b_{|R|} \neq 0$.</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>For each $X \subset R$, we have by explicit computation</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>\theta\lrb{\sum_{A_i \subset R} a_{A_i} \psi_{A_i}} = 1\lrs{f(|X|) \geq 0}</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>where $f(t) := \sum_{i=0}^{|R|}b_i \binom{t}{i}$ is a polynomial in $t$. Since this perceptron machine implements the parity function, as $t$ increases from $0$ to $|R|$, the function $f(t) + \epsilon$ must intersect the $x$-axis at least $|R|$ times for some real value $\epsilon$. Since $f$ is a polynomial, it must have at least order $|R|$, thus $b_{|R|} \neq 0$.</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>::: {#thm-one-in-a-box}</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 3.2, one-in-a-box</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>Let $A_1, A_2, \dots, A_m$ be disjoint subsets of $R$, each of size $4 m^2$, and define the predicate $\psi(X) = \forall i, \left|X \cap A_i\right|&gt;0$; that is, there is at least one point of $X$ in each $A_i$. The order of $\psi$ is $\geq m$.</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>Let the order of $\psi$ be $k$.</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>The predicate $\psi$ is invariant with respect to the group $S_{A_1} \times \cdots \times S_{A_m}$, so by the same construction as the proof of @thm-group-invariance, there exists a polynomial $P(t_1, \dots, t_m)$, where $P$ has order $k$, and</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\forall t_1, \dots , t_m \in <span class="sc">\{</span>0, 1, \dots , 4m^2<span class="sc">\}</span>, P(t_1, \dots , t_m) &lt; 0 \iff t_1 = 0 \vee \cdots \vee t_m = 0</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>Now define $Q(t) := P((t-1)^2, (t-3)^2, \dots, (t-2m+1)^2)$. By the above equation, $Q &lt; 0$ at $t=1, 3, \dots, 2m - 1$ and $Q \geq 0$ at $t = 0, 2, \dots, 2m$. Thus, $Q$ has order $\geq 2m$. Thus, $2k \geq 2m$.</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>::: {#thm-and-or}</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 4.0</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>There exist predicates $\psi_1$ and $\psi_2$ of order 1 such that $\psi_1 \wedge \psi_2$ and $\psi_1 \vee \psi_2$ are not of finite order. Specifically, if we partition $R$ into three equal subsets $A, B, C$, then the boolean function does not have bounded order:</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>(|X \cap A| &gt; |X \cap C|) \wedge (|X \cap B| &gt; |X \cap C|)</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>even though both $|X \cap A| &gt; |X \cap C|$ and $|X \cap B| &gt; |X \cap C|$ are of order $1$.</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>$|X \cap A| &gt; |X \cap C|$ is computed by the order-$1$ perceptron machine $\theta\lrb{\sum_{i\in A} x_i - \sum_{i \in C}x_i}$, and similarly for the other one.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>To show the composite boolean function is not of bounded order, suppose otherwise, then by the same argument as the previous proof, we can construct a sequence of polynomials $P_1(a, b, c), P_2(a, b, c), P_3(a, b, c), \dots$, such that each $P_n$ is the polynomial corresponding to the perceptron machine for the case where $|A| = |B| = |C| = n$, and each of them has order at most $M$, for some fixed $M$.</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>Being the polynomial corresponding to the perceptron machine for the case where $|A| = |B| = |C| = n$ means precisely that</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>a &gt; c \wedge b &gt; c \implies P_n(a, b, c) \geq 0; \neg (a &gt; c \wedge b &gt; c) \implies P_n(a, b, c) &lt; 0;</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>for all $a, b, c \in <span class="sc">\{</span>0, 1, \dots, n<span class="sc">\}</span>$. This implies that each $P_1, P_2, \dots \neq 0$. Now, divide each polynomial by the root-sum-square of its coefficients, so that if we interpret the coefficients of each $P_n$ as a point in a high-dimensional space, the point falls on the unit sphere in that space. Since the unit sphere is compact, there exists a limit point, which we write as $P(a, b, c)$.</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>By the limit construction, we have</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>\forall a, b, c \in \N, \quad a &gt; c \wedge b &gt; c \implies P(a, b, c) \geq 0; \neg (a &gt; c \wedge b &gt; c) \implies P(a, b, c) \leq 0;</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>If we color the points $\N^3$ with black for $P &lt; 0$ and white for $P \geq 0$, then we can see the dusty outlines of a pyramid. We can construct a solid pyramid by zooming.</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>Let $M'$ be the order of $P$, then we can "zoom out" by taking the <span class="co">[</span><span class="ot">projective limit</span><span class="co">](https://en.wikipedia.org/wiki/Homogeneous_function)</span> $Q(a, b, c) := \lim_{t \to \infty} t^{-M'} P(ta, tb, tc)$. This $Q$ is a homogeneous polynomial, and by continuity,</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>\forall a, b, c \geq 0, \quad a &gt; c \wedge b &gt; c \implies P(a, b, c) \geq 0; a &lt; c \vee b &lt; c \implies P(a, b, c) \leq 0;</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>This implies that $P$ is identically zero on the "creased curve" $<span class="sc">\{</span> a, b, c \geq 0, a = c \vee b = c<span class="sc">\}</span>$ in the projective plane, which is not an algebraic curve, therefore it is identically zero, contradiction.</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>::: {#thm-connectedness-order}</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 5.1</span></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>The connectedness function has order $\Omega(|R|^{1/3})$.</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>If we have a mask perceptron machine that can solve the connectedness problem, then it can be repurposed<span class="ot">[^the-collapsing-theorem]</span> to solve the one-in-a-box problem of the following kind:</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="al">![Figure 5.2](figure/perceptron_fig_5_2.png)</span></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>In the picture, the rectangle $R$ has width $4m^2$ and height $2m+1$. We fill in all the odd-numbered rows, and use the machine to solve the one-in-a-box problem for the even-numbered rows. By @thm-one-in-a-box, the machine has order $\geq m = \Omega(|R|^{1/3})$.</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a><span class="ot">[^the-collapsing-theorem]: </span>Repurposing one machine to solve another problem is a common trick in computational complexity, called "<span class="co">[</span><span class="ot">reduction</span><span class="co">]</span>(https://en.wikipedia.org/wiki/Reduction_(complexity))". For perceptron machines, they called it "Theorem 5.4.1: The Collapsing Theorem".</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>In fact, it turns out that essentially the only locally conjunctive topological invariant is the Euler number.</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>::: {#thm-euler-number}</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 5.8.1</span></span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>The Euler number itself is $E(X) = \sum_{i \in R} x_i - \sum_{i, j \in R} x_ix_j + \sum_{i, j, k, l  \in R} x_ix_jx_kx_l$, where the $i, j\in R$ ranges only over adjacent points, and $i, j, k, l  \in R$ ranges only over quadruples that form a square. Thus the Euler number itself has order $4$.</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>::: {#thm-euler-number}</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 5.9</span></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>If $\psi$ is a boolean function, and it depends only on the topology of the input figure, and its order is upper-bounded by some number $k$ that does not grow even as $R$ grows into a larger and larger rectangle, then $\psi$ is of form $f \circ E$, for some function $f: \N \to 2$.</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="fu">## Over infinite spaces</span></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>Chapters 6--9 continue in the same style, but move to the case where the input space is made of one or two copies of the infinite line $\Z$, or the infinite plane $\Z^2$, and the predicate to recognize is still translation-invariant. In order to avoid blowing up the perceptron machine with infinities, they restricted to the case where the input figure is of finite size, meaning that $\sum_i x_i$ is finite.</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>Chapter 6 develops the idea of "spectra" of images. For example, the following picture shows that if we were to design a perceptron machine using only masks of size up to 2, and translation-invariant weights, then it cannot possibly distinguish between the two tetris-like pieces, because both figures contain exactly 4 instances of single-square, 1 instance of two squares side by side, and so on.</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a><span class="al">![page 100](figure/perceptron_chap_6.png)</span></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>Sections 6.1--6.5 of the chapter studies the same idea for masks of size up to 4. For example, Section 6.4 shows that "$X$ is the perimeter of a complete circle" is of order $4$.</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>Section 6.6 claims that "recognizing figures in context" is generally not locally conjunctive, although they gave only two examples. Specifically, they showed that the predicate "$X$ is a horizontal line across the rectangle" is order 2, the predicate "$X$ contains one horizontal line across the plane" is not locally conjunctive. The same is true for the case with "a hollow square" instead of "a horizontal line".</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>Chapter 7 uses the "stratification" construction to explicitly demonstrate that several binocular predicates are of finite order, but requires exponentially growing weights. It is essentially the same idea as <span class="co">[</span><span class="ot">Gödel numbering</span><span class="co">](https://en.wikipedia.org/wiki/G%C3%B6del_numbering)</span>. A single illustrative example suffices to demonstrate the general point.</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>::: {#exm-line-symmetry}</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>Given a line $\Z$, how to construct a perceptron machine that detects that input figure is symmetric?</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>Suppose we know for certain that the input figure has leftmost point $m$ and rightmost point $n$, then we can test for symmetry by computing the value of:</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a>f_{m, n}(x) := \sum_{i = 0}^{n-m} x_{m+i}(x_{n-i} - 1)</span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>We have that $f_{m, n}(x) = 0$ if the figure is symmetric, and $f_{m, n}(x) \leq -1$ otherwise.</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>Now we define the entire perceptron machine by $\sum_{m \leq n} M_{n - m} x_m x_n (f_{m, n}(x) - 1/2)$. If the sequence of $M_0, M_1, \dots$ grows as $(d!)^2$ roughly <span class="ot">[^exm-line-symmetry-factorial-growth]</span>, then the largest bracket $(m, n)$ would "veto" every smaller bracket contained within, and so the entire perceptron machine is effectively first testing for the smallest interval containing the figure, before testing the symmetry within that interval.</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a><span class="ot">[^exm-line-symmetry-factorial-growth]</span>:</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a>    Expanding term by term, we have $|f_{m, n}(x) - 1/2| \leq 2(n-m) + \frac{1}{2}$. Therefore, in order for $M_d$ to "veto" every other bracket within, we need</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a><span class="in">    $$</span></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a><span class="in">    M_d \frac 12 &gt; \sum_{d' = 1}^{d-1} \lrb{M_{d'}(\frac 12 + 2d')(d-d' + 1)}</span></span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a><span class="in">    $$</span></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="in">    Here $d$ should be read as "distance between two ends of a bracket".</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="in">    To bound the growth rate, we bound the recurrence relation $M_d = \sum_{d' = 1}^{d-1} \lrb{M_{d'}(4d' + 1)(d-d' + 1)}$. The sum on the right is bounded by</span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="in">    $$</span></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a><span class="in">    \sum_{d' = 1}^{d-1} \lrb{M_{d'}(4d' + 1)(d-d' + 1)} \in \Theta{\lrs{</span></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a><span class="in">        \sum_{d' = 1}^{d-1} \lrb{M_{d'}d'},</span></span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a><span class="in">        d^2\sum_{d' = 1}^{d-1} \lrb{M_{d'}}}}</span></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a><span class="in">    $$</span></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="in">    The lower bound implies $M_d = \Omega((d!)^2 \times d^{-1})$ and the upper bound implies $M_d = O((d!)^2 \times (d+1)^2)$.</span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a>They made similar constructions for perceptron machines that decide whether two infinite lines or planes are translates of each other, whether two planes are translation and approximate dilations of each other, and whether a line or a plane is the translation of a fixed figure.</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a>Chapter 8 studies diameter-limited perceptron machines, meaning that not only are the hidden perceptrons assumed to be masks of bounded size, those masks are assumed to be contained in a circle of radius $M$ for some finite $M$. It is intended to be a formalization of the intuition that the perceptron machines should model a real pair of eyes scanning a plane, and each retina is a circular disk with finite radius. No results in chapter 8 were used further in the book, and they resemble the case of finite-order perceptron machines, so we do not discuss the results in detail.</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>Chapter 9 shows that the connectedness problem is easy to solve using small serial programs, a contrast to the case of perceptron machines. The overarching lesson is that connectedness is an "inherently serial" decision problem. The whole chapter is beautiful computational complexity theory, in the same style of solving fun mathematical puzzles, similar to <span class="co">[</span><span class="ot">@minskyComputationFiniteInfinite1967</span><span class="co">]</span>.</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a>They designed a robot (a finite state machine) that moves around the plane, and can solve the connectedness problem, with a memory size of just two. Namely, it only needs to store up to two locations $(x, y), (x', y')$ in its memory during its operation, and it eventually halts in one of three states "empty", "connected", and "disconnected". Notice that a robot with no memory can still remember a finite number of states. It is simply that those memory slots are its finite states, which do not scale with the size of the problem. The little robot with a "memory size of two" really has a memory size of $2 \log_2|R|$ bits, because it can remember two coordinates from the square $R$, no matter how large the square grows.</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>They then described a few other more exotic computational models, such as a "pebble machine", meaning a robot that has no memory, but provided with a single pebble. It can drop the pebble anywhere, and it can pick it up again. One can think of this as an intermediate level between pure finite state machines, which cannot write at all, and a Turing machine, which can write as much as it wants. They left as an exercise for the reader the task of translating the previous robot with two memory slots to this robot with one pebble. They conjectured that a finite state machine (a robot with no pebbles) would be unable to solve the task, but they could not prove it. I did a brief literature search and it seemed to be still unsolved.</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning theory</span></span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>They claimed that Chapter 10 is part of the learning theory. However, it does not actually involve learning. Whereas in the construction @exm-line-symmetry, we saw coefficients growing exponentially on an infinite plane, chapter 10 proves similar results on a finite plane.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>:::{#exm-parity-order}</span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theorem 10.1</span></span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>Suppose we have a perceptron machine that tests for parity; then, by @thm-parity-order, it must have order $|R|$. As in the construction given in the proof, we use the group invariance theorem to obtain a group-symmetric machine with the form $\theta(\sum_{i=0}^{|R|}\binom{|X|}{i} b_i)$, where $b_0, b_1, \dots, b_{|R|}$ are real numbers. Then, assuming the machine is "reliable", we can prove that $(-1)^{M} b_{M+1} \geq 2^{M}$ for any $M \in <span class="sc">\{</span>0, 1, \dots, |R|-1<span class="sc">\}</span>$.</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>Since the group-symmetric construction can only average out the most extreme values, this implies that, before the group-symmetric construction, our perceptron machine had even more extreme coefficients.</span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>A "reliable" machine is a <span class="co">[</span><span class="ot">support vector machine</span><span class="co">](https://en.wikipedia.org/wiki/Support_vector_machine)</span> with margin $\geq 1/2$. That is, it is a machine such that</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a>\sum_{i=0}^{|R|}\binom{|X|}{i} b_i \begin{cases}</span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a>\geq 1 &amp; \text{ if $|X|$ is odd}</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>\leq 0 &amp; \text{ if $|X|$ is even}</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>Define $A_n = \sum_{i=0}^n \binom{n}{i}b_i$. Since the machine is reliable, we have that $(-1)^{n}(A_{n+1} - A_n) \geq 1$ for each $n = 0, 1, \dots, |R|-1$. Simplifying the binomial coefficients, we have $A_{n+1} - A_n = \sum_i \binom{n}{i} b_{i+1}$. Note that we use the convenient convention that $\binom{x}{y} = 0$ if $x &lt; y$.</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a>Now fix any $M \in <span class="sc">\{</span>0, 1, \dots, |R|-1<span class="sc">\}</span>$, and evaluate the following inequality:</span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>2^{M} = \sum_n \binom{M}{n} \cdot 1 \leq \sum_n \binom{M}{n} (-1)^{n}(A_{n+1} - A_n)</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>By manipulating the binomial coefficients, the right side simplifies to $(-1)^M b_{M+1}$.</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>Sections 10.2 and 10.3 construct two pathological examples. By restricting the shapes of hidden perceptron masks, they proved that certain predicates required superexponential weights. Section 10.4 purports to extend the group invariance theorem to the infinite case.<span class="ot">[^theorem-10-4-1]</span> None of these are used elsewhere in the book. They are virtuoso performances. However, they interpreted this as a serious problem:</span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A proof, in Chapter 10, that coefficients can grow much faster than exponentially with $</span><span class="pp">|</span><span class="at">R</span><span class="pp">|</span><span class="at">$ has serious consequences both practically and conceptually: the use of more memory capacity to store the coefficients than to list all the figures strains the idea that the machine is making some kind of abstraction</span></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="ot">[^theorem-10-4-1]: </span>They purported to show in Theorem 10.4.1 that, if the weights are bounded, then the group invariance theorem applies again, but the proof is false, with a counterexample being $\sum_{n \in \Z}e^{-n^2}x_n - e^{-n^4}x_nx_{n+1}$. The theorem *might* still be correct with another proof, but I cannot find one.</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>In Chapter 11, they *finally* began discussing perceptron learning, which is of a very restrictive form.</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>::: {#def-perceptron-learning}</span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a><span class="fu">## Perceptron learning</span></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>To train a perceptron machine is to *fix* its hidden perceptrons, and adjust the weights and threshold of *only* the single output perceptron by the perceptron learning rule. For this chapter, it is cleaner to change the convention so that each perceptron outputs $-1, +1$ instead of $0, 1$.</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>Since only the output perceptron is adapted, it suffices to discuss the case where there are *no* hidden perceptrons, and we only need to adapt the weights of a single perceptron. That is, we have a dataset $D$, and we sample some $(x, y) \in D$, and verify that $y = \theta(\braket{w, x})$.</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>If this is true for all $(x, y) \in D$, then the perceptron learning has converged. Otherwise, we update $w$ using $w \leftarrow w + \alpha y x$, where $\alpha &gt; 0$ is the learning rate.</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>::: {#def-perceptron-learning}</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a><span class="fu">## Perceptron learning theorem</span></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>Let $D$ be a dataset with radius $R = \max_{(x, y) \in D} <span class="sc">\|</span>x<span class="sc">\|</span>$. If there exists some unit $w^*$ such that $\gamma = \min_{(x, y) \in D} y\braket{w^*, x}$, then the perceptron learning algorithm converges after making at most $(R/\gamma)^2$ updates.</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>By linearity of the learning rule, we can deal only with the case where $\alpha = 1$.</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>By multiplying each $x$ with its $y$, we can deal only with the case where all $y = +1$.</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a>By rotating and scaling the space, we can deal only with the case where $w^* = (1, 0, \dots, 0)$, and $\gamma = 1$.</span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a>Now, each weight update increases the first coordinate of $w$ by at least $1$, so after $n$ updates, $<span class="sc">\|</span>w<span class="sc">\|</span> \geq n$. However, each weight update of $w \leftarrow w + x$ uses a vector $x$ that is pointing in a direction perpendicular to $w$, or worse, pointing against $w$. Therefore, by Pythagorean theorem, $<span class="sc">\|</span>w<span class="sc">\|</span>^2$ increases by at most $<span class="sc">\|</span>x<span class="sc">\|</span>^2 \leq R^2$. So after $n$ updates, $<span class="sc">\|</span>w<span class="sc">\|</span>^2 \leq nR^2$.</span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a>Combining the two results, we have $n \leq R^2$.</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a>Modifying the proof slightly, and applying the conclusion of @exm-parity-order, we find that starting with the zero weight vector, it takes at least $2^{|R|}/|R|$ steps to learn the parity function.</span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a>They then suggested that, since gradient descent is "just" a more efficient perceptron learning rule, it also cannot escape local optima. No "local learning rule" can escape local optima, unlike symbolic programs that are provably capable of finding global optima.</span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>If the dataset is not linearly separable, then the perceptron weights will not converge. However, the perceptron cycling theorem shows that if the dataset is finite, then the perceptron weights will still be trapped within a large but finite disk, no matter how the dataset is sampled.</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>Chapter 12 is not very mathematical, and consists mainly of quick sketches<span class="ot">[^quick-sketches]</span> of other algorithms for learning. Those included: lookup table, nearest neighbor, k-means, ISODATA, maximum likelihood, Bayes, naive Bayes, etc. Sections 12.6 and 12.7 study variations on a toy problem: given a subset of $<span class="sc">\{</span>0, 1<span class="sc">\}</span>^n$, decide whether an $n$-bit word is in it or not. This had relevance to the time-space tradeoff, a perennial topic in computational complexity.</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="ot">[^quick-sketches]</span>:</span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>    <span class="at">&gt; In this chapter we will study a few of these to indicate points of contact with the perceptron and to reveal deep differences. ... The chapter is written more in the spirit of inciting students to research than of offering solutions to problems.</span></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 13</span></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>Chapter 13 is a summary of the "take-home lessons" for the readers. As the intended lessons were expounded in great length in the epilogue added in 1988, I would not analyze the chapter in detail.</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a>They discussed "Gamba perceptrons", which is just another name for two-layered perceptrons where the hidden layer consists of perceptrons, instead of merely masks. They made the infamous prediction that not only Gamba perceptrons but also arbitrary multilayer perceptrons would be a "sterile extension".</span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Well, we have considered Gamba machines, which could be described as “two layers of perceptron.” We have not found (by thinking or by studying the literature) any other really interesting class of multilayered machine, at least none whose principles seem to have a significant relation to those of the perceptron. To see the force of this qualification it is worth pondering the fact, trivial in itself, that a universal computer could be built entirely out of linear threshold modules. This does not in any sense reduce the theory of computation and programming to the theory of perceptrons. Some philosophers might like to express the relevant general principle by saying that the computer is so much more than the sum of its parts that the computer scientist can afford to ignore the nature of the components and consider only their connectivity. More concretely, we would call the student's attention to the following considerations: </span></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 1. Multilayer machines with loops clearly open all the questions of the general theory of automata. </span></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 2. A system with no loops but with an order restriction at each layer can compute only predicates of finite order. </span></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 3. On the other hand, if there is no restriction except for the absence of loops, the monster of vacuous generality once more raises its head.</span></span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The perceptron has shown itself worthy of study despite (and even because of!) its severe limitations. It has many features to attract attention: its linearity; its intriguing learning theorem; its clear paradigmatic simplicity as a kind of parallel computation. There is no reason to suppose that any of these virtues carry over to the many-layered version. Nevertheless, we consider it to be an important research problem to elucidate (or reject) **our intuitive judgment that the extension is sterile**.</span></span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>In short, their objection to multilayer perceptrons is that they suffer from Turing completeness, making it hard to prove anything concrete about them, except the trivial claim that they are Turing complete.<span class="ot">[^turing-tarpit]</span> A single perceptron is just a linear classifier, so it is possible to study mathematically. Experimental evidence is no justification, because:</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 13.5 Why Prove Theorems? </span></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *Why did you prove all these complicated theorems? Couldn't you just take a perceptron and see if it can recognize $\psi_{\text {CONNECTED}}$?*</span></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span></span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; No.</span></span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a><span class="al">![page 239](figure/perceptron_page_239.png)</span></span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a><span class="ot">[^turing-tarpit]</span>: </span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>    <span class="at">&gt; Beware of the </span><span class="co">[</span><span class="ot">Turing tar-pit</span><span class="co">](https://en.wikipedia.org/wiki/Turing_tarpit)</span><span class="at"> in which everything is possible but nothing of interest is easy.</span></span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>Since a perceptron-only architecture is not the right way, they illustrated what they believe to be the "right" way to do computer vision by describing in detail the scene analysis algorithm. In short, it starts by discovering edges, then performs some computational geometry on them to recover outlines of basic shapes, such as cubes, then fills in the faces, then fills in the bodies between the faces. The algorithm had to do something clever to deal with occlusions. In the end, a complete 3D scene populated with 3D objects is recovered.</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Scene analysis. Figure from [@guzmanDecompositionVisualScene1968]</span><span class="co">](figure/guzman_1968.png)</span></span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a>They ended the book with a brief discussion of how they discovered the various results, as well as a list of people they thanked.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>