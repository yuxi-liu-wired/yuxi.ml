<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hugo de Garis">
<meta name="dcterms.date" content="1989-05-01">
<meta name="description" content="Hugo de Garis’ publication of the Terran vs Cosmists Artilect War scenario. He would add the ‘Cyborgian’ camp later, and the ‘gigadeath’ prediction, but otherwise repeat the same scenario.">

<title>The 21st Century Artilect – Yuxi on the Wired</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../img/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-493ec8732bc442be923a7677f0a4f8b4.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-c590cb3103796f9f406a17afdd3881b8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<meta property="og:title" content="The 21st Century Artilect – Yuxi on the Wired">
<meta property="og:description" content="Hugo de Garis’ publication of the Terran vs Cosmists Artilect War scenario. He would add the ‘Cyborgian’ camp later, and the ‘gigadeath’ prediction, but otherwise repeat the same scenario.">
<meta property="og:image" content="https://yuxi.ml/docs/posts/1989-hugo-de-garis/img/blog icon.jpg">
<meta property="og:site_name" content="Yuxi on the Wired">
<meta name="twitter:title" content="The 21st Century Artilect – Yuxi on the Wired">
<meta name="twitter:description" content="Hugo de Garis’ publication of the Terran vs Cosmists Artilect War scenario. He would add the ‘Cyborgian’ camp later, and the ‘gigadeath’ prediction, but otherwise repeat the same scenario.">
<meta name="twitter:image" content="https://yuxi.ml/docs/posts/1989-hugo-de-garis/img/blog icon.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../img/favicon.ico" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Yuxi on the Wired</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../sketches/index.html"> 
<span class="menu-text">Sketches</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../logs/index.html"> 
<span class="menu-text">Logs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../docs/index.html"> 
<span class="menu-text">Docs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:yuxi_liu@berkeley.com"> <i class="bi bi-envelope" role="img" aria-label="email">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../feeds.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">The 21st Century Artilect</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Moral Dilemmas Concerning the Ultra Intelligent Machine</p>
                  <div>
        <div class="description">
          Hugo de Garis’ publication of the Terran vs Cosmists Artilect War scenario. He would add the ‘Cyborgian’ camp later, and the ‘gigadeath’ prediction, but otherwise repeat the same scenario.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hugo de Garis </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 1, 1989</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#a-moral-dilemma" id="toc-a-moral-dilemma" class="nav-link" data-scroll-target="#a-moral-dilemma">A Moral Dilemma</a></li>
  <li><a href="#a-world-divided" id="toc-a-world-divided" class="nav-link" data-scroll-target="#a-world-divided">A World Divided</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#appendix-metadata" id="toc-appendix-metadata" class="nav-link" data-scroll-target="#appendix-metadata">Appendix: Metadata</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Within one to two human generations, it is likely that computer technology will be capable of building brain-like computers containing millions if not billions of artificial neurons. This development will allow neuroengineers and neurophysiologists to combine forces to discover the principles of the functioning of the human brain. These principles will then be translated into more sophisticated computer architectures, until a point is reached in the 2Ist Century when the primary global politica! issue will become, “Who or what is to be dominant species on this planet — human beings, or artilects (artificial intellects)?”</p>
<p>A new branch of applied moral philosophy is needed to study the profound implications of the prospect of life in a world in which it is generally recognised to be only a question of time before our computers become smarter than we are. Since human beings could never be sure of the attitudes of advanced artilects towards us, due to their unfathomable complexity and possible “Darwinian” self modification, the prospect of possible hostilities between human beings and artilects cannot be excluded.</p>
<p>Keywords: Artilect, Ultra Intelligent Machine, Neuro-Engineering, Dominant Species, Artificial Neuron.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Artificial Intelligence is a branch of computer science which is at tempting to make machines intelligent, and in so doing, to cast light on the mysteries of biological intelligence. This valiant enterprise has not escaped the critical eye of the philosophers over the years, some of whom have attempted to show that certain claims of the intelligists (AI researchers) are excessive. (See for example, Searle 1981, Dennett 1981, Dreyfus 1986 and the replies of the intelligists, Hofstadter 1981, Gregory 1987). However, this article does not address itself to such traditional “philosophical-ΑI” concerns as the mind-brain distinction, the freedom of the will, or the impossibility or otherwise of artificial intelligence. It assumes that artificial intelligence is a reasonable endeavor, and raises new questions concerning the moral consequences for humanity when AI eventually succeeds.</p>
<p>A revolution is taking place in the field of Artificial Intelligence. This revolution, called “Connectionism”, attempts to understand the junctioning of the human brain in terms of interactions between artificial abstract neuron-like components, and hopes to provide computer science with design principles sufficiently powerful to be able to build genuine artificial electronic (optical, molecular) brains (Kohonen 1987, McClelland et al 1986, Mead 1987). Progress in micro electronics and related fields, such as optical Computing, has been so impressive over the last few years, that the possibility of building a true artilect within a human generation or two becomes a real possibility and not merely a science fiction pipe dream.</p>
<p>However, if the idea of the 21st Century artilect is to be taken seriously (and a growing number of Artificial Intelligence specialists are doing just that (Michie 1974, Waltz 1988, de Garis 1989)), then a large number of profound political and philosophical questions arise. This paper addresses itself to some of the philosophical and moral issues concerning the fundamental question “Who or what is to be dominant species on this planet — human beings or the artilects?”</p>
</section>
<section id="a-moral-dilemma" class="level2">
<h2 class="anchored" data-anchor-id="a-moral-dilemma">A Moral Dilemma</h2>
<p>In order to understand the disquiet which has been growing amongst an increasing number of intelligists (specialists in Artificial Intelligence) around the world in the late 1980s (Waltz 1988, de Garis 1989), it is useful to make a historical analogy with the development of the awareness of the nuclear physicists in the 1930s, of the possibility of a chain reaction when Splitting the uranium atom. At the time, that is immediately after the announcement of the Splitting, very few nuclear physicists thought hard about the consequences to humanity of life in a nuclear age and the possibility of a large scale nuclear war in which billions of human beings would die.</p>
<p>Some intelligists feel that a similar situation is developing now with the connectionist revolution. The intelligists concerned, are worried that if the artificial intelligence community simply rushes ahead with the construction of increasingly sophisticated artilects, without thinking about the possible long term political, social and philosophical consequences, then humanity may end up in the same sort of diabolical situation as in the present era of possible nuclear holocaust.</p>
<p>Within a single human generation, computer scientists will be building brain-like computers based on the technology of the 21st Century. These true “electronic (optical, molecular) brains” will allow neurophysiologists to perform experiments on machines instead of being confined to biological specimens. The marriage between neuroengineers and neurophysiologists will be extremely fruitful and artificial intelligence can expect to make rapid progress towards its long term goal of building a machine that can “think”, a machine usually called an “artificial intelligence”, or “artilect”. However, since an artilect is, by definition, highly intelligent, (and in the limit, ultra intelligent, that is, having an intelligence which is orders of magnitude superior to ours), if ever such a machine should turn against humanity, it could be extremely dangerous. An atomic bomb has the enormous advantage, from the point of view of human beings, of being totally stupid. It has no intelligence. It is human beings who control it. But an artilect is a different kettle of fish entirely.</p>
<p>Artilects, unlike the human species, will probably be capable of extremely rapid evolution and will, in a very short time (as judged by human standards), reach a state of sophistication beyond human comprehension. Remember, that human neurons communicate at hundreds of meters per second, whereas electronic components communicate near the speed of light, a million times faster. Remember, that our brains, although containing some trillion neurons, has a fixed architecture, as specified by our genes. The artilects could choose to undertake “Darwinian experiments” on themselves, or parts of themselves, and incorporate the more successful results into their structure. Artilects have no obvious limit as to the number of components they may choose to integrate into themselves. To them, our trillion neurons may seem puny. Not only may artilects be superior to humans in quantitative terms, they may be greatly our superiors in qualitative terms as well. They may discover whole new principles of “intelligence theory” which they may use in restructuring themselves. This continuous updating may grow exponentially — the smarter the machine, the better and faster the redesigning phase, so that a take-off point may be reached, beyond which, we human beings will appear to artilects as mice do to us.</p>
<p>This notion of Darwinian experimentation is important in this discussion, because it runs counter to the opinions of many people who believe (rather naively, in my view) that it will be possible to construct artilects which will obey human commands with docility. Such machines are not artilects according to my conception of the word.</p>
<p>I accept that machines will be built which will show some obvious signs of real intelligence and yet remain totally obedient. However, this is not the issue being discussed in this paper. What worries me is the type of machine which is so smart that it is capable of modifying itself, of searching out new structures and behaviours, that is, the “Darwinian Artilect”.</p>
<p>Since any machine, no matter how intelligent, is subject to the same physical laws as is any other material object in the universe, there will be upper limits to the level of self-control of its intellectual functions. At some level in its architectural design, there will be “givens”, that is, top level structures determining the artilect’s functioning, which are not “judged” by any higher level structures. If the artilect is to modify these top level structures, how can it judge the quality of the change? What is meant by quality in such a context?</p>
<p>This problem is universal for biological systems. Quality, in a biological context, is defined as increased survivability. Structural innovations such as reproduction, mutation, sex, death, etc., are ail “measured” according to the survivability criterion. It is just possible that there may be no other alternative for the artilect, than taking the same route. Survivability however, only has meaning in a context in which the concept of death has meaning. But would not an artilect be essentially immortal, as are cancer cells, and would a folly autonomous artilect, resulting from an artilectual reproductive process, but with modified structures, accept being “termina ted” by its “parent” artilects, if the latter consider the experiment to have been a failure?</p>
<p>If the offspring artilects do not agree to being “killed”, they might be allowed to live, but this would imply that every artilectual experiment would create a new immortal being, which would consume scarce re sources. There seem to be at least three possible solutions to this problem. Either a limit is placed on the number of experiments being performed, a philosophy inevitably leading to evolutionary stagnation, or artilects are replaced, by newer versions, (processes called reproduction and death, in biological contexts), or the growing population of artilects could under take a mass migration into the cosmos in search of other resources. This Darwinian modification is, by its nature, random and chancy. The problem for human beings is that an artilect, by definition, is beyond our control. As human beings, with our feeble intellects (by artilectual standards), we are unable to understand the implications of structural changes to the artilect’s “brain”, because this requires a greater intellect than we possess. We can only sit back and observe the impact of artilectual change upon us. But this change may not necessarily be to our advantage. The “moral circuits” of the artilects may change so that they no longer feel any “sympathy” for human beings and decide that, given a materials shortage on the planet, it might be advisable, from an artilectual point of view, to reduce the “ecological load” by removing the “hungriest” of the inferior species, namely human beings.</p>
<p>Since human moral attitudes, like any other psychological attitudes, are ultimately physical/chemical phenomena, human beings could not be sure of the attitudes of artilects towards human beings, once the artilects had evolved to a highly advanced State. What human beings consider as moral is merely the result of our biological evolution. As human beings we have no qualms about killing mosquitoes or cattle. To us, they are such inferior creatures we do not question our power of life or death over them. This uncertainty raises the inevitable fear of the unknown in human beings. With artilects undertaking experiments to “improve” themselves (however the artilects define improvement), we humans could never be sure that the changing intelligences and attitudes of the artilects would remain favorable to us, even if we humans did our best to instil some sort of initial “Asimovian”, human-oriented moral code into them. Personally, I believe that Asimov’s “Three Laws of Robotics” are inappropriate for machines making random changes to themselves to see whether they lead to “improvements”. Asimov’s robots were not artilects.</p>
</section>
<section id="a-world-divided" class="level2">
<h2 class="anchored" data-anchor-id="a-world-divided">A World Divided</h2>
<p>With many intelligists agreeing that it will be technologically possible to build electronic (optical, molecular) brains within a human generation or two, what are the moral problems presented to humanity, and particularly to applied moral philosophers? The biggest question in many peoples minds will be, “Do we, or do we not, allow such artilects to be built?” Given the time frame we are talking about, namely 20 to 50 years from now, it is unlikely that human societies will have evolved sufficiently to have formed a world State, having the power to enforce a world wid ban on artilectual development, beyond an agreed point. What wi probably happen, is that military powers will argue that they cannot afford to stop the development of artilects, in case the “other side” creates smarter “soldier robots” than they do. Military/political pressures may ensure artilect funding and research until it is too late.</p>
<p>The artilect question alone, is sufficient in itself, to provide a very strong motivation for the establishment of a skeleton world government within the next human generation. With the rapid development of global tel communications and the corresponding development of a world language the establishment of a skeleton world government within such a short time may not be as naive as it sounds.</p>
<p>For the purposes of discussion, imagine that such a ban, or at least a moratorium, on artilectual development is established. Should such a ban remain in force forever? Could one not argue that mankind has not onl the power, but the moral duty to initiate the next major phase in evolution and that it would be a “crime” on a universal or cosmic scale not to exercise that power?</p>
<p>One can imagine new ideological political factions being established, comparable with the capitalist/communist factions of the 20th Century. Those in favour of giving the artilects freedom to evolve as they wish, I have labelled the “Cosmists”, and those opposed, I have labelled the “Terras” (or Terrestrialists). I envisage a bitter ideological conflict bet ween these two groups, taking on a planetary and military scale. The Cosmists are so named because of the idea that it is unlikely, once the artilects have evolved beyond a certain point, that they will want to remain on this provincial little planet we call Earth. After all, there are some trillion trillion other stars to choose from. It seems more credible that the artilects will leave our planet and move into the Cosmos, perhaps in search of other ultraintelligences.</p>
<p>The Terras are so named because they wish to remain dominant on this planet. Their horizons are terrestrial. To the Cosmists, this attitude is provincial in the extreme. To the Terras, the aspirations of the Cosmists are fraught with danger, and are to be resisted at any cost. The survival of humanity is at stake.</p>
<p>There may be a way out of this moral dilemma. With 21st Century Space technology, it may be entirely feasible to transport whole populations of Cosmist scientists and technicians to some distant planet, where they can build their artilects and suffer the consequences. However, even this opinion may be too risky for some Terran politicians, because the artilects may choose to return to the Earth, and with their superior intellects, they could easily overcome the military precautions installed by the Terras.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>This article claims that intelligists will be able to construct true electronic (optical, molecular) brains, called artilects, within one to two human generations. It is argued that this possibility is not a piece of science fiction, but is an opinion held by a growing number of professional intelligists. This prospect raises the moral dilemma of whether human beings should or should not allow the artilects to be built, and whether they should or should not be allowed to modify themselves into super beings, beyond human comprehension. This dilemma will probably dominate political and philosophical discussion in the 2Ist Century. A new branch of applied moral philosophy needs to be established to consider the artilect problem.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>(de Garis 1989) “What if AI Succeeds? The Rise of the 21st Century Artilect” ; Artificial Intelligence Magazine, Summer 1989.</li>
<li>(Dennet 1981) Brainstorms – Philosophical Essays on Mind and Psychology, D. C. Dennet, M.I.T. Press, 1981</li>
<li>(Dreyfus 1986) Mind Over Machine, Dreyfus H., Blackwell Oxford, 1986</li>
<li>(Evans 1979) The Mighty Micro, Evans C., Coronet Books, London.</li>
<li>(Gregory 1987) “In Defense of Artificial Intelligence - A Reply to John Searle”, R. Gregory, in Mindwaves, eds C. Blakemore and S. Greenfield, Blackwell, 1987</li>
<li>(Hofstadter 1981) The Mind’s I, Hofstadter D. R., Bantam, 1981</li>
<li>(Jastrow 1981) The Enchanted Loom, Simon &amp; Schuster, New York.</li>
<li>(Kelly 1987) “Intelligent Machines. What Chance?”, Advances in Artificial Intelligence, Wiley.</li>
<li>(Kohonen 1987) Self-Organization and Associative Memory, 2nd edn. Kohonen T., Springer-Verlag, Belin, Heidelberg.</li>
<li>(McClelland et al 1986) Parallel Distributed Processing, Vols 1 and 2, McClelland J. L. &amp; Rumelhart D. E. (Eds), MIT Press, Cambridge, Mass.</li>
<li>(McCorduck 1979) Forging the Gods, in Machines Who Think, Freeman (Mead 1987) Analog VLSI and Neural Systems, Mead C., Addison Wesley, Reading, Mass.</li>
<li>(Michie 1974) On Machine Intelligence, Michie D., Edinburgh University Press, Edinburgh</li>
<li>(Searle 1981) “Minds, Brains, and Programs”, Searle J., in The Minds’s I, see (Hofstadter 1981)</li>
<li>(Waltz 1988) “The Prospects for Building Truly Intelligent Machines”, Waltz D., in The Artificial Intelligence Debate, Cambridge, Mass., MIT Press.</li>
</ul>
</section>
<section id="appendix-metadata" class="level2">
<h2 class="anchored" data-anchor-id="appendix-metadata">Appendix: Metadata</h2>
<p>This essay was written in 1989-05 and published in 1990 as De Garis, Hugo. “The 21st Century Artilect Moral Dilemmas Concerning the Ultra Intelligent Machine.” Revue Internationale de Philosophie (1990): 131-138.</p>
<p>It was hosted as a plaintext file at <a href="https://web.archive.org/web/19981202210056/http://www.hip.atr.co.jp/%7Edegaris/Artilect-phil.html">his homepage</a>, with the following author’s information:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode txt code-with-copy"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Dr. Hugo de Garis,</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Head, Brain Builder Group,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Evolutionary Systems Department,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>ATR Human Information Processing Research Labs,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>2-2 Hikaridai, Seika-cho, Soraku-gun,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>Kansai Science City, Kyoto-fu, 619-02, Japan.</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>tel. + 81 774 95 1079,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>fax. + 81 774 95 1008,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>email. degaris@hip.atr.co.jp</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>web. http://www.hip.atr.co.jp/~degaris</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

</section>

</main> <!-- /main -->
<!-- file: html/copy‑anchors-js.html -->

<script type="module">

document.addEventListener("DOMContentLoaded", () => {

  // 1. All little ¶ icons Quarto/AnchorJS adds

  document.querySelectorAll("a.anchorjs-link").forEach(anchor => {

    anchor.addEventListener("click", async (evt) => {

      // Keep normal scroll behaviour but stop full page reload

      evt.preventDefault();



      // Build absolute URL: origin + path + #hash

      const url = `${location.origin}${location.pathname}${anchor.getAttribute("href")}`;



      // 2. Try modern Clipboard API first

      try {

        await navigator.clipboard.writeText(url);

      } catch {

        // 3. Fallback for legacy browsers

        const helper = Object.assign(document.createElement("input"), { value: url });

        document.body.appendChild(helper);

        helper.select();

        document.execCommand("copy");

        helper.remove();

      }

      // TODO: The following two doesn't work yet

      // 4. Brief visual confirmation (optional)

      anchor.dataset.tooltip = "Copied!";

      setTimeout(() => delete anchor.dataset.tooltip, 1500);



      // 5. Still jump to the heading

      history.pushState(null, "", anchor.getAttribute("href"));

    }, false);

  });

});

</script>

<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/yuxi\.ml\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The 21st Century Artilect"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Moral Dilemmas Concerning the Ultra Intelligent Machine"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Hugo de Garis"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "1989-05"</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [AI]</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"> html:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">  toc: true</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Hugo de Garis' publication of the Terran vs Cosmists Artilect War scenario. He would add the 'Cyborgian' camp later, and the 'gigadeath' prediction, but otherwise repeat the same scenario."</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="an">status:</span><span class="co"> "finished"</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="an">confidence:</span><span class="co"> "log"</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="an">importance:</span><span class="co"> 7</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Abstract</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>Within one to two human generations, it is likely that computer technology will be capable of building brain-like computers containing millions if not billions of artificial neurons. This development will allow neuroengineers and neurophysiologists to combine forces to discover the principles of the functioning of the human brain. These principles will then be translated into more sophisticated computer architectures, until a point is reached in the 2Ist Century when the primary global politica! issue will become, "Who or what is to be dominant species on this planet — human beings, or artilects (artificial intellects)?"</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>A new branch of applied moral philosophy is needed to study the profound implications of the prospect of life in a world in which it is generally recognised to be only a question of time before our computers become smarter than we are. Since human beings could never be sure of the attitudes of advanced artilects towards us, due to their unfathomable complexity and possible "Darwinian" self modification, the prospect of possible hostilities between human beings and artilects cannot be excluded.</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>Keywords: Artilect, Ultra Intelligent Machine, Neuro-Engineering, Dominant Species, Artificial Neuron.</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>Artificial Intelligence is a branch of computer science which is at tempting to make machines intelligent, and in so doing, to cast light on the mysteries of biological intelligence. This valiant enterprise has not escaped the critical eye of the philosophers over the years, some of whom have attempted to show that certain claims of the intelligists (AI researchers) are excessive. (See for example, Searle 1981, Dennett 1981, Dreyfus 1986 and the replies of the intelligists, Hofstadter 1981, Gregory 1987). However, this article does not address itself to such traditional "philosophical-ΑI" concerns as the mind-brain distinction, the freedom of the will, or the impossibility or otherwise of artificial intelligence. It assumes that artificial intelligence is a reasonable endeavor, and raises new questions concerning the moral consequences for humanity when AI eventually succeeds.</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>A revolution is taking place in the field of Artificial Intelligence. This revolution, called "Connectionism", attempts to understand the junctioning of the human brain in terms of interactions between artificial abstract neuron-like components, and hopes to provide computer science with design principles sufficiently powerful to be able to build genuine artificial electronic (optical, molecular) brains (Kohonen 1987, McClelland et al 1986, Mead 1987). Progress in micro electronics and related fields, such as optical Computing, has been so impressive over the last few years, that the possibility of building a true artilect within a human generation or two becomes a real possibility and not merely a science fiction pipe dream.</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>However, if the idea of the 21st Century artilect is to be taken seriously (and a growing number of Artificial Intelligence specialists are doing just that (Michie 1974, Waltz 1988, de Garis 1989)), then a large number of profound political and philosophical questions arise. This paper addresses itself to some of the philosophical and moral issues concerning the fundamental question "Who or what is to be dominant species on this planet — human beings or the artilects?"</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Moral Dilemma</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>In order to understand the disquiet which has been growing amongst an increasing number of intelligists (specialists in Artificial Intelligence) around the world in the late 1980s (Waltz 1988, de Garis 1989), it is useful to make a historical analogy with the development of the awareness of the nuclear physicists in the 1930s, of the possibility of a chain reaction when Splitting the uranium atom. At the time, that is immediately after the announcement of the Splitting, very few nuclear physicists thought hard about the consequences to humanity of life in a nuclear age and the possibility of a large scale nuclear war in which billions of human beings would die.</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>Some intelligists feel that a similar situation is developing now with the connectionist revolution. The intelligists concerned, are worried that if the artificial intelligence community simply rushes ahead with the construction of increasingly sophisticated artilects, without thinking about the possible long term political, social and philosophical consequences, then humanity may end up in the same sort of diabolical situation as in the present era of possible nuclear holocaust.</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>Within a single human generation, computer scientists will be building brain-like computers based on the technology of the 21st Century. These true "electronic (optical, molecular) brains" will allow neurophysiologists to perform experiments on machines instead of being confined to biological specimens. The marriage between neuroengineers and neurophysiologists will be extremely fruitful and artificial intelligence can expect to make rapid progress towards its long term goal of building a machine that can "think", a machine usually called an "artificial intelligence", or "artilect". However, since an artilect is, by definition, highly intelligent, (and in the limit, ultra intelligent, that is, having an intelligence which is orders of magnitude superior to ours), if ever such a machine should turn against humanity, it could be extremely dangerous. An atomic bomb has the enormous advantage, from the point of view of human beings, of being totally stupid. It has no intelligence. It is human beings who control it. But an artilect is a different kettle of fish entirely.</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>Artilects, unlike the human species, will probably be capable of extremely rapid evolution and will, in a very short time (as judged by human standards), reach a state of sophistication beyond human comprehension. Remember, that human neurons communicate at hundreds of meters per second, whereas electronic components communicate near the speed of light, a million times faster. Remember, that our brains, although containing some trillion neurons, has a fixed architecture, as specified by our genes. The artilects could choose to undertake "Darwinian experiments" on themselves, or parts of themselves, and incorporate the more successful results into their structure. Artilects have no obvious limit as to the number of components they may choose to integrate into themselves. To them, our trillion neurons may seem puny. Not only may artilects be superior to humans in quantitative terms, they may be greatly our superiors in qualitative terms as well. They may discover whole new principles of "intelligence theory" which they may use in restructuring themselves. This continuous updating may grow exponentially — the smarter the machine, the better and faster the redesigning phase, so that a take-off point may be reached, beyond which, we human beings will appear to artilects as mice do to us.</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>This notion of Darwinian experimentation is important in this discussion, because it runs counter to the opinions of many people who believe (rather naively, in my view) that it will be possible to construct artilects which will obey human commands with docility. Such machines are not artilects according to my conception of the word.</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>I accept that machines will be built which will show some obvious signs of real intelligence and yet remain totally obedient. However, this is not the issue being discussed in this paper. What worries me is the type of machine which is so smart that it is capable of modifying itself, of searching out new structures and behaviours, that is, the "Darwinian Artilect".</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>Since any machine, no matter how intelligent, is subject to the same physical laws as is any other material object in the universe, there will be upper limits to the level of self-control of its intellectual functions. At some level in its architectural design, there will be "givens", that is, top level structures determining the artilect's functioning, which are not "judged" by any higher level structures. If the artilect is to modify these top level structures, how can it judge the quality of the change? What is meant by quality in such a context?</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>This problem is universal for biological systems. Quality, in a biological context, is defined as increased survivability. Structural innovations such as reproduction, mutation, sex, death, etc., are ail "measured" according to the survivability criterion. It is just possible that there may be no other alternative for the artilect, than taking the same route. Survivability however, only has meaning in a context in which the concept of death has meaning. But would not an artilect be essentially immortal, as are cancer cells, and would a folly autonomous artilect, resulting from an artilectual reproductive process, but with modified structures, accept being "termina ted" by its "parent" artilects, if the latter consider the experiment to have been a failure?</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>If the offspring artilects do not agree to being "killed", they might be allowed to live, but this would imply that every artilectual experiment would create a new immortal being, which would consume scarce re sources. There seem to be at least three possible solutions to this problem. Either a limit is placed on the number of experiments being performed, a philosophy inevitably leading to evolutionary stagnation, or artilects are replaced, by newer versions, (processes called reproduction and death, in biological contexts), or the growing population of artilects could under take a mass migration into the cosmos in search of other resources. This Darwinian modification is, by its nature, random and chancy. The problem for human beings is that an artilect, by definition, is beyond our control. As human beings, with our feeble intellects (by artilectual standards), we are unable to understand the implications of structural changes to the artilect's "brain", because this requires a greater intellect than we possess. We can only sit back and observe the impact of artilectual change upon us. But this change may not necessarily be to our advantage. The "moral circuits" of the artilects may change so that they no longer feel any "sympathy" for human beings and decide that, given a materials shortage on the planet, it might be advisable, from an artilectual point of view, to reduce the "ecological load" by removing the "hungriest" of the inferior species, namely human beings.</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>Since human moral attitudes, like any other psychological attitudes, are ultimately physical/chemical phenomena, human beings could not be sure of the attitudes of artilects towards human beings, once the artilects had evolved to a highly advanced State. What human beings consider as moral is merely the result of our biological evolution. As human beings we have no qualms about killing mosquitoes or cattle. To us, they are such inferior creatures we do not question our power of life or death over them. This uncertainty raises the inevitable fear of the unknown in human beings. With artilects undertaking experiments to "improve" themselves (however the artilects define improvement), we humans could never be sure that the changing intelligences and attitudes of the artilects would remain favorable to us, even if we humans did our best to instil some sort of initial "Asimovian", human-oriented moral code into them. Personally, I believe that Asimov's "Three Laws of Robotics" are inappropriate for machines making random changes to themselves to see whether they lead to "improvements". Asimov's robots were not artilects.</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="fu">## A World Divided</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>With many intelligists agreeing that it will be technologically possible to build electronic (optical, molecular) brains within a human generation or two, what are the moral problems presented to humanity, and particularly to applied moral philosophers? The biggest question in many peoples minds will be, "Do we, or do we not, allow such artilects to be built?" Given the time frame we are talking about, namely 20 to 50 years from now, it is unlikely that human societies will have evolved sufficiently to have formed a world State, having the power to enforce a world wid ban on artilectual development, beyond an agreed point. What wi probably happen, is that military powers will argue that they cannot afford to stop the development of artilects, in case the "other side" creates smarter "soldier robots" than they do. Military/political pressures may ensure artilect funding and research until it is too late.</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>The artilect question alone, is sufficient in itself, to provide a very strong motivation for the establishment of a skeleton world government within the next human generation. With the rapid development of global tel communications and the corresponding development of a world language the establishment of a skeleton world government within such a short time may not be as naive as it sounds.</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>For the purposes of discussion, imagine that such a ban, or at least a moratorium, on artilectual development is established. Should such a ban remain in force forever? Could one not argue that mankind has not onl the power, but the moral duty to initiate the next major phase in evolution and that it would be a "crime" on a universal or cosmic scale not to exercise that power?</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>One can imagine new ideological political factions being established, comparable with the capitalist/communist factions of the 20th Century. Those in favour of giving the artilects freedom to evolve as they wish, I have labelled the "Cosmists", and those opposed, I have labelled the "Terras" (or Terrestrialists). I envisage a bitter ideological conflict bet ween these two groups, taking on a planetary and military scale. The Cosmists are so named because of the idea that it is unlikely, once the artilects have evolved beyond a certain point, that they will want to remain on this provincial little planet we call Earth. After all, there are some trillion trillion other stars to choose from. It seems more credible that the artilects will leave our planet and move into the Cosmos, perhaps in search of other ultraintelligences.</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>The Terras are so named because they wish to remain dominant on this planet. Their horizons are terrestrial. To the Cosmists, this attitude is provincial in the extreme. To the Terras, the aspirations of the Cosmists are fraught with danger, and are to be resisted at any cost. The survival of humanity is at stake.</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>There may be a way out of this moral dilemma. With 21st Century Space technology, it may be entirely feasible to transport whole populations of Cosmist scientists and technicians to some distant planet, where they can build their artilects and suffer the consequences. However, even this opinion may be too risky for some Terran politicians, because the artilects may choose to return to the Earth, and with their superior intellects, they could easily overcome the military precautions installed by the Terras.</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>This article claims that intelligists will be able to construct true electronic (optical, molecular) brains, called artilects, within one to two human generations. It is argued that this possibility is not a piece of science fiction, but is an opinion held by a growing number of professional intelligists. This prospect raises the moral dilemma of whether human beings should or should not allow the artilects to be built, and whether they should or should not be allowed to modify themselves into super beings, beyond human comprehension. This dilemma will probably dominate political and philosophical discussion in the 2Ist Century. A new branch of applied moral philosophy needs to be established to consider the artilect problem.</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(de Garis 1989) "What if AI Succeeds? The Rise of the 21st Century Artilect" ; Artificial Intelligence Magazine, Summer 1989.</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Dennet 1981) Brainstorms -- Philosophical Essays on Mind and Psychology, D. C. Dennet, M.I.T. Press, 1981</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Dreyfus 1986) Mind Over Machine, Dreyfus H., Blackwell Oxford, 1986</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Evans 1979) The Mighty Micro, Evans C., Coronet Books, London.</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Gregory 1987) "In Defense of Artificial Intelligence - A Reply to John Searle", R. Gregory, in Mindwaves, eds C. Blakemore and S. Greenfield, Blackwell, 1987</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Hofstadter 1981) The Mind's I, Hofstadter D. R., Bantam, 1981</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Jastrow 1981) The Enchanted Loom, Simon &amp; Schuster, New York.</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Kelly 1987) "Intelligent Machines. What Chance?", Advances in Artificial Intelligence, Wiley.</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Kohonen 1987) Self-Organization and Associative Memory, 2nd edn. Kohonen T., Springer-Verlag, Belin, Heidelberg.</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(McClelland et al 1986) Parallel Distributed Processing, Vols 1 and 2, McClelland J. L. &amp; Rumelhart D. E. (Eds), MIT Press, Cambridge, Mass.</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(McCorduck 1979) Forging the Gods, in Machines Who Think, Freeman (Mead 1987) Analog VLSI and Neural Systems, Mead C., Addison Wesley, Reading, Mass.</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Michie 1974) On Machine Intelligence, Michie D., Edinburgh University Press, Edinburgh</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Searle 1981) "Minds, Brains, and Programs", Searle J., in The Minds's I, see (Hofstadter 1981)</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>(Waltz 1988) "The Prospects for Building Truly Intelligent Machines", Waltz D., in The Artificial Intelligence Debate, Cambridge, Mass., MIT Press.</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="fu">## Appendix: Metadata</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>This essay was written in 1989-05 and published in 1990 as De Garis, Hugo. "The 21st Century Artilect Moral Dilemmas Concerning the Ultra Intelligent Machine." Revue Internationale de Philosophie (1990): 131-138.</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>It was hosted as a plaintext file at <span class="co">[</span><span class="ot">his homepage</span><span class="co">](https://web.archive.org/web/19981202210056/http://www.hip.atr.co.jp/%7Edegaris/Artilect-phil.html)</span>, with the following author's information:</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a><span class="in">```txt</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a><span class="in">Dr. Hugo de Garis,</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="in">Head, Brain Builder Group,</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="in">Evolutionary Systems Department,</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="in">ATR Human Information Processing Research Labs,</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a><span class="in">2-2 Hikaridai, Seika-cho, Soraku-gun,</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a><span class="in">Kansai Science City, Kyoto-fu, 619-02, Japan.</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a><span class="in">tel. + 81 774 95 1079,</span></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a><span class="in">fax. + 81 774 95 1008,</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a><span class="in">email. degaris@hip.atr.co.jp</span></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a><span class="in">web. http://www.hip.atr.co.jp/~degaris</span></span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p><span class="faux-block">Yuxi on the Wired</span></p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><span class="faux-block">Everything <a href="https://en.wikipedia.org/wiki/Public_domainl">PD</a>; <a href="https://creativecommons.org/publicdomain/zero/1.0/legalcode.en/">CC0</a> fallback.</span></p>
</div>
  </div>
</footer>




</body></html>